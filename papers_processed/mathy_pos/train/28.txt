Journal of Machine Learning Research 11 (2010) 2057-2078 Submitted 6/09; Revised 4/10; Published 7/10
Matrix Completion from Noisy Entries
Raghunandan H. Keshavan RAGHURAM@STANFORD.EDU
Andrea Montanari∗ MONTANARI@STANFORD.EDU
Sewoong Oh SWOH@STANFORD.EDU
Department of Electrical Engineering
Stanford University
Stanford, CA 94304, USA
Editor: Tommi Jaakkola
Abstract
Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observa-
tions of a small, random subset of its entries. The problem arises in a variety of applications, from
collaborative filtering (the ‘Netflix problem’) to structure-from-motion and positioning. We study
a low complexity algorithm introduced by Keshavan, Montanari, and Oh (2010), based on a com-
bination of spectral techniques and manifold optimization, that we call here OPTSPACE. We prove
performance guarantees that are order-optimal in a number of circumstances.
Keywords: matrix completion, low-rank matrices, spectral methods, manifold optimization
1. Introduction
Spectral techniques are an authentic workhorse in machine learning, statistics, numerical analysis,
and signal processing. Given a matrix M, its largest singular values—and the associated singular
vectors—‘explain’ the most significant correlations in the underlying data source. A low-rank ap-
proximation of M can further be used for low-complexity implementations of a number of linear
algebra algorithms (Frieze et al., 2004).
In many practical circumstances we have access only to a sparse subset of the entries of an
m×n matrix M. It has recently been discovered that, if the matrix M has rank r, and unless it is too
‘structured’, a small random subset of its entries allow to reconstruct it exactly. This result was first
proved by Candès and Recht (2008) by analyzing a convex relaxation introduced by Fazel (2002). A
tighter analysis of the same convex relaxation was carried out by Candès and Tao (2009). A number
of iterative schemes to solve the convex optimization problem appeared soon thereafter (Cai et al.,
2008; Ma et al., 2009; Toh and Yun, 2009).
In an alternative line of work, Keshavan, Montanari, and Oh (2010) attacked the same problem
using a combination of spectral techniques and manifold optimization: We will refer to their al-
gorithm as OPTSPACE. OPTSPACE is intrinsically of low complexity, the most complex operation
being computing r singular values (and the corresponding singular vectors) of a sparse m×n matrix.
The performance guarantees proved by Keshavan et al. (2010) are comparable with the information
theoretic lower bound: roughly nrmax{r,logn} random entries are needed to reconstruct M exactly
(here we assume m of order n). A related approach was also developed by Lee and Bresler (2009),
although without performance guarantees for matrix completion.
∗. Also in Department of Statistics.
c 2010 Raghunandan H. Keshavan, Andrea Montanari and Sewoong Oh.
KESHAVAN, MONTANARI AND OH
The above results crucially rely on the assumption that M is exactly a rank r matrix. For many
applications of interest, this assumption is unrealistic and it is therefore important to investigate
their robustness. Can the above approaches be generalized when the underlying data is ‘well ap-
proximated’ by a rank r matrix? This question was addressed by Candès and Plan (2009) within the
convex relaxation approach of Candès and Recht (2008). The present paper proves a similar robust-
ness result for OPTSPACE. Remarkably the guarantees we obtain are order-optimal in a variety of
circumstances, and improve over the analogous results of Candès and Plan (2009).
1.1 Model Definition
Let M be an m×n matrix of rank r, that is
M = UΣVT
. (1)
where U has dimensions m×r, V has dimensions n×r, and Σ is a diagonal r×r matrix. We assume
that each entry of M is perturbed, thus producing an ‘approximately’ low-rank matrix N, with
Nij = Mij +Zij ,
where the matrix Z will be assumed to be ‘small’ in an appropriate sense.
Out of the m×n entries of N, a subset E ⊆ [m]×[n] is revealed. We let NE be the m×n matrix
that contains the revealed entries of N, and is filled with 0’s in the other positions
NE
ij =

Nij if (i, j) ∈ E ,
0 otherwise.
Analogously, we let ME and ZE be the m × n matrices that contain the entries of M and Z, re-
spectively, in the revealed positions and is filled with 0’s in the other positions. The set E will be
uniformly random given its size |E|.
1.2 Algorithm
For the reader’s convenience, we recall the algorithm introduced by Keshavan et al. (2010), which
we will analyze here. The basic idea is to minimize the cost function F(X,Y), defined by
F(X,Y) ≡ min
S∈Rr×r
F (X,Y,S), (2)
F (X,Y,S) ≡
1
2 ∑
(i, j)∈E
(Nij −(XSYT
)ij)2
.
Here X ∈ Rn×r, Y ∈ Rm×r are orthogonal matrices, normalized by XT X = mI, YTY = nI.
Minimizing F(X,Y) is an a priori difficult task, since F is a non-convex function. The key
insight is that the singular value decomposition (SVD) of NE provides an excellent initial guess,
and that the minimum can be found with high probability by standard gradient descent after this
initialization. Two caveats must be added to this description: (1) In general the matrix NE must be
‘trimmed’ to eliminate over-represented rows and columns; (2) For technical reasons, we consider
a slightly modified cost function to be denoted by e
F(X,Y).
2058
MATRIX COMPLETION FROM NOISY ENTRIES
OPTSPACE( matrix NE )
1: Trim NE, and let e
NE be the output;
2: Compute the rank-r projection of e
NE, Pr(e
NE) = X0S0YT
0 ;
3: Minimize e
F(X,Y) through gradient descent, with initial condition (X0,Y0).
We may note here that the rank of the matrix M, if not known, can be reliably estimated from
e
NE (Keshavan and Oh, 2009).
The various steps of the above algorithm are defined as follows.
Trimming. We say that a row is ‘over-represented’ if it contains more than 2|E|/m revealed
entries (i.e., more than twice the average number of revealed entries per row). Analogously, a
column is over-represented if it contains more than 2|E|/n revealed entries. The trimmed matrix e
NE
is obtained from NE by setting to 0 over-represented rows and columns.
Rank-r projection. Let
e
NE
=
min(m,n)
∑
i=1
σixiyT
i ,
be the singular value decomposition of e
NE, with singular values σ1 ≥ σ2 ≥ .... We then define
Pr(e
NE
) =
mn
|E|
r
∑
i=1
σixiyT
i .
Apart from an overall normalization, Pr(e
NE) is the best rank-r approximation to e
NE in Frobenius
norm.
Minimization. The modified cost function e
F is defined as
e
F(X,Y) = F(X,Y)+ρG(X,Y)
≡ F(X,Y)+ρ
m
∑
i=1
G1
kX(i)k2
3µ0r
!
+ρ
n
∑
j=1
G1
kY( j)k2
3µ0r
!
,
where X(i) denotes the i-th row of X, and Y( j) the j-th row of Y. The function G1 : R+ → R is such
that G1(z) = 0 if z ≤ 1 and G1(z) = e(z−1)2
−1 otherwise. Further, we can choose ρ = Θ(|E|).
Let us stress that the regularization term is mainly introduced for our proof technique to work
(and a broad family of functions G1 would work as well). In numerical experiments we did not find
any performance loss in setting ρ = 0.
One important feature of OPTSPACE is that F(X,Y) and e
F(X,Y) are regarded as functions
of the r-dimensional subspaces of Rm and Rn generated (respectively) by the columns of X and
Y. This interpretation is justified by the fact that F(X,Y) = F(XA,YB) for any two orthogonal
matrices A, B ∈ Rr×r (the same property holds for e
F). The set of r dimensional subspaces of Rm
is a differentiable Riemannian manifold G(m,r) (the Grassmann manifold). The gradient descent
algorithm is applied to the function e
F : M(m,n) ≡ G(m,r) × G(n,r) → R. For further details on
optimization by gradient descent on matrix manifolds we refer to Edelman et al. (1999) and Absil
et al. (2008).
2059
KESHAVAN, MONTANARI AND OH
1.3 Some Notations
The matrix M to be reconstructed takes the form (1) where U ∈ Rm×r, V ∈ Rn×r. We write U =
[u1,u2,...,ur] andV = [v1,v2,...,vr] for the columns of the two factors, with kuik =
√
m, kvik =
√
n,
and uT
i uj = 0, vT
i vj = 0 for i 6= j (there is no loss of generality in this, since normalizations can be
absorbed by redefining Σ).
We shall write Σ = diag(Σ1,...,Σr) with Σ1 ≥ Σ2 ≥ ··· ≥ Σr > 0. The maximum and minimum
singular values will also be denoted by Σmax = Σ1 and Σmin = Σr. Further, the maximum size of an
entry of M is Mmax ≡ maxij |Mij|.
Probability is taken with respect to the uniformly random subset E ⊆ [m] × [n] given |E| and
(eventually) the noise matrix Z. Define ε ≡ |E|/
√
mn. In the case when m = n, ε corresponds to the
average number of revealed entries per row or column. Then it is convenient to work with a model
in which each entry is revealed independently with probability ε/
√
mn. Since, with high probability
|E| ∈ [ε
√
αn − A
√
nlogn,ε
√
αn + A
√
nlogn], any guarantee on the algorithm performances that
holds within one model, holds within the other model as well if we allow for a vanishing shift in ε.
We will use C, C′ etc. to denote universal numerical constants.
It is convenient to define the following projection operator PE(·) as the sampling operator, which
maps an m×n matrix onto an |E|-dimensional subspace in Rm×n
PE(N)ij =

Nij if (i, j) ∈ E ,
0 otherwise.
Given a vector x ∈ Rn, kxk will denote its Euclidean norm. For a matrix X ∈ Rn×n′
, kXkF is its
Frobenius norm, and kXk2 its operator norm (i.e., kXk2 = supu6=0 kXuk/kuk). The standard scalar
product between vectors or matrices will sometimes be indicated by hx,yi or hX,Yi ≡ Tr(XTY),
respectively. Finally, we use the standard combinatorics notation [n] = {1,2,...,n} to denote the
set of first n integers.
1.4 Main Results
Our main result is a performance guarantee for OPTSPACE under appropriate incoherence assump-
tions, and is presented in Section 1.4.2. Before presenting it, we state a theorem of independent
interest that provides an error bound on the simple trimming-plus-SVD approach. The reader inter-
ested in the OPTSPACE guarantee can go directly to Section 1.4.2.
Throughout this paper, without loss of generality, we assume α ≡ m/n ≥ 1.
1.4.1 SIMPLE SVD
Our first result shows that, in great generality, the rank-r projection of e
NE provides a reasonable
approximation of M. We define e
ZE to be an m×n matrix obtained from ZE, after the trimming step
of the pseudocode above, that is, by setting to zero the over-represented rows and columns.
Theorem 1.1 Let N = M + Z, where M has rank r, and assume that the subset of revealed entries
E ⊆ [m]×[n] is uniformly random with size |E|. Let Mmax = max(i, j)∈[m]×[n] |Mij|. Then there exists
numerical constants C and C′ such that
1
√
mn
kM −Pr(e
NE
)kF ≤ CMmax
nrα3/2
|E|
!1/2
+ C′ n
√
rα
|E|
ke
ZE
k2 ,
2060
MATRIX COMPLETION FROM NOISY ENTRIES
with probability larger than 1−1/n3.
Projection onto rank-r matrices through SVD is a pretty standard tool, and is used as first analysis
method for many practical problems. At a high-level, projection onto rank-r matrices can be in-
terpreted as ‘treat missing entries as zeros’. This theorem shows that this approach is reasonably
robust if the number of observed entries is as large as the number of degrees of freedom (which is
about (m + n)r) times a large constant. The error bound is the sum of two contributions: the first
one can be interpreted as an undersampling effect (error induced by missing entries) and the second
as a noise effect. Let us stress that trimming is crucial for achieving this guarantee.
1.4.2 OPTSPACE
Theorem 1.1 helps to set the stage for the key point of this paper: a much better approximation
is obtained by minimizing the cost e
F(X,Y) (step 3 in the pseudocode above), provided M satisfies
an appropriate incoherence condition. Let M = UΣVT be a low rank matrix, and assume, without
loss of generality, UTU = mI and VTV = nI. We say that M is (µ0,µ1)-incoherent if the following
conditions hold.
A1. For all i ∈ [m], j ∈ [n] we have, ∑r
k=1U2
ik ≤ µ0r, ∑r
k=1V2
ik ≤ µ0r.
A2. For all i ∈ [m], j ∈ [n] we have, |∑r
k=1Uik(Σk/Σ1)Vjk| ≤ µ1r1/2.
Theorem 1.2 Let N = M + Z, where M is a (µ0,µ1)-incoherent matrix of rank r, and assume that
the subset of revealed entries E ⊆ [m] × [n] is uniformly random with size |E|. Further, let Σmin =
Σr ≤ ··· ≤ Σ1 = Σmax with Σmax/Σmin ≡ κ. Let b
M be the output of OPTSPACE on input NE. Then
there exists numerical constants C and C′ such that if
|E| ≥ Cn
√
ακ2
max

µ0r
√
αlogn; µ2
0r2
ακ4
; µ2
1r2
ακ4
,
then, with probability at least 1−1/n3,
1
√
mn
k b
M −MkF ≤ C′
κ2 n
√
rα
|E|
kZE
k2 . (3)
provided that the right-hand side is smaller than Σmin.
As discussed in the next section, this theorem captures rather sharply the effect of important
classes of noise on the performance of OPTSPACE.
1.5 Noise Models
In order to make sense of the above results, it is convenient to consider a couple of simple models
for the noise matrix Z:
Independent entries model. We assume that Z’s entries are i.i.d. random variables, with zero
mean E{Zij} = 0 and sub-Gaussian tails. The latter means that
P{|Zij| ≥ x} ≤ 2e
− x2
2σ2
,
for some constant σ2 uniformly bounded in n.
2061
KESHAVAN, MONTANARI AND OH
Worst case model. In this model Z is arbitrary, but we have an uniform bound on the size of its
entries: |Zij| ≤ Zmax.
The basic parameter entering our main results is the operator norm of e
ZE, which is bounded as
follows in these two noise models.
Theorem 1.3 If Z is a random matrix drawn according to the independent entries model, then for
any sample size |E| there is a constant C such that,
ke
ZE
k2 ≤ Cσ

|E|logn
n
1/2
, (4)
with probability at least 1−1/n3. Further there exists a constant C′ such that, if the sample size is
|E| ≥ nlogn (for n ≥ α), we have
ke
ZE
k2 ≤ C′
σ

|E|
n
1/2
, (5)
with probability at least 1−1/n3.
If Z is a matrix from the worst case model, then
ke
ZE
k2 ≤
2|E|
n
√
α
Zmax ,
for any realization of E.
It is elementary to show that, if |E| ≥ 15αnlogn, no row or column is over-represented with high
probability. It follows that in the regime of |E| for which the conditions of Theorem 1.2 are satisfied,
we have ZE = e
ZE and hence the bound (5) applies to ke
ZEk2 as well. Then, among the other things,
this result implies that for the independent entries model the right-hand side of our error estimate,
Eq. (3), is with high probability smaller than Σmin, if |E| ≥ Crαnκ4(σ/Σmin)2. For the worst case
model, the same statement is true if Zmax ≤ Σmin/C
√
rκ2.
1.6 Comparison with Other Approaches to Matrix Completion
Let us begin by mentioning that a statement analogous to our preliminary Theorem 1.1 was proved
by Achlioptas and McSherry (2007). Our result however applies to any number of revealed entries,
while the one of Achlioptas and McSherry (2007) requires |E| ≥ (8logn)4n (which for n ≤ 5 · 108
is larger than n2). We refer to Section 1.8 for further discussion of this point.
As for Theorem 1.2, we will mainly compare our algorithm with the convex relaxation approach
recently analyzed by Candès and Plan (2009), and based on semidefinite programming. Our basic
setting is indeed the same, while the algorithms are rather different.
Figures 1 and 2 compare the average root mean square error k b
M − MkF/
√
mn for the two al-
gorithms as a function of |E| and the rank-r respectively. Here M is a random rank r matrix of
dimension m = n = 600, generated by letting M = e
U e
VT with e
Uij, e
Vij i.i.d. N(0,20/
√
n). The noise
is distributed according to the independent noise model with Zij ∼ N(0,1). In the first suite of sim-
ulations, presented in Figure 1, the rank is fixed to r = 2. In the second one (Figure 2), the number
of samples is fixed to |E| = 72000. These examples are taken from Candès and Plan (2009, Figure
2062
MATRIX COMPLETION FROM NOISY ENTRIES
0
0.2
0.4
0.6
0.8
1
0 100 200 300 400 500 600
Convex Relaxation
Lower Bound
rank-r projection
OptSpace : 1 iteration
2 iterations
3 iterations
10 iterations
|E|/n
RMSE
Figure 1: Numerical simulation with random rank-2 600 × 600 matrices. Root mean square error
achieved by OPTSPACE is shown as a function of the number of observed entries |E| and
of the number of line minimizations. The performance of nuclear norm minimization and
an information theoretic lower bound are also shown.
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10
Convex Relaxation
Lower Bound
rank-r projection
OptSpace: 1 iteration
2 iterations
3 iterations
10 iterations
Rank
RMSE
Figure 2: Numerical simulation with random rank-r 600 × 600 matrices and number of observed
entries |E|/n = 120. Root mean square error achieved by OPTSPACE is shown as a
function of the rank and of the number of line minimizations. The performance of nuclear
norm minimization and an information theoretic lower bound are also shown.
2063
KESHAVAN, MONTANARI AND OH
0.0001
0.001
0.01
0.1
1
0 5 10 15 20 25 30 35 40 45 50
|E|/n=80, Fit error
RMSE
Lower Bound
|E|/n=160, Fit error
RMSE
Lower Bound
Iterations
Error
Figure 3: Numerical simulation with random rank-2 600 × 600 matrices and number of observed
entries |E|/n = 80 and 160. The standard deviation of the i.i.d. Gaussian noise is 0.001.
Fit error and root mean square error achieved by OPTSPACE are shown as functions of
the number of line minimizations. Information theoretic lower bounds are also shown.
2), from which we took the data points for the convex relaxation approach, as well as the informa-
tion theoretic lower bound described later in this section. After a few iterations, OPTSPACE has a
smaller root mean square error than the one produced by convex relaxation. In about 10 iterations
it becomes indistinguishable from the information theoretic lower bound for small ranks.
In Figure 3, we illustrate the rate of convergence of OPTSPACE. Two metrics, root mean squared
error(RMSE) and fit error kPE( b
M −N)kF/
p
|E|, are shown as functions of the number of iterations
in the manifold optimization step. Note, that the fit error can be easily evaluated since NE = PE(N)
is always available at the estimator. M is a random 600 × 600 rank-2 matrix generated as in the
previous examples. The additive noise is distributed as Zij ∼ N(0,σ2) with σ = 0.001 (A small noise
level was used in order to trace the RMSE evolution over many iterations). Each point in the figure
is the averaged over 20 random instances, and resulting errors for two different values of sample
size |E| = 80 and |E| = 160 are shown. In both cases, we can see that the RMSE converges to the
information theoretic lower bound described later in this section. The fit error decays exponentially
with the number iterations and converges to the standard deviation of the noise which is 0.001. This
is a lower bound on the fit error when r ≪ n, since even if we have a perfect reconstruction of M,
the average fit error is still 0.001.
For a more complete numerical comparison between various algorithms for matrix completion,
including different noise models, real data sets and ill conditioned matrices, we refer to Keshavan
and Oh (2009).
Next, let us compare our main result with the performance guarantee of Candès and Plan (2009,
Theorem 7). Let us stress that we require the condition number κ to be bounded, while the analysis
of Candès and Plan (2009) and Candès and Tao (2009) requires a stronger incoherence assumption
2064
MATRIX COMPLETION FROM NOISY ENTRIES
(compared to our A1). Therefore the assumptions are not directly comparable. As far as the error
bound is concerned, Candès and Plan (2009) proved that the semidefinite programming approach
returns an estimate b
M which satisfies
1
√
mn
k b
MSDP −MkF ≤ 7
r
n
|E|
kZE
kF +
2
n
√
α
kZE
kF . (6)
(The constant in front of the first term is in fact slightly smaller than 7 in Candès and Plan (2009),
but in any case larger than 4
√
2. We choose to quote a result which is slightly less accurate but
easier to parse.)
Theorem 1.2 improves over this result in several respects: (1) We do not have the second term on
the right-hand side of (6), that actually increases with the number of observed entries; (2) Our error
decreases as n/|E| rather than (n/|E|)1/2; (3) The noise enters Theorem 1.2 through the operator
norm kZEk2 instead of its Frobenius norm kZEkF ≥ kZEk2. For E uniformly random, one expects
kZEkF to be roughly of order kZEk2
√
n. For instance, within the independent entries model with
bounded variance σ, kZEkF = Θ(
p
|E|) while kZEk2 is of order
p
|E|/n (up to logarithmic terms).
Theorem 1.2 can also be compared to an information theoretic lower bound computed by Candès
and Plan (2009). Suppose, for simplicity, m = n and assume that an oracle provides us a linear
subspace T where the correct rank r matrix M = UΣVT lies. More precisely, we know that M ∈ T
where T is a linear space of dimension 2nr −r2 defined by
T = {UYT
+XVT
| X ∈ Rn×r
,Y ∈ Rn×r
} .
Notice that the rank constraint is therefore replaced by this simple linear constraint. The minimum
mean square error estimator is computed by projecting the revealed entries onto the subspace T,
which can be done by solving a least squares problem. Candès and Plan (2009) analyzed the root
mean squared error of the resulting estimator b
M and showed that
1
√
mn
k b
MOracle −MkF ≈
s
1
|E|
kZE
kF .
Here ≈ indicates that the root mean squared error concentrates in probability around the right-hand
side.
For the sake of comparison, suppose we have i.i.d. Gaussian noise with variance σ2. In this case
the oracle estimator yields (for r = o(n))
1
√
mn
k b
MOracle −MkF ≈ σ
s
2nr
|E|
.
The bound (6) on the semidefinite programming approach yields
1
√
mn
k b
MSDP −MkF ≤ σ

7
p
n|E|+
2
n
|E|

.
Finally, using Theorems 1.2 and 1.3 we deduce that OPTSPACE achieves
1
√
mn
k b
MOptSpace −MkF ≤ σ
s
Cnr
|E|
.
Hence, when the noise is i.i.d. Gaussian with small enough σ, OPTSPACE is order-optimal.
2065
KESHAVAN, MONTANARI AND OH
1.7 Related Work on Gradient Descent
Local optimization techniques such as gradient descent of coordinate descent have been intensively
studied in machine learning, with a number of applications. Here we will briefly review the recent
literature on the use of such techniques within collaborative filtering applications.
Collaborative filtering was studied from a graphical models perspective in Salakhutdinov et al.
(2007), which introduced an approach to prediction based on Restricted Boltzmann Machines (RBM).
Exact learning of the model parameters is intractable for such models, but the authors studied the
performances of a contrastive divergence, which computes an approximate gradient of the likeli-
hood function, and uses it to optimize the likelihood locally. Based on empirical evidence, it was
argued that RBM’s have several advantages over spectral methods for collaborative filtering.
An objective function analogous to the one used in the present paper was considered early on
in Srebro and Jaakkola (2003), which uses gradient descent in the factors to minimize a weighted
sum of square residuals. Salakhutdinov and Mnih (2008) justified the use of such an objective
function by deriving it as the (negative) log-posterior of an appropriate probabilistic model. This
approach naturally lead to the use of quadratic regularization in the factors. Again, gradient descent
in the factors was used to perform the optimization. Also, this paper introduced a logistic mapping
between the low-rank matrix and the recorded ratings.
Recently, this line of work was pushed further in Salakhutdinov and Srebro (2010), which em-
phasize the advantage of using a non-uniform quadratic regularization in the factors. The basic
objective function was again a sum of square residuals, and version of stochastic gradient descent
was used to optimize it.
This rich and successful line of work emphasizes the importance of obtaining a rigorous under-
standing of methods based on local minimization of the sum of square residuals with respect to the
factors. The present paper provides a first step in that direction. Hopefully the techniques developed
here will be useful to analyze the many variants of this approach.
The relationship between the non-convex objective function and convex relaxation introduced
by Fazel (2002) was further investigated by Srebro et al. (2005) and Recht et al. (2007). The basic
relation is provided by the identity
kMk∗ =
1
2
min
M=XYT

kXk2
F +kYk2
F , (7)
where kMk∗ denotes the nuclear norm of M (the sum of its singular values). In other words, adding a
regularization term that is quadratic in the factors (as the one used in much of the literature reviewed
above) is equivalent to weighting M by its nuclear norm, that can be regarded as a convex surrogate
of its rank.
In view of the identity (7) it might be possible to use the results in this paper to prove stronger
guarantees on the nuclear norm minimization approach. Unfortunately this implication is not im-
mediate. Indeed in the present paper we assume the correct rank r is known, while on the other
hand we do not use a quadratic regularization in the factors. (See Keshavan and Oh, 2009 for a
procedure that estimates the rank from the data and is provably successful under the hypotheses of
Theorem 1.2.) Trying to establish such an implication, and clarifying the relation between the two
approaches is nevertheless a promising research direction.
2066
MATRIX COMPLETION FROM NOISY ENTRIES
1.8 On the Spectrum of Sparse Matrices and the Role of Trimming
The trimming step of the OPTSPACE algorithm is somewhat counter-intuitive in that we seem to be
wasting information. In this section we want to clarify its role through a simple example. Before
describing the example, let us stress once again two facts: (i) In the last step of our the algorithm,
the trimmed entries are actually incorporated in the cost function and hence the full information
is exploited; (ii) Trimming is not the only way to treat over-represented rows/columns in ME, and
probably not the optimal one. One might for instance rescale the entries of such rows/columns. We
stick to trimming because we can prove it actually works.
Let us now turn to the example. Assume, for the sake of simplicity, that m = n, there is no
noise in the revealed entries, and M is the rank one matrix with Mij = 1 for all i and j. Within
the independent sampling model, the matrix ME has i.i.d. entries, with distribution Bernoulli(ε/n).
The number of non-zero entries in a column is Binomial(n,ε/n) and is independent for different
columns. It is not hard to realize that the column with the largest number of entries has more than
C logn/loglogn entries, with positive probability (this probability can be made as large as we want
by reducing C). Let i be the index of this column, and consider the test vector e(i) that has the i-th
entry equal to 1 and all the others equal to 0. By computing kMEe(i)k, we conclude that the largest
singular value of ME is at least
p
Clogn/loglogn. In particular, this is very different from the
largest singular value of E{ME} = (ε/n)M which is ε. This suggests that approximating M with
the Pr(ME) leads to a large error. Hence trimming is crucial in proving Theorem 1.1. Also, the
phenomenon is more severe in real data sets than in the present model, where each entry is revealed
independently.
Trimming is also crucial in proving Theorem 1.3. Using the above argument, it is possible to
show that under the worst case model,
kZE
k2 ≥ C′
(ε)Zmax
s
logn
loglogn
.
This suggests that the largest singular value of the noise matrix ZE is quite different from the largest
singular value of E{ZE} which is εZmax.
To summarize, Theorems 1.1 and 1.3 (for the worst case model) simply do not hold without
trimming or a similar procedure to normalize rows/columns of NE. Trimming allows to overcome
the above phenomenon by setting to 0 over-represented rows/columns.
2. Proof of Theorem 1.1
As explained in the introduction, the crucial idea is to consider the singular value decomposition
of the trimmed matrix e
NE instead of the original matrix NE. Apart from a trivial rescaling, these
singular values are close to the ones of the original matrix M.
Lemma 1 There exists a numerical constant C such that, with probability greater than 1−1/n3,
σq
ε
−Σq ≤ CMmax
r
α
ε
+
1
ε
ke
ZE
k2 ,
where it is understood that Σq = 0 for q > r.
2067
KESHAVAN, MONTANARI AND OH
Proof For any matrix A, let σq(A) denote the qth singular value of A. Then, σq(A+B) ≤ σq(A)+
σ1(B), whence
σq
ε
−Σq ≤
σq( e
ME)
ε
−Σq +
σ1(e
ZE)
ε
≤ CMmax
r
α
ε
+
1
ε
ke
ZE
k2 ,
where the second inequality follows from the next Lemma as shown by Keshavan et al. (2010).
Lemma 2 (Keshavan, Montanari, Oh, 2009) There exists a numerical constant C such that, with
probability larger than 1−1/n3,
1
√
mn
M −
√
mn
ε
e
ME
2
≤ CMmax
r
α
ε
.
We will now prove Theorem 1.1.
Proof (Theorem 1.1) For any matrix A of rank at most 2r, kAkF ≤
√
2rkAk2, whence
1
√
mn
kM −Pr(e
NE
)kF ≤
√
2r
√
mn
M −
√
mn
ε

e
NE
− ∑
i≥r+1
σixiyT
i

2
=
√
2r
√
mn
M −
√
mn
ε

e
ME
+ e
ZE
− ∑
i≥r+1
σixiyT
i

2
=
√
2r
√
mn

M −
√
mn
ε
e
ME

+
√
mn
ε
e
ZE
−

∑
i≥r+1
σixiyT
i

!
2
≤
√
2r
√
mn

M −
√
mn
ε
e
ME
2
+
√
mn
ε
ke
ZE
k2 +
√
mn
ε
σr+1

≤ 2CMmax
r
2αr
ε
+
2
√
2r
ε
ke
ZE
k2
≤ C′
Mmax
nrα3/2
|E|
!1/2
+ 2
√
2

n
√
rα
|E|

ke
ZE
k2 .
where on the fourth line, we have used the fact that for any matrices Ai, k∑i Aik2 ≤ ∑i kAik2. This
proves our claim.
3. Proof of Theorem 1.2
Recall that the cost function is defined over the Riemannian manifold M(m,n) ≡ G(m,r)×G(n,r).
The proof of Theorem 1.2 consists in controlling the behavior of F in a neighborhood of u = (U,V)
(the point corresponding to the matrix M to be reconstructed). Throughout the proof we let K (µ)
be the set of matrix couples (X,Y) ∈ Rm×r ×Rn×r such that kX(i)k2 ≤ µr, kY( j)k2 ≤ µr for all i, j.
2068
MATRIX COMPLETION FROM NOISY ENTRIES
3.1 Preliminary Remarks and Definitions
Given x1 = (X1,Y1) and x2 = (X2,Y2) ∈ M(m,n), two points on this manifold, their distance is
defined as d(x1,x2) =
p
d(X1,X2)2 +d(Y1,Y2)2, where, letting (cosθ1,...,cosθr) be the singular
values of XT
1 X2/m,
d(X1,X2) = kθk2 .
The next remark bounds the distance between two points on the manifold. In particular, we will
use this to bound the distance between the original matrix M = UΣVT and the starting point of the
manifold optimization b
M = X0S0YT
0 .
Remark 3 (Keshavan, Montanari, Oh, 2009) Let U,X ∈ Rm×r with UTU = XT X = mI, V,Y ∈
Rn×r with VTV = YTY = nI, and M = UΣVT , b
M = XSYT for Σ = diag(Σ1,...,Σr) and S ∈ Rr×r.
If Σ1,...,Σr ≥ Σmin, then
d(U,X) ≤
π
√
2αnΣmin
kM − b
MkF , d(V,Y) ≤
π
√
2αnΣmin
kM − b
MkF
Given S achieving the minimum in Eq. (2), it is also convenient to introduce the notations
d−(x,u) ≡
q
Σ2
mind(x,u)2 +kS−Σk2
F ,
d+(x,u) ≡
q
Σ2
maxd(x,u)2 +kS−Σk2
F .
3.2 Auxiliary Lemmas and Proof of Theorem 1.2
The proof is based on the following two lemmas that generalize and sharpen analogous bounds in
Keshavan et al. (2010).
Lemma 4 There exist numerical constants C0,C1,C2 such that the following happens. Assume
ε ≥ C0µ0r
√
α max{logn; µ0r
√
α(Σmin/Σmax)4 } and δ ≤ Σmin/(C0Σmax). Then,
F(x)−F(u) ≥ C1nε
√
αd−(x,u)2
−C1n
√
rαkZE
k2d+(x,u), (8)
F(x)−F(u) ≤ C2nε
√
αΣ2
max d(x,u)2
+C2n
√
rαkZE
k2d+(x,u), (9)
for all x ∈ M(m,n)∩K (4µ0) such that d(x,u) ≤ δ, with probability at least 1−1/n4. Here S ∈ Rr×r
is the matrix realizing the minimum in Eq. (2).
Corollary 3.1 There exist a constant C such that, under the hypotheses of Lemma 4
kS−ΣkF ≤ CΣmaxd(x,u)+C
√
r
ε
kZE
k2 .
Further, for an appropriate choice of the constants in Lemma 4, we have
σmax(S) ≤ 2Σmax +C
√
r
ε
kZE
k2 , (10)
σmin(S) ≥
1
2
Σmin −C
√
r
ε
kZE
k2 . (11)
2069
KESHAVAN, MONTANARI AND OH
Lemma 5 There exist numerical constants C0,C1,C2 such that the following happens. Assume
ε ≥ C0µ0r
√
α(Σmax/Σmin)2 max{logn; µ0r
√
α(Σmax/Σmin)4 } and δ ≤ Σmin/(C0Σmax). Then,
kgrad e
F(x)k2
≥ C1 nε2
Σ4
min

d(x,u)−C2
√
rΣmax
εΣmin
kZEk2
Σmin
2
+
, (12)
for all x ∈ M(m,n)∩K (4µ0) such that d(x,u) ≤ δ, with probability at least 1−1/n4. (Here [a]+ ≡
max(a,0).)
We can now turn to the proof of our main theorem.
Proof (Theorem 1.2). Let δ = Σmin/C0Σmax with C0 large enough so that the hypotheses of Lemmas
4 and 5 are verified.
Call {xk}k≥0 the sequence of pairs (Xk,Yk) ∈ M(m,n) generated by gradient descent. By as-
sumption the right-hand side of Eq. (3) is smaller than Σmin. The following is therefore true for
some numerical constant C:
kZE
k2 ≤
ε
C
√
r

Σmin
Σmax
2
Σmin . (13)
Notice that the constant appearing here can be made as large as we want by modifying the constant
appearing in the statement of the theorem. Further, by using Corollary 3.1 in Eqs. (8) and (9) we get
F(x)−F(u) ≥ C1nε
√
αΣ2
min

d(x,u)2
−δ2
0,− , (14)
F(x)−F(u) ≤ C2nε
√
αΣ2
max

d(x,u)2
+δ2
0,+ , (15)
with C1 and C2 different from those in Eqs. (8) and (9), where
δ0,− ≡ C
√
rΣmax
εΣmin
kZEk2
Σmin
, δ0,+ ≡ C
√
rΣmax
εΣmin
kZEk2
Σmax
.
By Eq. (13), with large enough C, we can assume δ0,− ≤ δ/20 and δ0,+ ≤ (δ/20)(Σmin/Σmax).
Next, we provide a bound on d(u,x0). Using Remark 3, we have d(u,x0) ≤ (π/n
√
αΣmin)kM −
X0S0YT
0 kF. Together with Theorem 1.1 this implies
d(u,x0) ≤
CMmax
Σmin
rα
ε
1/2
+
C′ √
r
εΣmin
ke
ZE
k2 .
Since ε ≥ C′′αµ2
1r2(Σmax/Σmin)4 as per our assumptions and Mmax ≤ µ1
√
rΣmax for incoherent M,
the first term in the above bound is upper bounded by Σmin/20C0Σmax, for large enough C′′. Using
Eq. (13), with large enough constant C, the second term in the above bound is upper bounded by
Σmin/20C0Σmax. Hence we get
d(u,x0) ≤
δ
10
.
We make the following claims :
2070
MATRIX COMPLETION FROM NOISY ENTRIES
1. xk ∈ K (4µ0) for all k.
First we notice that we can assume x0 ∈ K (3µ0). Indeed, if this does not hold, we can
‘rescale’ those rows of X0, Y0 that violate the constraint. A proof that this rescaling is possible
was given in Keshavan et al. (2010) (cf. Remark 6.2 there). We restate the result here for the
reader’s convenience in the next Remark.
Remark 6 Let U,X ∈ Rn×r with UTU = XT X = nI and U ∈ K (µ0) and d(X,U) ≤ δ ≤ 1
16.
Then there exists X′ ∈ Rn×r such that X′T X′ = nI, X′ ∈ K (3µ0) and d(X′,U) ≤ 4δ. Further,
such an X′ can be computed from X in a time of O(nr2).
Since x0 ∈ K (3µ0) , e
F(x0) = F(x0) ≤ 4C2nε
√
αΣ2
maxδ2/100. On the other hand e
F(x) ≥
ρ(e1/9 − 1) for x 6∈ K (4µ0). Since e
F(xk) is a non-increasing sequence, the thesis follows
provided we take ρ ≥ C2nε
√
αΣ2
min.
2. d(xk,u) ≤ δ/10 for all k.
Since ε ≥ Cαµ2
1r2(Σmax/Σmin)6 as per our assumptions in Theorem 1.2, we have d(x0,u)2 ≤
(C1Σ2
min/C2Σ2
max)(δ/20)2. Also assuming Eq. (13) with large enough C, we have δ0,− ≤ δ/20
and δ0,+ ≤ (δ/20)(Σmin/Σmax). Then, by Eq. (15),
F(x0) ≤ F(u)+C1nε
√
αΣ2
min
2δ2
400
.
Also, using Eq. (14), for all xk such that d(xk,u) ∈ [δ/10,δ], we have
F(x) ≥ F(u)+C1nε
√
αΣ2
min
3δ2
400
.
Hence, for all xk such that d(xk,u) ∈ [δ/10,δ], we have e
F(x) ≥ F(x) ≥ F(x0). This contra-
dicts the monotonicity of e
F(x), and thus proves the claim.
Since the cost function is twice differentiable, and because of the above two claims, the sequence
{xk} converges to
Ω =

x ∈ K (4µ0)∩M(m,n) : d(x,u) ≤ δ,grad e
F(x) = 0 .
By Lemma 5 for any x ∈ Ω,
d(x,u) ≤ C
√
rΣmax
εΣmin
kZEk2
Σmin
. (16)
Using Corollary 3.1, we have d+(x,u) ≤ Σmaxd(x,u)+kS−ΣkF ≤CΣmaxd(x,u)+C(
√
r/ε)kZEk2.
Together with Eqs. (18) and (16), this implies
1
n
√
α
kM −XSYT
kF ≤ C
√
rΣ2
maxkZEk2
εΣ2
min
,
which finishes the proof of Theorem 1.2.
2071
KESHAVAN, MONTANARI AND OH
3.3 Proof of Lemma 4 and Corollary 3.1
Proof (Lemma 4) The proof is based on the analogous bound in the noiseless case, that is, Lemma
5.3 in Keshavan et al. (2010). For readers’ convenience, the result is reported in Appendix A,
Lemma 7. For the proof of these lemmas, we refer to Keshavan et al. (2010).
In order to prove the lower bound, we start by noticing that
F(u) ≤
1
2
kPE(Z)k2
F ,
which is simply proved by using S = Σ in Eq. (2). On the other hand, we have
F(x) =
1
2
kPE(XSYT
−M −Z)k2
F
=
1
2
kPE(Z)k2
F +
1
2
kPE(XSYT
−M)k2
F −hPE(Z),(XSYT
−M)i (17)
≥ F(u)+Cnε
√
αd−(x,u)2
−
√
2rkZE
k2kXSYT
−MkF ,
where in the last step we used Lemma 7. Now by triangular inequality
kXSYT
−Mk2
F ≤ 3kX(S−Σ)YT
k2
F +3kXΣ(Y −V)T
k2
F +3k(X −U)ΣVT
k2
F
≤ 3nmkS−Σk2
F +3n2
αΣ2
max(
1
m
kX −Uk2
F +
1
n
kY −Vk2
F)
≤ Cn2
αd+(x,u)2
, (18)
In order to prove the upper bound, we proceed as above to get
F(x) ≤ 1
2kPE(Z)k2
F +Cnε
√
αΣ2
max d(x,u)2 +
√
2rαkZEk2Cnd+(x,u).
Further, by replacing x with u in Eq. (17)
F(u) ≥
1
2
kPE(Z)k2
F −hPE(Z),(U(S−Σ)VT
)i
≥
1
2
kPE(Z)k2
F −
√
2rαkZE
k2Cnd+(x,u).
By taking the difference of these inequalities we get the desired upper bound.
Proof (Corollary 3.1) By putting together Eq. (8) and (9), and using the definitions of d+(x,u),
d−(x,u), we get
kS−Σk2
F ≤
C1 +C2
C1
Σ2
maxd(x,u)2
+
(C1 +C2)
√
r
C1ε
kZE
k2
q
Σ2
maxd(x,u)2 +kS−Σk2
F .
Let x ≡ kS−ΣkF, a2 ≡ (C1 +C2)/C1

Σ2
maxd(x,u)2, and b ≡ (C1 +C2)
√
r/C1ε

kZEk2. The above
inequality then takes the form
x2
≤ a2
+b
p
x2 +a2 ≤ a2
+ab+bx,
which implies our claim x ≤ a+b.
2072
MATRIX COMPLETION FROM NOISY ENTRIES
The singular value bounds (10) and (11) follow by triangular inequality. For instance
σmin(S) ≥ Σmin −CΣmaxd(x,u)−C
√
r
ε
kZE
k2 .
which implies the inequality (11) for d(x,u) ≤ δ = Σmin/C0Σmax and C0 large enough. An analo-
gous argument proves Eq. (10).
3.4 Proof of Lemma 5
Without loss of generality we will assume δ ≤ 1, C2 ≥ 1 and
√
r
ε
kZE
k2 ≤ Σmin , (19)
because otherwise the lower bound (12) is trivial for all d(x,u) ≤ δ.
Denote by t 7→ x(t), t ∈ [0,1], the geodesic on M(m,n) such that x(0) = u and x(1) = x,
parametrized proportionally to the arclength. Let b
w = ẋ(1) be its final velocity, with b
w = ( b
W, b
Q).
Obviously b
w ∈ Tx (with Tx the tangent space of M(m,n) at x) and
1
m
k b
Wk2
+
1
n
k b
Qk2
= d(x,u)2
,
because t 7→ x(t) is parametrized proportionally to the arclength.
Explicit expressions for b
w can be obtained in terms of w ≡ ẋ(0) = (W,Q) (Keshavan et al.,
2010). If we let W = LΘRT be the singular value decomposition of W, we obtain
b
W = −URΘsinΘRT
+LΘcosΘRT
. (20)
It was proved in Keshavan et al. (2010) that hgradG(x), b
wi ≥ 0. It is therefore sufficient to lower
bound the scalar product hgradF, b
wi. By computing the gradient of F we get
hgradF(x), b
wi = hPE(XSYT
−N),(XS b
QT
+ b
WSYT
)i
= hPE(XSYT
−M),(XS b
QT
+ b
WSYT
)i−hPE(Z),(XS b
QT
+ b
WSYT
)i
= hgradF0(x), b
wi−hPE(Z),(XS b
QT
+ b
WSYT
)i (21)
where F0(x) is the cost function in absence of noise, namely
F0(X,Y) = min
S∈Rr×r
(
1
2 ∑
(i, j)∈E
(XSYT
)ij −Mij
2
)
. (22)
As proved in Keshavan et al. (2010),
hgradF0(x), b
wi ≥ Cnε
√
αΣ2
mind(x,u)2
(23)
(see Lemma 9 in Appendix).
We are therefore left with the task of upper bounding hPE(Z),(XS b
QT + b
WSYT )i. Since XS b
QT
has rank at most r, we have
hPE(Z),XS b
QT
i ≤
√
rkZE
k2 kXS b
QT
kF .
2073
KESHAVAN, MONTANARI AND OH
Since XT X = mI, we get
kXS b
QT
k2
F = mTr(ST
S b
QT b
Q) ≤ nασmax(S)2
k b
Qk2
F
≤ Cn2
α

Σmax +
√
r
ε
kZE
kF
2
d(x,u)2
(24)
≤ 4Cn2
αΣ2
max d(x,u)2
,
where, in inequality (24), we used Corollary 3.1 and in the last step, we used Eq. (19). Proceeding
analogously for hPE(Z), b
WSYT i, we get
hPE(Z),(XS b
QT
+ b
WSYT
)i ≤ C′
nΣmax
√
rαkZE
k2 d(x,u).
Together with Eq. (21) and (23) this implies
hgradF(x), b
wi ≥ C1nε
√
αΣ2
mind(x,u)
n
d(x,u)−C2
√
rΣmax
εΣmin
kZEk2
Σmin
o
,
which implies Eq. (12) by Cauchy-Schwartz inequality.
4. Proof of Theorem 1.3
Proof (Independent entries model ) We start with a claim that for any sampling set E, we have
ke
ZE
k2 ≤ kZE
k2 .
To prove this claim, let x∗ and y∗ be m and n dimensional vectors, respectively, achieving the opti-
mum in maxkxk≤1,kyk≤1{xT e
ZEy}, that is, such that ke
ZEk2 = x∗T e
ZEy∗. Recall that, as a result of the
trimming step, all the entries in trimmed rows and columns of e
ZE are set to zero. Then, there is no
gain in maximizing xT e
ZEy to have a non-zero entry x∗
i for i corresponding to the rows which are
trimmed. Analogously, for j corresponding to the trimmed columns, we can assume without loss of
generality that y∗
j = 0. From this observation, it follows that x∗T e
ZEy∗ = x∗T ZEy∗, since the trimmed
matrix e
ZE and the sample noise matrix ZE only differ in the trimmed rows and columns. The claim
follows from the fact that x∗T ZEy∗ ≤ kZEk2, for any x∗ and y∗ with unit norm.
In what follows, we will first prove that kZEk2 is bounded by the right-hand side of Eq. (4)
for any range of |E|. Due to the above observation, this implies that ke
ZEk2 is also bounded by
Cσ
p
ε
√
αlogn, where ε ≡ |E|/
√
αn. Further, we use the same analysis to prove a tighter bound in
Eq. (5) when |E| ≥ nlogn.
First, we want to show that kZEk2 is bounded by Cσ
p
ε
√
αlogn, and Zij’s are i.i.d. random
variables with zero mean and sub-Gaussian tail with parameter σ2. The proof strategy is to show that
E

kZEk2

is bounded, using the result of Seginer (2000) on expected norm of random matrices, and
use the fact that k · k2 is a Lipschitz continuous function of its arguments together with concentration
inequality for Lipschitz functions on i.i.d. Gaussian random variables due to Talagrand (1996).
Note that k · k2 is a Lipschitz function with a Lipschitz constant 1. Indeed, for any M and M′,
kM′k2 − kMk2 ≤ kM′ − Mk2 ≤ kM′ − MkF, where the first inequality follows from triangular
inequality and the second inequality follows from the fact that k · k2
F is the sum of the squared
singular values.
2074
MATRIX COMPLETION FROM NOISY ENTRIES
To bound the probability of large deviation, we use the result on concentration inequality for
Lipschitz functions on i.i.d. sub-Gaussian random variables due to Talagrand (1996). For a 1-
Lipschitz function k·k2 on m×n i.i.d. random variables ZE
ij with zero mean, and sub-Gaussian tails
with parameter σ2,
P kZE
k2 −E[kZE
k2] > t

≤ exp
n
−
t2
2σ2
o
. (25)
Setting t =
p
8σ2 logn, this implies that kZEk2 ≤ E

kZk2

+
p
8σ2 logn with probability larger
than 1−1/n4.
Now, we are left to bound the expectation E

kZEk2

. First, we symmetrize the possibly asym-
metric random variables ZE
ij to use the result of Seginer (2000) on expected norm of random matrices
with symmetric random variables. Let Z′
ij’s be independent copies of Zij’s, and ξij’s be independent
Bernoulli random variables such that ξij = +1 with probability 1/2 and ξij = −1 with probability
1/2. Then, by convexity of E

kZE −Z′Ek2|Z′E

and Jensen’s inequality,
E

kZE
k2

≤ E

kZE
−Z′E
k2

= E

k(ξij(ZE
ij −Z′E
ij ))k2

≤ 2E

k(ξijZE
ij)k2

,
where (ξijZE
ij) denotes an m×n matrix with entry ξijZE
ij in position (i, j). Thus, it is enough to show
that E

kZEk2

is bounded by Cσ
p
ε
√
αlogn in the case of symmetric random variables Zij’s.
To this end, we apply the following bound on expected norm of random matrices with i.i.d.
symmetric random entries, proved by Seginer (2000, Theorem 1.1).
E

kZE
k2

≤ C

E

max
i∈[m]
kZE
i•k

+E

max
j∈[n]
kZE
• jk

, (26)
where ZE
i• and ZE
• j denote the ith row and jth column of A respectively. For any positive parameter
β, which will be specified later, the following is true.
E

max
j
kZE
• jk2

≤ βσ2
ε
√
α+
Z ∞
0
P max
j
kZE
• jk2
≥ βσ2
ε
√
α+z

dz. (27)
To bound the second term, we can apply union bound on each of the n columns, and use the follow-
ing bound on each column kZE
• jk2 resulting from concentration of measure inequality for the i.i.d.
sub-Gaussian random matrix Z.
P
 m
∑
k=1
(ZE
k j)2
≥ βσ2
ε
√
α+z

≤ exp
n
−
3
8

(β−3)ε
√
α+
z
σ2
o
. (28)
To prove the above result, we apply Chernoff bound on the sum of independent random vari-
ables. Recall that ZE
k j = ξ̃k jZk j where ξ̃’s are independent Bernoulli random variables such that
ξ̃ = 1 with probability ε/
√
mn and zero with probability 1 − ε/
√
mn. Then, for the choice of
λ = 3/8σ2 < 1/2σ2,
E
h
exp

λ
m
∑
k=1
(ξ̃k jZk j)2
i
=

1−
ε
√
mn
+
ε
√
mn
E[eλZ2
k j ]
m
≤

1−
ε
√
mn
+
ε
p
mn(1−2σ2λ)
m
= exp
n
mlog

1+
ε
√
mn
o
≤ exp

ε
√
α ,
2075
KESHAVAN, MONTANARI AND OH
where the first inequality follows from the definition of Zk j as a zero mean random variable with
sub-Gaussian tail, and the second inequality follows from log(1 + x) ≤ x. By applying Chernoff
bound, Eq. (28) follows. Note that an analogous result holds for the Euclidean norm on the rows
kZE
i•k2.
Substituting Eq. (28) and P maxj kZE
• jk2 ≥ z

≤ mP kZE
• jk2 ≥ z

in Eq. (27), we get
E

max
j
kZE
• jk2

≤ βσ2
ε
√
α+
8σ2m
3
e− 3
8 (β−3)ε
√
α
. (29)
The second term can be made arbitrarily small by taking β = Clogn with large enough C. Since
E

maxj kZE
• jk

≤
q
E

maxj kZE
• jk2

, applying Eq. (29) with β = Clogn in Eq. (26) gives
E

kZE
k2

≤ Cσ
q
ε
√
αlogn .
Together with Eq. (25), this proves the desired thesis for any sample size |E|.
In the case when |E| ≥ nlogn, we can get a tighter bound by similar analysis. Since ε ≥C′ logn,
for some constant C′, the second term in Eq. (29) can be made arbitrarily small with a large constant
β. Hence, applying Eq. (29) with β = C in Eq. (26), we get
E

kZE
k2

≤ Cσ
q
ε
√
α .
Together with Eq. (25), this proves the desired thesis for |E| ≥ nlogn.
Proof (Worst Case Model ) Let D be the m×n all-ones matrix. Then for any matrix Z from the worst
case model, we have ke
ZEk2 ≤ Zmaxke
DEk2, since xT e
ZEy ≤ ∑i, j Zmax|xi|e
DE
ij|yj|, which follows from
the fact that Zij’s are uniformly bounded. Further, e
DE is an adjacency matrix of a corresponding
bipartite graph with bounded degrees. Then, for any choice of E the following is true for all positive
integers k:
ke
DE
k2k
2 ≤ max
x,kxk=1
xT
((e
DE
)T e
DE
)k
x ≤ Tr ((e
DE
)T e
DE
)k

≤ n(2ε)2k
.
Now Tr ((e
DE)T e
DE)k

is the number of paths of length 2k on the bipartite graph with adjacency
matrix e
DE, that begin and end at i for every i ∈ [n]. Since this graph has degree bounded by 2ε, we
get
ke
DE
k2k
2 ≤ n(2ε)2k
.
Taking k large, we get the desired thesis.
Acknowledgments
This work was partially supported by a Terman fellowship, the NSF CAREER award CCF-0743978
and the NSF grant DMS-0806211. SO was supported by a fellowship from the Samsung Scholarship
Foundation.
2076
MATRIX COMPLETION FROM NOISY ENTRIES
Appendix A. Three Lemmas on the Noiseless Problem
Lemma 7 There exists numerical constants C0,C1,C2 such that the following happens. Assume
ε ≥ C0µ0r
√
α max{logn; µ0r
√
α(Σmin/Σmax)4 } and δ ≤ Σmin/(C0Σmax). Then,
C1
√
αΣ2
min d(x,u)2
+C1
√
αkS0 −Σk2
F ≤
1
nε
F0(x) ≤ C2
√
αΣ2
maxd(x,u)2
,
for all x ∈ M(m,n) ∩ K (4µ0) such that d(x,u) ≤ δ, with probability at least 1 − 1/n4. Here S0 ∈
Rr×r is the matrix realizing the minimum in Eq. (22).
Lemma 8 There exists numerical constants C0 and C such that the following happens. Assume
ε ≥ C0µ0r
√
α(Σmax/Σmin)2 max{logn; µ0r
√
α(Σmax/Σmin)4 } and δ ≤ Σmin/(C0Σmax). Then
kgrad e
F0(x)k2
≥ Cnε2
Σ4
mind(x,u)2
,
for all x ∈ M(m,n)∩K (4µ0) such that d(x,u) ≤ δ, with probability at least 1−1/n4.
Lemma 9 Define b
w as in Eq. (20). Then there exists numerical constants C0 and C such that the
following happens. Under the hypothesis of Lemma 8
hgradF0(x), b
wi ≥ Cnε
√
αΣ2
mind(x,u)2
,
for all x ∈ M(m,n)∩K (4µ0) such that d(x,u) ≤ δ, with probability at least 1−1/n4.
References
P.-A. Absil, R. Mahony, and R. Sepulchrer. Optimization Algorithms on Matrix Manifolds. Prince-
ton University Press, 2008.
D. Achlioptas and F. McSherry. Fast computation of low-rank matrix approximations. J. ACM, 54
(2):9, 2007.
J-F Cai, E. J. Candès, and Z. Shen. A singular value thresholding algorithm for matrix completion.
arXiv:0810.3286, 2008.
E. J. Candès and Y. Plan. Matrix completion with noise. arXiv:0903.3131, 2009.
E. J. Candès and B. Recht. Exact matrix completion via convex optimization. arxiv:0805.4471,
2008.
E. J. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion.
arXiv:0903.1476, 2009.
A. Edelman, T. A. Arias, and S. T. Smith. The geometry of algorithms with orthogonality con-
straints. SIAM J. Matr. Anal. Appl., 20:303–353, 1999.
M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.
A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for finding low-rank approxi-
mations. J. ACM, 51(6):1025–1041, 2004. ISSN 0004-5411.
2077
KESHAVAN, MONTANARI AND OH
R. H. Keshavan and S. Oh. Optspace: A gradient descent algorithm on the grassman manifold for
matrix completion. arXiv:0910.5260, 2009.
R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans.
Inform. Theory, 56(6):2980–2998, June 2010.
K. Lee and Y. Bresler. Admira: Atomic decomposition for minimum rank approximation.
arXiv:0905.0044, 2009.
S. Ma, D. Goldfarb, and L. Chen. Fixed point and Bregman iterative methods for matrix rank
minimization. arXiv:0905.1643, 2009.
B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions of matrix equations via
nuclear norm minimization. arxiv:0706.4138, 2007.
R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in Neural Informa-
tion Processing Systems, volume 20, 2008.
R. Salakhutdinov and N. Srebro. Collaborative filtering in a non-uniform world: Learning with the
weighted trace norm. arXiv:1002.2780, 2010.
R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted Boltzmann machines for collaborative fil-
tering. In Proceedings of the International Conference on Machine Learning, volume 24, pages
791–798, 2007.
Y. Seginer. The expected norm of random matrices. Comb. Probab. Comput., 9:149–
166, March 2000. ISSN 0963-5483. doi: 10.1017/S096354830000420X. URL
http://portal.acm.org/citation.cfm?id=971471.971475.
N. Srebro and T. S. Jaakkola. Weighted low-rank approximations. In In 20th International Confer-
ence on Machine Learning, pages 720–727. AAAI Press, 2003.
N. Srebro, J. D. M. Rennie, and T. S. Jaakola. Maximum-margin matrix factorization. In Advances
in Neural Information Processing Systems 17, pages 1329–1336. MIT Press, 2005.
M. Talagrand. A new look at independence. The Annals of Probability, 24(1):1–34, 1996. ISSN
00911798. URL http://www.jstor.org/stable/2244830.
K. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least
squares problems. http://www.math.nus.edu.sg/∼matys, 2009.
2078
