arXiv:1306.3917v1
[stat.ML]
17
Jun
2013
On Finding the Largest Mean Among Many
Kevin Jamiesonâ€ 
, Matthew Malloyâ€ âˆ—
, Robert Nowakâ€ 
, and SeÌbastien Bubeckâ€¡
â€ 
Department of Electrical and Computer Engineering,
University of Wisconsin-Madison
â€¡
Princeton University,
Department of Operations Research and Financial Engineering
Abstract
Sampling from distributions to find the one with the largest mean arises in a broad range of applica-
tions, and it can be mathematically modeled as a multi-armed bandit problem in which each distribution
is associated with an arm. This paper studies the sample complexity of identifying the best arm (largest
mean) in a multi-armed bandit problem. Motivated by large-scale applications, we are especially in-
terested in identifying situations where the total number of samples that are necessary and sufficient to
find the best arm scale linearly with the number of arms. We present a single-parameter multi-armed
bandit model that spans the range from linear to superlinear sample complexity. We also give a new
algorithm for best arm identification, called PRISM, with linear sample complexity for a wide range of
mean distributions. The algorithm, like most exploration procedures for multi-armed bandits, is adaptive
in the sense that the next arms to sample are selected based on previous samples. We compare the sam-
ple complexity of adaptive procedures with simpler non-adaptive procedures using new lower bounds.
For many problem instances, the increased sample complexity required by non-adaptive procedures is a
polynomial factor of the number of arms.
1 Introduction
This paper studies the sample complexity of finding the best arm in a multi-armed bandit problem. Consider
n + 1 arms with mean payoffs Âµ0 > Âµ1 > Â· Â· Â· > Âµn. The mean values and the ordering of the arms are
unknown. The goal is to identify the arm with the largest mean (i.e., the â€œbest armâ€) by sampling the arms.
A sample of arm i is an independent realization of a random variable Xi âˆˆ [0, 1] with mean Âµi âˆˆ [0, 1]
(it is straightforward to extend all results presented in this paper to sub-Gaussian realizations with bounded
means and variances).
The main focus of this paper is identifying necessary and sufficient conditions under which the sample
complexity (total number of samples) of finding the best arm grows linearly in the number of arms. This
is motivated by applications involving very large numbers of arms, such as virus replication experiments
testing thousands of cell strains [3], cognitive radio problems searching over hundreds of communication
channels [1, 2], and network surveillance of large social networks. These applications are time-consuming
and/or costly, so minimizing the number of samples required to find the most influential genes, best chan-
nels, or malicious agents is crucial. This paper quantifies the minimum number of samples needed in such
applications and gives a new algorithm called PRISM that succeeds using a total number of samples within
a negligible factor of the minimum.
âˆ—
The first two authors are listed in alphabetical order as both contributed equally.
1
Mannor and Tsitsiklis [4] showed that for any procedure that finds the best arm with probability at least
1âˆ’Î´ requires on the order of H log(1/Î´) samples, where H :=
Pn
i=1(Âµ0 âˆ’Âµi)âˆ’2. This lower bound shows
that the sample complexity can be much greater than the number of arms. For example, if the gap between
Âµ0 and Âµ1 is 1/n, then H, and the sample complexity, are at least O(n2). On the other hand, scenarios can
arise in which H grows linearly with n. For instance, if Âµ0 âˆ’ Âµi is greater than a positive constant for all i,
then H = O(n). This case is â€œsparseâ€ in the sense that Âµ0 is bounded away from all others, and recent work
has shown that O(n) samples are sufficient in such cases [5].
Most bandit exploration algorithms are sequential and adaptive in the sense that the selection of the arms
to sample next is based on previous samples. This is necessary in order to achieve linear sample complexity
in the sparse case mentioned above. Non-adaptive methods, in which every arm is sampled an equal number
of times, require at least O(n log n)1 samples [6]. The factor of log n is significant if n is large, which is
one motivation for adaptive strategies like the one used in [3]. Contrasting the differences between adaptive
and non-adaptive sampling is a second focus of this paper.
Of particular interest here is the scaling of the sample complexity as a function of the number of arms
and the behavior of the gaps between their means. The sparse model discussed above is an enlightening
idealization, but unlikely to arise in practice. A smoothly decaying distribution of means may be a more
reasonable model for the biological and radio applications discussed above. Fig. 1 depicts the means in
three different arm configurations. The left plot (a) represents a sparse model in which Âµ0 is bounded
away from all others means by a fixed constant (all gaps greater than a fixed constant). In this case, the
sample complexities of non-adaptive and adaptive strategies differs by a factor of log n. The other two plots
represent cases in which the gaps between means are shrinking as n increases. In Figure 1(b) Âµ0 âˆ’ Âµi =
( i
n ).49 and in (c) Âµ0 âˆ’ Âµi = i
n . The difference between the sample complexities is much more significant
for these non-sparse cases. Also note that there are non-sparse cases in which adaptive strategies can find
the best arm in O(n) samples like the one shown in Fig. 1(b).
adaptive â€“ O(n)
non-adaptive â€“ O(n logn)
adaptive â€“ O(n)
non-adaptive â€“ O(n1.98
)
adaptive â€“ O(n2
)
non-adaptive â€“ O(n3
)
(a) (b) (c)
Figure 1: Three different configurations of means, each ordered Âµ0 > Âµ1 > Â· Â· Â· > Âµn. In (a) Âµ0 âˆ’ Âµi â‰¥ 0.95, in (b)
Âµ0 âˆ’ Âµi = ( i
n ).49
, and in (c) Âµ0 âˆ’ Âµi = i
n , for i = 1, . . . , n. The necessary sample complexities (sufficient to within
log log n factors) of non-adaptive and adaptive strategies are indicated in each case. Finding the best arm becomes
increasingly difficult as the gaps between the means decrease, but in all cases adaptive strategies have significantly
lower sample complexities.
1
In this paper our focus is on how the number of samples scales with n, not necessarily the probability of failure Î´. For instance,
if we were to say an algorithm requires just O(n) samples then it is understood that this many samples suffices to find the best arm
with a fixed probability of error. However, in the theorem statements the dependence on Î´ is explicit.
2
1.1 Contributions and Organization
The paper is organized as follows. In Sec. 2, we present a single-parameter model for the distribution of
means that spans the range from linear to superlinear sample complexity. In Sec. 3 we present an algorithm
for best arm identification with sample complexity O(H log(1/Î´)) for a wide range of mean distributions.
In particular, we show that the algorithm has linear sample complexity for all of the single-parameter dis-
tributions satisfying H = O(n). Our bounds apply to the PAC (probably approximately correct) setting
(generalizing the algorithm to the fixed budget case of [7, 8] is a challenging open problem). While com-
pleting this paper2, we became aware of independent work on the best arm problem to appear at ICML 2013
[9]. The algorithm and theoretical analysis in that paper are essentially the same as ours, but in fact the
upper bound on sample complexity bound given in [9] is slightly tighter.
Sec. 4 discusses the limitations of non-adaptive sampling strategies and shows that any non-adaptive
sampling strategy may require drastically more samples than our adaptive algorithm. Using new lower
bounds for non-adaptive sampling procedures, we show that there exist problems in which the difference
between the sample complexities of non-adaptive and adaptive procedures grows polynomially with the
number of arms. This is somewhat surprising, as the as the advantage of adaptivity in the sparse setting
(where the gaps are bound by a fixed constant) is known to be a factor of log n at best. To be more concrete,
consider the following. We demonstrate problem instances where adaptive procedures, for example, suc-
ceed with just O(n) samples, but all non-adaptive procedures fail without at least O(n1.98) samples. This
observation is crucial since it shows that adaptive designs can be vastly superior to simpler non-adaptive
methods often used in practice (e.g., biological applications mentioned above). The take-away message is
that the added implementational burden of adaptive sampling methods may be well worth the investment.
Notation, in general, follows convention. Since the order and means are unknown, we denote the index
of the best arm as iâˆ— throughout the paper. An estimate of the best arm is denoted as b
i. Proofs of all
Theorems are found in the Appendix.
2 A Single-Parameter Family of Mean Distributions
A lower bound on the sample complexity of finding the best arm follows in a straightforward way from [4,
Theorem 5]; see Theorem 3 in Sec. 4 for the derivation. To find the best arm with probability at least 1 âˆ’ Î´
requires at least
c1H log(1/Î´)
samples, where c1 > 0 is a universal constant. The quantity H, refereed to as the hardness of the problem,
is given by
H =
n
X
i=1
âˆ†âˆ’2
i . (1)
where
âˆ†i = Âµ0 âˆ’ Âµi (2)
is the gap between the best arm and the ith arm.
2
The main results of this paper were presented in a lecture (but not as a publication) at the Information Theory and Applications
(ITA) Workshop in San Diego in February 2013.
3
In this paper we focus on a specific parametric family that spans the hardness of the best arm problem
with a single parameter. Consider a model in which the means are given by
Âµi = Âµ0 âˆ’ (i/n)Î±
(3)
for i = 1, . . . , n and some Î± â‰¥ 0. We refer to this model as an Î±-parameterization. Under this model,
âˆ†i = (i/n)Î±. The Î±-parameterization spans the range from â€œhardâ€ problems in which the gaps âˆ†i are
shrinking quickly as n grows, to â€œeasyâ€ or sparse cases in which the gaps are greater than a constant (when
Î± = 0). Theorem 3 in Sec. 4 yields the following lower bounds on the sample complexity of identifying the
best arm (ignoring constant and log(1/Î´) factors):
sample complexity â‰¥
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
n if Î± < 1/2
n log n if Î± = 1/2
n2Î± if Î± > 1/2.
(4)
The focus of this paper is on finding sufficient conditions under which the best arm can be found with
O(n) samples. If the gaps follow the Î±-parameterization with Î± â‰¥ 1/2, (4) implies that the best arm cannot
be found with O(n) samples. Conversely, if the gaps satisfy âˆ†i â‰¥ C( i
n)Î±, for Î± < 1/2, the lower bound
in (4) does not preclude the possibility that order n samples are sufficient to find the best arm. In the next
section, we show that when Î± < 1/2, order n samples are indeed sufficient.
3 PRISM Algorithm for Best Arm Identification
To show that a linear number of pulls is sufficient for a number of problem instances, we propose and analyze
the algorithm for best arm identification outlined in Fig. 2. The algorithm follow a multi-phase approach,
with a specific allocation of confidence and sampling budgets across phases. The algorithm relies on the
output of Median Elimination [10] to establish a threshold on each phase. We mention again independent
work to appear at ICML 2013 [9], which proposes and analyzes essentially the same algorithm.
Input Î´. Let A1 = {0, 1, . . . , n}, nâ„“ = â„“2â„“, and Îµâ„“ =
q
log(1/Î´)
2â„“ .
For each phase â„“ = 1, 2, . . . ,
(1) Let iâ„“ be the output of Median Elimination [10] run on Aâ„“ with accuracy Îµâ„“, Î´â„“

.
(2) For each arm i âˆˆ Aâ„“, sample nâ„“ times arm i and let b
Âµi(â„“) be the corresponding average.
(3) Let
Aâ„“+1 = {i âˆˆ Aâ„“ : b
Âµi(â„“) â‰¥ b
Âµiâ„“
âˆ’ 2Îµâ„“} .
Stop when Aâ„“ contains a unique elementb
i and outputb
i.
Figure 2: PRISM algorithm for the best arm identification problem.
The following theorem is our main result. The sample complexity of identifying the best arm with
probability at least 1 âˆ’ Î´ is bounded in terms of H and a novel measure of complexity denoted by G :=
4
Pn
i=1 âˆ†âˆ’2
i log2(âˆ†âˆ’2
i ). In general H â‰¤ G â‰¤ H log(H) but in many cases, G = H and the bound implies
that the best arm can be found using O(H) samples with a fixed probability of error. This is the best known
bound for the best arm problem. The proof is left to the appendix.
Theorem 1. Let Î´ âˆˆ (0, 1). Let H =
Pn
i=1 âˆ†âˆ’2
i where âˆ†1 is the minimum gap. Then with probability at
least 1 âˆ’ 3Î´2
1âˆ’Î´2 âˆ’ Î´
1âˆ’Î´ âˆ’ 4Î´2
(1âˆ’Î´2)2 , the PRISM algorithm of Fig. 2 stops after at most
O log(1/Î´)
"
H log(log(1/Î´)) +
n
X
i=1
âˆ†âˆ’2
i log2(âˆ†âˆ’2
i )
#!
samples and outputs armb
i = iâˆ—.
Corollary 1. Consider the problem instance Âµ0 = 1 and Âµi = 1 âˆ’ (i/n)Î±, for i = 1, . . . , n and some
0 < Î±. Then for a fixed probability error, using the PRISM algorithm of Fig. 2 we have that
number of total samples =
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
O(n) if Î± < 1/2
O(n log2
n) if Î± = 1/2
O(n2Î± log(n)) if Î± > 1/2.
(5)
and the algorithm outputs armb
i = iâˆ—.
Proof. For âˆ†i = i
n
Î±
we have the relevant quantities given in the following table (ignoring lower order
terms):
H
Pn
i=1 âˆ†âˆ’2
i log2(âˆ†âˆ’2
i )
Î± > 1/2 2Î±
2Î±âˆ’1 n2Î± (2Î±)2
2Î±âˆ’1 n2Î± log(n)
Î± = 1/2 n log(n) 2Î±n log2
(n)
Î± < 1/2 2Î±
1âˆ’2Î± n 2Î±
(1âˆ’2Î±)2 n
The result follows from plugging the above quantities into Theorem 1.
Conservative PRISM. Consider the algorithm of Fig. 2, but in the first line set nâ„“ = 2â„“ and Îµâ„“ =
p
log(â„“2/Î´)/2â„“ and for item (2), run Median Elimination with input (Îµâ„“, Î´/â„“2).
Theorem 2. Let Î´ âˆˆ (0, 0.6]. With probability at least 1 âˆ’ 2Î´ âˆ’ 6Î´2 âˆ’ 6Î´4, Conservative PRISM stops after
at most O

H log

log(H)
Î´

pulls and outputsb
i = iâˆ—.
Theorem 1 matches the lower bound when Î± < 1/2 and comes within a log(n) factor for Î± â‰¥ 1/2. On
the other hand, Theorem 2 comes within a factor of log log n of the lower bound for all Î± > 0. We note that
the upper bound of [9] also comes no closer than log log n of the lower bound for Î± â‰¥ 1/2.
4 Lower Bounds on the Sample Complexity of Non-Adaptive Algorithms
Here we examine the limitations of non-adaptive sampling strategies (which sample all arms an equal num-
ber of times), since these simpler procedures are not uncommon in applications like the biological problems
that partially motivate this paper. A non-adaptive procedure is any procedure that samples each arm m
5
times, where m is fixed a-priori, and outputs a single arm as an estimate of the best arm. We show that all
non-adaptive methods may require drastically more samples than PRISM. The take-away message is that
the small added difficulty (for the practitioner) in applying PRISM may be well worth the investment.
We begin by formally stating the adaptive lower bound developed in [4].
Theorem 3. Adaptive Lower Bound [4, Theorem 5]. For every set of means, {Âµi}n
i=0, Âµi âˆˆ (3/8, 1/2],
there exists a joint distribution on the arms such that the arms take mean values {Âµ0, . . . , Âµn}, each arm is
sub-Gaussian, and any adaptive procedure with fewer than
c1H log
1
8Î´
(6)
samples in expectation has P(b
i 6= iâˆ—) â‰¥ Î´ for any Î´ âˆˆ (0, eâˆ’8/8) and some constant c1.
Note that the restriction on the means to (3/8, 1/2] in Theorem 3 can be relaxed (see [4] for details). We
proceed with the non-adaptive lower bounds which allow us to compare the best non-adaptive procedures
against adaptive procedures.
Theorem 4. Non-Adaptive Lower Bound. Consider any Î´ âˆˆ (0, eâˆ’3/24). For every set of means
{Âµ1, . . . , Âµn}, there exists a joint distribution on the arms such that the arms take mean values {Âµ1, . . . , Âµn},
each arm is sub-Guassian, and any non-adaptive procedure with fewer than
H log
 n
25Î´

samples in expectation has P(b
i 6= iâˆ—) â‰¥ Î´. Moreover, for any value of H there exists a joint sub-Gaussian
distribution over arms with means {Âµ0, . . . , Âµn} satisfying
Pn
i=1(Âµ0 âˆ’ Âµi)âˆ’2 = H, such that any non-
adaptive procedure with fewer than
Hn
2
log

1
24Î´

samples in expectation has P(iÌ‚ 6= iâˆ—) â‰¥ Î´.
The lower bound of Theorem 4 consists of two statements, the first which implies that for any set
of means, the sample complexity must be at least order H log n. The second statement, on the other hand,
implies the existence of particular problem instances that are especially difficult for non-adaptive procedures,
requiring order Hn samples. Inspecting the proofs of the two parts of Theorem 4 one sees that the minimum
gap âˆ†1, not H, is governing the query complexity for non-adaptive procedures. Using this fact we have the
following Theorem which is proved in the appendix.
Theorem 5. Non-adaptive Lower Bound, Î±-parameterization. Consider arms with mean values accord-
ing to the parameterization of (3) for some Î± â‰¥ 0. There exists a joint sub-Gaussian distribution on the
arms such that any non-adaptive procedure with fewer than
(
n log n
25Î´

if Î± = 0
n2Î±+1 log 1
24Î´

if Î± > 0
samples to find the best arm with a fixed probability of failure.
6
To see that Theorem 5 is indeed tight, it is straightforward to show that the non-adaptive procedure
which chooses the arm with the largest empirical mean after sampling each arm the same number of times
does indeed meet the lower bound. Letting m be the number of times each arm is sampled. Then
P

b
i 6= iâˆ—

â‰¤
X
i6=iâˆ—
P (b
Âµiâˆ— â‰¤ b
Âµi) â‰¤
X
i6=iâˆ—
(P (b
Âµiâˆ— â‰¤ Âµiâˆ— âˆ’ âˆ†i/2) + P (b
Âµi â‰¥ Âµi + âˆ†i/2))
â‰¤
X
i6=iâˆ—
2 exp(âˆ’mâˆ†2
i ) =
X
i6=iâˆ—
2 exp âˆ’m

i
n
2Î±
!
which follow from a union bound and Hoeffdingâ€™s inequality. For Î± 6= 0, if m â‰¥ n2Î± (which implies the
total number of samples is greater than n2Î±+1) the above sum is convergent, and the probability that the
wrong arm is returned is controlled. The case where Î± = 0 is also controlled if m â‰¥ log n.
We conclude that for Î± âˆˆ (0, 1/2), when compared to adaptive procedures that require just O(n) sam-
ples, any non-adaptive procedure requires a factor of n2Î± more samples to identify the best arm. The
implications of this observation can be somewhat surprising: for many problem instances, the improvement
in the sample complexity resulting from adaptivity is polynomial in n, compared with the typical log(n)
improvement observed for sparse problems (Î± = 0).
References
[1] Simon Haykin. Cognitive radio: brain-empowered wireless communications. Selected Areas in Com-
munications, IEEE Journal on, 23(2):201â€“220, 2005.
[2] David LoÌpez-PeÌrez, Alvaro Valcarce, Guillaume De La Roche, and Jie Zhang. OFDMA femtocells: A
roadmap on interference avoidance. Communications Magazine, IEEE, 47(9):41â€“48, 2009.
[3] L. Hao, A. Sakurai, T. Watanabe, E. Sorensen, C. Nidom, M. Newton, P. Ahlquist, and Y. Kawaoka.
Drosophila RNAi screen identifies host genes important for influenza virus replication. Nature, page
8903, 2008.
[4] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
problem. The Journal of Machine Learning Research, 5:623â€“648, 2004.
[5] Matthew L Malloy and Robert Nowak. Sequential testing for sparse recovery. arXiv preprint
arXiv:1212.1801, 2012.
[6] Matt Malloy and Robert Nowak. On the limits of sequential testing in high dimensions. In Signals,
Systems and Computers (ASILOMAR), 2011 Conference Record of the Forty Fifth Asilomar Conference
on, pages 1245â€“1249. IEEE, 2011.
[7] J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In Proceed-
ings of the 23rd Annual Conference on Learning Theory (COLT), 2010.
[8] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings
of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.
[9] Z. Karnin, T. Koren, and O. Somekh. Almost optimal exploration in multi-armed bandits. Proceedings
of the 30th International Conference on Machine Learning, June 2013.
7
[10] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research,
7:1079â€“1105, 2006.
[11] G. Abreu. Very simple tight bounds on the q-function. Communications, IEEE Transactions on,
60(9):2415â€“2420, 2012.
[12] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer Science and Business Media,
2006.
A Appendix
A.1 Proof of Theorem 1
Proof. It will be useful to consider the following â€˜slicingâ€™ of arms:
â„¦s = {i âˆˆ [n] : 5
âˆš
2 Îµs+1 < âˆ†i â‰¤ 5
âˆš
2 Îµs}, s â‰¥ 1.
Note that
25 log (1/Î´)
X
iâˆˆâ„¦s
âˆ†âˆ’2
i â‰¤ 2s
|â„¦s| â‰¤ 50 log (1/Î´)
X
iâˆˆâ„¦s
âˆ†âˆ’2
i . (7)
Step 1: A good event. In this step we describe the event of probability 1 âˆ’ Î´ on which we will prove the
result. We want the following to hold:
b
Âµiâˆ— (â„“) âˆ’ Âµâˆ—
â‰¥ âˆ’Îµâ„“ , âˆ€â„“ â‰¥ 1 , (8)
|b
Âµiâ„“
(â„“) âˆ’ Âµiâ„“
| â‰¤ Îµâ„“ , âˆ€â„“ â‰¥ 1 , (9)
max
jâˆˆAâ„“
Âµj âˆ’ Âµiâ„“
â‰¤ Îµâ„“ , âˆ€â„“ â‰¥ 1 , (10)

i âˆˆ Aâ„“ âˆ© â„¦s : b
Âµi(â„“) â‰¥ b
Âµiâ„“
âˆ’ 2Îµâ„“ â‰¤
|Aâ„“ âˆ© â„¦s|
4
, âˆ€â„“ â‰¥ s â‰¥ 1. (11)
We first bound the probability that the above events do not hold. By Hoeffdingâ€™s inequality we have
P (ÂµÌ‚i(â„“) âˆ’ Âµi â‰¥ Îµâ„“) â‰¤ exp(âˆ’2nâ„“Îµ2
â„“ ) = Î´2â„“
for any i and note that an analogous inequality holds for the deviation away from its mean in the other
direction. Thus, applying Hoeffdingâ€™s and a union bound we have that the probability that 8) is not satisfied
is less than Î´2
1âˆ’Î´2 . The probability that (9) is not satisfied is bound in the exact same manner with an additional
factor of two to satisfy both inequality directions. The only subtlety is that after the union bound one needs
to condition on the value of iâ„“ before using Hoeffdingâ€™s inequality, and this is possible since the random
variables obtain in Step (2) of the algorithm are independent of iâ„“. By the properties of median elimination
and a union bound we have that the probability that (10) is not satisfied is less than Î´
1âˆ’Î´ . Observe that by (8)
and (9) one always has:
b
Âµiâˆ— (â„“) â‰¥ Âµâˆ—
âˆ’ Îµâ„“ â‰¥ Âµiâ„“
âˆ’ Îµâ„“ â‰¥ b
Âµiâ„“
(â„“) âˆ’ 2Îµâ„“.
which implies that the best arm is never removed from the set.
8
It remains to bound the probability that (11) is not satisfied while (8), (9) and (10) are satisfied.
P


i âˆˆ Aâ„“ âˆ© â„¦s : b
Âµi(â„“) â‰¥ b
Âµiâ„“
âˆ’ 2Îµâ„“ >
|Aâ„“ âˆ© â„¦s|
4
|(8), (9), (10), Aâ„“, iâ„“

â‰¤ P


i âˆˆ Aâ„“ âˆ© â„¦s : b
Âµi(â„“) â‰¥ Âµâˆ—
âˆ’ 4Îµâ„“ >
|Aâ„“ âˆ© â„¦s|
4
|Aâ„“

â‰¤
4
|Aâ„“ âˆ© â„¦s|
E


i âˆˆ Aâ„“ âˆ© â„¦s : b
Âµi(â„“) â‰¥ Âµâˆ—
âˆ’ 4Îµâ„“ | Aâ„“

=
4
|Aâ„“ âˆ© â„¦s|
X
iâˆˆAâ„“âˆ©â„¦s
P (b
Âµi(â„“) â‰¥ Âµâˆ—
âˆ’ 4 Îµâ„“ | Aâ„“)
â‰¤ 4 exp(âˆ’2nâ„“Îµ2
â„“ ) = 4Î´2â„“
.
where the last inequality follows since all elements of â„¦s have gaps greater than 5
âˆš
2Îµs+1 = 5Îµs. Summing
over all s â‰¤ â„“ and then over â„“ â‰¥ 1 gives the probability that (11) is not satisfied:
Pâˆ
â„“=1
P
sâ‰¤â„“ 4Î´2â„“ = 4Î´2
(1âˆ’Î´2)2 .
In the next steps we will assume that (8)-(9)-(10)-(11) are satisfied as they all hold with probability at least
1 âˆ’ 3Î´2
1âˆ’Î´2 âˆ’ Î´
1âˆ’Î´ âˆ’ 4Î´2
(1âˆ’Î´2)2 .
Step 2: Bound on the total number of phases. It suffices to bound the number of phases given that (8), (9),
(10) and (11) hold. Let L denote the first phase such that |Aâ„“| = 1 (if there is no such phase then L = +âˆ).
Observe that using (11) one can show by induction that
|Aâ„“| â‰¤ 1 +
â„“
X
s=1
|â„¦s|
4â„“âˆ’s
+
+âˆ
X
s=â„“+1
|â„¦s|. (12)
Define sâˆ— = log2(âˆ†âˆ’2
1 log(1/Î´)) so that â„¦s = âˆ… for all s > sâˆ—. By definition, when â„“ â‰¥ sâˆ— the third term in
the equation immediately above is equal to zero so that
|Aâ„“| â‰¤ 1 + 2âˆ’â„“
sâˆ—
X
s=1
2sâˆ’sâˆ—
2s
|â„¦s| âˆ€â„“ â‰¥ sâˆ—
where we have
sâˆ—
X
s=1
2sâˆ’sâˆ—
2s
|â„¦s| =
âˆ†2
1
log(1/Î´)
âˆ
X
s=1
22s
|â„¦s|
=
âˆ†2
1
log(1/Î´)
âˆ
X
s=1
22s
n
X
i=1
1
(
5
âˆš
2
r
log(1/Î´)
2s+1
< âˆ†i â‰¤ 5
âˆš
2
r
log(1/Î´)
2s
)
â‰¤
âˆ†2
1
log(1/Î´)
n
X
i=1
502 log(1/Î´)2
âˆ†2
i
= 502
log(1/Î´)Hâˆ†2
1.
We conclude that for
L := 1 + max

sâˆ—
, log2(502
log(1/Î´)Hâˆ†2
1) = log2(2 log(1/Î´)) + max

log2(âˆ†âˆ’2
1 ), log2(502
Hâˆ†2
1)
we have that |Aâ„“| < 2 whenever â„“ â‰¥ L. Hence, L is an upper bound on the stopping time.
Step 3: Bound on the total number of pulls. Recall that Median Elimination applied to a set Aâ„“ with
parameters Îµâ„“, Î´â„“ takes no more than cME
Îµ2
â„“
|Aâ„“|â„“ log(1/Î´) = cMEâ„“2â„“|Aâ„“| pulls. Thus, the total number of pulls
9
on phase â„“ is bounded by câ„“2â„“|Aâ„“| pulls with c = cME + 1. Using the results from the previous step (in
particular (12) and the stopping time L) one has that the total number of pulls is bounded from above by
L
X
â„“=1
câ„“2â„“
|Aâ„“| â‰¤
L
X
â„“=1
câ„“2â„“
1 +
â„“
X
s=1
|â„¦s|
4â„“âˆ’s
+
âˆ
X
s=â„“+1
|â„¦s|
!
â‰¤ cL2L+1
+ c
+âˆ
X
â„“=1
â„“
+âˆ
X
s=1

2s
2â„“
1sâ‰¤â„“ +
2â„“
2s
1s>â„“

2s
|â„¦s|
â‰¤ cL2L+1
+ 3c
+âˆ
X
s=1
s2s
|â„¦s| + 2c(n âˆ’ 1)
= cL2L+1
+ 150c log(1/Î´) [log2(50 log(1/Î´))H + G] + 2c(n âˆ’ 1)
where
H â‰¤ G :=
n
X
i=1
âˆ†âˆ’2
i log2 âˆ†âˆ’2
i

â‰¤ H log(H)
which follows directly from
âˆ
X
s=1
s2s
|â„¦s| =
âˆ
X
s=1
s2s
n
X
i=1
1
(
5
âˆš
2
r
log(1/Î´)
2s+1
< âˆ†i â‰¤ 5
âˆš
2
r
log(1/Î´)
s2s
)
â‰¤ 50 log(1/Î´)
n
X
i=1
1
âˆ†2
i
log2

50 log(1/Î´)
âˆ†2
i

.
Evaluating L2L+1 and collecting terms obtains the result.
A.2 Proof of Theorem 3
Proof. Assume some procedure has P(b
i 6= iâˆ—) â‰¤ Î´ and requires fewer than c1H log 1
8Î´ samples for some
{Âµi}n
i=0, Âµi âˆˆ (3/8, 1/2]. This procedure is by definition (Îµ, Î´) PAC (probably approximately correct) for
any Îµ âˆˆ (0, âˆ†1). [4, Theorem 5] implies any (Îµ, Î´), procedure, Îµ âˆˆ (0, âˆ†1), requires more than
c1
X
iâˆˆN
1
Âµiâˆ— âˆ’ Âµi
log
1
8Î´
samples in expectation, where
N =
(
i : Âµi â‰¤ Âµiâˆ— âˆ’ Îµ, Âµi â‰¥
Îµ + Âµiâˆ—
1 +
p
1/2
)
Since Âµi âˆˆ (3/8, 1/2], N := [n]. Any procedure requires more than
ciH log
1
8Î´
samples in expectation. This negates the original assumption.
10
A.3 Proof of Theorem 4
Proof. We restrict our attention to reward distributions of the form N(Âµi, 1). Assume that Âµi, i = 0, . . . , n,
are know up to a permutation, and let each arm be assigned a mean uniformly at random. We first show
that the test with minimum average probability of error simply picks the largest empirical mean among all
arms, i.e., b
i = arg maxi b
Âµi, where b
Âµi = 1/m
Pm
j=1 Xi,j, and Xi,j represents the reward of arm i on the
jth play of that arm, and m is the total number of samples of each arm. This can be seen by considering
the maximum a-posteriori (MAP) estimator of the best arm, which by definition has the smallest probably
of error. Under the assumption that the arms are assigned means uniformly at random, the MAP estimator
reduces to the maximum likelihood (ML) estimator:
b
iMAP = b
iML = arg max
i
P(Xm
0 , . . . , Xm
n |Hi),
where Xm
i = Xi,1, . . . , Xi,m and Hi is event that arm i is the best arm. Consider comparing between events
Hi and Hiâ€² , i 6= iâ€²: the ML test is P(Xm
0 , . . . , Xm
n |Hi) â‰¶iâ€²
i P(Xm
0 , . . . , Xm
n |Hiâ€² ). By the independence
across arms, it is straightforward to show this test reduces to:
P(Xm
i |Hi)P(Xm
iâ€² |Hi) â‰¶iâ€²
i P(Xm
i |Hiâ€² )P(Xm
iâ€² |Hiâ€² ) (13)
The distribution of Xm
i , given Hi, is simply
P(Xm
i |Hi) =
1
âˆš
2Ï€
exp

âˆ’ ||Xm
i âˆ’ Âµiâˆ— 1||2
/2

(14)
where Xm
i = [Xi,1, . . . , Xi,m]T âˆˆ Rm. The marginal distribution on arm iâ€², given that arm i is the largest,
follows a mixture distribution:
P(Xm
iâ€² |Hi) =
1
n
âˆš
2Ï€
n
X
j=1
exp

âˆ’ ||Xm
iâ€² âˆ’ Âµj1||2
/2

. (15)
Combining (13), (14), and (15), after a number of straightforward manipulations, excluded for brevity, it can
be shown that the ML estimate prefers arm i to iâ€² if and only if
Pm
â„“=1 Xi,â„“ >
Pm
â„“=1 Xiâ€²,â„“, or equivalently,
b
Âµi > b
Âµiâ€² . The estimate with minimum probability of error is simplyb
i = arg maxi b
Âµi.
We continue by bounding the probability of error of the maximum likelihood test. For any estimator,
P(b
i 6= iâˆ—
) â‰¥ P
ï£«
ï£­
[
i6=iâˆ—
b
Âµiâˆ— âˆ’ b
Âµi â‰¤ 0
ï£¶
ï£¸
= P(b
Âµiâˆ— â‰¥ Âµiâˆ— ) P
ï£«
ï£­
[
i6=iâˆ—
b
Âµiâˆ— âˆ’ b
Âµi â‰¤ 0 b
Âµiâˆ— â‰¥ Âµiâˆ—
ï£¶
ï£¸
+ P(b
Âµiâˆ— < Âµiâˆ— ) P
ï£«
ï£­
[
i6=iâˆ—
b
Âµiâˆ— âˆ’ b
Âµi â‰¤ 0 b
Âµiâˆ— < Âµiâˆ—
ï£¶
ï£¸
â‰¥
1
2
P
ï£«
ï£­
[
i6=iâˆ—
b
Âµi â‰¥ Âµiâˆ—
ï£¶
ï£¸ =
1
2
ï£«
ï£­1 âˆ’ P
ï£«
ï£­
\
i6=iâˆ—
b
Âµi â‰¤ Âµiâˆ—
ï£¶
ï£¸
ï£¶
ï£¸
=
1
2
ï£«
ï£­1 âˆ’
Y
i6=iâˆ—
FN
q
mâˆ†2
i

ï£¶
ï£¸ â‰¥
1
2
ï£«
ï£­1 âˆ’
Y
i6=iâˆ—

1 âˆ’
1
12
exp âˆ’mâˆ†2
i


ï£¶
ï£¸ (16)
â‰¥ min
âˆ†1,...,âˆ†n:
P
i
1
âˆ†2
i
=H
1
2
ï£«
ï£­1 âˆ’
Y
i6=iâˆ—

1 âˆ’
1
12
exp âˆ’mâˆ†2
i


ï£¶
ï£¸ (17)
11
where FN (x) is the standard Gaussian CDF. The inequality in (16) follows since FN (x) â‰¤ 1âˆ’exp(âˆ’x2)/12
for x â‰¥ 0 [11, Eqn. 13]. The next step in the proof will be showing that error probabilities smaller than a
fixed constant, (17) is minimized when the gaps are equal, i.e., when âˆ†1 = âˆ†2 = ... =
p
n/H. First define
z âˆˆ Rn
+ with elements zi := 1/(mâˆ†2
i ). We can recover the minimum of (17) by solving
argmax
zâˆˆRn
+:1T z=H/m
n
X
i=1
log

1 âˆ’
1
12
exp (âˆ’1/zi)

. (18)
Define the Lagrangian of (18) as
L(z, Î») = âˆ’
n
X
i=1
log

1 âˆ’
1
12
exp zâˆ’1
i


âˆ’ Î»(1T
z âˆ’ H/m).
From [12, p. 321], any z that maximizes (18) necessarily satisfies
âˆ‚L
âˆ‚zi
=
zâˆ’2
i
12 exp(zâˆ’1
i ) âˆ’ 1
âˆ’ Î»zi = 0 âˆ€ i (19)
1T
z = H.
The above system of equations is satisfied by pairs (z, Î») that satisfy

zi : Î» =
zâˆ’3
i
12 exp(zâˆ’1
i ) âˆ’ 1

âˆ€ i (20)
and 1T z = H simultaneously. Differentiation of (20) shows the function Î»(zi) is monotonically increasing
in zi for zi â‰¤ 1/3. First, consider a solution to (19) which has one or more zi â‰¥ 1/3. This would imply
mâˆ†2
i â‰¤ 3 for some i, and from (17), P(b
i 6= iâˆ—) â‰¥ 1
24 exp(âˆ’3). For any Î», (20) is satisfied by at most
one zi âˆˆ (0, 1/3] by the monotonicity of the function on this range; this implies implies either 1) the z that
maximizes (18) has the form z1 = z2 = Â· Â· Â· = zn, or 2) P(b
i 6= iâˆ—) â‰¥ 1
24 exp(âˆ’3). We focus our attention
on the case when z1 = Â· Â· Â· = zn (and thus âˆ†1 = Â· Â· Â· = âˆ†n). Since
P
i 1/âˆ†2
i = H, âˆ†i =
p
n/H for all i.
(17) gives
P(b
i 6= iâˆ—
) â‰¥
1
2

1 âˆ’

1 âˆ’
1
12
exp

âˆ’
mn
H
n
.
Recall the total number of samples is given by nm. If mn â‰¤ H(log n + log (25Î´)âˆ’1)

, then for Î´ âˆˆ
(0, eâˆ’3/24)
P(b
i 6= iâˆ—
) â‰¥
1
2

1 âˆ’

1 âˆ’
25Î´
12n
n
(21)
â‰¥
1
2

1 âˆ’ exp

âˆ’
25Î´
12

for all n â‰¥ 1 (22)
â‰¥ Î´ (23)
which completes the proof of the first statement of the theorem.
To prove the second statement of the theorem, consider the following set of gaps â€“
âˆ†i =
ï£±
ï£²
ï£³
q
2
H i = 1
q
2(nâˆ’1)
H i > 1.
12
Note that {âˆ†1, ..., âˆ†n} satisfy
Pn
i=1 1/âˆ†2
i = H. From (16), and by considering only the arm with the
smallest gap,
P(b
i 6= i) â‰¥
1
2
ï£«
ï£­1 âˆ’
Y
i6=iâˆ—

1 âˆ’
1
12
exp âˆ’mâˆ†2
i


ï£¶
ï£¸ â‰¥
1
24
exp

âˆ’
2m
H

. (24)
If m â‰¤ H
2 log 1
24Î´

, we have P

b
i 6= iâˆ—

â‰¥ Î´. This implies that if the total number of measurements is less
than Hn
2 log 1
24Î´

, then P(b
i 6= iâˆ—) â‰¥ Î´, completing the proof of the second statement of Thm. 4.
A.4 Proof of Corollary 2
Proof. When Î± = 0, Theorem 4 implies the result. When Î± > 0, we can bound (16) by dropping all terms
in the product except the term corresponding to the smallest gap. This gives
P(b
i 6= i) â‰¥
1
24
exp âˆ’mâˆ†2
1

=
1
24
exp âˆ’mnâˆ’2Î±

(25)
Setting m â‰¤ n2Î± log 1
24Î´

implies the result.
13
