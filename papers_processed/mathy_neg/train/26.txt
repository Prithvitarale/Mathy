DoNeuralLanguageRepresentationsLearnPhysicalCommonsense?MaxwellForbes†,AriHoltzman†‡,andYejinChoi†‡{mbforbes,ahai,yejin}@cs.washington.edu†PaulG.AllenSchoolofComputerScienceandEngineering,UniversityofWashington‡AllenInstituteforArtificialIntelligenceAbstractHumansunderstandlanguagebasedonrichbackgroundknowledgeabouthowthephysicalworldworks,whichinturnallowsustoreasonaboutthephysicalworldthroughlanguage.Inadditiontothepropertiesofobjects(e.g.,boatsrequirefuel)andtheiraffordances,i.e.,theactionsthatarecentcontextualizedrepresentationslikeELMo(Petersetal.,2018)andBERT(Devlin,Chang,Lee,&Toutanova,2018).Suchmodelsaretrainedwithoutsupervisionbyexposingthemtobillionsofwords,andallowingthemtoextractpat-ternspurelyfromtokenpredictiontasksthatcanbederiveddirectlyfromrawtext.Theselanguagerepresentationmodelshaveestablishedunprecedentedperformanceonawiderangeofevaluations,includingnaturallanguageinferenceandcom-monsensereasoning.Howmuchdotheselarge,unsupervisedmodelsoflan-StatisticsTotalStatisticsAbstractObjects514411train/103testProperties50obj/prop:60median(3min,302max)prop/obj:8median(1min,23max)Annotations77,1003anns/datumSituatedObjects1,02480unique,split:64train/16testProperties50Affordances30723affordances/object(bydesign)Annotations156,6723anns/datumExamplesObjectsPropertiesAffordancesharmonica,vanexpensive,squishypickup,removepotato,shovelusedasatoolforcookingpet,talktocat,beddecorative,AbstractSituatedO←→PO←→PO←→AA←→PobjpropµF1sigobjpropµF1sigobjaffµF1sigaffpropµF1sigRANDOM0.250.260.26***0.240.250.22***0.530.620.51***0.240.260.23***MAJORITY0.340.110.31***0.160.050.17***0.820.680.82***0.180.050.17***GLOVE0.630.470.63***0.550.390.570.850.730.860.270.130.29DEP-EMBS0.620.420.60***0.540.360.54***0.840.670.84*0.260.120.28ELMO0.670.550.67**0.580.440.58***0.840.710.85**0.310.170.34BERT0.740.670.74←0.640.590.67←0.870.770.88←0.360.250.37←HUMAN0.780.800.670.700.690.610.830.930.800.650.670.40Table2:MacroF1scorespercategory(object,property,affordance)andmicroF1score(µF1)onboththeabstractandsituatedtestsets.Highestmodelvaluesarebolded.Statisticalsignificance(sig)iscalculatedwithMcNemar’stest,comparingthebest-scoringmodel(byµF1,denoted←)witheachothermodel.Stratifiedp-valuesareshown,with*forp<0.05,**forp<0.01,and***forp<0.001.Humanperformanceisestimatedby50expert-annotatedrandomsamplesfromthetestset(noMcNemar’stest).aggregatetheverbschosentopickthetopthreemostcommonaffordancesforeachobject.Weendupwithasetofsparselylabeledaffordancesforeachsituatedobject.Weperformbal-ancednegativesamplingbyselectingk=3affordancesforeachdatumandsettingtheirlabelstozero.DetailedstatisticsandexamplesforbothdatasetsareshowninTable1.Fulllistsoftheobjects,properties,andaffordances,aswellastheannotationinterfaces,areprovidedintheAppendix.ModelsWordembeddingsWeconsiderfourrepresentationsofthewordsinvolvedinthetasks.Twooftherepresentationsarewordembeddings.ThesemapsinglewordstovectorsinRd.WeuseGloVeembeddings(Penningtonetal.,2014)astheyhaveproveneffectiveatobject-propertytasksinthepast(Lucy&Gauthier,2017).WealsouseDependencyBasedWordEmbeddings(Levy&Goldberg,2014),astheymaymoredirectlycapturetherelationsbetweenobjectsandtheiraffordances.Inbothcases,d=300,andweusetheGloVeembeddingvariantwiththelargestamountofpretrain-ing(840billionwords).ContextualizedrepresentationsTheothertworepresen-tationsareELMo(Petersetal.,2018)andBERT(Devlinetal.,2018),whicharecontextualized.Theserequirefullsen-tences(asopposedtosinglewords)tocomputeavector,butinturnproduceresultsmorespecifictowords’linguisticsur-roundings.Forexample,ELMoandBERTproducedifferentrepresentationsforbookin“Ireadthebook”versus“Pleasebooktheflight,”whilewordembeddingshaveonlyasinglerepresentation.Toaccountforthis,wegeneratesentencesusingtherele-vantobjects,properties,andaffordancesforthetaskathand.Forexample,tojudgeaccordionandsquishy,wewouldgen-erate“Anaccordionissquishy.”ForELMo,wethentakethefinallayerrepresentationsforthetwocomparedwords,eachofwhichisad=1024lengthvector.ForBERT,wetaketheoverallsentencerepresenta-tionasthefinallayer’shiddenstateofthe[CLS](sentencesummary)symbol,whichproducesasingled=1024vector.FinetuningGiventhewordrepresentationsabove,wefine-tuneeachofthemodelsbyaddingtrainablemultilayerper-ceptron(MLP)aftertheinputrepresentations.Thisallowsmodelstolearninterrelationsbetweenthetwocategoriesathand,essentiallycalibratingtheunsupervisedrepresentationsintoacompatibilityfunction.WeuseasinglehiddenlayerintheMLP,andtrainusingmeansquarederrorlosswithL2regularization.ForBERT,wefindthestandardprocedureoffinetuningtheentiremodelvitalforgoodperformance.Tosummarize,fortwowords(wi,wj)whichcanbewrittentogetherinasentences=w1...wn,wehaveforamodelm,r(wi,wj)=hm(wi),m(wj)iifm∈{GL.,D.E.}m−1{i,j}(s)ifm=ELMOm−1[CLS](s)ifm=BERTŷwi,wj∝σ(W2×a(W1×r(wi,wj)+b1)+b2)L(wi,wj,y,θ,λ)=(y−ŷwi,wj)2+λkθk22wherem(·)`iisanembeddingoftheithtokeninthelayer`,aisanonlinearactivationfunction,y∈{0,1}isthegroundtruthlabel,θ={W1,W2,b1,b2}aretrainableparameters,andλistheregularizationstrength.4Weoptimizemodelsusinggradientdescent(orAdam(Kingma&Ba,2014)forBERT),andPropertyandAﬀordanceF1ScoresbyClassPropertyAccuracybyCategory(a)(b)(c)(d)BERTFigure2:Detailedresultsoftopperformingmodel(BERT)ontheaffordance-propertycompatibilitytask(A←→P)inthesituateddataset.(a)F1scoresareplottedperproperty(left)andaffordance(right).(b)Propertiesaredividedintofourcategoriesandplottedbyaccuracy.(c),(d)BothpropertyandaffordanceF1plottedagainstwordfrequencyinnaturallanguagetext.BaselinesWecompareperformanceforthesemodelsagainsttwosimpleapproaches.Therandombaselinesim-plyflipsacoinforeachcompatibilitydecision.Themajoritybaselineusestheper-classmajoritylabelforthetrainingset,aggregatingbypropertyfortheO←→PandA←→Ptasks,andbyaffordancefortheO←→Atask.HumanperformanceFinally,weestimatehumanperfor-manceonthistask.Wesample50samplesatrandomfromthetestsetforeachtask,andhaveanexpertannotatethem.Forfairnesstothemodels,wedonotshowtheexpertthepho-tographsdances.Incontrast,perceptualpropertiesexhibitgenerallylowerandinconsistentperformancethanothercategories.Wesuspectthatperceptualobservationsobservedintextarenotexpressedwithaffordances,makingthisconnectiondifficultformodels.Largelyperceptualfeaturescanbewrittenaboutwithsimpleverbs(hear,see,feel),givingthemlessimplicitevidencethanmorenuancedproperties.Finally,encyclope-dicandcommonsensepropertiesfallsomewhereinthemid-dle.Theseproperties,whichinvolveanobject’sgeneralchar-acteristics(likerequiresgasoline,livesinphysicalsupport.Proceedingsofthe39thAnnualConfer-enceoftheCognitiveScienceSociety.Gibson,J.J.(1966).Thesensesconsideredasperceptualsystems.Gover,M.R.(1996).Theembodiedmind:Cognitivescienceandhumanexperience(book).Mind,Culture,andActivity,3(4),295–299.Herbelot,A.,&Vecchi,E.M.(2015).Fromconceptstomodels:someissuesinquantifyingfeaturenorms.InLilt(Vol.2).Kingma,D.P.,&Ba,J.(2014).Adam:Amethodforstochas-ticoptimization.arXivpreprintarXiv:1412.6980.Lake,B.M.,Linzen,T.,&Baroni,M.(2019).Humanfew-shotlearningofcompositionalinstructions.arXivpreprintarXiv:1901.04587.Levesque,H.J.,Davis,E.,&Morgenstern,L.(2012).Thewinogradschemachallenge.InProceedingsofthethirteenthinternationalconferenceonprin-ciplesofknowledgerepresentationandreason-ing(pp.552–561).AAAIPress.Retrievedfromhttp://dl.acm.org/citation.cfm?id=3031843.3031909Levy,O.,&Goldberg,Y.(2014).Dependency-basedwordembeddings.InProceedingsofthe52ndannualmeetingoftheassociationforcomputationallinguistics(volume2:Shortpapers)(Vol.2,pp.302–308).Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ra-manan,D.,...Zitnick,C.L.(2014).Microsoftcoco:Commonobjectsincontext.InEuropeanconferenceoncomputervision(pp.740–755).Lucy,L.,&Gauthier,J.(2017).Aredistributionalrep-resentationsreadyfortherealworld?evaluatingwordvectorsforgroundedperceptualmeaning.arXivpreprintarXiv:1705.11168.McRae,K.,Cree,G.S.,Seidenberg,M.S.,&McNorgan,C.(2005).Semanticfeatureproductionnormsforalargesetoflivingandnonlivingthings.Behaviorresearchmethods,37(4),547–559.Norman,D.(1988).Thedesignofeverydaythings:RevisedandAppendixObjectsWeprovidebelowafulllistoftheobjectsconsideredinbothofourdatasets.Wenotethesplitthatitbelongstoineachdataset(trainortest),andtheoriginoftheobject(MR=McRaeetal.(2005);C=MSCOCO(Linetal.,2014)).Ingeneral,thislististheunionofobjectsfoundinMcRae(2005)andMSCOCO.Forcaseswherewedonotusetheob-jectineitherdataset(i.e.,ObjectAbstractSituatedOriginNotecard(greeting)--MR(polysemy)cariboutrain-MRcarpettrain-MRcarrottraintrainMR,Ccarttest-MRcattraintrainMR,Ccatapulttrain-MRcaterpillartrain-MRcatfish--MR(obscure)cathedraltrain-MRcauliflowertest-MRcedar--MR→treecelerytrain-MRcellphonetraintrainCcellartrain-MRcellotrain-MRcertificatetrain-MRchaintrain-MRchairtesttestMR,Cchandeliertrain-MRchapeltrain-MRcheesetest-MRcheetahtrain-MRcherrytrain-MRchickadee--MR→birdchickentest-MRchimptrain-MRchipmunktrain-MRchiseltrain-MRchurchtrain-MRcigartrain-MRcigarettetrain-MRclamtrain-MRclamptrain-MRclarinettrain-MRcloaktrain-MRclocktraintrainMR,Cclosettrain-MRcoattrain-MRcockroachtrain-MRcoconuttrain-MRcod--MR(obscure)cointrain-MRcolandertrain-MRcombtest-MRcorktrain-MRcorkscrewtrain-MRcorntest-MRcottagetrain-MRcouchtraintrainMR,Ccougartrain-MRcowtesttrainMR,Ccoyotetrain-MRcrabtest-MRcranberrytrain-MRcrane(machine)--MR(polysemy)crayontrain-MRcrocodiletrain-MRcrossbowtrain-MRcrowtrain-MRcrowbartest-ObjectAbstractSituatedOriginNotegrapefruittrain-MRgrasshoppertrain-MRgratertrain-MRgrenadetrain-MRgroundhogtrain-MRguitartrain-MRguntrain-MRguppy--MR(obscure)hairdriertraintestChammertrain-MRhamstertrain-MRhandbagtraintrainCharetrain-MRharmonicatrain-MRharptrain-MRharpoontrain-MRharpsichordtrain-MRhatchettrain-MRhawktestObjectAbstractSituatedOriginNotepiertrain-MRpigtrain-MRpigeontrain-MRpillowtest-MRpintrain-MRpine--MR→treepineappletrain-MRpipe--MR(polysemy)pipe(smoking)--MR(polysemy)pistoltest-MRpizzatesttrainCplatetest-MRplatypustest-MRplierstrain-MRplug(electric)--MR(polysemy)plumtrain-MRponytrain-MRporcupinetrain-MRpottrain-MRpotatotrain-MRpottedplanttraintrainCprojectortrain-MRprunetrain-MRpumpkintrain-MRpyramidtest-MRpythontrain-MRrabbittrain-MRraccoontrain-MRracquettrain-MRradiotrain-MRradishtrain-MRrafttrain-MRraisintest-MRraketest-MRraspberrytrain-MRrattrain-MRrattletrain-MRrattlesnaketrain-MRraven--MR→birdrazortrain-MRrefrigeratortraintrainCremotetraintrainCrevolvertrain-MRrhubarbtrain-MRricetrain-MRrifletrain-MRring(jewelry)--MR(polysemy)robetrain-MRrobin--MR→birdrocktest-MRrockertest-MRrockettrain-MRroostertrain-MRropetest-MRrulertrain-MRsacktrain-MRsaddletrain-MRsailboattest-MRsalamandertrain-MRsalmontrain-MRsandalstrain-MRsandpapertrainObjectAbstractSituatedOriginNoteswordtrain-MRtabletrain-MRtacktest-MRtangerinetrain-MRtank(army)--MR(polysemy)tank(container)--MR(polysemy)tap--MR→faucettape(scotch)--MR(polysemy)taxitrain-MRteddybeartraintrainCtelephonetest-MRtennisrackettraintestCtenttrain-MRthermometertrain-MRthimbletrain-MRtietraintrainMR,CtigerPropertyCategorizationOriginisloudperceptualMRisman-madeencyclopedicnewisusedformusicfunctionalMRcomesinpairscommonsenseMRhasapeelencyclopedicMRissharpcommonsenseMRhasshelvescommonsenseMRisshinyperceptualMRisslimyperceptualMRissmellyperceptualMRissmoothperceptualMRproducessoundperceptualMRissquishyperceptualnewiseateninsummercommonsenseMRcanswimencyclopedicMRistallperceptualMRisatoolfunctionalMRisatoyfunctionalMRisusedfortransportationfunctionalMRisunhealthycommonsensenewisfoundonwallscommonsenseMRiswornforwarmthfunctionalMRlivesinwaterencyclopedicMRisusuallywetcommonsensenewhaswordsonitcommonsensenewAffordancesWeprovidebelowthelistofcandidateverbs,alongwiththeassistiveparticlesandprepositions,weusedforannotatingaffordances.Whenannotating“WhatmightyoudototheX?”foranobjectX,itcanbehelpfulornecessarytouseaparticleorprepositionwhenwritingananswer.Herearesomeexam-ples:Feedthedog.(verbonly)Takeoutthetrash.(verb+particle)Diveintothewater.(verb+preposition)Weusetwostrategiestoenableannotatorstowritegram-maticalconstructionsliketheabove.Forparticles,wepro-videcommonvariantsofeachverbthatuseparticles,suchasbuckleupandbuckleininadditiontobuckle.Forpreposi-tions,weallowanadditionalchoiceofaprepositionaftertheannotatorhasselectedaverb(orverb+particle)fromthelist.Wediscardbothparticlesandprepositionswhenbuildingourtaskdata.Wedothisfortworeasons.First,wewishtoeliminateerrorswhereanannotatormistakesthesubtledis-tinctionbetweenaparticleandapreposition.Second,wewanttoconstraintheinputandoutputspaceforthemodels,whichwouldotherwisemultiplicativelyscaletheverbsbythenumberofparticlesandprepositions.Topicktheverbs,wetakethesetof504verbsusedintheimSitudataset(Yatskaretal.,2016)andlemmatizethem.Forparticlesandprepositions,werunadependencyparseronatwirl,twist,type,uncork,unload,unlock,unpack,unplug,unveil,urinate,vacuum,vault,videotape,vote,wad,waddle,wag,wait,walk,wash,water,wave,wax,weed,weep,weigh,weld,wet,wheel,whip,whirl,whisk,whistle,wilt,wink,wipe,work,wrap,wring,wrinkle,write,yank,yawnParticlesandPrepositionsabout,after,against,around,as,at,before,behind,by,down,for,from,in,into,like,of,off,on,onto,out,over,through,to,towards,up,with,withoutDataCollectionWeprovidebelowthedatacollection