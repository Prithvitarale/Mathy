HumanEvaluationofSpokenvs.VisualExplanationsforOpen-DomainQAAnaValeriaGonzález∗1,GaganBansal2,AngelaFan3,4,YasharMehdad3,RobinJia3,andSrinivasanIyer31UniversityofCopenhagen,2UniversityofWashington,3FacebookAI,4LORIAana@di.ku.dkbansalg@cs.washington.edu{angelafan,mehdad,robinjia,sviyer}@fb.comAbstractWhileresearchonexplainingpredictionsofopen-domainQAsystems(ODQA)tousersisgainingmomentum,mostworkshavefailedtoevaluatetheextenttowhichexplanationsimproveusertrust.Whilefewworksevalu-ateexplanationsusinguserstudies,theyem-ploysettingsthatmaydeviatefromtheend-user’susagein-the-wild:ODQAismostubiq-uitousinvoice-assistants,yetcurrentresearchonlyevaluatesexplanationsusingavisualdis-play,andmayerroneouslyshowedthatprovidinganevidencesentenceaswellascoreferenceandentailmentinformation,improveserror-detectabilitymarginally.However,thisstudylackedastrongbaselinethatusescalibratedconfidenceandwhichonotherdomainshasbeenshownratersdeterminewhetheramodeldecisioniscor-rectorincorrect,andfindmarginalimprovementsonrateraccuracy.Unliketheseworks,wesimplifythepresentationsetupsothatwecanadaptexpla-nationsacrossdifferentmodalities.Bansaletal.(2020)observedthatforsentimentanalysisandansweringLSATquestions,state-of-theartexpla-nationmethodsarenotbetterthanrevealingmodelconfidencescoresandtheyincreasethelikelihoodofusersacceptingwrongmodelpredictions.Wecompareconfidencetovariousexplanationstrate-giesforODQA,butunlikepreviouswork,weusecalibratedmodelconfidence.Open-domainQAODQAconsistsofanswer-ingquestionsfromacorpusofunstructureddocu-ments2.Currently,ODQAmodelsconsistoftwocomponents:(1)adocumentretrieverwhichfindsthemostrelevantdocumentsfromalargecollec-tionand(2)amachinecomprehensionmodelorreadercomponent,whichselectstheanswerwithinthechosendocuments(Chenetal.,2017;Dasetal.,2018;Leeetal.,2019a;Karpukhinetal.,2020).RecentworkfocusesonidentifyinganswersinWikipedia(Karpukhinetal.,2020)aswellastheweb(Joshietal.,2017),encompassingbothshortextractiveanswers(Rajpurkaretal.,2016)andlongexplanatoryanswers(Fanetal.,2019).3Visualvs.SpokenModalitiesWhenpresentingNLexplanationstousers,wemustkeepinmindthatuserstypicallyprocessin-formationdifferentlyacrossthespokenandvisualmodalities.Inthissectionwediscussworkinlearn-ingandpsychologyresearch,whichpointtothedifferencesmotivatingourevaluation.1.Real-timeprocessing:Flowerdewetal.(1994)observethatoneofthemaindiffer-encesinhowpeopleprocessspokenversuswritteninformationislinearity.Whenlisten-ing,asopposedtoreading,informationpro-gresseswithoutyou.Readers,ontheotherhand,areabletogobackanddwellonspe-cificpointsinthetext,skipoverandjumpbackandforth(Buck,1991;Lund,1991).Al-thoughinsomescenariositispossibletogetspokeninformationrepeated,itmaynotbeaseffectiveasre-reading(seebelow).2https://trec.nist.gov/data/qamain.html2.Recallofinformation:Peopletendtore-calllessafterlisteningversusreading(Osada,2004).Lund(1991)foundthatforsomelis-teners,listeningtoinformationagainwasnotaseffectiveasre-reading.Whileadvancedlis-tenersbenefitedfromlisteningmultipletimes,thiswasacontrolledlearningscenariosim-ulatingstudentslearningclassroommaterial;wewouldexpectusersinanODQAsettingtobeslightlymorepassive.3.Effectonconcentration:Theheaviercog-nitiveloadimposedbylisteningtoinforma-tioncanmakepeopleloseconcentrationmoreeasily.ThompsonandRubin(1996)foundthatoptimallengthforlisteningmaterialswasaround30secondsto2minutes.Beyondthat,listenerswouldlosefullconcentration.Whenpeopleinteractwithvoiceassistantstheymaybeonthego,ormaybesurroundedbyad-ditionaldistractionsnotpresentinalearningenvironment.Thisinturnmaymaketheop-timallengthofmaterial(explanations,inourcase)muchshorter.Wearguethatthesedifferencesinprocessingofspokenandwritteninformationcanhavetremen-dousconsequencesintheeffectivenessofnaturallanguageexplanationsinODQA.Ourexperimentalsetupisthefirsttoaddressthesedifferences.4ExperimentalSetupWedesignouruserstudytoevaluateexplanationef-fectivenessforODQAbyvaryingtwofactors:typeofexplanationandmodalityofcommunication.Wecombinevariationsofeachfactortoobtainexplana-tionconditions(Section4.1)andobtainthemusingastate-of-the-artODQAmodel(Section4.3).WethendeploytheseconditionsasHITsonAmazonMechanicalTurk(MTurk)tovalidatefivehypoth-esis,eachstatingrelativeeffectivenessofcondi-tionsatimprovingerror-detectability(Section4.2).SinceMTurkstudiescanbepronetonoise,toen-surequality-control,wemakeandjustifyvarious•EXTRACTIVE-SENT:Extractsandcommuni-catesasentencecontainingthepredictedan-swerasevidence.•EXTRACTIVE-LONG:Extractsandcommuni-catesalonger,multi-sentenceparagraphcon-tainingtheanswerFigure2:UIforvisual(left)andspokenmodalities(right)forEXTRACTIVE-SENTexplanationtype.UserseitherreadorhearanexplanationanddecidewhethertotrustordiscardtheQAsystem’sprediction.astheentirepassage.SinceDPRdoesnotgenerateabstractiveexplanations,wesimulateABSTRAC-TIVEbymanuallycreatingasinglesentencethatcapturesthemaininformationofEXTRACTIVE-SENTandaddsadditionalrelevantinformationfromEXTRACTIVE-LONG,whilstremainingthesamelengthas(orunsuccessfully)withAIassistants(Bansaletal.,2019).Table1showsthefinalpay-offmatrixthatweused.PREDICTION/DECISIONACCEPTREJECTCORRECT+$0.15$0INCORRECT-$0.15$0Modelconfidenceimprovedaccuracyoferror-detectabilityInFigure3,CONFIDENCEachieveshigheraccuracythanBASELINE–68.1%vs.57.2%.Thisdifferencewasstatisticallysignificant(p<0.01),thusvalidatingH1.Whilepreviousguidelinesrecommenddisplayingconfidencetousers(Amershietal.,2019)andshowitsbenefitforsentimentclassificationandLSAT(Bansaletal.,2020),ourobservationsprovidethefirstempiricalevidencethatconfidenceservesasasimpleyetstrongerbaselineagainstwhichexplanationsforODQAshouldbecompared.Explainingviaanevidencesentencefurtherim-provedperformance.Themoreinterestingcom-parisonsarebetweenexplanationtypesandCON-FIDENCE.Inbothmodalities,EXTRACTIVE-SENTperformedbetterthanCONFIDENCE.Forexam-ple,inthespokenmodality,EXTRACTIVE-SENTimprovedaccuracyoverCONFIDENCEfrom68.1%to75.6%(p<0.01);thusvalidatingH2.Contrarytorecentpriorworksthatobservednobenefitfromexplainingpredictions,thisresultprovidesandcon-firmsaconcreteapplicationofexplanationswheretheyhelpusersinanend-to-endtask.Whilelongerexplanationsimprovedperfor-manceovermoreconciseexplanationsinthevisualmodality,theyworsenedperformanceinthespokenmodality.Figure3showsthat,forthevisualmodality,EXTRACTIVE-LONGoutper-formsEXTRACTIVE-SENTexplanationsinthevi-sualmodality–77.6%vs.74.7%(p<0.4).Con-versely,forspoken,EXTRACTIVE-SENTisbet-terthanEXTRACTIVE-LONG–75.6%vs.70.4%(p<0.01);thussupportingH3.Infact,thede-creasewassevereenoughthatwenolongerob-servedastatisticallysignificantdifferencebetweenlongexplanationsandsimplycommunicatingcon-fidence(p=0.9).Althoughcommunicatingthesamecontent,vi-sualEXTRACTIVE-LONGledtosignificantlybetteraccuracythantheirspokenversion—77.6%vs.70.4%(p<0.01);thusvalidatingH5.Thesere-sultsindicatelargedifferences,acrossmodalities,inuserabilitytoprocessandutilizeexplanations,andhowthesedifferencesneedtobeaccountedforwhileevaluatinganddevelopingexplanations.Despiteimprovingconciseness,abstractivesummaries(ofthelongerexplanations)didnothelpimproveperformanceinthespokenmodal-ity.Figure3showsthatwhileABSTRACTIVEper-Figure4:(Left)Explanationssignificantlyincreasedparticipantabilitytodetectcorrectanswerscomparedtosimplydisplayingconfidence.(Right)However,onlyEXTRACTIVE-SENTinthespokenmodalityandbothexplanationsinthevisualmodalitydecreasedtherateatwhichusersaremisled.formsmarginallybetterthanconfidence–71.3%vs.68.1%,thedifferenceisnotstatisticallysignificant(p=0.4)andthuswecouldnotvalidateH4.Thisresultsindicatesthatthelengthoftheexplanation(e.g.,numberoftokens)isnottheonlyfactorthataffectsuserperformance,insteadthedensityofin-formationalsoincreasescognitiveloadonusers.ThisfindingisinlinewiththeTimeBasedRe-sourceSharing(TBRS)model(Barrouilletetal.,2007),atheoryonworkingmemoryestablishingthattimeaswellasthecomplexityofwhatisbeingcommunicated,bothplayaroleincognitivede-mand.Wealsoobservethesameeffectinuserssub-jectiveratingoflengthofexplanation(Section5.2).Allexplanationssignificantlyincreasedpartici-pants’abilitytodetectcorrectanswers,butonlysomeexplanationsimprovedtheirabilitytode-tectincorrectanswers.Insteadofaggregateac-curacy,Figure4splitsandvisualizeshowoftenusersacceptcorrectandincorrectanswers.Foracceptingcorrectmodelpredictions,allvisualandspokenexplanationconditionssignficantlyhelpedcomparedtoCONFIDENCE(atleastp<0.05).Intermsofacceptingincorrectpredictions,inthespokenmodality,onlyEXTRACTIVE-SENTissig-nificantlybetter(i.e.,lower)thanCONFIDENCE—34%vs.40%(p<0.05).Whereasinthevisualmodality,bothEXTRACTIVE-LONGandEXTRACTIVE-SENTleadtoimprovementsoverCONFIDENCE—30%(p<0.01)andFigure5:Top:Usersperceivethesameexplanationtobelongerinthespokenmodality.Bottom:WhileEXTRACTIVE-SENTandABSTRACTIVEwerethesamelength,participantsratethelatteraslongermoreoftenperhapslikethelevelofdetailtoadapttothemodelcer-tainty.Morespecifically,userswouldliketohavemoredetailsoradditionalanswersonlywhenthemodelisnotconfidentintheprediction.Thisstrat-egyseemssimilartoadaptiveexplanationspro-posedby(Bansaletal.,2020).FortheEXTRACTIVE-LONGconditioninthespokenmodalitythefeedbackwasmostlyaboutthelengthoftheresponses.78%ofparticipantsmentionedthatresponsesshouldbeshorter,whichalignswiththehigherperceivedlengthoftheexpla-nationsinFigure5.Forthevisualmodality,40%ofparticipantsmentionthathighlightingsomekeyitemswouldhavemadeiteveneasierandfaster.Infact,introducinghighlightswouldimprovethevisualinterface,andwouldlikelyQuestion:Whereisthelongestboneinthebodyfound?Response:Iam17percentconfidentthattheansweris,femur.Ifoundthefollowingevidenceinawikipediapassagetitled,Femur:TheFemurorkeptasrealisticaspossible,inrealityusersmayhavetheoptiontodouble-checkthemodel’sanswerusingexternaltools,suchasWebsearch.Accom-modatingthatcase,wouldrequireencodingtheadditionalcostoftherejectaction(e.g.,duetotimespentandeffort)intothepayoff.Inaddition,un-likeaninteractionin-the-wild,questionswerenotposedbyparticipantsthemselves,whichmayleadtodifferentkindsofbiasesintheinterpretationofthequestions,asdiscussedbefore.Second,wecon-ductedstudieswithMTurkworkerswhomaynothavethesamemotivationforperformingthetaskasrealusers.Toaddressthisweincentivizedthembyrewardinghigh-performancethroughbonusesandpenaltiesspecifiedusingapayoffmatrix.Inpractice,thevaluesofthepayoffmatrixcanvarydependingonthestakeofthedomainandmayvarywithusers.Finally,weonlyregisteredhypothesisthatcomparedperformanceononemetric–accu-racyoferror-detectability.However,theremaybeothermetricsthatmaybeofinterest,e.g.,improve-mentsinspeedandusersatisfaction.7ConclusionContrarytorecentuser-studiesforothertasks(suchasclassification),ourssuggestthatforODQA,ex-planationsofmodel’spredictionssignificantlyhelpend-usersdecidewhentotrustthemodel’sanswers,overstrongbaselinessuchasdisplayingcalibratedconfidence.WeobservedthisfortwoscenarioswhereusersinteractwithODQAmodelusingspo-kenorvisualmodalities.However,thebestex-planationmaychangewiththemodality,e.g.,duetodifferencesinusers’cognitiveabilitiesacrossmodalities.Forexample,forthespokenmodality,conciseexplanationsthathighlightthesentencecontainingtheanswerworkedwell.Incontrast,forthevisualmodality,performanceimproveduponshowinglongerexplanations.Thus,developersandresearchersofexplainableODQAsystemsshouldevaluateexplanationsonthetaskandmodalitieswherethesemodelswillbeeventuallydeployed,andtunetheseexplanationswhileaccountingforuserneedsandlimitations.Despitesuccessofexplanationsonourdomain,explanationssometimesstillmisleadusersintotrustinganincorrectprediction,andsometimesasoftenasdisplayingthebaselines.Theseresultsindi-catetheneedtodevelopbetterexplanationsorothermechanismstofurtherappropriateuserrelianceonODQAagents,e.g.,byenablingabstractiveexpla-nationsthatbalanceconcisenessanddetailwhiletakingintoaccountuser’scognitivelimitations,in-teractiveexplanationsthatcanexplainmultiplean-swersourcesandcandidatesandadaptiveexplana-tionswheremodelstrategychangesbasedonitsconfidence.ReferencesSaleemaAmershi,DanWeld,MihaelaVorvoreanu,AdamFourney,BesmiraNushi,PennyCollisson,JinaSuh,ShamsiIqbal,PaulNBennett,KoriInkpen,etal.2019.Guidelinesforhuman-aiinteraction.InCHI.PepaAtanasova,JakobGrueSimonsen,ChristinaLi-oma,andIsabelleAugenstein.2020.Generat-ingfactcheckingexplanations.arXivpreprintarXiv:2004.05773.GaganBansal,BesmiraNushi,EceKamar,WalterSLasecki,DanielSWeld,andEricHorvitz.2019.Beyondaccuracy:Theroleofmentalmodelsinhuman-aiteamperformance.InProceedingsoftheAAAIConferenceonHumanComputationandCrowdsourcing,volume7,pages2–11.GaganBansal,TongshuangWu,JoyceZhu,RaymondFok,BesmiraNushi,EceKamar,MarcoTulioRibeiro,andDanielSWeld.2020.Doesthewholeexceeditsparts?theeffectofaiexplanationsoncomplementaryteamperformance.arXivpreprintarXiv:2006.14779.PierreBarrouillet,SophieBernardin,SophiePortrat,EvieVergauwe,andValérieCamos.2007.Timeandcognitiveloadinworkingmemory.JournalofExperimentalPsychology:Learning,Memory,andCognition,33(3):570.ZanaBuçinca,PhoebeLin,KrzysztofZGajos,andElenaLGlassman.2020.Proxytasksandsubjectivemeasurescanbemisleadinginevaluatingexplain-ableaisystems.InProceedingsofthe25thInter-nationalConferenceonIntelligentUserInterfaces,pages454–464.GaryBuck.1991.Thetestingoflisteningcompre-hension:anintrospectivestudy1.Languagetesting,8(1):67–91.Oana-MariaCamburu,TimRocktäschel,ThomasLukasiewicz,andPhilBlunsom.2018.e-snli:Nat-urallanguageinferencewithnaturallanguageexpla-nations.InJianboChen,LeSong,MartinJWainwright,andMichaelIJordan.2018.Learningtoexplain:Aninformation-theoreticperspectiveonmodelinterpre-tation.arXivpreprintarXiv:1802.07814.EricChu,DebRoy,andJacobAndreas.2020.ArevisualElizabethJMarshandShardaUmanath.2013.Knowl-edgeneglect:Failurestonoticecontradictionswithstoredknowledge.Processinginaccurateinforma-tion:Theoreticalandappliedperspectivesfromcog-nitivescienceandtheeducationalsciences.RThomasMcCoy,ElliePavlick,andTalLinzen.2019.Rightforthewrongreasons:Diagnosingsyntacticheuristicsinnaturallanguageinference.InACL.TimMiller.2019.Explanationinartificialintelligence:Insightsfromthesocialsciences.ArtificialIntelli-gence,267:1–38.SewonMin,JulianMichael,HannanehHajishirzi,andLukeZettlemoyer.2020.AExplanationExamplesInTable2,weshowanexampleofhowthere-sponsesandexplanationslookedforeachoftheconditions.Wealsoindicateinwhichmodalitieseachexplanationisshowninourexperiments.BTemperatureScalingTemperaturescaling(Guoetal.,2017),amulti-classextensionofPlattScaling(Plattetal.,1999),isapost-processingmethodappliedonthelogitsofaneuralnetwork,beforethesoftmaxlayer.Itconsistsoflearningascalarparametert,whichde-creasesorincreasesconfidence.tisusedtorescalethelogitvectorz,whichisinputtosoftmaxσ,sothatthepredictedprobabilitiesareobtainedbyσ(z/t),insteadofσ(z).Inourexperiments,themodelissettopickfromthetop100solutions,EXPLANATIONTYPERESPONSE+EXPLANATIONMODALITYBaselineTheansweris,two.SpokenConfidenceIam41percentconfidentthattheansweris,two.SpokenAbstractiveIam41percentconfidentthattheansweris,two.Isummarizedevidencefromawikipediapassagetitled,MarcoPolo(TVseries).Netflixcancelledtheshowaftertwoseasons,asithadresultedina200milliondollarloss.SpokenExtractive-sentIam41percentconfidentthattheansweris,two.Ifoundthefollowingevidenceinawikipediapassagetitled,MarcoPolo(TVseries).OnDecember12,2016,Netflixannouncedtheyhadcanceled"MarcoPolo"aftertwoseasons.Spoken/visual.Extractive-longIam41percentconfidentthattheansweris,two.Ifoundthefollowingevidenceinawikipediapassagetitled,MarcoPolo(TVseries).OnDecember12,2016,Netflixannouncedtheyhadcanceled"MarcoPolo"aftertwoseasons.Sourcestold"TheHollywoodReporter"thattheseries’twoseasonsresultedina200milliondollarlossforNetflix,andthedecisiontocanceltheserieswasjointlytakenbyNetflixandtheWeinsteinCompany.LuthiportraysLingLinginseason1,Chewinseason2.Theserieswasoriginallydevelopedatstarz,whichhadpickeduptheseriesinJanuary2012.Spoken/visualTable2:Explanationexamples:Exampleofhowsystemresponseslookedforeachexplanationtypeandbaseline,forthequestionHowmanyseasonsofMarcoPoloarethere?Werecruited525participantsintotal,withapprovalratingsgreaterthan95%andhadamaximumof8daysforapprovalofresponsesinordertominimizetheamountofspamming.Weusearandomsampleof120questionsfromourdatasetwhichremainsthesameacrossallcon-ditions.Inordertokeepeachsessionperpartici-pantatareasonabletimeandensurethequalityofthedatawouldn’tbeaffectedbyworkersbecomingexhausted,weoptedforthreefixedbatchesof40questions,allsplitas50%correctand50%incor-rect.Workerscouldonlyparticipateonce(onlyonebatchinonecondition).Participantstookaroundfrom35-45minutestocompletetheHITs,butweregivenupto70minutestocomplete.Wemonitorediftheirscreenwentoutoffocus,toensurethatparticipantsdidnotcheat.Ween-suredthatwehad25userannotationsperquestion.Whenanalyzingthedata,weremovethefirst4questionsofeachbatch,asitmaytakeparticipantsafewtriesbeforegettingusedtotheinterface.Intheend,wecollectabout21,000testinstances.TaskInstructionsImagineaskingNorbyaques-tionandNorbyrespondswithananswer.Norby’sanswercanbecorrectorwrong.IfyoubelieveNorby’sansweriscorrect,youcanacceptthean-swer.Ifyoubelieveitiswrong,youcanrejectit.Iftheanswerisactuallycorrectandyouacceptit,youwillearnabonusof$0.15.But,iftheansweriswrong,andyouacceptit,youwilllose$0.15fromyourbonus.Ifyourejecttheanswer,yourbonusisnotaffected.(Don’tworry,thebonusisextra!Evenifitshowsnegativeduringtheexperiment,intheendtheminimumbonusis0).Intotalyouwillsee40questionsinthisHIT(youwillonlybeallowedtoparticipateonce)andthetaskwilltakeabout40to45minutes.Youcanbecompensatedamaximumof$13.50forabout40-45minutesofwork.Somethingstonote:1.Youmustlistentotheaudiobeforetheoptionsbecomeavailable.2.Ifyoumakeittotheendthereisasubmitbuttonthere,however,incaseofanemergencyyoucanhitthequitearlybuttonaboveandyouwillgetrewardedforprovided.3.Youcanplaytheaudioasmanytimesasyouneedbutassoonasyouclickachoiceyouwillbedirectedtothenextitem.4.IMPORTANT!!PleasedonotlookFigure8:Voiceclarity:Mostparticipantsfoundthevoiceoftheassistanttobegoodorexcellent.Figure9:Helpfulness:Participantsindicatedhowhelpfulresponseswere.Theseresultsreflectthelargedifferencesweseeinperformance(BASELINEvstherestofthesettings),butarenotabletocapturethemoresubtledifferencesamongexplanationstrategiesandCONFIDENCE.F.5UserFeedbackUsersprovidedfree-formwrittenfeedbackonpos-siblewaystoimprovethesystem.Theprompttheysawwas:CODEDESCRIPTIONCATEGORYlen-concisenessuserswishexplanationwasshorterimprovementonlengthlen-expanduserswishexplanationwasshorteradapt-detailuserswishdetailsadaptedwithconfi-denceadaptabilityfeatureadapt-voiceuserswishvoiceadaptedtoconfidencepres-change-confidenceuserswishconfidencewouldbecom-municateddifferentlye.g.theanswerisprobably....improvepresentationpres-highlightinguserswishimportantfactswouldbehighlightedneed-more-sourcesuserswishmoresourcewereprovidedneed-confidenceuserswishconfidencewasprovidedneed-sourceuserswishedasourcewasprovidedneedCONDITIONCODE%PARTICIPANTSbaselineadapt-voice50need-confidence36need-explanation25need-source17confidenceneed-explanation38adapt-voice29pres-change-confidence14adapt-detail10need-multiple-answers10need-link5extractive-sent(spoken)need-more-sources44adapt-detail28len-conciseness22need-multiple-answers