Proceedingsofthe52ndAnnualMeetingoftheAssociationforComputationalLinguistics,pages902–912,Baltimore,Maryland,USA,June23-252014.c2014AssociationforComputationalLinguisticsHierarchicalSummarization:ScalingUpMulti-DocumentSummarizationJanaraChristensenStephenmentionthattheUSretaliatedbystrikingAfghanistanandSudan.Theusercanclickonthisinformationtolearnmoreabouttheseattacks.Inthisway,thesystemcanpresentlargeamountsofinformationwithoutHierarchicalClusteringHs1...sNsj+1...sNsl+1...sNsj+1...slsi+1...sjs1...sisk+1...sis1...skHierarchicalSummaryXx1,1x1,2x1,3x4,1x4,2x8,1x8,2x7,1x7,2x7,3x3,1x3,2x3,3x2,1x2,2x6,1x6,2x5,1x5,2x5,3Figure2:Examplesofahierarchicalclusteringandahier-archicalsummary,wheretheinputsentencesares2S,thenumberofinputsentencesisN,andthesummarysentencesarex2X.Thehierarchicalclusteringdeterminesthestruc-tureofthehierarchicalsummary.hierarchicalstructure,processinganorderofmag-nitudebiggerinput,generatingamuchlargerout-put,andenforcingcoherencebetweenparentandchildsummaries.Wesimplifytheproblembydecomposingitintotwosteps:hierarchicalclusteringandsummariz-ingovertheclustering(seeFigure2foranexam-ple).Ahierarchicalclusteringisatreeinwhichifaclustergpistheparentofclustergc,theneachsentenceingcisalsoingp.Thisorganizestheinformationintomanageable,semantically-relatedsectionsandinducesahierarchicalstructureovertheinput.Thehierarchicalclusteringservesasinputtothesecondstep–summarizinggiventhehierarchy.Thehierarchicalsummaryfollowsthehierarchi-calstructureoftheclustering.Eachnodeinthehierarchyhasanassociatedflatsummary,whichsummarizesthesentencesinthatcluster.More-over,thenumberofsentencesinaflatsummaryisexactlyequaltothenumberofchildclustersofthenode,sincetheuserwillclickasentencetogettothechildsummary.SeeFigure2foranillustrationofthiscorrespondence.Becauseweareinterestedintemporalhierar-chicalsummarization,wehierarchicallyclusterallthesentencesintheinputdocumentsbytime.Unfortunately,neitheragglomerativenordivisiveclusteringissuitable,sincebothassumeabinarysplitateachnode(Berkhin,2006).Thenumberofclustersateachsplitshouldbewhatismostnaturalfortheinputdata.Wedesignarecursiveclusteringalgorithmthatautomaticallychoosestheappropri-atenumberofclustersateachsplit.Beforeclustering,wetimestampallsentences.WeuseSUTime(ChangandManning,2012)tonormalizetemporalreferences,andweparsethesentenceswiththeStanfordparser(KleinandManning,2003)anduseasetofsimpleheuristicstodetermineifthetimestampsinthesentencere-fertotherootverb.Ifnotimestampisgiven,weusethearticledate.3.1TemporalClusteringAfteracquiringthetimestamps,wemusthierar-chicallyclusterthesentencesintosetsthatmakesensetosummarizetogether.Sincewewishtopartitionalongthetemporaldimension,ourprob-lemreducestoidentifyingthebestdatesatwhichtosplitaclusterintosubclusters.Weidentifythesedatesbylookingforburstsofactivity.Newstendstobebursty–manyarticlesonatopicappearatonceandthentaperout(Kleinberg,2002).Forexample,Figure3showsthenumberofarticlesperdayrelatedto1998embassybombingspublishedintheNewYorkTimes(identifiedusingakeywordsearch).Thereweretwomainevents–onthe7th,theembassieswerebombedandonthe20th,USretaliatedthroughmissilestrikes.Thefigureshowsacorrespondencebetweentheseeventsandnewsspikes.Idealsplitsforthisexamplewouldoccurjustbeforeeachspikeincoverage.However,whenthereislittledifferentiationinnewscoverage,wepreferclustersevenlyspacedacrosstime.WethuschooseclustersC={c1,...,ck}asfollows:maximizeCB(C)+↵E(C)(1)whereCisaclustering,B(C)istheburstinessofthesetofclusters,E(C)istheevennessoftheclusters,and↵isthetradeoffparameter.B(C)=Xc2Cburst(c)(2)burst(c)isthedifferenceinthenumberofsen-tencespublishedthedaybeforethefirstdateincandtheaveragenumberofsentencespublishedonthefirstandseconddateofc:burst(c)=pub(di)+pub(di+1)2pub(di1)(3)wheredisadateindexedovertime,suchthatdjisadaybeforedj+1,anddiisthefirstdateinc.Figure2:Examplesofinputandoutputtohierarchicalsum-marization.Theinputsentencesares∈S,thenumberofinputsentencesisN,andthesummarysentencesarex∈X.childsummaries.Wesimplifytheproblembydecomposingitintotwosteps:hierarchicalclusteringandsummariz-ingovertheclustering(seeFigure2foranexam-ple).Ahierarchicalclusteringisatreeinwhichifaclustergpistheparentofclustergc,theneachsentenceingcisalsoingp.Thisorganizestheinformationintomanageable,semantically-relatedsectionsandinducesahierarchicalstructureovertheinput.Thehierarchicalclusteringservesasinputtothesecondstep–summarizinggiventhehierarchy.Thehierarchicalsummaryfollowsthehierarchi-calstructureoftheclustering.Eachnodeinthehierarchyhasanassociatedflatsummary,whichsummarizesthesentencesinthatcluster.More-over,thenumberofsentencesinaflatsummaryisexactlyequaltothenumberofchildclustersofthenode,sincetheuserwillclickasentencetogettothechildsummary.SeeFigure2foranillustrationofthiscorrespondence.Becauseweareinterestedintemporalhierar-chicalsummarization,wehierarchicallyclusterallthesentencesintheinputdocumentsbytime.Unfortunately,neitheragglomerativenordivisiveclusteringissuitable,sincebothassumeabinarysplitateachnode(Berkhin,2006).Thenumberofclustersateachsplitshouldbewhatismostnaturalfortheinputdata.Wedesignarecursiveclusteringalgorithmthatautomaticallychoosestheappropri-atenumberofclustersateachsplit.Beforeclustering,wetimestampallsentences.WeuseSUTime(ChangandManning,2012)tonormalizetemporalreferences,andweparsethesentenceswiththeStanfordparser(KleinandManning,2003)anduseasetofsimpleheuristicstodetermineifthetimestampsinthesentencere-fertotherootverb.Ifnotimestampisgiven,weusethearticledate.3.1TemporalClusteringAfteracquiringthetimestamps,wemusthierar-chicallyclusterthesentencesintosetsthatmakesensetosummarizetogether.Sincewewishtopartitionalongthetemporaldimension,ourprob-lemreducestoidentifyingthebestdatesatwhichtosplitaclusterintosubclusters.Weidentifythesedatesbylookingforburstsofactivity.Newstendstobebursty–manyarticlesonatopicappearatonceandthentaperout(Kleinberg,2002).Forexample,Figure3showsthenumberofarticlesperdayrelatedtothe1998embassybomb-ingspublishedintheNewYorkTimes(identifiedusingakeywordsearch).Thereweretwomainevents–onthe7th,theembassieswerebombedandonthe20th,theUSretaliatedthroughmis-silestrikes.Thefigureshowsacorrespondencebetweentheseeventsandnewsspikes.Idealsplitsforthisexamplewouldoccurjustbeforeeachspikeincoverage.However,whenthereislittledifferentiationinnewscoverage,wepreferclustersevenlyspacedacrosstime.WethuschooseclustersC={c1,...,ck}asfollows:maximizeCB(C)+αE(C)(1)whereCisaclustering,B(C)istheburstinessofthesetofclusters,E(C)istheevennessoftheclusters,andαisthetradeoffparameter.B(C)=Xc∈Cburst(c)(2)burst(c)isthedifferenceinthenumberofsen-tencespublishedthedaybeforethefirstdateincandtheaveragenumberofsentencespublishedonthefirstandseconddateofc:burst(c)=pub(di)+pub(di+1)2−pub(di−1)(3)wheredisadateindexedovertime,suchthatdjisadaybeforedj+1,anddiisthefirstdateinc.pub(di)isthenumberofsentencespublishedondi.Theevennessofthesplitismeasuredby:E(C)=minc∈Csize(c)(4)wheresize(c)isthenumberofdatesinclusterc.Weperformhierarchicalclusteringtop-down,ateachpointsolving68101214161820222402040DayofMonthNumberofArticles1Figure3:NewscoveragebydatefortheembassybombingsinTanzaniaandKenya.There4.3SummaryCoherenceWerequiretwotypesofcoherence:coherencebe-tweentheparentandchildsummariesandcoher-encewithineachsummaryXi.Werelyontheapproximatediscoursegraph(ADG)thatsearchthroughthespaceofpartialsummariesandlocalsearch(hillclimbingwithrandomrestarts)intheinnerlooptopickthebestsentencetoaddtotheexistingpartialsummary.Weuseabeamofsizeteninourimplementation.5ExperimentsOurexperimentsaredesignedtoevaluatehowef-fectivehierarchicalsummarizationisinsumma-rizingalarge,complextopicandhowwellthishelpsuserslearnaboutthetopic.Ourevaluationaddressesthefollowingquestions:•Dousersedgegainbyfollowingthemethodologyof(Sha-hafetal.,2012)–askinguserstowriteaparagraphsummarizingtheinformationlearned.Usingthesamesetupasinthepreviousexper-iment,foreachtopic,fiveAMTworkersspentthreeminutesreadingthroughatimelineorsum-maryandwerethenaskedtowriteadescriptionofwhattheyhadlearned.Workerswerenotal-lowedtoseethetimelineorsummarywhilewrit-ing.WecollectedfivedescriptionsforEventsSUMMATIMELINEFLAT-MDSPrim.96%74%93%Sec.76%53%64%ThedifferenceinrecallbetweenSUMMAandTIMELINEwassignificantinbothcases,andthedifferencebetweensamedomain(SauperandBarzilay,2009),byanentity-aspectLDAmodel(Lietal.,2010),orbyWikipediatemplatesofrelatedtopics(Yaoetal.,2011).Thesemethodsassumeacommonstruc-tureforallReferencesC.G.Akcora,M.A.Bayir,M.Demirbas,andH.Fer-hatosmanoglu.2010.Identifyingbreakpointsinpublicopinion.In1stKDDWorkshoponSocialMe-diaAnalytics.BerkhinBerkhin.2006.Asurveyofclusteringdataminingtechniques.GroupingMultidimensionalData,pages25–71.OrkutBuyukkokten,HectorGarcia-Molina,andAn-dreasPaepcke.2001.Seeingthewholeinparts:Textsummarizationforwebbrowsingonhandhelddevices.InProceedingsofWWW2001,pages652–662.JaimeCarbonellandJadeGoldstein.1998.TheuseofMMR,diversity-basedrerankingforreorderingdoc-umentsandproducingsummaries.InProceedingsofSIGIR1998,pages335–336.AsliCelikyilmazandDilekHakkani-Tur.2010.Ahy-bridhierarchicalmodelformulti-documentsumma-rization.InProceedingsofACL2010,pages815–824.AngelChangandChristopherManning.2012.SU-Time:Alibraryforrecognizingandnormalizingtimeexpressions.InProceedingsofLREC2012.HaiLeongChieuandYoongKeokLee.2004.Querybasedeventextractionalongatimeline.InProceed-ingsofSIGIR2004,pages425–432.JanaraChristensen,Mausam,StephenSoderland,andOrenRussellSwanandJamesAllen.2000.Automaticgen-erationofoverviewtimelines.InProceedingsofSI-GIR2000,pages49–56.KouTakahashi,TakaoMiura,andIsamuShioya.2007.Hierarchicalsummarizingandevaluatingforwebpages.