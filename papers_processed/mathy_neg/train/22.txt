Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing,pages4069–4082,HongKong,China,November3–7,2019.c2019AssociationforComputationalLinguistics4069Don’tTaketheEasyWayOut:EnsembleBasedMethodsforAvoidingKnownDatasetBiasesChristopherClark∗,MarkYatskar†,LukeZettlemoyer∗∗PaulG.AllenSchoolofCSE,UniversityofWashington{csquared,lsz}@cs.uw.edu†AllenInstituteforArtificialIntelligence,SeattleWAmarky@allenai.orgAbstractState-of-the-artmodelsoftenmakeuseofsu-perficialpatternsinthedatathatdonotgener-alizewelltoout-of-domainoradversarialset-tings.Forexample,textualentailmentmod-elsoftenlearnthatparticularkeywordsim-plyentailment,irrespectiveofcontext,and4070Whatcoloristhegrass?Bias-OnlyModelRobustModelEnsembleDataGradientsBrownYellowGoldGreenBlueGrayOtherp(answer|model)BrownYellowGoldGreenBlueGrayOtherp(answer|ensemble)BrownYellowGoldGreenBlueGray4071Recentworkhasfocusedonbiasesthatcomefromignoringpartsoftheinput(e.g.,guessingtheanswertoaquestionbeforeseeingtheevi-dence).Solutionsincludegenerativeobjectivestoforcemodelstounderstandalltheinput(LewisandFan,2019),carefullydesignedmodelarchi-tecture(Agrawaletal.,2018;Zhangetal.,2016),oradversarialremovalofclass-indicativefeaturesfrommodel’sinternalrepresentations(Ramakrish-nanetal.,2018;Zhangetal.,2018a;Belinkovetal.,2019;GrandandBelinkov,2019).Incontrast,weconsiderbiasesbeyondpartial-inputcases(Fengetal.,2019),andshowourmethodissuperioronVQA-CP.Concurrently,Heetal.(2019)alsosuggestedusingaproduct-of-expertsensembletotrainunbiasedmodels,butwecon-siderawidervarietyofensembling40723.2.3BiasProductOursimplestensembleisaproductofex-perts(Hinton,2002):ˆpi=softmax(log(pi)+log(bi))Equivalently,ˆpi∝pi◦bi,where◦iselement-wisemultiplication.ProbabilisticJustification:Foragivenex-ample,x,letxbbethebiasoftheexample.Thatis,itisthefeatureswewilluseinourbias-onlymodel.Letx−bbeaviewoftheexamplethatcapturesallinformationaboutthatexampleexceptthebias.Assumethatx−bandxbareconditionallyindependentgiventhelabel,c.Thentocomputep(c|x)wehave:p(c|x)=p(c|xb,x−b)(1)∝p(c|x−b)p(xb|c,x−b)(2)=p(c|x−b)p(xb|c)(3)=p(c|x−b)p(c|xb)p(xb)p(c)(4)∝p(c|x−b)p(c|xb)p(c)(5)Where2isfromapplyingBayesRulewhileconditioningonx−b,3followsfromthecondi-tionalindependenceassumption,and4appliesBayesRuleasecondtimetop(xb|c).Wecannotdirectlymodelp(c|x−b)becauseitisusuallynotpossibletocreateaviewofthedatathatexcludesthebias.Instead,withthegoalofencouragingthemodeltofallintotheroleofcom-putingp(c|x−b),wecomputep(c|xb)/p(c)usingthebias-onlymodel,andtraintheproductofthetwomodelstocomputep(c|x).Inpractice,weignorethep(c)factorbecause,onourdatasets,eithertheclassesareuniformlydistributed(MNLI),thebias-onlymodelcannoteasilycaptureaclasspriorsinceitisusingapointernetwork(QA),orbecausewewanttore-moveclasspriorsfrommodelanyway(VQA).3.2.4Learned-MixinTheassumptionofconditionalindependence(Equation3)willoftenbetoostrong.Forexam-ple,insomecasestherobustmodelmightbeabletopredictthebias-onlymodelwillbeunreliableforcertainkindsoftrainingexamples.Wefindthatthiscancausetherobustmodeltoselectivelyadjustitsbehaviorinordertocompensatefortheinaccuracyofthebias-onlymodel,leadingtoer-rorsintheout-of-domainsetting(seeSection5.1).Insteadweallowthemodeltoexplicitlydeter-minehowmuchtotrustthebiasgiventheinput:ˆpi=softmax(log(pi)+g(xi)log(bi))wheregisalearnedfunction.Wecomputegassoftplus(w·hi)wherewisalearnedvector,hiisthelasthiddenlayerofthemodelforexamplexi,andthesoftplus(x)=log(1+ex)functionisusedtopreventthemodelreversingthebiasbymulti-plyingitbyanegativeweight.wistrainedwiththerestofthemodelparameters.Thisreducestobiasproductwheng(xi)=1.Adifficultywiththismethodisthatthemodelcouldlearntointegratethebiasintopiandsetg(xi)=0.Wefindthisdoessometimesoccursinpractice,andournextmethodalleviatesthischal-lenge.3.2.5Learned-Mixin+HTopreventthelearned-mixinensemblefromig-noringbi,weaddanentropypenaltytotheloss:R=wH(softmax(g(xi)log(bi)))WhereH(z)=−Pjzjlog(zj)istheentropyandwisahyperparameter.Penalizingtheentropyencouragesthebiascomponenttobenon-uniform,andthushaveagreaterimpactontheensemble.4EvaluationMethodologyWeevaluateourmethodsonseveraldatasetsthathaveout-of-domaintestsets.Someofthesetasks,suchasHANS(McCoyetal.,2019)orAdversar-ialSQuAD(JiaandLiang,2017),canbesolvedeasilybygeneratingadditionaltrainingexamplessimilartotheonesinthetestset(e.g.,WangandBansal(2018)).We,instead,demonstratethatitispossibletoimproveperformanceonthesetasksbyexploitingknowledgeofgeneral,biasedstrategiesthemodelislikelytoadopt.Ourevaluationsetupconsistsofatrainingset,anout-of-domaintestset,abias-onlymodel,andamainmodel.Torunanevaluationwetrainthebias-onlymodelonthetrainset,trainthemainmodelonthetrain4073TaskDatasetDomainShiftBias-OnlyModelMainModelNLISyntheticMNLISyntheticindicatorfeaturesarerandomizedIndicatorfeaturesCo-AttentionVQAVQA-CPv2.0Correlationsbetweenquestion-typesandanswersarealteredQuestion-typeBottomUpTopDownNLIHANS4074timethetokencorrespondstotheexample’slabel(i.e.,“0”iftheclassis“entailment”,“1”iftheclassiscontradiction,ect.).Intheout-of-domaintestset,thetokenisselectedrandomly.Excluder:ThesameasIndicator,butwitha3%chancetheaddedtokencorrespondstotheexample’slabel,meaningthetokencanusuallybeusedtoeliminateoneofthethreeoutputclasses.Dependent:Intheprevioustwosettings,theaddedbiasisindependentoftheexamplegiven4075DebiasingMethodIndicatorExcluderDependentAcc.w/BiasAcc.w/BiasAcc.w/BiasNone69.3686.4968.0683.5663.2387.90Reweight75.4482.7470.3683.2969.8185.50BiasProduct76.2781.3277.3380.4171.8584.98Learned-Mixin4076Isthisa....?NoQuestionTypeBiasAnswerHigherBiasWeightLowerBiasWeightHowmany….?2Howmanyanimals?[2]G=5.61G+=5.89Howmanybirds?[17]G=0.17G+=1.95Isthisablackbear?[No]G=4.65G+=5.96Isthisaphotoorpainting?[Painting]WhiteWhatcoloristhe….?G=0.87G+=4.32Whatcoloristhedoor?[White]Whatcoloristhetenniscourt?[Purple]G=0.00G+=0.48PizzaWhatkindof….?Whatkindoffoodisinthebox?[Pizza]G=0.06G+=2.93Whatkindofbirdsareinthepicture?[Seagull]G=0.11G+=2.34G=0.00G+=1.89Figure2:Qualitativeexamplesofthevaluesofg(xi)on4077DebiasingMethodTF-IDFFilteredTF-IDFAddSentAddSentOneDevAddSentAddSentOneDevNone42.5453.9180.6142.5453.9180.61Reweight41.5553.0680.5942.7453.8380.51BiasProduct47.1757.7478.6344.4155.7378.22Learned-Mixin4078ReferencesAishwaryaAgrawal,DhruvBatra,DeviParikh,andAniruddhaKembhavi.2018.Don’tJustAssume;LookandAnswer:OvercomingPriorsforVisualQuestionAnswering.InCVPR.AnkeshAnand,EugeneBelilovsky,KyleKastner,HugoLarochelle,and4079RobinJiaandPercyLiang.2017.AdversarialEx-amplesforEvaluatingReadingComprehensionSys-tems.InEMNLP.MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer.2017.TriviaQA:ALargeScaleDis-tantly4080DatasetExperimentPenaltySyntheticIndicator0.01SyntheticExcluder0.005SyntheticDependent0.005VQA-CP-0.36HANSRecurrent0.03HANSBERT0.03Adver.SQuADTF-IDFFiltered2.0Adver.SQuADTF-IDF2.0TriviaQA-CPLocation0.4TriviaQA-CP4081ModelDebiasingMethodMNLICorrect:EntailmentCorrect:Non-entailmentLexicalSubseq.ConstLexicalSubseq.ConstCo-AttentionNone78.7397.8399.6797.281.373.683.68Reweight77.0380.1077.8473.7615.6834.2735.44BiasProduct76.634082MethodAccuracyLocationPersonPrecisionRecallF1PrecisionRecallF1Patterns72.8498.3070.6182.1999.1233.4350.00Yago88.5695.8785.3190.2894.7080.0086.73Yago+Patterns91.7395.4493.8894.6594.7085.3789.80Dist.SupervisedModel92.7396.6092.6594.5898.2885.3791.37SupervisedModel94.4695.0894.6994.8993.3195.8294.55Table10:Accuracy,andper-classscores,onthemanuallyannotatedquestionsforthevariousquestionclassifica-tionmethodsweusedwhenbuildingTriviaQA-CP.