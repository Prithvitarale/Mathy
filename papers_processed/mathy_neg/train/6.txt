City,UniversityofLondonInstitutionalRepositoryCitation:Kulesza,T.,Burnett,M.,Wong,W-K.&Stumpf,S.(2015).PrinciplesofExplanatoryDebuggingtopersonalizeinteractivemachinelearning.In:O.Brdiczka&PChau(Eds.),ProceedingsofPrinciplesofExplanatoryDebuggingtoPersonalizeInteractiveMachineLearningABSTRACTHowcanendusersefficientlyinfluencethepredictionsthatmachinelearningsystemsmakeontheirbehalf?ThispaperpresentsExplanatoryDebugging,anapproachinwhichimagesegmentationsystem[11].Sincethen,researchershaveexploredusingthiscycleofquickinteractionstotraininstance-basedclassifiers[5,13],enablebettermodelselectionbyendusers[2,12,37],elicitlabelsforthemostimportantinstances(e.g.,activelearning)[7,34],andtoimprovereinforcementlearningforautomatedagents[20].Theseusecaseslargelytreatthemachinelearningsystemasa“blackbox”—userscantrytopersonalizethesystembyprovidingitwithdifferentinputs(e.g.,labeledinstances),Principle1.3:ButDon’tOverwhelmBalancedagainstthesoundnessandcompletenessprinciplesistheneedtoremaincomprehensibleandtoengageuserattention.Findingsfrom[23]suggestthatonewaytoengageuserattentionisFigure1.TheEluciDebugprototype.(A)Listoffolders.(B)Listofmessagesintheselectedfolder.(C)Theselectedmessage.(D)Explanationoftheselectedmessage’spredictedfolder.(E)Overviewofwhichmessagescontaintheselectedword.(F)Completelistofwordsthelearningsystemusestomakepredictions.inthecollectionofpotentialoutputclassesC,ditorepresentanindividualdocumenttoclassify,andassumethattheonlyfeaturestheclassifierusesareindividualwordsintheFigure2.TheWhyexplanationtellsusershowfeaturesandfoldersizewereusedtopredicteachmessage’stopic.Thisfigureisaclose-upofFigure1partD.thefeature’sfontsize,whileitsfontcolorisbasedontheclasswiththelargerprobability.Forexample,inFigure2thewordstanleyislargerthantigerbecauseitsratioofwordprobabilityiscorrespondinglylarger,anditisbluebecauseitsprobabilityofoccurringinthehockeyclassis0.00.51.001020304050F1score#offeaturesFigure3.Selectingthe10highestinformationgainfeaturesresultedinsimilarclassifierperformanceaslargerfeaturesets.50wouldbetheupperlimitonfeaturesetsize.Offlinetests,however,revealedthatMNB’sF1scoredidnotimprovewhilethefeaturesetsizeincreasedfrom10to50(Figure3),sowedecidedourclassifierwouldautomaticallyselectonlythe10featureswiththehighestinformationgain(untiltheuserspecifiesotherwisebyaddingorremovingfeatures).BeingActionableTheImportantwordsexplanation(Figure4)isthemostac-tionableofEluciDebug’sexplanations.Userscanaddwordsto—andremovewordsfrom—thisexplanation,whichinturnwilladdthosewordsto(or“up”arrows,whiledecreasingnumericvaluesareidentifiedbyred“down”arrows.ExamplesofeachareshowninFigure1,partB.Hoveringovereitherofthesearrowiconsyieldsatooltipdetailingwhatjustsentenceslong,andaskedparticipantswhichtopictheclas-sifierwouldassigntoeachmessage,andwhy.Themessageswereconstructedsuchthatonlyonecomponent—eitheranobviousfeature,asubtlefeature,orclassratios—wasentirelyresponsiblefortheclassification.Tounderstandparticipants’reactionstoEluciDebug,thepost-taskquestionnairealsoaskedparticipantsaboutvariousfea-turesoftheprototypeandtheirperceivedtaskloadduringtheexperiment(viatheNASA-TLXquestionnaire[14]).DataanalysisWeusednon-parametricmethodsforallstatisticalanalyses.Assuggestedin[29],weusedMann–WhitneyU-testsforordinaldata,Wilcoxonsignedranktestsforintervaldata,andSpearman’sρforcorrelations.Toanalyzeparticipants’mentalmodels,aresearchergradedparticipantresponsestothepost-taskmentalmodelquestion-naires.Becausesomeparticipantsmayhaverandomlyguessedwhichtopictheclassifierwouldpredictforeachmessage,weignoredallpredictedtopicsandonlygradedthereasonparticipants’statedfortheclassifier’sprediction.Participantsearnedtwopointsforcorrectreasonsandonepointforpartiallycorrectreasons.Theresearchergradedparticipantresponseswithoutknowingwhichconditiontheparticipantwasin(i.e.,blindly).Eachparticipants’pointsweresummedtoyieldamentalmodelscorewithamaximumpossiblevalueof24.WeanalyzedclassifierperformanceviatheF1score.Thiscombinestwosimplermeasures,precisionandrecall,eachofwhichcanrangefrom0to1.Inthecontextofabinaryclas-sificationsystemthatpredictswhethereachinputispositiveornegative,aprecisionof0indicatesthatnoneofitspositivepredictionswerecorrect,whileaprecisionof1indicatesthatallofitspositivepredictionswerecorrect.Forthesamesystem,arecallof0indicatestheclassifierdidnotcorrectlyidentifyanyofthepositiveitems,whilearecallof1indicatesthatitcorrectlyidentifiedallofthem.Astheharmonicmeanofprecisionandrecall,F1alsorangesfrom0(noprecisionandnorecall)to1(perfectprecisionandrecall).Wesupplementedourevaluationofclassifieraccuracywithanadditionalofflineexperimentusingaseparatefeatureselectionmethod.RecallthatEluciDebuglimitsitsclassifiertothe10featureswiththehighestinformationgain.Textclassifiers,however,oftenincludemost—ifnotall—ofthewordsinthetrainingsetasfeatures.Thus,weanalyzedparticipants’classi-fiersusingbothHighIGfeaturesandComprehensivefeatures.Forcontrolparticipants(whocouldnotprovidefeature-basedfeedback),HighIGwasrecomputedaftereachmessagewaslabeledandkeptthe10highestinformationgainfeatures.Fortreatmentparticipants,HighIGwasneverrecomputed;instead,participantsneededtomodifyitmanuallybyadding,removing,oradjustingfeatures.TheComprehensivefeaturesetincludedallwordsfromthesetoflabeledmessagesattheendoftheexperiment.TheclassifiersparticipantsinteractedwithusedtheHighIGfeatures;theComprehensivefeatureswereonlyusedforofflineanalysis.RESULTSExplainingCorrectionstoEluciDebugEluciDebugincludesseveralmethodsforuserstoexplaincorrectionstotheclassifier,andtreatmentparticipantsmadefrequentuseofallofthem.Onaverage,theyadded34.5newfeatures,removed8.2features(including7.4ofEluciDebug’s10initialfeatures),andmade18.3featureadjustments(e.g.,increasingordecreasingafeature’simportancetoatopic).Controlparticipants—whocouldnotprovidefeature-basedfeedback—insteadreliedoninstance-basedfeedbacktoadjustEluciDebug’spredictions,labelinganaverageof182messagesvs.thetreatmentaverageof47(W=1395,p<.001).Controlparticipantsalsoexaminedmoremessages,averaging296messageviewsvs.151views(W=1306,p<.001).Treatmentparticipantsthusprovidedlessfeedbackoverall(andneededtoexplorelessofthedatasetwhileprovidingit),insteadleveragingEluciDebug’sabilitiestotargettheirfeedbackatfeaturesratherthaninstances.Thisfeature-basedfeedbackprovedefficientatimprovingparticipants’classifiers.WeexaminedthechangeinF1foreachparticipant’sclassifierduringtheexperimentanddividedthisbythenumberofactionstheparticipantmadethatcouldinfluencetheclassifier’spredictions(instanceslabeledandfeaturesadded,removed,oradjusted).Theresults,showninFigure6(left),werethattreatmentparticipantsperformedfeweractions,buteachoftheiractionsresultedinlargerclassifierimprovementsthanthoseofcontrolparticipants.Treatmentparticipants’feedbackwastwiceasefficientascontrolparticipants’usingHighIGfeatures(0.16%vs.0.34%F1improvementperaction,W=207,p<.001),andremainedsuperiorwhenusingtheComprehensivefeatureset(0.65%vs.0.97%,W=367,p<.001).Wethushaveevidencethatwhenuserscanonlyprovidealimitedamountoffeedbacktoalearningsystem(suchaswhenlabelinginstancesisexpensive,insufficientinstancesareavailableforlabeling,ortheuser’stimeisconstrained),ExplanatoryDebuggingcanresultinsuperiorclassifiersthanatraditionalblack-boxinstancelabelingapproach.Indeed,Figure6(right)showsthatbytheendofthe30-minuteex-periment,treatmentparticipantshadcreatedclassifiersthatwereroughly10%moreaccuratethancontrolparticipants,averagingF1scoresof0.85vs.0.77(W=237,p<.001).However,ouranalysiswiththeComprehensivefeaturesetsuggeststhatwhentheusercanlabelmanyinstances,instancelabelingwithalargefeaturesetmaybepreferabletoEx-planatoryDebugging—atleasttoinitiallytrainaclassifier.Thecombinationofalargetrainingsetandmanyfeaturesallowedcontrolparticipants’classifierstoedgeoutthoseoftreatmentparticipantsbyabout8%(Figure6,right).Eventhoughtreatmentparticipants’feedbackwasuptotwiceasefficient,controlparticipantsprovided0.0%0.5%1.0%HighIGComp.F1change0.00.51.0HighIGComp.F1scoreFigure6.(Left)AverageclassifierF1improvementperuseractionforcontrol(darkblue)andtreatment(lightorange);treatmentparticipantsModelcomponentMaxscoreControlmean(SD)Treatmentmean(SD)p-valueObviousfeatures86.7(2.7)7.3(1.8).345Subtlefeatures82.8(2.6)6.8(1.9)<.001Classratios80.6(1.5)1.8(3.0)2.Amershi,S.,Fogarty,J.,Kapoor,A.,andTan,D.Examiningmultiplepotentialmodelsinend-userinteractiveconceptlearning.InProceedingsoftheACMConferenceonHumanFactorsinComputingSystems(2010),1357–1360.3.Blackwell,46(2008),38–41.30.Norman,D.A.Someobservationsonmentalmodels.InHuman-ComputerInteraction,R.M.BaeckerandW.A.S.Buxton,Eds.SanFrancisco,CA,USA,1987,241–244.31.Norman,D.A.The