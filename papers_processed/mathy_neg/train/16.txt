Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages6323–6330Florence,Italy,July28-August2,2019.c2019AssociationforComputationalLinguistics6323WhyDidn’tYouListentoMe?ComparingUserControlofHuman-in-the-LoopTopicModelsVarunKumar∗AmazonAlexaCambridge,MAvarunk@cs.umd.eduAlisonSmith-RennerUniversityofMarylandCollegePark,MDamsmit@cs.umd.eduLeahFindlaterUniversityofWashingtonSeattle,Washingtonleahkf@uw.eduKevinSeppiBrighamYoungUniversityProvo,UTkseppi@gmail.comJordanBoyd-GraberUniversityofMarylandCollegePark,MDjordanbg@umiacs.eduAbstractToaddressthelackofcomparativeevalua-tionofHuman-in-the-LoopTopicModeling(HLTM)systems,weimplementandevaluatethreecontrastingHLTMmodelingapproachesusingsimulationexperiments.Theseap-proachesextendpreviouslyproposedframe-works,includingconstraintsandinformedprior-basedmethods.UsersshouldhaveasenseofcontrolinHLTMsystems,sowepro-poseacontrolmetrictomeasurewhetherre-finementoperations’resultsmatchusers’ex-pectations.Informedprior-basedmethodspro-videbettercontrolthanconstraints,butcon-straintsyieldhigherqualitytopics.1Human-in-the-LoopTopicModelingTopicmodelshelpexplorelarge,unstructuredtextcorporabyautomaticallydiscoveringthetopicsdiscussedinthedocuments(Bleietal.,2003).However,generatedtopicmodelsarenotperfect;theymaycontainincoherentorlooselyconnectedtopics(Changetal.,2009;Mimnoet6324Wealsointroducemetricstoassessthede-greetowhichHLTMsystemslistentousers—usercontrol—akeyuserinterfacedesignprinci-pleforhuman-in-the-loopsystems(Amershietal.,2014;Duetal.,2017).Ingeneral,informedpri-orsprovidemorecontrolwhileconstraintspro-ducehigherqualitytopics.Thispaperprovidesthreecontributions:(1)im-plementationofanHLTMsystemusinginformedpriorsandvariationalinference,(2)experimentalcomparisonofthreeHLTMsystems,and(3)met-ricstoevaluateusercontrolinHLTMsystems.2HumanFeedbackandLDAWebrieflydescribeLatentDirichletAlloca-tion(Bleietal.,2003,LDA)andoutlinetheex-perimentalconditionsandourimplementation.2.1LDAInferenceLDAisgenerative,modelingdocumentsasmix-turesofktopicswhereeachtopicisamultinomialdistribution,φz,overthevocabulary,V.Eachdoc-umentdisanadmixtureoftopicsθd.Eachwordindexedbyiindocumentdisgeneratedbyfirstsamplingatopicassignmentzd,ifromθdandthensamplingawordfromthecorrespondingtopicφzi.CollapsedGibbssampling(GriffithsandSteyvers,2004)andvariationalExpectation-Maximization(Bleietal.,2003,EM)aretwopopularinferencemethodstocomputetheposterior,p(z,φ,θ|w,α,β).Gibbssamplingiterativelysamplesatopicassignment,zd,i=tgivenanobservedtokenwd,iindocumentdandothertopicassignments,z−d,n,withprobabilityP(zd,i=t|z−d,n,w)∝(nd,t+α)6325InformedpriorsusingGibbssampling(info-gibbs)forgetstopic-wordassignmentsziandin-jectsnewinformationbymodifyingDirichletpa-rameters,αandβ.Smithetal.(2018)implementsevenrefinementsforthisapproach.Weextendtheirworkwithacreatetopicrefinement.Informedpriorsusingvariationalinference(info-vb)forgetstopic-wordassignmentsforawordwintopictbyresettingthevalueofλt,w.Thisapproachmanipulatespriors,αandβ,toin-corporatenewknowledgelikeinfo-gibbs.Wede-fineandimplementsevenuser-preferredrefine-mentoperationsforthisapproach.ConstraintsusingGibbssampling(const-gibbs)forgetstopicassignmentslikeininfo-gibbs,butinsteadofpriormanipulation,injectsnewinfor-mationintothemodelusingpotentialfunctions,fm(z,m,d)(Yangetal.,2015).Wedefineandimplementsevenuser-preferredrefinementopera-tionsforthisapproach.2.3RefinementImplementationsOurthreesystemssupportthefollowingsevenrefinementsthatusersrequestinHLTMsystems(Musialeketal.,2016;Leeetal.,2017):Removewordwfromtopict.Forallthreesystems,firstforgetallw’stokenswifromt.Then,forinfo-gibbsandinfo-vb,assignavery6326rM1wtdenotetherankofawordwintopictinmodelM1.Afterapplyingaword-levelrefine-ment,therankofwintheupdatedmodelM2,isrM2wt.Forword-levelrefinements,suchasaddword,removeword,andchangewordorder,computecontrolastheratiooftheactualrankchange,theabsolutedifference(rM1wt−rM2wt),andtheexpectedrankchange.Ascoreof1.0indicatesthatthemodelperfectlyappliedtherefinement,whileanegativescoreindicatesthemodeldidtheoppositeofwhatwasdesired.Forremovedoc-ument,usethesamedefinitionasremovewordexceptconsideratopicasarankeddocumentlist.Forcreatetopic,computecontrolastheratioofthenumberofseedwordsinthecreatedtopicoutofthetotalnumberofprovidedseedwords.Formergetopics,controlisdefinedastheratioofthenumberofwordsinthemergedtopicwhichcamefromeitheroftheparenttopics,andthetotalnum-berofwordsshowntoauser.Forsplittopic,con-trolistheaverageofthecontrolscoresofparenttopicandchildtopic,computedusingthecontroldefinitionforcreatetopic.4HLTMSystemComparisonTocomparehowthethreeHLTMsystemsmodeldataandadheretouserfeedback(i.e.,providecon-trol),weneeduserdata;however,realuserin-teractionisexpensivetoobtain.So,wesimu-latearangeofuserbehaviorwiththesesystems:usersthataimtoimprovetopics,“goodusers”,andthosethatbehaveunexpectedly,“randomusers”.Thesimulationsuseadatasetof7000newsar-ticles,500articleseachforfourteendifferentnewscategories,suchasbusiness,law,andmoney,col-lectedusingtheGuardianAPI.34.1SimulatedUsersThe“randomuser”refinesrandomly.Forexam-ple,removedocument,deletesarandomlyse-lecteddocumentfromarandomlyselectedtopic.Our“gooduser”reflectsarealisticuserbehav-iorpattern:identifyamixedcategorytopicandap-plyrefinementstofocusthetopiconitsmostdom-inantcategory.Thusthe“gooduser”—withaccesstotruedocumentcategories—firstchoosesatopicassociatedwithmultiplecategoriesofdocumentsanddeterminesthedominantcategoryofthetopdocumentsforthetopic.Then,refinementopera-tionspushthetopictothedominantcategory.For3https://open-platform.theguardian.comexample,the“gooduser”mayremoveadocumentwhichdoesnotbelongtothedominantcategory.AdditionalsimulationarefoundinAppendixA.4.2MethodWetrainfortyinitialLDAmodels,twentywithtentopicsandtwentywithtwentytopicsforthenewsarticles,resultinginmodelswithlessandmoretopicsthanthetruenumberofcategories.ForeachofthethreeHLTMsystemsandeachofthesevenrefinementtypes,werandomlyselectoneofthepre-trainedmodels.Thecreateandsplittopicrefinementtypesselectfromthemodelswithtentopics,ensuringthattopicshaveoverlappingcategories,whiletheothersselectfromthemod-elswithtwentytopics.Wethenapplyarefinementasdictatedbythesimulateduser.Forthe“ran-domuser”,werandomlyselectrefinementparam-eters,suchastopicandword(AppendixA.1),andforthe“gooduser”,wechoosetopicandrefine-mentparametersintendingtoimprovethetopics(AppendixA.2).Weapplytherefinement(Sec-tion2.3)andruninferenceuntilthemodelcon-vergesorreachesathresholdoftwentyGibbssam-plingandthreeEMiterations.Wecomputecontrol(Section3)oftherefinementandchangeintopiccoherenceusingNPMIderivedfromWikipediaforthetoptwentytopicwords(Lauetal.,2014).Werepeatthisprocess100timesforeachrefinementtype,simulateduser,andHLTMsystem.5InformedPriorsListentoUsers,whileConstraintsProduceCoherentTopicsTable1showstheper-refinementcontrolandco-herencedeltasforthethreedifferentHLTMsys-tems.AsdetailedinAppendixB,Kruskal-WallistestsshowthatHLTMsystemshavesignificantlydifferent(p<.05)controlscoresforallrefine-mentsforthe“gooduser”andforallbutremovewordforthe“randomuser.”Coherencedeltaswerealsosignificantlydifferentforallrefinementsexceptaddword,whereconst-gibbsyieldscon-sistentlyhighercoherenceimprovementsthantheotherconditionsasidefromremovedocument.Forremoveword,andmergetopics,allmeth-odsprovidegoodcontrol(scorescloseto1.0).However,theinformedpriormethods,info-vbandinfo-gibbs,providemorecontrol,forboththerandom(CRand)andgood(CGood)users,com-paredtoconst-gibbs.Informedpriormethodsalsoexcelatrefinementsthatpromotetopicwords,6327const-gibbsinfo-gibbsinfo-vbCRandCGoodQGood∗CRandCGoodQGood∗CRandCGoodQGood∗removew1.0(0.0)1.0(0.0)5.4(9.7)1.0(0.0)1.0(0.0)3.0(8.9)1.0(0.0)1.0,(0.0)1.2(5.0)removed1.0(0.0)1.0(0.0)-1.7(10.8)1.0(0.0)1.0(0.0).8(4.5).72(.4).85(.25)-6.0(13.2)merget.97(.05)1.0(0.0)6.3(8.7).96(.05)1.0(0.0)-.43(9.3).99(.02).99(.02)1.4(9.8)addw.82(.29).86(.24)3.0(9.4)1.0(0.0).98(.03)3.1(6.4).98(.04).98(.02)1.7(5.6)createt.08(.10).81(.13)-6.6(13.7).98(.11).98(.04)-11(10.4)1.0(0.0)1.0(0.0)-13.0(8.4)splitt.91(.09).79(.19)1.9(17.9).93(.06).87(.19)-7.9(13.5)1.0(0.0).93(.16)-1.6(8)reorderw.41(.53).19(.20)1.6(7)1.19(.46).56(.24)-1.0(5.5)1.02(.27).44(.24)-1.0(5.1)Table1:Simulationresults,reportedasmean(SD):controlwiththerandom(CRand)andgood(CGood)users,andcoherencedeltas(QGood)forthegooduser(weomitcoherencefortherandomuserasthegoalthereisnottoimprovethetopics).∗valuesreportedasE-04.suchasaddwordandcreatetopic.Ontheotherhand,const-gibbssupportsdefiningtokenanddocument-levelconstraints,whichensureal-mostperfectcontrolforrefinementsthatrequirerestrictingcertainwordsordocuments,suchasre-movewordandremovedocument.Additionally,comparinggoodandrandomusers,allsystemsprovidesimilarcontrolexceptforconst-gibbsforcreatetopic:.81forgood(CGood)comparedto.08forrandom6328ReferencesSaleemaAmershi,MayaCakmak,WilliamBradleyKnox,andToddKulesza.2014.Powertothepeo-ple:Theroleofhumansininteractivemachinelearning.AIMagazine,35(4):105–120.DavidAndrzejewski,XiaojinZhu,andMark6329KeZhai,JordanL.Boyd-Graber,NimaAsadi,andMo-hamadL.Alkhouja.2012.Mr.LDA:aflexiblelargescaletopicmodelingpackageusingvariationalin-ferenceinmapreduce.InProceedingsofthe21stInternationalConferenceonWorldWideWeb.ASimulationDetailsTosimulatethebehaviorofthe“randomuser”and“gooduser”forthethreeHLTMsystems,wetrain40initialLDAmodels,20with10topicsand20with20topicsforthenewsarticles,resultinginmodelswithlessandmoretopicsthanthetruenumberofcategories.A.1RandomUserSimulationTosimulaterandomuserbehavior,foreachofthethreesystemsandforeachofthesevenre-finementtypes,werandomlyselectapre-trainedLDAmodelfromthepoolofmodelswith20top-ics.Then,weapplyarefinementofthatrefine-menttypetotheselectedmodel.Werandomlyselectrefinementspecificparameters,suchascan-didatetopic,wordtobeadded,anddocumenttobedeleted.Weruninferenceuntilthemodelcon-vergesorreachesalimit.ForGibbssamplingmodels,info-gibbsandconst-gibbs,weuse20iter-ationsaslimitandforthevariationalmodel,info-vb,weuse3EMiterationsasthelimit.Afterap-plyingtherefinement,wecomputethecontrolandcoherencegiventheupdatedandinitialmodel.Weperformthis100timesforeachoftherefinementtypesandHLTMsystems.A.2GoodUserSimulationForeachcategorycofthe14categoriesoftheGuardiannewsdataset(art&design,business,ed-ucation,environment,fashion,film,football,law,money,music,politics,science,sports,technol-ogy),wecomputethemostimportantwordsinc,Sc,usingaLogisticregressionclassifier.WeuseScasalistofrepresentativewordsforcategoryc.Givenalabeledcorpus,werandomlychooseoneofthepre-trainedmodels.Whenapplyingcre-ateorsplittopicrefinementtypes,weselectfromthemodelswith10topics,ensuringthattopicshaveoverlappingcategories.Whileapplyingallotherrefinementtypes,weselectfromthemodelswith20topics.Wethensimulategooduserbehav-iorforeachoftherefinementtypesasfollows:1.Addword:Randomlyselectatopictfromthosewherethetop20documentsarefrommorethanonecategory.Then,findthecor-respondinglabeledcategorycbyanalyzingtop20documentsintheselectedcategory.Toimprovethetopiccoherenceoft,addtoprankedwords(fromonetofivewords)fromSc,whicharenotalreadyinthetopwordsoft.2.Removeword:Randomlyselectatopictfromthosewheretop20documentsarefrommorethanonecategory.Then,findthecorre-spondinglabeledcategorycbyanalyzingtop20documentsintheselectedcategory.Forselectedtopict,removewordswhicharenotpartofSc.3.Changewordorder:Randomlyselectatopictamongalltopics.Then,findthecorre-spondinglabeledcategorycbyanalyzingtop20documentsintheselectedcategory.Then,findwordsbetweenindex10to20,whichareathigherrankinSc.Promotesuchwordstoahigherrankusingchangewordorder.4.Removedocument:Randomlyselectatopictfromthosewheretop20documentsarefrommorethanonecategory.Then,findthecorre-spondinglabeledcategorycbyanalyzingtop20documentsintheselectedcategory.Forselectedtopict,deletedocuments(fromonetofivedocuments),whicharenotinc.5.Mergetopics:Randomlychooseatopicpairtomergewhichrepresentsacommoncate-goryc.6.Createtopic:Randomlyselectacategorycwhichisnotadominantcategoryinanyofthetopics.Createatopicbyprovidingtop10wordsasseedwordsfromSc.7.Splittopic:Randomlyselectatopicfromthosewhichhavedocumentsfromtwodif-ferentcategories,c1andc2.Splitthetop20wordsinthattopicintotwolistsusingtherepresentativewordsfromSc1andSc2.Then,splitthetopicusingoneofthelists.BKruskalWallisTestsWeprovidedetailsontheKruskalWallistestsusedtoassesswhethertherearesignificantdiffer-encesinhow6330coherence.Themeansreportedhererepeatwhatisprovidedinthemainpaper,butwiththeadditionalχ2andpvaluesoutputfromtheKruskalWallistests;p<.05isconsideredto