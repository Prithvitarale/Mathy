InterpretabilityBeyondFeatureAttribution:QuantitativeTestingwithConceptActivationVectors(TCAV)BeenKimMartinWattenbergJustinGilmerCarrieCaiJamesWexlerFernandaViegasRorySayresAbstractTheinterpretationofdeeplearningmodelsisachallengeTestingwithConceptActivationVectors(TCAV)Figure1.TestingwithConceptActivationVectors:Givenauser-definedsetofexamplesforaconcept(e.g.,‘striped’),andrandomexamplesa,labeledtraining-dataexamplesforthestudiedTestingwithConceptActivationVectors(TCAV)point,buttrueforeachclass(i.e.,globalexplanation).2.2.InterpretabilitymethodsinneuralnetworksThegoalofTCAVistointerprethighdimensionalEmsuchasthatofneuralnetworkmodels.Saliencymethodsareoneofthemostpopularlocalexplanationmethodsforimageclassification(Erhanetal.,2009;Smilkovetal.,2017;Selvarajuetal.,2016;Sundararajanetal.,2017;Dabkowski&Gal,2017).Thesetechniquestypicallyproduceamapshowinghowimportanteachpixelofaparticularpictureisforitsclassification,asshowninFigure8.Whileasaliencymapoftenidentifiesrelevantregionsandprovidesatypeofquantification(i.e.,importanceforeachpixel),thereareacoupleoflimitations:1)sinceasaliencymapTestingwithConceptActivationVectors(TCAV)ThisclassifiervlC∈RmisalinearCAVfortheconceptC.3.3.DirectionalDerivativesandConceptualSensitivityInterpretabilitymethodslikesaliencymapsusethegradientsoflogitvalueswithrespecttoindividualinputfeatures,likepixels,andcompute∂hk(x)∂xa,bwherehk(x)isthelogitforadatapointxforclasskandxa,bisapixelatposition(a,b)inx.TestingwithConceptActivationVectors(TCAV)usingTCAV(Section4.2.1).Forfurthervalidation,wecre-ateadatasetandsettingswherewehaveanapproximatedgroundtruthforTCAVQ.WeshowthatTCAVTestingwithConceptActivationVectors(TCAV)Figure4.RelativeTCAVforalllayersinGoogleNet(Szegedyetal.,2015)andlastthreelayersinInceptionV3(Szegedyetal.,2016)forconfirmation(e.g.,fireengine),discoveringbiases(e.g.,rugby,apron),andquantitativeconfirmationforpreviouslyqualita-tivefindingsin(Mordvintsevetal.,2015;Stock&Cisse,2017)(e.g.,dumbbell,ping-pongball).TCAVQsinlayersclosetothelogitlayer(red)representmoredirectinfluenceonthepredictionthanlowerTestingwithConceptActivationVectors(TCAV)Tothisendwecreateadatasetofthreearbitraryclasses(zebra,cab,andcucumber)withpotentiallynoisycaptionswrittenintheimage(exampleshowninFigure6).Thenoiseparameterp∈[0,1.0]controlstheprobabilitythattheimagecaptionagreeswiththeimageclass.Ifthereisnonoise(p=0),thecaptionalwaysagreeswiththeimagelabel,e.g.apictureofacabalwayscontainstheword“cab”atthebottom.Atp=.3,eachpicturehasa30%chanceofhavingthecorrectcaptionreplacedwitharandomword(e.g.“rabbit”).Wethentrain4networks,eachonadatasetwithadiffer-entnoiseparameterpin[0,1].Eachnetworkmaylearntopayattentiontoeitherimagesorcaptions(orboth)intheclassificationtask.Toobtainanapproximatedgroundtruthforwhichconcepteachnetworkpaidattention,wecantestthenetwork’sperformanceonimageswithoutcaptions.Ifthenetworkusedtheimageconceptforclassification,theperformanceshouldremainhigh.Ifnot,thenetworkperfor-mancewillsuffer.WecreateimageCAVsusingeachclass’simages,andcaptionCAVsusingcaptionswithotherpixelsintheimagerandomlyshuffled.4.3.1.QUANTITATIVEEVALUATIONOFTCAVFigure7.TCAVresultswithapproximatedgroundtruth:Bothcabandcucumberclasses,TCAVQcloselymatchesthegroundtruth.Forthecabclass,thenetworkusedimageconceptmorethanthecaptionconceptregardlessofthemodels.Overall,wefindthattheTCAVscorecloselymirrorstheconceptthatthenetworkpaidattentionto(Figure7).Accu-racyresultssuggestthat,whenclassifyingcabs,thenetworkusedtheimageconceptmorethanthecaptionconcept,re-gardlessofthenoiseparameter.However,whenclassifyingcucumbers,thenetworksometimespaidattentiontothecaptionconceptandsometimestheimageconcept.Figure7showsthattheTCAVQcloselymatchesthisgroundtruth.Inthecabclass,theTCAVQfortheimageconceptishigh,con-sistentwithitshightestperformanceoncaption-lessimages.Inthecucumberclass,theTCAVQforTestingwithConceptActivationVectors(TCAV)Figure9.Forthecabclass,thegroundtruthwasthattheimageconceptwasmoreimportantthanthecaptionconcept.However,whenlookingatsaliencymaps,humansperceivedTestingwithConceptActivationVectors(TCAV)ACKNOWLEDGMENTSWewouldliketothankDanielSmilkovforhelpfuldiscus-sions.WethankAlexanderMordvintsevforprovidingtfzoocode.WealsothankEthanRElenberg,DavidAlvarezTestingwithConceptActivationVectors(TCAV)AppendixInthisappendix,weshowotherexperimentsweconductedandadditionalresultsandfigures.A.TCAVonadversarialexamplesprediction:zebrazebrazebraFigure11.TwotypesofTestingTestingwithConceptActivationVectors(TCAV)Figure14.EmpiricaldeepdreamusingCAVsforeachlayerinGooglenet.TestingwithTestingwithConceptActivationVectors(TCAV)ReferencesAdebayo,Julius,Gilmer,Justin,Goodfellow,Ian,andKim,Been.Localexplanationmethodsfordeepneuralnet-workslacksensitivitytoparametervalues.arXivpreprint,2018.Alain,GuillaumeandTestingwithConceptActivationVectors(TCAV)Russakovsky,Olga,Deng,Jia,Su,Hao,Krause,Jonathan,Satheesh,Sanjeev,Ma,Sean,Huang,Zhiheng,Karpa-thy,Andrej,Khosla,Aditya,Bernstein,Michael,etal.Imagenetlargescalevisualrecognitionchallenge.Inter-nationalJournalofComputerVision,115(3):211–252,2015.Salvatore,Christian,Cerasa,Antonio,Castiglioni,Isabella,Gallivanone,F,Augimeri,A,Lopez,M,Arabia,G,Morelli,M,Gilardi,MC,andQuattrone,A.Machinelearningonbrainmridatafordifferentialdiagnosisofparkinson’sdiseaseandprogressivesupranuclearpalsy.JournalofNeuroscienceMethods,222:230–237,2014.Selvaraju,RamprasaathR,Das,Abhishek,Vedantam,Ra-makrishna,Cogswell,Michael,Parikh,Devi,andBatra,Dhruv.Grad-cam:Whydidyousaythat?arXivpreprintarXiv:1611.07450,2016.Smilkov,Daniel,Thorat,Nikhil,Kim,Been,Viégas,Fer-nanda,andWattenberg,Martin.Smoothgrad:removingnoisebyaddingnoise.arXivpreprintarXiv:1706.03825,2017.Stock,PierreandCisse,Moustapha.Convnetsandim-agenetbeyondaccuracy:Explanations,biasdetection,adversarialexamplesandmodelcriticism.arXivpreprintarXiv:1711.11443,2017.Sundararajan,Mukund,Taly,Ankur,andYan,Qiqi.Ax-iomaticattributionfordeepnetworks.arXivpreprintarXiv:1703.01365,2017.Szegedy,Christian,Zaremba,Wojciech,Sutskever,Ilya,Bruna,Joan,Erhan,Dumitru,Goodfellow,Ian,andFer-gus,Rob.Intriguingpropertiesofneuralnetworks.arXivpreprintarXiv:1312.6199,2013.Szegedy,Christian,Liu,Wei,Jia,Yangqing,Sermanet,Pierre,Reed,Scott,Anguelov,Dragomir,Erhan,Dumitru,Vanhoucke,Vincent,Rabinovich,Andrew,etal.Goingdeeperwithconvolutions.ComputerVisionandPatternRecognition,2015.Szegedy,Christian,Vanhoucke,Vincent,Ioffe,Sergey,Shlens,Jon,andWojna,Zbigniew.Rethinkingthein-ceptionarchitectureforcomputervision.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pp.2818–2826,2016.Tibshirani,Robert.Regressionshrinkageandselectionviathelasso.JournaloftheRoyalStatisticalSociety,SeriesB,58:267–288,1994.Ustun,Berk,Tracà,Stefano,andRudin,Cynthia.Super-sparselinearintegermodelsforinterpretableclassifica-tion.arXivpreprintarXiv:1306.6677,2013.Zeiler,MatthewDandFergus,Rob.Visualizingandunder-standingconvolutionalnetworks.InEuropeanconferenceoncomputervision,pp.818–833.Springer,2014.Zhu,Jun-Yan,Park,Taesung,Isola,Phillip,andEfros,AlexeiA.Unpairedimage-to-imagetranslationusingcycle-consistentadversarialnetworks.arXivpreprintarXiv:1703.10593,2017.Zou,Hui,Hastie,Trevor,andTibshirani,Robert.Sparseprincipalcomponentanalysis.Journalof