Multi-View Clustering via Canonical Correlation Analysis
Kamalika Chaudhuri kamalika@soe.ucsd.edu
UC San Diego, 9500 Gilman Drive, La Jolla, CA
Sham M. Kakade sham@tti-c.org
Karen Livescu klivescu@tti-c.org
Karthik Sridharan karthik@tti-c.org
Toyota Technological Institute at Chicago, 6045 S. Kenwood Ave., Chicago, IL
Keywords: multi-view learning, clustering, canonical correlation analysis
Abstract
Clustering data in high dimensions is be-
lieved to be a hard problem in general. A
number of efficient clustering algorithms de-
veloped in recent years address this prob-
lem by projecting the data into a lower-
dimensional subspace, e.g. via Principal
Components Analysis (PCA) or random pro-
jections, before clustering. Here, we consider
constructing such projections using multiple
views of the data, via Canonical Correlation
Analysis (CCA).
Under the assumption that the views are un-
correlated given the cluster label, we show
that the separation conditions required for
the algorithm to be successful are signifi-
cantly weaker than prior results in the lit-
erature. We provide results for mixtures
of Gaussians and mixtures of log concave
distributions. We also provide empirical
support from audio-visual speaker clustering
(where we desire the clusters to correspond to
speaker ID) and from hierarchical Wikipedia
document clustering (where one view is the
words in the document and the other is the
link structure).
1. Introduction
The multi-view approach to learning is one in which
we have ‚Äòviews‚Äô of the data (sometimes in a rather ab-
stract sense) and the goal is to use the relationship be-
tween these views to alleviate the difficulty of a learn-
Appearing in Proceedings of the 26th
International Confer-
ence on Machine Learning, Montreal, Canada, 2009. Copy-
right 2009 by the author(s)/owner(s).
ing problem of interest [BM98, KF07, AZ07]. In this
work, we explore how having two ‚Äòviews‚Äô makes the
clustering problem significantly more tractable.
Much recent work has been done on understanding
under what conditions we can learn a mixture model.
The basic problem is as follows: We are given indepen-
dent samples from a mixture of k distributions, and
our task is to either: 1) infer properties of the under-
lying mixture model (e.g. the mixing weights, means,
etc.) or 2) classify a random sample according to which
distribution in the mixture it was generated from.
Under no restrictions on the underlying mixture, this
problem is considered to be hard. However, in many
applications, we are only interested in clustering the
data when the component distributions are ‚Äúwell sep-
arated‚Äù. In fact, the focus of recent clustering algo-
rithms [Das99, VW02, AM05, BV08] is on efficiently
learning with as little separation as possible. Typ-
ically, the separation conditions are such that when
given a random sample from the mixture model, the
Bayes optimal classifier is able to reliably recover
which cluster generated that point.
This work makes a natural multi-view assumption:
that the views are (conditionally) uncorrelated, con-
ditioned on which mixture component generated the
views. There are many natural applications for which
this assumption applies. For example, we can con-
sider multi-modal views, with one view being a video
stream and the other an audio stream, of a speaker ‚Äî
here, conditioned on the speaker identity and maybe
the phoneme (both of which could label the generat-
ing cluster), the views may be uncorrelated. A second
example is the words and link structure in a document
from a corpus such as Wikipedia ‚Äì here, conditioned
on the category of each document, the words in it and
Multi-View Clustering via Canonical Correlation Analysis
its link structure may be uncorrelated. In this paper,
we provide experiments for both settings.
Under this multi-view assumption, we provide a sim-
ple and efficient subspace learning method, based
on Canonical Correlation Analysis (CCA). This algo-
rithm is affine invariant and is able to learn with some
of the weakest separation conditions to date. The in-
tuitive reason for this is that under our multi-view
assumption, we are able to (approximately) find the
low-dimensional subspace spanned by the means of
the component distributions. This subspace is impor-
tant, because, when projected onto this subspace, the
means of the distributions are well-separated, yet the
typical distance between points from the same distri-
bution is smaller than in the original space. The num-
ber of samples we require to cluster correctly scales
as O(d), where d is the ambient dimension. Finally,
we show through experiments that CCA-based algo-
rithms consistently provide better performance than
standard PCA-based clustering methods when applied
to datasets in the two quite different domains of audio-
visual speaker clustering and hierarchical Wikipedia
document clustering by category.
Our work adds to the growing body of results which
show how the multi-view framework can alleviate the
difficulty of learning problems.
Related Work. Most provably efficient clustering
algorithms first project the data down to some low-
dimensional space and then cluster the data in this
lower dimensional space (an algorithm such as sin-
gle linkage usually suffices here). Typically, these al-
gorithms also work under a separation requirement,
which is measured by the minimum distance between
the means of any two mixture components.
One of the first provably efficient algorithms for learn-
ing mixture models is due to [Das99], who learns
a mixture of spherical Gaussians by randomly pro-
jecting the mixture onto a low-dimensional subspace.
[VW02] provide an algorithm with an improved sepa-
ration requirement that learns a mixture of k spheri-
cal Gaussians, by projecting the mixture down to the
k-dimensional subspace of highest variance. [KSV05,
AM05] extend this result to mixtures of general Gaus-
sians; however, they require a separation propor-
tional to the maximum directional standard deviation
of any mixture component. [CR08] use a canonical
correlations-based algorithm to learn mixtures of axis-
aligned Gaussians with a separation proportional to
œÉ‚àó
, the maximum directional standard deviation in the
subspace containing the means of the distributions.
Their algorithm requires a coordinate-independence
property, and an additional ‚Äúspreading‚Äù condition.
None of these algorithms are affine invariant.
Finally, [BV08] provide an affine-invariant algorithm
for learning mixtures of general Gaussians, so long as
the mixture has a suitably low Fisher coefficient when
in isotropic position. However, their separation in-
volves a large polynomial dependence on 1
wmin
.
The two results most closely related to ours are the
work of [VW02] and [CR08]. [VW02] show that it is
sufficient to find the subspace spanned by the means
of the distributions in the mixture for effective cluster-
ing. Like our algorithm, [CR08] use a projection onto
the top k ‚àí1 singular value decomposition subspace of
the canonical correlations matrix. They also require
a spreading condition, which is related to our require-
ment on the rank. We borrow techniques from both of
these papers.
[BL08] propose a similar algorithm for multi-view clus-
tering, in which data is projected onto the top direc-
tions obtained by kernel CCA across the views. They
show empirically that for clustering images using the
associated text as a second view (where the target clus-
tering is a human-defined category), CCA-based clus-
tering methods out-perform PCA-based algorithms.
This Work. Our input is data on a fixed set of ob-
jects from two views, where View j is assumed to be
generated by a mixture of k Gaussians (Dj
1, . . . , Dj
k),
for j = 1, 2. To generate a sample, a source i is picked
with probability wi, and x(1)
and x(2)
in Views 1 and
2 are drawn from distributions D1
i and D2
i . Following
prior theoretical work, our goal is to show that our al-
gorithm recovers the correct clustering, provided the
input mixture obeys certain conditons.
We impose two requirements on these mixtures. First,
we require that conditioned on the source, the two
views are uncorrelated. Notice that this is a weaker
restriction than the condition that given source i, the
samples from D1
i and D2
i are drawn independently.
Moreover, this condition allows the distributions in the
mixture within each view to be completely general, so
long as they are uncorrelated across views. Although
we do not prove this, our algorithm seems robust to
small deviations from this assumption.
Second, we require the rank of the CCA matrix across
the views to be at least k ‚àí 1, when each view is in
isotropic position, and the k ‚àí 1-th singular value of
this matrix to be at least Œªmin. This condition ensures
that there is sufficient correlation between the views.
If the first two conditions hold, then we can recover
the subspace containing the means in both views.
In addition, for mixtures of Gaussians, if in at least
one view, say View 1, we have that for every pair of
Multi-View Clustering via Canonical Correlation Analysis
distributions i and j in the mixture,
||¬µ1
i ‚àí ¬µ1
j || > CœÉ‚àó
k1/4
p
log(n/Œ¥)
for some constant C, then our algorithm can also de-
termine which component each sample came from.
Here ¬µ1
i is the mean of the i-th component in View
1 and œÉ‚àó
is the maximum directional standard devi-
ation in the subspace containing the means in View
1. Moreover, the number of samples required to learn
this mixture grows (almost) linearly with d.
This separation condition is considerably weaker than
previous results in that œÉ‚àó
only depends on the direc-
tional variance in the subspace spanned by the means,
which can be considerably lower than the maximum di-
rectional variance over all directions. The only other
algorithm which provides affine-invariant guarantees is
due to [BV08] ‚Äî the implied separation in their work
is rather large and grows with decreasing wmin, the
minimum mixing weight. To get our improved sam-
ple complexity bounds, we use a result due to [RV07]
which may be of independent interest.
We stress that our improved results are really due to
the multi-view condition. Had we simply combined the
data from both views, and applied previous algorithms
on the combined data, we could not have obtained our
guarantees. We also emphasize that for our algorithm
to cluster successfully, it is sufficient for the distribu-
tions in the mixture to obey the separation condition
in one view, so long as the multi-view and rank condi-
tions are obeyed.
Finally, we study through experiments the perfor-
mance of CCA-based algorithms on data sets from two
different domains. First, we experiment with audio-
visual speaker clustering, in which the two views are
audio and face images of a speaker, and the target
cluster variable is the speaker. Our experiments show
that CCA-based algorithms perform better than PCA-
based algorithms on audio data and just as well on
image data, and are more robust to occlusions of the
images. For our second experiment, we cluster docu-
ments in Wikipedia. The two views are the words and
the link structure in a document, and the target cluster
is the category. Our experiments show that a CCA-
based hierarchical clustering algorithm out-performs
PCA-based hierarchical clustering for this data.
2. The Setting
We assume that our data is generated by a mixture
of k distributions. In particular, we assume that we
obtain samples x = (x(1)
, x(2)
), where x(1)
and x(2)
are the two views, which live in the vector spaces V1
of dimension d1 and V2 of dimension d2, respectively.
We let d = d1+d2. Let ¬µj
i , for i = 1, . . . , k and j = 1, 2,
be the mean of distribution i in view j, and let wi be
the mixing weight for distribution i.
For simplicity, we assume that the data have mean 0.
We denote the covariance matrix of the data as:
Œ£ = E[xx>
], Œ£11 = E[x(1)
(x(1)
)>
]
Œ£22 = E[x(2)
(x(2)
)>
], Œ£12 = E[x(1)
(x(2)
)>
]
Hence, we have: Œ£ =

Œ£11 Œ£21
Œ£12 Œ£22

(1)
The multi-view assumption we work with is as follows:
Assumption 1 (Multi-View Condition) We assume
that conditioned on the source distribution l in the mix-
ture (where l = i is picked with probability wi), the two
views are uncorrelated. More precisely, we assume that
for all i ‚àà [k],
E[x(1)
(x(2)
)>
|l = i] = E[x(1)
|l = i]E[(x(2)
)>
|l = i]
This assumption implies that: Œ£12 =
P
i wi¬µ1
i ¬∑ (¬µ2
i )T
.
To see this, observe that
E[x(1)
(x(2)
)>
] =
X
i
EDi [x(1)
(x(2)
)>
] Pr[Di]
=
X
i
wiEDi
[x(1)
] ¬∑ EDi [(x(2)
)>
]
=
X
i
wi¬µ1
i ¬∑ (¬µ2
i )T
(2)
As the distributions are in isotropic position, we ob-
serve that
P
i wi¬µ1
i =
P
i wi¬µ2
i = 0. Therefore, the
above equation shows that the rank of Œ£12 is at most
k ‚àí1. We now assume that it has rank precisely k ‚àí1.
Assumption 2 (Non-Degeneracy Condition) We as-
sume that Œ£12 has rank k ‚àí 1 and that the minimal
non-zero singular value of Œ£12 is Œªmin > 0 (where we
are working in a coordinate system where Œ£11 and Œ£22
are identity matrices).
For clarity of exposition, we also work in an isotropic
coordinate system in each view. Specifically, the ex-
pected covariance matrix of the data, in each view, is
the identity matrix, i.e. Œ£11 = Id1
, Œ£22 = Id2
.
As our analysis shows, our algorithm is robust to er-
rors, so we assume that data is whitened as a pre-
processing step.
One way to view the Non-Degeneracy Assumption is
in terms of correlation coefficients. Recall that for two
Multi-View Clustering via Canonical Correlation Analysis
directions u ‚àà V1 and v ‚àà V2, the correlation coeffi-
cient is defined as:
œÅ(u, v) =
E[(u ¬∑ x(1)
)(v ¬∑ x(2)
)]
p
E[(u ¬∑ x(1))2]E[(v ¬∑ x(2))2]
.
An alternative definition of Œªmin is the min-
imal non-zero correlation coefficient, Œªmin =
minu,v:œÅ(u,v)6=0 œÅ(u, v). Note 1 ‚â• Œªmin > 0.
We use b
Œ£11 and b
Œ£22 to denote the sample covariance
matrices in views 1 and 2 respectively. We use b
Œ£12 to
denote the sample covariance matrix combined across
views 1 and 2. We assume these are obtained through
empirical averages from i.i.d. samples from the under-
lying distribution.
3. The Clustering Algorithm
The following lemma provides the intuition for our al-
gorithm.
Lemma 1 Under Assumption 2, if U, D, V is the
‚Äòthin‚Äô SVD of Œ£12 (where the thin SVD removes all
zero entries from the diagonal), then the subspace
spanned by the means in view 1 is precisely the col-
umn span of U (and we have the analogous statement
for view 2).
The lemma is a consequence of Equation 2 and the
rank assumption. Since samples from a mixture are
well-separated in the space containing the means of the
distributions, the lemma suggests the following strat-
egy: use CCA to (approximately) project the data
down to the subspace spanned by the means to get
an easier clustering problem, and then apply standard
clustering algorithms in this space.
Our clustering algorithm, based on the above idea, is
stated below. We can show that this algorithm clusters
correctly with high probability, when the data in at
least one of the views obeys a separation condition, in
addition to our assumptions.
The input to the algorithm is a set of samples S, and
a number k, and the output is a clustering of these
samples into k clusters. For this algorithm, we assume
that the data obeys the separation condition in View
1; an analogous algorithm can be applied when the
data obeys the separation condition in View 2 as well.
Algorithm 1.
1. Randomly partition S into two subsets A and B
of equal size.
2. Let b
Œ£12(A) (b
Œ£12(B) resp.) denote the empirical
covariance matrix between views 1 and 2, com-
puted from the sample set A (B resp.). Com-
pute the top k ‚àí 1 left singular vectors of b
Œ£12(A)
(b
Œ£12(B) resp.), and project the samples in B (A
resp.) on the subspace spanned by these vectors.
3. Apply single linkage clustering [DE04] (for mix-
tures of log-concave distributions), or the algo-
rithm in Section 3.5 of [AK05] (for mixtures of
Gaussians) on the projected examples in View 1.
We note that in Step 3, we apply either single link-
age or the algorithm of [AK05]; this allows us to show
theoretically that if the distributions in the mixture
are of a certain type, and given the right separation
conditions, the clusters can be recovered correctly. In
practice, however, these algorithms do not perform as
well due to lack of robustness, and one would use an
algorithm such as k-means or EM to cluster in this
low-dimensional subspace. In particular, a variant of
the EM algorithm has been shown [DS00] to cluster
correctly mixtures of Gaussians, under certain condi-
tions.
Moreover, in Step 1, we divide the data set into two
halves to ensure independence between Steps 2 and 3
for our analysis; in practice, however, these steps can
be executed on the same sample set.
Main Results. Our main theorem is as follows.
Theorem 1 (Gaussians) Suppose the source distri-
bution is a mixture of Gaussians, and suppose As-
sumptions 1 and 2 hold. Let œÉ‚àó
be the maximum di-
rectional standard deviation of any distribution in the
subspace spanned by {¬µ1
i }k
i=1. If, for each pair i and j
and for a fixed constant C,
||¬µ1
i ‚àí ¬µ1
j || ‚â• CœÉ‚àó
k1/4
r
log(
kn
Œ¥
)
then, with probability 1‚àíŒ¥, Algorithm 1 correctly clas-
sifies the examples if the number of examples used is
c ¬∑
d
(œÉ‚àó)2Œª2
minw2
min
log2
(
d
œÉ‚àóŒªminwmin
) log2
(1/Œ¥)
for some constant c.
Here we assume that a separation condition holds in
View 1, but a similar theorem also applies to View 2.
An analogous theorem can also be shown for mixtures
of log-concave distributions.
Theorem 2 (Log-concave Distributions)
Suppose the source distribution is a mixture of
log-concave distributions, and suppose Assumptions
Multi-View Clustering via Canonical Correlation Analysis
1 and 2 hold. Let œÉ‚àó
be the maximum directional
standard deviation of any distribution in the subspace
spanned by {¬µ1
i }k
i=1. If, for each pair i and j and for
a fixed constant C,
||¬µ1
i ‚àí ¬µ1
j || ‚â• CœÉ‚àó
‚àö
k log(
kn
Œ¥
)
then, with probability 1‚àíŒ¥, Algorithm 1 correctly clas-
sifies the examples if the number of examples used is
c ¬∑
d
(œÉ‚àó)2Œª2
minw2
min
log3
(
d
œÉ‚àóŒªminwmin
) log2
(1/Œ¥)
for some constant c.
The proof of this theorem is very similar to Theorem 1,
and follows from the proof of Theorem 1, along with
standard results on log-concave probability distribu-
tions ‚Äì see [KSV05, AM05]. We do not provide a proof
here due to space constraints.
4. Analyzing Our Algorithm
In this section, we prove our main theorems.
Notation. In the sequel, we assume that we are given
samples from a mixture which obeys Assumptions 2
and 1. We use the notation S1
(resp. S2
) to denote
the subspace containing the centers of the distributions
in the mixture in View 1 (resp. View 2), and notation
S01
(resp. S02
) to denote the orthogonal complement to
the subspace containing the centers of the distributions
in the mixture in View 1 (resp. View 2).
For any matrix A, we use ||A|| to denote the L2 norm
or maximum singular value of A.
Proofs. Now, we are ready to prove our main the-
orem. First, we show the following two lemmas,
which demonstrate properties of the expected cross-
correlational matrix across the views. Their proofs
are immediate from Assumptions 2 and 1.
Lemma 2 Let v1
and v2
be any vectors in S1
and S2
respectively. Then, |(v1
)T
Œ£12v2
| > Œªmin.
Lemma 3 Let v1
(resp. v2
) be any vector in S01
(resp. S02
). Then, for any u1
‚àà V1 and u2 ‚àà V2,
(v1
)T
Œ£12u2
= (u1
)T
Œ£12v2
= 0.
Next, we show that given sufficiently many samples,
the subspace spanned by the top k ‚àí 1 singular vec-
tors of b
Œ£12 still approximates the subspace containing
the means of the distributions comprising the mixture.
Finally, we use this fact, along with some results in
[AK05] to prove Theorem 1. Our main lemma of this
section is the following.
Lemma 4 (Projection Subspace Lemma) Let v1
(resp. v2
) be any vector in S1
(resp. S2
). If the num-
ber of samples n > c d
œÑ2Œª2
minwmin
log2
( d
œÑŒªminwmin
) log2
(1
Œ¥ )
for some constant c, then, with probability 1 ‚àí Œ¥, the
length of the projection of v1
(resp. v2
) in the sub-
space spanned by the top k ‚àí 1 left (resp. right) sin-
gular vectors of b
Œ£12 is at least
‚àö
1 ‚àí œÑ2||v1
|| (resp.
‚àö
1 ‚àí œÑ2||v2
||).
The main tool in the proof of Lemma 4 is the following
lemma, which uses a result due to [RV07].
Lemma 5 (Sample Complexity Lemma) If the
number of samples
n > c ¬∑
d
2wmin
log2
(
d
wmin
) log2
(
1
Œ¥
)
for some constant c, then, with probability at least 1‚àíŒ¥,
||b
Œ£12 ‚àí Œ£12|| ‚â§ .
A consequence of Lemmas 5, 2 and 3 is the following.
Lemma 6 Let n > C d
2wmin
log2
( d
wmin
) log2
(1
Œ¥ ), for
some constant C. Then, with probability 1 ‚àí Œ¥, the
top k ‚àí 1 singular values of b
Œ£12 have value at least
Œªmin ‚àí . The remaining min(d1, d2) ‚àí k + 1 singular
values of b
Œ£12 have value at most .
The proof follows by a combination of Lemmas 2,3, 5
and a triangle inequality.
Proof:(Of Lemma 5) To prove this lemma, we apply
Lemma 7. Observe the block representation of Œ£ in
Equation 1. Moreover, with Œ£11 and Œ£22 in isotropic
position, we have that the L2 norm of Œ£12 is at most
1. Using the triangle inequality, we can write:
||b
Œ£12 ‚àíŒ£12|| ‚â§
1
2
(||b
Œ£‚àíŒ£||+||b
Œ£11 ‚àíŒ£11||+||b
Œ£22 ‚àíŒ£22||)
(where we applied the triangle inequality to the 2 √ó 2
block matrix with off-diagonal entries b
Œ£12 ‚àí Œ£12 and
with 0 diagonal entries). We now apply Lemma 7 three
times, on b
Œ£11 ‚àí Œ£11, b
Œ£22 ‚àí Œ£22, and a scaled version
of b
Œ£ ‚àí Œ£. The first two applications follow directly.
For the third application, we observe that Lemma 7
is rotation invariant, and that scaling each covariance
value by some factor s scales the norm of the matrix
by at most s. We claim that we can apply Lemma
7 on b
Œ£ ‚àí Œ£ with s = 4. Since the covariance of any
two random variables is at most the product of their
standard deviations, and since Œ£11 and Œ£22 are Id1
and Id2 respectively, the maximum singular value of
Œ£12 is at most 1; so the maximum singular value of Œ£
is at most 4. Our claim follows. The lemma follows by
plugging in n as a function of , d and wmin 
Multi-View Clustering via Canonical Correlation Analysis
Lemma 7 Let X be a set of n points generated by
a mixture of k Gaussians over Rd
, scaled such that
E[x ¬∑ xT
] = Id. If M is the sample covariance matrix
of X, then, for n large enough, with probability at least
1 ‚àí Œ¥,
||M ‚àí E[M]|| ‚â§ C ¬∑
q
d log n log(2n
Œ¥ ) log(1/Œ¥)
‚àö
wminn
where C is a fixed constant, and wmin is the minimum
mixing weight of any Gaussian in the mixture.
Proof: To prove this lemma, we use a concentra-
tion result on the L2-norms of matrices due to [RV07].
We observe that each vector xi in the scaled space
is generated by a Gaussian with some mean ¬µ and
maximum directional variance œÉ2
. As the total vari-
ance of the mixture along any direction is at most 1,
wmin(¬µ2
+ œÉ2
) ‚â§ 1. Therefore, for all samples xi, with
probability at least 1‚àíŒ¥/2, ||xi|| ‚â§ ||¬µ||+œÉ
q
d log(2n
Œ¥ ).
We condition on the fact that the event ||xi|| ‚â§
||¬µ|| + œÉ
q
d log(2n
Œ¥ ) happens for all i = 1, . . . , n. The
probability of this event is at least 1 ‚àí Œ¥/2.
Conditioned on this event, the distributions of the vec-
tors xi are independent. Therefore, we can apply The-
orem 3.1 in [RV07] on these conditional distributions,
to conclude that:
Pr[||M ‚àí E[M]|| > t] ‚â§ 2e‚àícnt2
/Œõ2
log n
where c is a constant, and Œõ is an upper bound on the
norm of any vector ||xi||. The lemma follows by plug-
ging in t =
q
Œõ2 log(4/Œ¥) log n
cn , and Œõ ‚â§
2
‚àö
d log(2n/Œ¥)
‚àö
wmin
. 
Proof: (Of Lemma 4) For the sake of contradiction,
suppose there exists a vector v1
‚àà S1
such that the
projection of v1
on the top k‚àí1 left singular vectors of
b
Œ£12 is equal to
‚àö
1 ‚àí œÑÃÉ2||v1
||, where œÑÃÉ > œÑ. Then, there
exists some unit vector u1
in V1 in the orthogonal com-
plement of the space spanned by the top k ‚àí1 left sin-
gular vectors of b
Œ£12 such that the projection of v1
on
u1
is equal to œÑÃÉ||v1
||. This vector u1
can be written as:
u1
= œÑÃÉv1
+(1‚àíœÑÃÉ2
)1/2
y1
, where y1
is in the orthogonal
complement of S1
. From Lemma 2, there exists some
vector u2
in S2
, such that (v1
)>
Œ£12u2
‚â• Œªmin; from
Lemma 3, for this vector u2
, (u1
)>
Œ£12u2
‚â• œÑÃÉŒªmin.
If n > c d
œÑÃÉ2Œª2
minwmin
log2
( d
œÑÃÉŒªminwmin
) log2
(1
Œ¥ ), then, from
Lemma 6, (u1
)T b
Œ£12u2
‚â• œÑÃÉ
2 Œªmin.
Now, since u1 is in the orthogonal complement of
the subspace spanned by the top k ‚àí 1 left singu-
lar vectors of b
Œ£12, for any vector y2
in the subspace
spanned by the top k ‚àí1 right singular vectors of b
Œ£12,
(u1)> b
Œ£12y2
= 0. This, in turn, means that there ex-
ists a vector z2
‚àà V2, the orthogonal complement of
the subspace spanned by the top k ‚àí 1 right singular
vectors of b
Œ£12 such that (u1
)T b
Œ£12z2
‚â• œÑÃÉ
2 Œªmin. This
implies that the k-th singular value of b
Œ£12 is at least
œÑÃÉ
2 Œªmin. However, from Lemma 6, all except the top
k ‚àí 1 singular values of b
Œ£12 are at most œÑ
3 Œªmin, which
is a contradiction. 
Proof:(Of Theorem 1) From Lemma 4, if n >
Cd
œÑ2Œª2
minwmin
log2
( d
œÑŒªminwmin
) log2
(1
Œ¥ ), then, with proba-
bility at least 1 ‚àí Œ¥, the projection of any vector v
in S1
or S2
onto the subspace returned by Step 2 of
Algorithm 1 has length at least
‚àö
1 ‚àí œÑ2||v||. There-
fore, the maximum directional variance of any Di in
this subspace is at most (1 ‚àí œÑ2
)(œÉ‚àó
)2
+ œÑ2
œÉ2
, where
œÉ2
is the maximum directional variance of any Di.
When œÑ ‚â§ œÉ‚àó
œÉ , this is at most 2(œÉ‚àó
)2
. From the
isotropic condition, œÉ ‚â§ 1
‚àö
wmin
. Therefore, when
n > Cd
(œÉ‚àó)2Œª2
minw2
min
log2
( d
œÉ‚àóŒªminwmin
) log2
(1
Œ¥ ), the maxi-
mum directional variance of any Di in the mixture in
the space output by Step 2 is at most 2(œÉ‚àó
)2
.
Since A and B are random partitions of the sample
set S, the subspace produced by the action of Step
2 of Algorithm 1 on the set A is independent of B,
and vice versa. Therefore, when projected onto the
top k ‚àí 1 SVD subspace of b
Œ£12(A), the samples from
B are distributed as a mixture of (k ‚àí 1)-dimensional
Gaussians. The theorem follows from the bounds in
the previous paragraph, and Theorem 1 of [AK05]. 
5. Experiments
5.1. Audio-visual speaker clustering
In the first set of experiments, we consider clustering
either audio or face images of speakers. We use 41
speakers from the VidTIMIT database [San08], speak-
ing 10 sentences (about 20 seconds) each, recorded at
25 frames per second in a studio environment with no
significant lighting or pose variation. The audio fea-
tures are standard 12-dimensional mel cepstra [DM80]
and their derivatives and double derivatives computed
every 10ms over a 20ms window, and finally concate-
nated over a window of 440ms centered on the current
frame, for a total of 1584 dimensions. The video fea-
tures are pixels of the face region extracted from each
image (2394 dimensions). We consider the target clus-
ter variable to be the speaker. We use either CCA
or PCA to project the data to a lower dimensional-
ity N. In the case of CCA, we initially project to an
intermediate dimensionality M using PCA to reduce
Multi-View Clustering via Canonical Correlation Analysis
PCA CCA
Images 1.1 1.4
Audio 35.3 12.5
Images + occlusion 6.1 1.4
Audio + occlusion 35.3 12.5
Images + translation 3.4 3.4
Audio + translation 35.3 13.4
Table 1. Conditional perplexities of the speaker given the
cluster, using PCA or CCA bases. ‚Äú+ occlusion‚Äù and ‚Äú+
translation‚Äù indicate that the images are corrupted with
occlusion/translation; the audio is unchanged, however.
the effects of spurious correlations. For the results re-
ported here, typical values (selected using a held-out
set) are N = 40 and M = 100 for images and 1000
for audio. For CCA, we randomize the vectors of one
view in each sentence, to reduce correlations between
the views due to other latent variables such as the
current phoneme. We then cluster either view using
k-means into 82 clusters (2 per speaker). To alleviate
the problem of local minima found by k-means, each
clustering consists of 5 runs of k-means, and the one
with the lowest score is taken as the final clustering.
Similarly to [BL08], we measure clustering perfor-
mance using the conditional entropy of the speaker
s given the cluster c, H(s|c). We report the results
in terms of conditional perplexity, 2H(s|c)
, which is
the mean number of speakers corresponding to each
cluster. Table 1 shows results on the raw data, as
well as with synthetic occlusions and translations of
the image data. Considering the clean visual environ-
ment, we expect PCA to do very well on the image
data. Indeed, PCA provides an almost perfect clus-
tering of the raw images and CCA does not improve
it. However, CCA far outperforms PCA when cluster-
ing the more challenging audio view. When synthetic
occlusions or translations are applied to the images,
the performance of PCA-based clustering is greatly de-
graded. CCA is unaffected in the case of occlusion; in
the case of translation, CCA-based image clustering
is degraded similarly to PCA, but audio clustering is
almost unaffected. In other words, even when the im-
age data are degraded, CCA is able to recover a good
clustering in at least one of the views. 1
For a more
detailed look at the clustering behavior, Figures 1(a-d)
show the distributions of clusters for each speaker.
1
The audio task is unusually challenging, as each fea-
ture vector corresponds to only a few phonemes. A typ-
ical speaker classification setting uses entire sentences. If
we force the cluster identity to be constant over each sen-
tence (the most frequent cluster label in the sentence), per-
formance improves greatly; e.g., in the ‚Äúaudio+occlusion‚Äù
case, the perplexity improves to 8.5 (PCA) and 2.1 (CCA).
5.2. Clustering Wikipedia articles
Next we consider the task of clustering Wikipedia ar-
ticles, based on either their text or their incoming and
outgoing links. The link structure L is represented as
a concatenation of ‚Äúto‚Äùand ‚Äúfrom‚Äù link incidence vec-
tors, where each element L(i) is the number of times
the current article links to/from article i. The article
text is represented as a bag-of-words feature vector,
i.e. the raw count of each word in the article. A lex-
icon of about 8 million words and a list of about 12
million articles were used to construct the two feature
vectors. Since the dimensionality of the feature vec-
tors is very high (over 20 million for the link view), we
use random projection to reduce the dimensionality to
a computationally manageable level.
We present clustering experiments on a subset of
Wikipedia consisting of 128,327 articles. We use either
PCA or CCA to reduce the feature vectors to the final
dimensionality, followed by clustering. In these experi-
ments, we use a hierarchical clustering procedure, as a
flat clustering is poor with either PCA or CCA (CCA
still usually outperforms PCA, however). In the hier-
archical procedure, all points are initially considered
to be in a single cluster. Next, we iteratively pick the
largest cluster, reduce the dimensionality using PCA
or CCA on the points in this cluster, and use k-means
to break the cluster into smaller sub-clusters (for some
fixed k), until we reach the total desired number of
clusters. The intuition for this is that different clus-
ters may have different natural subspaces.
As before, we evaluate the clustering using the condi-
tional perplexity of the article category a (as given by
Wikipedia) given the cluster c, 2H(a|c)
. For each arti-
cle we use the first category listed in the article. The
128,327 articles include roughly 15,000 categories, of
which we use the 500 most frequent ones, which cover
73,145 articles. While the clustering is performed on
all 128,327 articles, the reported entropies are for the
73,145 articles. Each sub-clustering consists of 10 runs
of k-means, and the one with the lowest k-means score
is taken as the final cluster assignment.
Figure 1(e) shows the conditional perplexity versus the
number of clusters for PCA and CCA based hierarchi-
cal clustering. For any number of clusters, CCA pro-
duces better clusterings, i.e. ones with lower perplex-
ity. In addition, the tree structures of the PCA/CCA-
based clusterings are qualitatively different. With
PCA based clustering, most points are assigned to a
few large clusters, with the remaining clusters being
very small. CCA-based hierarchical clustering pro-
duces more balanced clusters. To see this, in Fig-
ure 1(f) we show the perplexity of the cluster distribu-
Multi-View Clustering via Canonical Correlation Analysis
cluster
speaker
(a) AV: Audio, PCA basis
20 40 60 80
5
10
15
20
25
30
35
40
(c) AV: Images + occlusion, PCA basis
cluster
speaker
20 40 60 80
5
10
15
20
25
30
35
40
0 20 40 60 80 100 120
20
40
60
80
100
120
140
160
number of clusters
perplexity
(e) Wikipedia: Category perplexity
hierarchical CCA
hierarchical PCA
(b) AV: Audio, CCA basis
cluster
speaker
20 40 60 80
5
10
15
20
25
30
35
40
(d) AV: Images + occlusion, CCA basis
cluster
speaker
20 40 60 80
5
10
15
20
25
30
35
40
0 20 40 60 80 100 120
0
20
40
60
80
100
120
number of clusters
2
Entropy
(f) Wikipedia: Cluster perplexity
balanced clustering
hierarchical CCA
hierarchical PCA
Figure 1. (a-d) Distributions of cluster assignments per speaker in audio-visual experiments. The color of each cell (s, c)
corresponds to the empirical probability p(c|s) (darker = higher). (e-f) Wikipedia experiments: (e) Conditional perplexity
of article category given cluster (2H(a|c)
). (f) Perplexity of the cluster distribution (2H(c)
)
tion versus number of clusters. For about 25 or more
clusters, the CCA-based clustering has higher perplex-
ity, indicating a more uniform distribution of clusters.
References
[AK05] S. Arora and R. Kannan. Learning mixtures
of separated nonspherical Gaussians. Ann.
Applied Prob., 15(1A):69‚Äì92, 2005.
[AM05] D. Achlioptas and F. McSherry. On spec-
tral learning of mixtures of distributions. In
COLT, pages 458‚Äì469, 2005.
[AZ07] R. Kubota Ando and T. Zhang. Two-view
feature generation model for semi-supervised
learning. In ICML, pages 25‚Äì32, 2007.
[BL08] M. B. Blaschko and C. H. Lampert. Corre-
lational spectral clustering. In CVPR, 2008.
[BM98] A. Blum and T. Mitchell. Combining la-
beled and unlabeled data with co-training.
In COLT, pages 92‚Äì100, 1998.
[BV08] S. C. Brubaker and S. Vempala. Isotropic
PCA and affine-invariant clustering. In
FOCS, pages 551‚Äì560, 2008.
[CR08] K. Chaudhuri and S. Rao. Learning mixtures
of distributions using correlations and inde-
pendence. In COLT, pages 9‚Äì20, 2008.
[Das99] S. Dasgupta. Learning mixtures of Gaus-
sians. In FOCS, pages 634‚Äì644, 1999.
[DE04] G. Dunn and B. Everitt. An Introduction
to Mathematical Taxonomy. Dover Books,
2004.
[DM80] S. B. Davis and P. Mermelstein. Com-
parison of parametric representations for
monosyllabic word recognition in continu-
ously spoken sentences. IEEE Trans. Acous-
tics, Speech, and Signal Proc., 28(4):357‚Äì
366, 1980.
[DS00] S. Dasgupta and L. Schulman. A two-round
variant of EM for Gaussian mixtures. In
UAI, pages 152‚Äì159, 2000.
[KF07] S. M. Kakade and D. P. Foster. Multi-view
regression via canonical correlation analysis.
In COLT, pages 82‚Äì96, 2007.
[KSV05] R. Kannan, H. Salmasian, and S. Vempala.
The spectral method for general mixture
models. In COLT, pages 444‚Äì457, 2005.
[RV07] M. Rudelson and R. Vershynin. Sampling
from large matrices: An approach through
geometric functional analysis. Journal of the
ACM, 2007.
[San08] C. Sanderson. Biometric Person Recogni-
tion: Face, Speech and Fusion. VDM-Verlag,
2008.
[VW02] V. Vempala and G. Wang. A spectral algo-
rithm for learning mixtures of distributions.
In FOCS, pages 113‚Äì123, 2002.
