LearningtoWritewithCooperativeDiscriminatorsAriHoltzman†JanBuys†MaxwellForbes†AntoineBosselut†DavidGolub†YejinChoi†‡†PaulG.AllenSchoolofComputerScience&Engineering,Universitythenlearnstobalancethesediscriminatorsbyini-tiallyweighingthemuniformly,thencontinuallyupdatingitsweightsbycomparingthescoresthesystemgivestoitsowngeneratedcontinuationsandtothereferencecontinuation.Empiricalresults(§5)demonstratethatourlearningframeworkishighlyeffectiveinconvert-ingagenericRNNlanguagemodelintoasubstan-tiallystrongergenerator.Humanevaluationcon-firmsthatlanguagegeneratedbyourmodelispre-ferredoverthatofcompetitivebaselinesbyalargemargintrainingexamplestoguideeachmodeltofocusonadifferentaspectofGrice’sMaxims.Thediscrim-inatorscoresareinterpretedasclassificationprob-abilities(scaledwiththelogisticfunctionwherenecessary)andinterpolatedinData:contextx,beamsizek,samplingtemperaturetResult:bestcontinuationbest=Nonebeam=[x]forstep=0;step<maxsteps;step=step+1donextbeam=[]forcandidateinbeamdonextbeam.extend(nextk(candidate))ifterminationscore(candidate)>best.scorethenbest=candidate.append(term)endendforcandidateinnextbeamdo.scorewithmodelscandidate.score+=fλ(candidate)end.samplekcandidatesbyscorebeam=sample(nextbeam,k,t)endiflearningthenupdateλwithgradientdescentbycomparingbestagainstthegold.endreturnbestAlgorithm1:Inference/LearningintheLearningtoWriteFramework.sentenceisnotdirectlyentailedorcontradictedbyaprevioussentenceandnotthereverse.2Incontrasttoourothermodels,thescorethismodelreturnsonlycorrespondstoasubsequenceofthegivencontinuation,asthescoreisnotaccumu-latedacrosssentencesduringbeamsearch.Insteadthedecoderisguidedlocallytocontinuecompletesentencesthatarenotentailedorcontradictedbytheprevioustext.RelevanceModelTherelevancemodelencodesthemaximofRela-tionbypredictingwhetherthecontentofacandi-datecontinuationisrelevanttothegivencontext.Wetrainthemodeltodistinguishbetweentruecontinuationsandrandomcontinuationssampledfromother(human-written)endingsinthecorpus,conditionedonthegivencontext.Firstboththecontextandcontinuationse-quencesarepassedthroughaconvolutionallayer,followedbymaxpoolingtoobtainvectorrepresen-tationsofthesequences:a=maxpool(conva(e(x))),(7)b=maxpool(convb(e(y))).(8)2Ifthecurrentsentenceentailsapreviousoneitmaysim-plybeaddingmorespecificinformation,forinstance:“Hehatedbroccoli.Everytimeheatebroccolihewasremindedthatitwasthethinghehatedmost.”Thegoalofmaxpoolingistoobtainavectorrep-resentingthemostimportantsemanticinformationineachdimension.Thescoringfunctionisthendefinedassrel=wTl·(a◦b),(9)whereelement-wisemultiplicationofthecontextandcontinuationvectorswillamplifysimilarities.WeoptimizetherankingloglikelihoodLrel=X(x,yg)∈D,yr∼Dylogσ(srel(x,yg)−srel(x,yr)),(10)whereygisthegoldendingandyrisarandomlysampledending.LexicalStyleModelInpracticeRNNsgeneratetextthatexhibitmuchlesslexicaldiversitythantheirtrainingdata.Tocounterthiseffectweintroduceasimpledis-criminatorbasedonobservedlexicaldistributionswhichcaptureswritingstyleasexpressedthroughwordchoice.Thisclassifierthereforeencodesas-pectsBookCorpusTripAdvisorModelBLEUMeteorLengthVocabTrigramsBLEUMeteorLengthVocab%TrigramsL2W0.526.843.673.898.91.711.083.864.196.2ADAPTIVELM0.526.343.559.092.71.9411.294.152.692.5CACHELM0.334.637.931.044.91.367.252.139.257.0SEQ2SEQ0.324.036.723.033.71.848.059.233.957.0SEQGAN0.185.028.473.499.30.736.747.057.693.4REFERENCE100.0BookCorpusSpecificCriteriaOverallQualityL2Wvs.RepetitionContradictionRelevanceClarityBetterEqualWorseADAPTIVELM+0.48+0.18+0.12+0.1147%20%32%CACHELM+1.61+0.37+1.23+1.2186%6%8%SEQ2SEQ+1.01+0.54+0.83+0.8372%7%21%SEQGAN+0.20+0.32+0.61+0.6263%20%17%LMVS.REFERENCE-0.10-0.07-0.18-0.1041%7%52%L2WVS.REFERENCE+0.49+0.37+0.46+0.5553%18%29%TripAdvisorSpecificCriteriaOverallQualityL2Wvs.RepetitionContradictionRelevanceClarityBetterEqualWorseADAPTIVELM+0.23-0.02+0.19-0.0347%19%34%CACHELM+1.25+0.12+0.94+0.6977%9%14%SEQ2SEQ+0.64+0.04+0.50+0.4158%12%30%SEQGAN+0.53+0.01+0.49+0.0655%22%22%LMVS.REFERENCE-0.10-0.04-0.15-0.0638%10%52%L2WVS.REFERENCE-0.49-0.36-0.47-0.5025%18%57%Table2:Resultsofcrowd-sourcedevaluationondifferentaspectsofthegenerationqualityaswellasoverallqualityjudgments.Foreachsub-criteriawereporttheaverageofcomparativescoresonascalefrom-2to2.Fortheoverallqualityevaluationdecisionsareaggregatedover3annotatorsperexample.SEQ2SEQAsourevaluationcanbeframedassequence-to-sequencetransduction,wecompareagainstaseq2seqmodeldirectlytrainedtopredict5sentencecontinuationsfrom5sentencesofcon-text,usingtheOpenNMTattention-basedseq2seqimplementation(Kleinetal.,2017).SimilarlytoCACHELM,a50kvocabularywasusedandbeamsearchdecodingwasperformedwithabeamsizeof5.SEQGANFinally,asouruseofdiscrimina-torsisrelatedtoGenerativeAdversarialNetworks(GANs),weuseSeqGAN(Yuetal.,2017a),aGANfordiscretesequencestrainedwithpolicygradients.5Thismodelistrainedon10sentencesequences,whichissignificantlylongerthanpre-viousexperimentswithGANsfortext;thevocab-ularyisrestrictedto25kwordstomaketrainingtractable.Greedysamplingwasfoundtooutper-formbeamsearch.ForimplementationdetailsseeAppendixB.4.3EvaluationSetupWeposetheevaluationofourmodelasthetaskofgeneratinganappropriatecontinuationgivenaninitialcontext.Inouropen-endedgenerationset-tingthecontinuationisnotrequiredtobeaspe-cificlength,sowerequireourmodelsandbase-linestogenerate5-sentencecontinuations,consis-tentwiththewaythediscriminatorandseq2seqbaselinedatasetsareconstructed.Previousworkhasreportedthatautomaticmea-5Weusetheimplementationavailableathttps://github.com/nhynes/abc.suressuchasBLEU(Papinenietal.,2002)andMeteor(DenkowskiandLavie,2010)donotleadtomeaningfulevaluationwhenusedforlongorcreativetextgenerationwheretherecanbehighvarianceamongacceptablegenerationout-puts(Wisemanetal.,2017;Vedantametal.,2015).However,westillreportthesemeasuresasonecomponentofourevaluation.Additionallywere-portanumberofcustommetricswhichcaptureimportantpropertiesofthegeneratedtext:Length–Averagesequencelengthperexample;Trigrams–percentageofuniquetrigramsperexample;Vo-cab–percentageofuniquewordsperexample.EndingsgeneratedbyourmodelandthebaselinesCONTEXTWethoroughlyenjoyedstayingatthishotel.Wetravelledwithourkidsaged4and6.Theroomswereclean,spaciousandwellmaintained.Thebedsweregreatandallofussleptwell.Thehotelprovidedvoucherstowardsbreakfastattheadjoiningrestaurant.L2WWedidn’thavebreakfast,socan’tcommentonthat.Theconciergewasveryhelpful,andgaveusdirec-tions,etc.Overall,wewoulddefinitelyrecommendthishoteltoothers.Wewouldstayhereagaininaheartbeat.Ifyou’relookingforagreatlocation,andgoodvalue,I’drecommendthishotel.ADAPTIVELMThebreakfastbuffetwasverygood,andthestaffwasveryfriendlyandhelpful,anditwasagreatwaytostartourday!Iwoulddefinitelystayhereagain,andwouldrecommendittomyfriends,andwouldstaythereagainifIevergobacktoMontreal.Thelocationwasgreat,andthestaffwasfriendlyandhelpful.Iwoulddefinitelystayhereagain!!CACHELMThestaffwasveryfriendlyandhelpful.Thehotelislocatedinagreatlocation.Itisrightinthemiddleofthegaslampdistrict.Iwouldrecommendthishoteltoanyone.Iwoulddefinitelystayhereagain.SEQ2SEQThestaffwereveryfriendlyandhelpful.Thehotelisinagreatlocation.Thestaffwereveryfriendlyandhelpful.Thehotelisinagreatlocation.Thestaffwereveryfriendlyandhelpful.SEQGANWehadabreakfastatShula’s&adeliciousbreakfast.Thestaffwasveryhelpfulandhelpful.Thebreakfastwasgreataswell.Thestaffwasveryhelpfulandfriendly.Wehadagreatserviceandthefoodwasexcellent.REFERENCETherestaurantwasgreatandweusedthevoucherstowardswhateverbreakfastweordered.Thehotelhadamazinggroundswithaputtinggolfcoursethatwasfunforeveryone.Thepoolwasfantasticandweluckedoutwithgreatweather.Wespentmanyhoursinthepool,lounging,playingshuffleboardandsnackingfromtheattachedbar.Thehappyhourwasgreatperk.Table3:ExampleTripAdvisorAblationAblationvs.LMRepetitionContradictionRelevanceClarityBetterNeitherWorseREPETITIONONLY+0.63+0.30+0.37+0.4250%23%27%ENTAILMENTONLY+0.01+0.02+0.05-0.1039%20%41%RELEVANCEONLY-0.19+0.09+0.10+0.06036%22%42%LEXICALSTYLEONLY+0.11+0.16+0.20+0.1638%25%38%ALL+0.23-0.02+0.19-0.0347%19%34%Table4:Crowd-sourcedablationevaluationofgenerationsonTripAdvisor.Eachablationusesonlyonediscriminativecommunicationmodel,andiscomparedtoADAPTIVELM.AblationToinvestigatetheeffectofindividualdiscrimina-torsontheoverallperformance,wereportthere-sultsofablationsofourmodelinTable4.Foreachablationweincludeonlyoneofthecommunica-tionmodules,andtrainasinglemixturecoeffi-cientforcombiningthatmoduleandthelanguagemodel.ThediagonalofTable4containsonlypos-itivenumbers,indicatingthateachdiscriminatordoeshelpwiththepurposeitwasdesignedfor.Interestingly,mostdiscriminatorshelpwithmostaspectsofwriting,butallexceptrepetitionfailtoactuallyimprovetheoverallqualityoverADAP-TIVELM.Therepetitionmodulegivesthelargestboostbyfar,consistentwiththeintuitionthatmanyofthedeficienciesofRNNasatextgeneratorlieinse-manticrepetition.Theentailmentmodule(whichwasintendedtoreducecontradiction)istheweak-est,whichwehypothesizeisthecombinationof(a)mismatchbetweentrainingandtestdata(sincetheentailmentmodulewastrainedonSNLIandMultiNLI)and(b)thelackofpeatsitselflessthananyofourbaselines,itstillparaphrasesitself,albeitmoresubtly:“wewoulddefinitelyrecommendthishoteltoothers.”com-paredto“I’drecommendthishotel.”Thisex-amplealsoexposesamorefine-grainedissue:L2Wswitchesfromusing“we”tousing“I”mid-generation.Suchsubtledistinctionsarehardtocaptureduringbeamre-rankingandnoneofourmodelsaddressthelinguisticissuesofthissub-tlety.6RelatedWorkAlternativeDecodingObjectivesAnumberofpapershaveproposedalternativedecodingob-jectivesforgeneration(Shaoetal.,2017).Lietal.(2016a)proposedadiversity-promotingob-jectivethatinterpolatestheconditionalprobabil-ityscorewithnegativemarginalorreversecondi-tionalprobabilities.Yuetal.(2017b)alsoincor-poratethereverseconditionalprobabilitythroughanoisychannelmodelinordertoalleviatetheexplaining-awayproblem,butatthecostofsig-nificantdecodingcomplexity,makingitimpracti-calforparagraphgeneration.Modifieddecodingobjectiveshavelongbeenacommonpracticeinstatisticalmachinetranslation(Koehnetal.,2003;Och,2003;Watanabeetal.,2007;Chiangetal.,2009)andremaincommonwithneuralmachinetranslation,evenwhenanextremelylargeamountofdataisavailable(Wuetal.,2016).Inspiredbyalltheaboveapproaches,ourworkpresentsagenerallearningframeworktogetherwithamorecomprehensivesetofcompositecommunicationmodels.PragmaticCommunicationModelsModelsforpragmaticreasoningaboutcommunicativegoalssuchasGrice’smaximshavebeenpro-posedinthecontextofreferringexpressiongen-eration(FrankandGoodman,2012).AndreasandKlein(2016)proposedaneuralmodelwherecan-didatedescriptionsaresampledfromagenera-tivelytrainedspeaker,whicharethenre-rankedbyinterpolatingthescorewiththatofthelis-tener,adiscriminatorthatpredictsadistributionoverchoicesgiventhespeaker’sdescription.Sim-ilartoourworkthegeneratoranddiscriminatorscoresarecombinedtoselectutteranceswhichfol-lowGrice’smaxims.Yuetal.(2017c)proposedamodelwherethespeakerconsistsofaconvolu-tionalencoderandanLSTMdecoder,trainedwitharankinglossonnegativesamplesinadditiontooptimizinglog-likelihood.GenerativeAdversarialNetworksGANs(Goodfellowetal.,2014)areanotheralternativetomaximumlikelihoodestimationforgenerativemodels.However,backpropagatingthroughdiscretesequencesandtheinherentinstabilityofthetrainingobjective(Cheetal.,2017)bothpresentsignificantchallenges.WhilesolutionshavebeenproposedtomakeitpossibletotrainGANsforlanguage(Cheetal.,2017;Yuetal.,2017a)theyhavenotyetbeenshowntoproducehighqualitylong-formtext,asourresultsconfirm.GenerationwithLong-termContextSeveralpriorworksstudiedparagraphgenerationusingsequence-to-sequencemodelsforimagecaptions(Krauseetal.,2017),productreviews(Liptonetal.,2015;Dongetal.,2017),sportreports(Wisemanetal.,2017),andrecipes(Kiddonetal.,2016).Whilethesepriorworksfocusondevelop-ingneuralarchitecturesforlearningdomainspe-cificdiscoursepatterns,ourworkproposesagen-eralframeworkforlearningageneratorthatismorepowerfulthanmaximumlikelihooddecod-ingfromanRNNlanguagemodelforanarbitrarytargetdomain.7ConclusionWeproposedaunifiedlearningframeworkforthegenerationoflong,coherenttexts,whichover-comessomeofthecommonlimitationsofRNNsastextgenerationmodels.Ourframeworklearnsadecodingobjectivesuitableforgenerationthroughalearnedcombinationofsub-modelsthatcapturelinguistically-motivatedqualitiesofgoodwriting.Humanevaluationshowsthatthequalityofthetextproducedbyourmodelexceedsthatofcom-petitivebaselinesbyalargemargin.AcknowledgmentsWethanktheanonymousreviewersfortheirin-sightfulfeedbackandOmerLevyforhelpfuldis-cussions.ThisresearchwassupportedinpartbyNSF(IIS-1524371),DARPACwCthroughARO(W911NF-15-1-0543),SamsungAIResearch,andProceedingsoftheConferenceonEmpiricalMeth-odsinNaturalLanguageProcessing,pages1173–1182.AssociationforComputationalLinguistics.DzmitryBahdanau,KyunghyunCho,andYoshuaBen-gio.2015.Neuralmachinetranslationbyjointlylearningtoalignandtranslate.InInternationalCon-ferenceonLearningRepresentations.SamuelR.Bowman,GaborAngeli,ChristopherPotts,andChristopherD.Manning.2015.Alargean-notatedcorpusforlearningnaturallanguageinfer-ence.InProceedingsoftheConferenceonEmpiri-calMethodsinNaturalLanguageProcessing,pages632–642.AssociationforComputationalLinguis-tics.TongChe,YanranLi,RuixiangZhang,R.DevonHjelm,WenjieLi,YangqiuSong,andYoshuaBengio.2017.Maximum-likelihoodaugmenteddiscretegenerativeadversarialnetworks.CoRR,abs/1702.07983.DavidChiang,KevinKnight,andWeiWang.2009.11,001newfeaturesforstatisticalmachinetrans-lation.InProceedingsofHumanLanguageTech-nologies:The2009AnnualConferenceoftheNorthAmericanChapteroftheAssociationforCompu-tationalLinguistics,pages218–226,Boulder,Col-orado.AssociationforComputationalLinguistics.KyunghyunCho,BartvanMerriënboer,DzmitryBah-danau,andYoshuaBengio.2014.Onthepropertiesofneuralmachinetranslation:Encoder–decoderap-proaches.InProceedingsofSSST-8,EighthWork-shoponSyntax,SemanticsandStructureinStatisti-calTranslation,pages103–111.SumitChopra,MichaelAuli,andAlexanderM.Rush.2016.Abstractivesentencesummarizationwithat-tentiverecurrentneuralnetworks.InProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages93–98,SanDiego,California.AssociationforComputationalLinguistics.IdoDagan,OrenGlickman,andBernardoMagnini.2006.Thepascalrecognisingtextualentailmentchallenge.InProceedingsoftheFirstInter-nationalConferenceonMachineLearningChal-lenges:EvaluatingPredictiveUncertaintyVisualObjectClassification,andRecognizingTextualEn-tailment,MLCW’05,pages177–190,Berlin,Hei-delberg.Springer-Verlag.MichaelDenkowskiandAlonLavie.2010.Extend-ingtheMETEORMachineTranslationEvaluationMetrictothePhraseLevel.InPhilippKoehn,FranzJosefOch,andDanielMarcu.2003.Statisticalphrase-basedtranslation.InPro-ceedingsofthe2003ConferenceoftheNorthAmer-icanChapteroftheAssociationforComputationalLinguisticsonHumanLanguageTechnology-Vol-ume1,pages48–54.AssociationforComputationalLinguistics.JonathanKrause,JustinJohnson,RanjayKrishna,andLiFei-Fei.2017.Ahierarchicalapproachforgener-atingdescriptiveimageparagraphs.InProceedingsoftheConferenceonComputerVisionandPatternRecognition.JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,andBillDolan.2016a.Adiversity-promotingob-jectivefunctionforneuralconversationmodels.InConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages110–119,SanDiego,California.AssociationforComputationalLinguis-tics.JiweiLi,WillMonroe,AlanRitter,DanJurafsky,MichelGalley,andJianfengGao.2016b.Deeprein-forcementlearningfordialoguegeneration.InPro-ceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1192–1202,Austin,Texas.AssociationforComputationalLin-guistics.ZacharyChaseLipton,SharadVikram,andJulianMcAuley.2015.Capturingmeaninginproductre-viewswithcharacter-levelgenerativetextmodels.CoRR,abs/1511.03683.Chia-WeiLiu,RyanLowe,IulianSerban,MikeNose-worthy,LaurentCharlin,andJoellePineau.2016.Hownottoevaluateyourdialoguesystem:Anem-piricalstudyofunsupervisedevaluationmetricsfordialogueresponsegeneration.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLan-guageProcessing,pages2122–2132,Austin,Texas.AssociationforComputationalLinguistics.MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.1993.Buildingalargeannotatedcorpusofenglish:Thepenntreebank.Computa-tionalLinguistics,19(2):313–330.StephenMerity,NitishShirishKeskar,andRichardSocher.2018.Regularizingandoptimizinglstmlan-guagemodels.ICLR.JekaterinaNovikova,OndřejDušek,AmandaCer-casCurry,andVerenaRieser.2017.Whyweneednewevaluationmetricsfornlg.InProceed-ingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2241–2252,Copenhagen,Denmark.AssociationforComputa-tionalLinguistics.FranzJosefOch.2003.Minimumerrorratetrain-inginstatisticalmachinetranslation.InProceed-ingsofthe41stAnnualMeetingoftheAssociationforComputationalLinguistics,pages160–167,Sap-poro,Japan.AssociationforComputationalLinguis-tics.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.Bleu:amethodforautomaticevaluationofmachinetranslation.InProceedingsoftheAssociationforComputationalLinguistics,pages311–318.AssociationforComputationalLin-guistics.AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit.2016.Adecomposableattentionmodelfornaturallanguageinference.InProceed-ingsoftheConferenceonEmpiricalMethodsinNat-uralLanguageProcessing,pages2249–2255.Asso-ciationforComputationalLinguistics.RazvanPascanu,TomasMikolov,andYoshuaBengio.2013.Onthedifficultyoftrainingrecurrentneuralnetworks.InInternationalConferenceonMachineLearning(ICML),pages1310–1318.RomainPaulus,CaimingXiong,andRichardSocher.2018.Adeepreinforcedmodelforabstractivesum-marization.CoRR,abs/1705.04304.JeffreyPennington,RichardSocher,andChristopherManning.2014.Glove:Globalvectorsforwordrepresentation.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcess-ing,pages1532–1543,Doha,Qatar.AssociationforComputationalLinguistics.AllenSchmaltz,AlexanderM.Rush,andStuartShieber.2016.Wordorderingwithoutsyntax.InProceedingsoftheConferenceonEmpiricalMeth-odsinNaturalLanguageProcessing,pages2319–2324,Austin,Texas.AssociationforComputationalLinguistics.YuanlongShao,StephanGouws,DennyBritz,AnnaGoldie,BrianStrope,andRayKurzweil.2017.Generatinghigh-qualityandinformativeconversa-tionresponseswithsequence-to-sequencemodels.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2210–2219.AssociationforComputationalLinguis-tics.NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.2014.Dropout:asimplewaytopreventneuralnetworksfromoverfitting.JournalofMachineLearningRe-search,15(1):1929–1958.RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh.2015.Cider:Consensus-basedHongningWang,YueLu,andChengXiangZhai.2010.Latentaspectratinganalysisonreviewtextdata:aratingregressionapproach.InSIGKDDConferenceonKnowledgeDiscoveryandDataMining.TaroWatanabe,JunSuzuki,HajimeTsukada,andHidekiIsozaki.2007.Onlinelarge-margintrain-ingforstatisticalmachinetranslation.InProceed-ingsofthe2007JointConferenceonEmpiricalMethodsinNaturalLanguageProcessingandCom-putationalNaturalLanguageLearning(EMNLP-CoNLL),pages764–773,Prague,CzechRepublic.AssociationforComputationalLinguistics.AdinaWilliams,NikitaNangia,andSamuelR.Bow-man.2017.Abroad-coveragechallengecorpusforsentenceunderstandingthroughinference.CoRR,abs/1704.05426.SamWiseman,StuartShieber,andAlexanderRush.2017.Challengesindata-to-documentgeneration.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2253–2263,Copenhagen,Denmark.AssociationforComputationalLinguistics.YonghuiWu,MikeSchuster,ZhifengChen,QuocV.Le,MohammadNorouzi,WolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,JeffKlingner,ApurvaShah,MelvinJohnson,XiaobingLiu,LukaszKaiser,StephanGouws,YoshikiyoKato,TakuKudo,HidetoKazawa,KeithStevens,GeorgeKurian,NishantPatil,WeiWang,CliffYoung,JasonSmith,JasonRiesa,AlexRudnick,OriolVinyals,GregCorrado,MacduffHughes,andJeffreyDean.2016.Google’sneuralmachinetranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation.CoRR,abs/1609.08144.LantaoYu,WeinanZhang,JunWang,andYongYu.2017a.Seqgan:Sequencegenerativeadversarialnetswithpolicygradient.InProceedingsoftheAs-sociationfortheAdvancementofArtificialIntelli-gence,pages2852–2858.LeiYu,PhilBlunsom,ChrisDyer,EdwardGrefen-stette,andTomasKocisky.2017b.Theneuralnoisychannel.InInternationalConferenceonLearningRepresentations.LichengYu,HaoTan,MohitBansal,andTamaraLBerg.2017c.Ajointspeaker-listener-reinforcermodelforreferringexpressions.InProceedingsoftheConferenceonComputerVisionandPatternRecognition,volume2.YukunZhu,RyanKiros,RichardZemel,RuslanSalakhutdinov,RaquelUrtasun,AntonioTorralba,sequencesarepaddedsuchthattheinputandout-putlengthsareequal.BBaselineDetailsCACHELMDuetomemoryconstraints,weuseavocabularysizeof50kforCACHELM.BeamBookCorpusTripAdvisorTable7:The