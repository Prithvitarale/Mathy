Journal of Machine Learning Research 15 (2014) 2773-2832 Submitted 2/13; Revised 3/14; Published 8/14
Tensor Decompositions for Learning Latent Variable Models
Animashree Anandkumar a.anandkumar@uci.edu
Electrical Engineering and Computer Science
University of California, Irvine
2200 Engineering Hall
Irvine, CA 92697
Rong Ge rongge@microsoft.com
Microsoft Research
One Memorial Drive
Cambridge, MA 02142
Daniel Hsu djhsu@cs.columbia.edu
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, #0401
New York, NY 10027
Sham M. Kakade skakade@microsoft.com
Microsoft Research
One Memorial Drive
Cambridge, MA 02142
Matus Telgarsky mtelgars@cs.ucsd.edu
Department of Statistics
Rutgers University
110 Frelinghuysen Road
Piscataway, NJ 08854
Editor: Benjamin Recht
Abstract
This work considers a computationally and statistically efficient parameter estimation
method for a wide class of latent variable models—including Gaussian mixture models,
hidden Markov models, and latent Dirichlet allocation—which exploits a certain tensor
structure in their low-order observable moments (typically, of second- and third-order).
Specifically, parameter estimation is reduced to the problem of extracting a certain (orthog-
onal) decomposition of a symmetric tensor derived from the moments; this decomposition
can be viewed as a natural generalization of the singular value decomposition for matrices.
Although tensor decompositions are generally intractable to compute, the decomposition
of these specially structured tensors can be efficiently obtained by a variety of approaches,
including power iterations and maximization approaches (similar to the case of matrices).
A detailed analysis of a robust tensor power method is provided, establishing an analogue
of Wedin’s perturbation theorem for the singular vectors of matrices. This implies a ro-
bust and computationally tractable estimation approach for several popular latent variable
models.
c 2014 Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky.
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Keywords: latent variable models, tensor decompositions, mixture models, topic models,
method of moments, power method
1. Introduction
The method of moments is a classical parameter estimation technique (Pearson, 1894) from
statistics which has proved invaluable in a number of application domains. The basic
paradigm is simple and intuitive: (i) compute certain statistics of the data—often empirical
moments such as means and correlations—and (ii) find model parameters that give rise to
(nearly) the same corresponding population quantities. In a number of cases, the method of
moments leads to consistent estimators which can be efficiently computed; this is especially
relevant in the context of latent variable models, where standard maximum likelihood ap-
proaches are typically computationally prohibitive, and heuristic methods can be unreliable
and difficult to validate with high-dimensional data. Furthermore, the method of moments
can be viewed as complementary to the maximum likelihood approach; simply taking a
single step of Newton-Raphson on the likelihood function starting from the moment based
estimator (Le Cam, 1986) often leads to the best of both worlds: a computationally efficient
estimator that is (asymptotically) statistically optimal.
The primary difficulty in learning latent variable models is that the latent (hidden)
state of the data is not directly observed; rather only observed variables correlated with
the hidden state are observed. As such, it is not evident the method of moments should
fare any better than maximum likelihood in terms of computational performance: match-
ing the model parameters to the observed moments may involve solving computationally
intractable systems of multivariate polynomial equations. Fortunately, for many classes of
latent variable models, there is rich structure in low-order moments (typically second- and
third-order) which allow for this inverse moment problem to be solved efficiently (Cattell,
1944; Cardoso, 1991; Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar
et al., 2012c,a; Hsu and Kakade, 2013). What is more is that these decomposition problems
are often amenable to simple and efficient iterative methods, such as gradient descent and
the power iteration method.
1.1 Contributions
In this work, we observe that a number of important and well-studied latent variable
models—including Gaussian mixture models, hidden Markov models, and Latent Dirichlet
allocation—share a certain structure in their low-order moments, and this permits certain
tensor decomposition approaches to parameter estimation. In particular, this decompo-
sition can be viewed as a natural generalization of the singular value decomposition for
matrices.
While much of this (or similar) structure was implicit in several previous works (Chang,
1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c,a; Hsu and
Kakade, 2013), here we make the decomposition explicit under a unified framework. Specif-
ically, we express the observable moments as sums of rank-one terms, and reduce the pa-
rameter estimation task to the problem of extracting a symmetric orthogonal decomposition
of a symmetric tensor derived from these observable moments. The problem can then be
solved by a variety of approaches, including fixed-point and variational methods.
2774
Tensor Decompositions for Learning Latent Variable Models
One approach for obtaining the orthogonal decomposition is the tensor power method
of Lathauwer et al. (2000, Remark 3). We provide a convergence analysis of this method for
orthogonally decomposable symmetric tensors, as well as a detailed perturbation analysis
for a robust (and a computationally tractable) variant (Theorem 5.1). This perturbation
analysis can be viewed as an analogue of Wedin’s perturbation theorem for singular vectors
of matrices (Wedin, 1972), providing a bound on the error of the recovered decomposition
in terms of the operator norm of the tensor perturbation. This analysis is subtle in at least
two ways. First, unlike for matrices (where every matrix has a singular value decomposi-
tion), an orthogonal decomposition need not exist for the perturbed tensor. Our robust
variant uses random restarts and deflation to extract an approximate decomposition in a
computationally tractable manner. Second, the analysis of the deflation steps is non-trivial;
a naı̈ve argument would entail error accumulation in each deflation step, which we show can
in fact be avoided. When this method is applied for parameter estimation in latent variable
models previously discussed, improved sample complexity bounds (over previous work) can
be obtained using this perturbation analysis.
Finally, we also address computational issues that arise when applying the tensor de-
composition approaches to estimating latent variable models. Specifically, we show that the
basic operations of simple iterative approaches (such as the tensor power method) can be
efficiently executed in time linear in the dimension of the observations and the size of the
training data. For instance, in a topic modeling application, the proposed methods require
time linear in the number of words in the vocabulary and in the number of non-zero entries
of the term-document matrix. The combination of this computational efficiency and the
robustness of the tensor decomposition techniques makes the overall framework a promising
approach to parameter estimation for latent variable models.
1.2 Related Work
The connection between tensor decompositions and latent variable models has a long history
across many scientific and mathematical disciplines. We review some of the key works that
are most closely related to ours.
1.2.1 Tensor Decompositions
The role of tensor decompositions in the context of latent variable models dates back to early
uses in psychometrics (Cattell, 1944). These ideas later gained popularity in chemometrics,
and more recently in numerous science and engineering disciplines, including neuroscience,
phylogenetics, signal processing, data mining, and computer vision. A thorough survey of
these techniques and applications is given by Kolda and Bader (2009). Below, we discuss a
few specific connections to two applications in machine learning and statistics, independent
component analysis and latent variable models (between which there is also significant
overlap).
Tensor decompositions have been used in signal processing and computational neuro-
science for blind source separation and independent component analysis (ICA) (Comon and
Jutten, 2010). Here, statistically independent non-Gaussian sources are linearly mixed in
the observed signal, and the goal is to recover the mixing matrix (and ultimately, the orig-
inal source signals). A typical solution is to locate projections of the observed signals that
2775
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
correspond to local extrema of the so-called “contrast functions” which distinguish Gaussian
variables from non-Gaussian variables. This method can be effectively implemented using
fast descent algorithms (Hyvarinen, 1999). When using the excess kurtosis (i.e., fourth-order
cumulant) as the contrast function, this method reduces to a generalization of the power
method for symmetric tensors (Lathauwer et al., 2000; Zhang and Golub, 2001; Kofidis and
Regalia, 2002). This case is particularly important, since all local extrema of the kurtosis
objective correspond to the true sources (under the assumed statistical model) (Delfosse
and Loubaton, 1995); the descent methods can therefore be rigorously analyzed, and their
computational and statistical complexity can be bounded (Frieze et al., 1996; Nguyen and
Regev, 2009; Arora et al., 2012b).
Higher-order tensor decompositions have also been used to develop estimators for com-
monly used mixture models, hidden Markov models, and other related latent variable mod-
els, often using the the algebraic procedure of R. Jennrich (as reported in the article of
Harshman, 1970), which is based on a simultaneous diagonalization of different ways of
flattening a tensor to matrices. Jennrich’s procedure was employed for parameter estima-
tion of discrete Markov models by Chang (1996) via pair-wise and triple-wise probability
tables; and it was later used for other latent variable models such as hidden Markov models
(HMMs), latent trees, Gaussian mixture models, and topic models such as latent Dirichlet
allocation (LDA) by many others (Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar
et al., 2012c,a; Hsu and Kakade, 2013). In these contexts, it is often also possible to es-
tablish strong identifiability results, without giving an explicit estimators, by invoking the
non-constructive identifiability argument of Kruskal (1977)—see the article by Allman et al.
(2009) for several examples.
Related simultaneous diagonalization approaches have also been used for blind source
separation and ICA (as discussed above), and a number of efficient algorithms have been
developed for this problem (Bunse-Gerstner et al., 1993; Cardoso and Souloumiac, 1993;
Cardoso, 1994; Cardoso and Comon, 1996; Corless et al., 1997; Ziehe et al., 2004). A rather
different technique that uses tensor flattening and matrix eigenvalue decomposition has
been developed by Cardoso (1991) and later by De Lathauwer et al. (2007). A significant
advantage of this technique is that it can be used to estimate overcomplete mixtures, where
the number of sources is larger than the observed dimension.
The relevance of tensor analysis to latent variable modeling has been long recognized in
the field of algebraic statistics (Pachter and Sturmfels, 2005), and many works characterize
the algebraic varieties corresponding to the moments of various classes of latent variable
models (Drton et al., 2007; Sturmfels and Zwiernik, 2013). These works typically do not
address computational or finite sample issues, but rather are concerned with basic questions
of identifiability.
The specific tensor structure considered in the present work is the symmetric orthogo-
nal decomposition. This decomposition expresses a tensor as a linear combination of simple
tensor forms; each form is the tensor product of a vector (i.e., a rank-1 tensor), and the
collection of vectors form an orthonormal basis. An important property of tensors with
such decompositions is that they have eigenvectors corresponding to these basis vectors.
Although the concepts of eigenvalues and eigenvectors of tensors is generally significantly
more complicated than their matrix counterpart—both algebraically (Qi, 2005; Cartwright
and Sturmfels, 2013; Lim, 2005) and computationally (Hillar and Lim, 2013; Kofidis and
2776
Tensor Decompositions for Learning Latent Variable Models
Regalia, 2002)—the special symmetric orthogonal structure we consider permits simple
algorithms to efficiently and stably recover the desired decomposition. In particular, a gen-
eralization of the matrix power method to symmetric tensors, introduced by Lathauwer
et al. (2000, Remark 3) and analyzed by Kofidis and Regalia (2002), provides such a de-
composition. This is in fact implied by the characterization of Zhang and Golub (2001),
which shows that iteratively obtaining the best rank-1 approximation of such orthogonally
decomposable tensors also yields the exact decomposition. We note that in general, ob-
taining such approximations for general (symmetric) tensors is NP-hard (Hillar and Lim,
2013).
1.2.2 Latent Variable Models
This work focuses on the particular application of tensor decomposition methods to estimat-
ing latent variable models, a significant departure from many previous approaches in the
machine learning and statistics literature. By far the most popular heuristic for parameter
estimation for such models is the Expectation-Maximization (EM) algorithm (Dempster
et al., 1977; Redner and Walker, 1984). Although EM has a number of merits, it may suffer
from slow convergence and poor quality local optima (Redner and Walker, 1984), requir-
ing practitioners to employ many additional heuristics to obtain good solutions. For some
models such as latent trees (Roch, 2006) and topic models (Arora et al., 2012a), maximum
likelihood estimation is NP-hard, which suggests that other estimation approaches may be
more attractive. More recently, algorithms from theoretical computer science and machine
learning have addressed computational and sample complexity issues related to estimating
certain latent variable models such as Gaussian mixture models and HMMs (Dasgupta,
1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004;
Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker
and Vempala, 2008; Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010;
Hsu and Kakade, 2013; Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar
et al., 2012c; Arora et al., 2012a; Anandkumar et al., 2012a). See the works by Anandku-
mar et al. (2012c) and Hsu and Kakade (2013) for a discussion of these methods, together
with the computational and statistical hardness barriers that they face. The present work
reviews a broad range of latent variables where a mild non-degeneracy condition implies
the symmetric orthogonal decomposition structure in the tensors of low-order observable
moments.
Notably, another class of methods, based on subspace identification (Overschee and
Moor, 1996) and observable operator models/multiplicity automata (Schützenberger, 1961;
Jaeger, 2000; Littman et al., 2001), have been proposed for a number of latent variable
models. These methods were successfully developed for HMMs by Hsu et al. (2012b), and
subsequently generalized and extended for a number of related sequential and tree Markov
models models (Siddiqi et al., 2010; Bailly, 2011; Boots et al., 2010; Parikh et al., 2011; Rodu
et al., 2013; Balle et al., 2012; Balle and Mohri, 2012), as well as certain classes of parse
tree models (Luque et al., 2012; Cohen et al., 2012; Dhillon et al., 2012). These methods
use low-order moments to learn an “operator” representation of the distribution, which can
be used for density estimation and belief state updates. While finite sample bounds can be
given to establish the learnability of these models (Hsu et al., 2012b), the algorithms do not
2777
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
actually give parameter estimates (e.g., of the emission or transition matrices in the case of
HMMs).
1.3 Organization
The rest of the paper is organized as follows. Section 2 reviews some basic definitions of
tensors. Section 3 provides examples of a number of latent variable models which, after
appropriate manipulations of their low order moments, share a certain natural tensor struc-
ture. Section 4 reduces the problem of parameter estimation to that of extracting a certain
(symmetric orthogonal) decomposition of a tensor. We then provide a detailed analysis of
a robust tensor power method and establish an analogue of Wedin’s perturbation theorem
for the singular vectors of matrices. The discussion in Section 6 addresses a number of
practical concerns that arise when dealing with moment matrices and tensors.
2. Preliminaries
We introduce some tensor notations borrowed from Lim (2005). A real p-th order tensor
A ∈
Np
i=1 Rni is a member of the tensor product of Euclidean spaces Rni , i ∈ [p]. We
generally restrict to the case where n1 = n2 = · · · = np = n, and simply write A ∈
Np
Rn.
For a vector v ∈ Rn, we use v⊗p := v ⊗ v ⊗ · · · ⊗ v ∈
Np
Rn to denote its p-th tensor power.
As is the case for vectors (where p = 1) and matrices (where p = 2), we may identify a
p-th order tensor with the p-way array of real numbers [Ai1,i2,...,ip : i1, i2, . . . , ip ∈ [n]], where
Ai1,i2,...,ip is the (i1, i2, . . . , ip)-th coordinate of A (with respect to a canonical basis).
We can consider A to be a multilinear map in the following sense: for a set of matrices
{Vi ∈ Rn×mi : i ∈ [p]}, the (i1, i2, . . . , ip)-th entry in the p-way array representation of
A(V1, V2, . . . , Vp) ∈ Rm1×m2×···×mp is
[A(V1, V2, . . . , Vp)]i1,i2,...,ip :=
X
j1,j2,...,jp∈[n]
Aj1,j2,...,jp [V1]j1,i1 [V2]j2,i2 · · · [Vp]jp,ip .
Note that if A is a matrix (p = 2), then
A(V1, V2) = V >
1 AV2.
Similarly, for a matrix A and vector v ∈ Rn, we can express Av as
A(I, v) = Av ∈ Rn
,
where I is the n × n identity matrix. As a final example of this notation, observe
A(ei1 , ei2 , . . . , eip ) = Ai1,i2,...,ip ,
where {e1, e2, . . . , en} is the canonical basis for Rn.
Most tensors A ∈
Np
Rn considered in this work will be symmetric (sometimes called
supersymmetric), which means that their p-way array representations are invariant to
permutations of the array indices: i.e., for all indices i1, i2, . . . , ip ∈ [n], Ai1,i2,...,ip =
Aiπ(1),iπ(2),...,iπ(p)
for any permutation π on [p]. It can be checked that this reduces to the
usual definition of a symmetric matrix for p = 2.
2778
Tensor Decompositions for Learning Latent Variable Models
The rank of a p-th order tensor A ∈
Np
Rn is the smallest non-negative integer k such
that A =
Pk
j=1 u1,j ⊗ u2,j ⊗ · · · ⊗ up,j for some ui,j ∈ Rn, i ∈ [p], j ∈ [k], and the symmetric
rank of a symmetric p-th order tensor A is the smallest non-negative integer k such that
A =
Pk
j=1 u⊗p
j for some uj ∈ Rn, j ∈ [k].1 The notion of rank readily reduces to the usual
definition of matrix rank when p = 2, as revealed by the singular value decomposition.
Similarly, for symmetric matrices, the symmetric rank is equivalent to the matrix rank as
given by the spectral theorem. A decomposition into such rank-one terms is known as a
canonical polyadic decomposition (Hitchcock, 1927a,b).
The notion of tensor (symmetric) rank is considerably more delicate than matrix (sym-
metric) rank. For instance, it is not clear a priori that the symmetric rank of a tensor
should even be finite (Comon et al., 2008). In addition, removal of the best rank-1 approx-
imation of a (general) tensor may increase the tensor rank of the residual (Stegeman and
Comon, 2010).
Throughout, we use kvk = (
P
i v2
i )1/2 to denote the Euclidean norm of a vector v, and
kMk to denote the spectral (operator) norm of a matrix. We also use kTk to denote the
operator norm of a tensor, which we define later.
3. Tensor Structure in Latent Variable Models
In this section, we give several examples of latent variable models whose low-order moments
can be written as symmetric tensors of low symmetric rank; some of these examples can be
deduced using the techniques developed in the text by McCullagh (1987). The basic form
is demonstrated in Theorem 3.1 for the first example, and the general pattern will emerge
from subsequent examples.
3.1 Exchangeable Single Topic Models
We first consider a simple bag-of-words model for documents in which the words in the
document are assumed to be exchangeable. Recall that a collection of random variables
x1, x2, . . . , x` are exchangeable if their joint probability distribution is invariant to permu-
tation of the indices. The well-known De Finetti’s theorem (Austin, 2008) implies that such
exchangeable models can be viewed as mixture models in which there is a latent variable h
such that x1, x2, . . . , x` are conditionally i.i.d. given h (see Figure 1(a) for the corresponding
graphical model) and the conditional distributions are identical at all the nodes.
In our simplified topic model for documents, the latent variable h is interpreted as
the (sole) topic of a given document, and it is assumed to take only a finite number of
distinct values. Let k be the number of distinct topics in the corpus, d be the number of
distinct words in the vocabulary, and ` ≥ 3 be the number of words in each document. The
generative process for a document is as follows: the document’s topic is drawn according to
the discrete distribution specified by the probability vector w := (w1, w2, . . . , wk) ∈ ∆k−1.
This is modeled as a discrete random variable h such that
Pr[h = j] = wj, j ∈ [k].
1. For even p, the definition is slightly different (Comon et al., 2008).
2779
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Given the topic h, the document’s ` words are drawn independently according to the dis-
crete distribution specified by the probability vector µh ∈ ∆d−1. It will be convenient to
represent the ` words in the document by d-dimensional random vectors x1, x2, . . . , x` ∈ Rd.
Specifically, we set
xt = ei if and only if the t-th word in the document is i, t ∈ [`],
where e1, e2, . . . ed is the standard coordinate basis for Rd.
One advantage of this encoding of words is that the (cross) moments of these random
vectors correspond to joint probabilities over words. For instance, observe that
E[x1 ⊗ x2] =
X
1≤i,j≤d
Pr[x1 = ei, x2 = ej] ei ⊗ ej
=
X
1≤i,j≤d
Pr[1st word = i, 2nd word = j] ei ⊗ ej,
so the (i, j)-the entry of the matrix E[x1 ⊗ x2] is Pr[1st word = i, 2nd word = j]. More
generally, the (i1, i2, . . . , i`)-th entry in the tensor E[x1 ⊗ x2 ⊗ · · · ⊗ x`] is Pr[1st word =
i1, 2nd word = i2, . . . , `-th word = i`]. This means that estimating cross moments, say, of
x1 ⊗ x2 ⊗ x3, is the same as estimating joint probabilities of the first three words over all
documents. (Recall that we assume that each document has at least three words.)
The second advantage of the vector encoding of words is that the conditional expectation
of xt given h = j is simply µj, the vector of word probabilities for topic j:
E[xt|h = j] =
d
X
i=1
Pr[t-th word = i|h = j] ei =
d
X
i=1
[µj]i ei = µj, j ∈ [k]
(where [µj]i is the i-th entry in the vector µj). Because the words are conditionally inde-
pendent given the topic, we can use this same property with conditional cross moments,
say, of x1 and x2:
E[x1 ⊗ x2|h = j] = E[x1|h = j] ⊗ E[x2|h = j] = µj ⊗ µj, j ∈ [k].
This and similar calculations lead one to the following theorem.
Theorem 3.1 (Anandkumar et al., 2012c) If
M2 := E[x1 ⊗ x2]
M3 := E[x1 ⊗ x2 ⊗ x3],
then
M2 =
k
X
i=1
wi µi ⊗ µi
M3 =
k
X
i=1
wi µi ⊗ µi ⊗ µi.
2780
Tensor Decompositions for Learning Latent Variable Models
As we will see in Section 4.3, the structure of M2 and M3 revealed in Theorem 3.1 implies
that the topic vectors µ1, µ2, . . . , µk can be estimated by computing a certain symmetric
tensor decomposition. Moreover, due to exchangeability, all triples (resp., pairs) of words
in a document—and not just the first three (resp., two) words—can be used in forming M3
(resp., M2); see Section 6.1.
3.2 Beyond Raw Moments
In the single topic model above, the raw (cross) moments of the observed words directly
yield the desired symmetric tensor structure. In some other models, the raw moments do
not explicitly have this form. Here, we show that the desired tensor structure can be found
through various manipulations of different moments.
3.2.1 Spherical Gaussian Mixtures: Common Covariance
We now consider a mixture of k Gaussian distributions with spherical covariances. We start
with the simpler case where all of the covariances are identical; this probabilistic model is
closely related to the (non-probabilistic) k-means clustering problem (MacQueen, 1967).
Let wi ∈ (0, 1) be the probability of choosing component i ∈ [k], {µ1, µ2, . . . , µk} ⊂ Rd
be the component mean vectors, and σ2I be the common covariance matrix. An observation
in this model is given by
x := µh + z,
where h is the discrete random variable with Pr[h = i] = wi for i ∈ [k] (similar to the ex-
changeable single topic model), and z ∼ N(0, σ2I) is an independent multivariate Gaussian
random vector in Rd with zero mean and spherical covariance σ2I.
The Gaussian mixture model differs from the exchangeable single topic model in the way
observations are generated. In the single topic model, we observe multiple draws (words in
a particular document) x1, x2, . . . , x` given the same fixed h (the topic of the document). In
contrast, for the Gaussian mixture model, every realization of x corresponds to a different
realization of h.
Theorem 3.2 (Hsu and Kakade, 2013) Assume d ≥ k. The variance σ2 is the smallest
eigenvalue of the covariance matrix E[x ⊗ x] − E[x] ⊗ E[x]. Furthermore, if
M2 := E[x ⊗ x] − σ2
I
M3 := E[x ⊗ x ⊗ x] − σ2
d
X
i=1
E[x] ⊗ ei ⊗ ei + ei ⊗ E[x] ⊗ ei + ei ⊗ ei ⊗ E[x]

,
then
M2 =
k
X
i=1
wi µi ⊗ µi
M3 =
k
X
i=1
wi µi ⊗ µi ⊗ µi.
2781
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
3.2.2 Spherical Gaussian Mixtures: Differing Covariances
The general case is where each component may have a different spherical covariance. An
observation in this model is again x = µh + z, but now z ∈ Rd is a random vector whose
conditional distribution given h = i (for some i ∈ [k]) is a multivariate Gaussian N(0, σ2
i I)
with zero mean and spherical covariance σ2
i I.
Theorem 3.3 (Hsu and Kakade, 2013) Assume d ≥ k. The average variance σ̄2 :=
Pk
i=1 wiσ2
i is the smallest eigenvalue of the covariance matrix E[x ⊗ x] − E[x] ⊗ E[x]. Let v
be any unit norm eigenvector corresponding to the eigenvalue σ̄2. If
M1 := E[x(v>
(x − E[x]))2
]
M2 := E[x ⊗ x] − σ̄2
I
M3 := E[x ⊗ x ⊗ x] −
d
X
i=1
M1 ⊗ ei ⊗ ei + ei ⊗ M1 ⊗ ei + ei ⊗ ei ⊗ M1

,
then
M2 =
k
X
i=1
wi µi ⊗ µi
M3 =
k
X
i=1
wi µi ⊗ µi ⊗ µi.
As shown by Hsu and Kakade (2013), M1 =
Pk
i=1 wiσ2
i µi. Note that for the common
covariance case, where σ2
i = σ2, we have that M1 = σ2E[x] (cf. Theorem 3.2).
3.2.3 Independent Component Analysis (ICA)
The standard model for ICA (Comon, 1994; Cardoso and Comon, 1996; Hyvärinen and
Oja, 2000; Comon and Jutten, 2010), in which independent signals are linearly mixed and
corrupted with Gaussian noise before being observed, is specified as follows. Let h ∈ Rk be
a latent random vector with independent coordinates, A ∈ Rd×k the mixing matrix, and z
be a multivariate Gaussian random vector. The random vectors h and z are assumed to be
independent. The observed random vector is
x := Ah + z.
Let µi denote the i-th column of the mixing matrix A.
Theorem 3.4 (Comon and Jutten, 2010) Define
M4 := E[x ⊗ x ⊗ x ⊗ x] − T
where T is the fourth-order tensor with
[T]i1,i2,i3,i4 := E[xi1 xi2 ]E[xi3 xi4 ] + E[xi1 xi3 ]E[xi2 xi4 ]
+ E[xi1 xi4 ]E[xi2 xi3 ], 1 ≤ i1, i2, i3, i4 ≤ k
2782
Tensor Decompositions for Learning Latent Variable Models
( i.e., T is the fourth derivative tensor of the function v 7→ 8−1E[(v>
x)2]2, so M4 is the
fourth cumulant tensor). Let κi := E[h4
i ] − 3 for each i ∈ [k]. Then
M4 =
k
X
i=1
κi µi ⊗ µi ⊗ µi ⊗ µi.
Note that κi corresponds to the excess kurtosis, a measure of non-Gaussianity as κi = 0 if
hi is a standard normal random variable. Furthermore, note that A is not identifiable if h
is a multivariate Gaussian.
We may derive forms similar to that of M2 and M3 from Theorem 3.1 using M4 by
observing that
M4(I, I, u, v) =
k
X
i=1
κi(µ>
i u)(µ>
i v) µi ⊗ µi,
M4(I, I, I, v) =
k
X
i=1
κi(µ>
i v) µi ⊗ µi ⊗ µi
for any vectors u, v ∈ Rd.
3.2.4 Latent Dirichlet Allocation (LDA)
An increasingly popular class of latent variable models are mixed membership models, where
each datum may belong to several different latent classes simultaneously. LDA is one such
model for the case of document modeling; here, each document corresponds to a mixture
over topics (as opposed to just a single topic). The distribution over such topic mixtures is a
Dirichlet distribution Dir(α) with parameter vector α ∈ Rk
++ with strictly positive entries;
its density over the probability simplex ∆k−1 := {v ∈ Rk : vi ∈ [0, 1]∀i ∈ [k],
Pk
i=1 vi = 1}
is given by
pα(h) =
Γ(α0)
Qk
i=1 Γ(αi)
k
Y
i=1
hαi−1
i , h ∈ ∆k−1
where
α0 := α1 + α2 + · · · + αk.
As before, the k topics are specified by probability vectors µ1, µ2, . . . , µk ∈ ∆d−1. To
generate a document, we first draw the topic mixture h = (h1, h2, . . . , hk) ∼ Dir(α), and
then conditioned on h, we draw ` words x1, x2, . . . , x` independently from the discrete
distribution specified by the probability vector
Pk
i=1 hiµi (i.e., for each xt, we independently
sample a topic j according to h and then sample xt according to µj). Again, we encode a
word xt by setting xt = ei iff the t-th word in the document is i.
The parameter α0 (the sum of the “pseudo-counts”) characterizes the concentration of
the distribution. As α0 → 0, the distribution degenerates to a single topic model (i.e., the
limiting density has, with probability 1, exactly one entry of h being 1 and the rest are 0).
At the other extreme, if α = (c, c, . . . , c) for some scalar c > 0, then as α0 = ck → ∞, the
distribution of h becomes peaked around the uniform vector (1/k, 1/k, . . . , 1/k) (further-
more, the distribution behaves like a product distribution). We are typically interested in
2783
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
h
x1 x2 · · · x`
(a) Multi-view models
h1 h2 · · · h`
x1 x2 x`
(b) Hidden Markov model
Figure 1: Examples of latent variable models.
the case where α0 is small (e.g., a constant independent of k), whereupon h typically has
only a few large entries. This corresponds to the setting where the documents are mainly
comprised of just a few topics.
Theorem 3.5 (Anandkumar et al., 2012a) Define
M1 := E[x1]
M2 := E[x1 ⊗ x2] −
α0
α0 + 1
M1 ⊗ M1
M3 := E[x1 ⊗ x2 ⊗ x3]
−
α0
α0 + 2

E[x1 ⊗ x2 ⊗ M1] + E[x1 ⊗ M1 ⊗ x2] + E[M1 ⊗ x1 ⊗ x2]

+
2α2
0
(α0 + 2)(α0 + 1)
M1 ⊗ M1 ⊗ M1.
Then
M2 =
k
X
i=1
αi
(α0 + 1)α0
µi ⊗ µi
M3 =
k
X
i=1
2αi
(α0 + 2)(α0 + 1)α0
µi ⊗ µi ⊗ µi.
Note that α0 needs to be known to form M2 and M3 from the raw moments. This,
however, is a much weaker than assuming that the entire distribution of h is known (i.e.,
knowledge of the whole parameter vector α).
3.3 Multi-View Models
Multi-view models (also sometimes called naı̈ve Bayes models) are a special class of Bayesian
networks in which observed variables x1, x2, . . . , x` are conditionally independent given a
latent variable h. This is similar to the exchangeable single topic model, but here we
do not require the conditional distributions of the xt, t ∈ [`] to be identical. Techniques
developed for this class can be used to handle a number of widely used models including
hidden Markov models (Mossel and Roch, 2006; Anandkumar et al., 2012c), phylogenetic
tree models (Chang, 1996; Mossel and Roch, 2006), certain tree mixtures (Anandkumar
et al., 2012b), and certain probabilistic grammar models (Hsu et al., 2012a).
2784
Tensor Decompositions for Learning Latent Variable Models
As before, we let h ∈ [k] be a discrete random variable with Pr[h = j] = wj for all j ∈ [k].
Now consider random vectors x1 ∈ Rd1 , x2 ∈ Rd2 , and x3 ∈ Rd3 which are conditionally
independent given h, and
E[xt|h = j] = µt,j, j ∈ [k], t ∈ {1, 2, 3}
where the µt,j ∈ Rdt are the conditional means of the xt given h = j. Thus, we allow the
observations x1, x2, . . . , x` to be random vectors, parameterized only by their conditional
means. Importantly, these conditional distributions may be discrete, continuous, or even a
mix of both.
We first note the form for the raw (cross) moments.
Proposition 3.1 We have that:
E[xt ⊗ xt0 ] =
k
X
i=1
wi µt,i ⊗ µt0,i, {t, t0
} ⊂ {1, 2, 3}, t 6= t0
E[x1 ⊗ x2 ⊗ x3] =
k
X
i=1
wi µ1,i ⊗ µ2,i ⊗ µ3,i.
The cross moments do not possess a symmetric tensor form when the conditional distri-
butions are different. Nevertheless, the moments can be “symmetrized” via a simple linear
transformation of x1 and x2 (roughly speaking, this relates x1 and x2 to x3); this leads
to an expression from which the conditional means of x3 (i.e., µ3,1, µ3,2, . . . , µ3,k) can be
recovered. For simplicity, we assume d1 = d2 = d3 = k; the general case (with dt ≥ k) is
easily handled using low-rank singular value decompositions.
Theorem 3.6 (Anandkumar et al., 2012a) Assume that {µv,1, µv,2, . . . , µv,k} are lin-
early independent for each v ∈ {1, 2, 3}. Define
x̃1 := E[x3 ⊗ x2]E[x1 ⊗ x2]−1
x1
x̃2 := E[x3 ⊗ x1]E[x2 ⊗ x1]−1
x2
M2 := E[x̃1 ⊗ x̃2]
M3 := E[x̃1 ⊗ x̃2 ⊗ x3].
Then
M2 =
k
X
i=1
wi µ3,i ⊗ µ3,i
M3 =
k
X
i=1
wi µ3,i ⊗ µ3,i ⊗ µ3,i.
We now discuss three examples (taken mostly from Anandkumar et al., 2012c) where the
above observations can be applied. The first two concern mixtures of product distributions,
and the last one is the time-homogeneous hidden Markov model.
2785
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
3.3.1 Mixtures of Axis-Aligned Gaussians and Other Product Distributions
The first example is a mixture of k product distributions in Rn under a mild incoherence as-
sumption (Anandkumar et al., 2012c). Here, we allow each of the k component distributions
to have a different product distribution (e.g., Gaussian distribution with an axis-aligned co-
variance matrix), but require the matrix of component means A := [µ1|µ2| · · · |µk] ∈ Rn×k
to satisfy a certain (very mild) incoherence condition. The role of the incoherence condition
is explained below.
For a mixture of product distributions, any partitioning of the dimensions [n] into three
groups creates three (possibly asymmetric) “views” which are conditionally independent
once the mixture component is selected. However, recall that Theorem 3.6 requires that
for each view, the k conditional means be linearly independent. In general, this may not
be achievable; consider, for instance, the case µi = ei for each i ∈ [k]. Such cases, where
the component means are very aligned with the coordinate basis, are precluded by the
incoherence condition.
Define coherence(A) := maxi∈[n]{e>
i ΠAei} to be the largest diagonal entry of the orthog-
onal projector to the range of A, and assume A has rank k. The coherence lies between k/n
and 1; it is largest when the range of A is spanned by the coordinate axes, and it is k/n when
the range is spanned by a subset of the Hadamard basis of cardinality k. The incoherence
condition requires, for some ε, δ ∈ (0, 1), coherence(A) ≤ (ε2/6)/ ln(3k/δ). Essentially, this
condition ensures that the non-degeneracy of the component means is not isolated in just
a few of the n dimensions. Operationally, it implies the following.
Proposition 3.2 (Anandkumar et al., 2012c) Assume A has rank k, and
coherence(A) ≤
ε2/6
ln(3k/δ)
for some ε, δ ∈ (0, 1). With probability at least 1−δ, a random partitioning of the dimensions
[n] into three groups (for each i ∈ [n], independently pick t ∈ {1, 2, 3} uniformly at random
and put i in group t) has the following property. For each t ∈ {1, 2, 3} and j ∈ [k], let
µt,j be the entries of µj put into group t, and let At := [µt,1|µt,2| · · · |µt,k]. Then for each
t ∈ {1, 2, 3}, At has full column rank, and the k-th largest singular value of At is at least
p
(1 − ε)/3 times that of A.
Therefore, three asymmetric views can be created by randomly partitioning the observed
random vector x into x1, x2, and x3, such that the resulting component means for each
view satisfy the conditions of Theorem 3.6.
3.3.2 Spherical Gaussian Mixtures, Revisited
Consider again the case of spherical Gaussian mixtures (cf. Section 3.2). As we shall see
in Section 4.3, the previous techniques (based on Theorem 3.2 and Theorem 3.3) lead to
estimation procedures when the dimension of x is k or greater (and when the k component
means are linearly independent). We now show that when the dimension is slightly larger,
say greater than 3k, a different (and simpler) technique based on the multi-view structure
can be used to extract the relevant structure.
2786
Tensor Decompositions for Learning Latent Variable Models
We again use a randomized reduction. Specifically, we create three views by (i) applying
a random rotation to x, and then (ii) partitioning x ∈ Rn into three views x̃1, x̃2, x̃3 ∈ Rd
for d := n/3. By the rotational invariance of the multivariate Gaussian distribution, the
distribution of x after random rotation is still a mixture of spherical Gaussians (i.e., a
mixture of product distributions), and thus x̃1, x̃2, x̃3 are conditionally independent given
h. What remains to be checked is that, for each view t ∈ {1, 2, 3}, the matrix of conditional
means of x̃t for each view has full column rank. This is true with probability 1 as long as
the matrix of conditional means A := [µ1|µ2| · · · |µk] ∈ Rn×k has rank k and n ≥ 3k. To
see this, observe that a random rotation in Rn followed by a restriction to d coordinates
is simply a random projection from Rn to Rd, and that a random projection of a linear
subspace of dimension k to Rd is almost surely injective as long as d ≥ k. Applying this
observation to the range of A implies the following.
Proposition 3.3 (Hsu and Kakade, 2013) Assume A has rank k and that n ≥ 3k. Let
R ∈ Rn×n be chosen uniformly at random among all orthogonal n × n matrices, and set
x̃ := Rx ∈ Rn and Ã := RA = [Rµ1|Rµ2| · · · |Rµk] ∈ Rn×k. Partition [n] into three groups
of sizes d1, d2, d3 with dt ≥ k for each t ∈ {1, 2, 3}. Furthermore, for each t, define x̃t ∈ Rdt
(respectively, Ãt ∈ Rdt×k) to be the subvector of x̃ (resp., submatrix of Ã) obtained by
selecting the dt entries (resp., rows) in the t-th group. Then x̃1, x̃2, x̃3 are conditionally
independent given h; E[x̃t|h = j] = Ãtej for each j ∈ [k] and t ∈ {1, 2, 3}; and with
probability 1, the matrices Ã1, Ã2, Ã3 have full column rank.
It is possible to obtain a quantitative bound on the k-th largest singular value of each At
in terms of the k-th largest singular value of A (analogous to Proposition 3.2). One avenue
is to show that a random rotation in fact causes Ã to have low coherence, after which we
can apply Proposition 3.2. With this approach, it is sufficient to require n = O(k log k)
(for constant ε and δ), which results in the k-th largest singular value of each At being
a constant fraction of the k-th largest singular value of A. We conjecture that, in fact,
n ≥ c · k for some c > 3 suffices.
3.3.3 Hidden Markov Models
Our last example is the time-homogeneous HMM for sequences of vector-valued observations
x1, x2, . . . ∈ Rd. Consider a Markov chain of discrete hidden states y1 → y2 → y3 → · · ·
over k possible states [k]; given a state yt at time t, the observation xt at time t (a random
vector taking values in Rd) is independent of all other observations and hidden states. See
Figure 1(b).
Let π ∈ ∆k−1 be the initial state distribution (i.e., the distribution of y1), and T ∈ Rk×k
be the stochastic transition matrix for the hidden state Markov chain: for all times t,
Pr[yt+1 = i|yt = j] = Ti,j, i, j ∈ [k].
Finally, let O ∈ Rd×k be the matrix whose j-th column is the conditional expectation of xt
given yt = j: for all times t,
E[xt|yt = j] = Oej, j ∈ [k].
2787
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Proposition 3.4 (Anandkumar et al., 2012c) Define h := y2, where y2 is the second
hidden state in the Markov chain. Then
• x1, x2, x3 are conditionally independent given h;
• the distribution of h is given by the vector w := Tπ ∈ ∆k−1;
• for all j ∈ [k],
E[x1|h = j] = O diag(π)T>
diag(w)−1
ej
E[x2|h = j] = Oej
E[x3|h = j] = OTej.
Note the matrix of conditional means of xt has full column rank, for each t ∈ {1, 2, 3},
provided that: (i) O has full column rank, (ii) T is invertible, and (iii) π and Tπ have
positive entries.
4. Orthogonal Tensor Decompositions
We now show how recovering the µi’s in our aforementioned problems reduces to the prob-
lem of finding a certain orthogonal tensor decomposition of a symmetric tensor. We start by
reviewing the spectral decomposition of symmetric matrices, and then discuss a generaliza-
tion to the higher-order tensor case. Finally, we show how orthogonal tensor decompositions
can be used for estimating the latent variable models from the previous section.
4.1 Review: The Matrix Case
We first build intuition by reviewing the matrix setting, where the desired decomposi-
tion is the eigendecomposition of a symmetric rank-k matrix M = V ΛV >
, where V =
[v1|v2| · · · |vk] ∈ Rn×k is the matrix with orthonormal eigenvectors as columns, and Λ =
diag(λ1, λ2, . . . , λk) ∈ Rk×k is diagonal matrix of non-zero eigenvalues. In other words,
M =
k
X
i=1
λi viv>
i =
k
X
i=1
λi v⊗2
i . (1)
Such a decomposition is guaranteed to exist for every symmetric matrix.
Recovery of the vi’s and λi’s can be viewed at least two ways. First, each vi is fixed
under the mapping u 7→ Mu, up to a scaling factor λi:
Mvi =
k
X
j=1
λj(v>
j vi)vj = λivi
as v>
j vi = 0 for all j 6= i by orthogonality. The vi’s are not necessarily the only such fixed
points. For instance, with the multiplicity λ1 = λ2 = λ, then any linear combination of v1
and v2 is similarly fixed under M. However, in this case, the decomposition in (1) is not
unique, as λ1v1v>
1 + λ2v2v>
2 is equal to λ(u1u>
1 + u2u>
2 ) for any pair of orthonormal vectors,
2788
Tensor Decompositions for Learning Latent Variable Models
u1 and u2 spanning the same subspace as v1 and v2. Nevertheless, the decomposition is
unique when λ1, λ2, . . . , λk are distinct, whereupon the vj’s are the only directions fixed
under u 7→ Mu up to non-trivial scaling.
The second view of recovery is via the variational characterization of the eigenvalues.
Assume λ1 > λ2 > · · · > λk; the case of repeated eigenvalues again leads to similar non-
uniqueness as discussed above. Then the Rayleigh quotient
u 7→
u>
Mu
u>
u
is maximized over non-zero vectors by v1. Furthermore, for any s ∈ [k], the maximizer of
the Rayleigh quotient, subject to being orthogonal to v1, v2, . . . , vs−1, is vs. Another way
of obtaining this second statement is to consider the deflated Rayleigh quotient
u 7→
u>

M −
Ps−1
j=1 λjvjv>
j

u
u>
u
and observe that vs is the maximizer.
Efficient algorithms for finding these matrix decompositions are well studied (Golub
and van Loan, 1996, Section 8.2.3), and iterative power methods are one effective class of
algorithms.
We remark that in our multilinear tensor notation, we may write the maps u 7→ Mu
and u 7→ u>
Mu/kuk2
2 as
u 7→ Mu ≡ u 7→ M(I, u), (2)
u 7→
u>
Mu
u>
u
≡ u 7→
M(u, u)
u>
u
. (3)
4.2 The Tensor Case
Decomposing general tensors is a delicate issue; tensors may not even have unique decom-
positions. Fortunately, the orthogonal tensors that arise in the aforementioned models have
a structure which permits a unique decomposition under a mild non-degeneracy condition.
We focus our attention to the case p = 3, i.e., a third order tensor; the ideas extend to
general p with minor modifications.
An orthogonal decomposition of a symmetric tensor T ∈
N3
Rn is a collection of or-
thonormal (unit) vectors {v1, v2, . . . , vk} together with corresponding positive scalars λi > 0
such that
T =
k
X
i=1
λiv⊗3
i . (4)
Note that since we are focusing on odd-order tensors (p = 3), we have added the require-
ment that the λi be positive. This convention can be followed without loss of generality
since −λiv⊗p
i = λi(−vi)⊗p whenever p is odd. Also, it should be noted that orthogonal
decompositions do not necessarily exist for every symmetric tensor.
In analogy to the matrix setting, we consider two ways to view this decomposition: a
fixed-point characterization and a variational characterization. Related characterizations
based on optimal rank-1 approximations are given by Zhang and Golub (2001).
2789
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
4.2.1 Fixed-Point Characterization
For a tensor T, consider the vector-valued map
u 7→ T(I, u, u) (5)
which is the third-order generalization of (2). This can be explicitly written as
T(I, u, u) =
d
X
i=1
X
1≤j,l≤d
Ti,j,l(e>
j u)(e>
l u)ei.
Observe that (5) is not a linear map, which is a key difference compared to the matrix case.
An eigenvector u for a matrix M satisfies M(I, u) = λu, for some scalar λ. We say a
unit vector u ∈ Rn is an eigenvector of T, with corresponding eigenvalue λ ∈ R, if
T(I, u, u) = λu.
(To simplify the discussion, we assume throughout that eigenvectors have unit norm; oth-
erwise, for scaling reasons, we replace the above equation with T(I, u, u) = λkuku.) This
concept was originally introduced by Lim (2005) and Qi (2005). For orthogonally decom-
posable tensors T =
Pk
i=1 λiv⊗3
i ,
T(I, u, u) =
k
X
i=1
λi(u>
vi)2
vi .
By the orthogonality of the vi, it is clear that T(I, vi, vi) = λivi for all i ∈ [k]. Therefore
each (vi, λi) is an eigenvector/eigenvalue pair.
There are a number of subtle differences compared to the matrix case that arise as a
result of the non-linearity of (5). First, even with the multiplicity λ1 = λ2 = λ, a linear
combination u := c1v1 + c2v2 may not be an eigenvector. In particular,
T(I, u, u) = λ1c2
1v1 + λ2c2
2v2 = λ(c2
1v1 + c2
2v2)
may not be a multiple of c1v1 + c2v2. This indicates that the issue of repeated eigenvalues
does not have the same status as in the matrix case. Second, even if all the eigenvalues
are distinct, it turns out that the vi’s are not the only eigenvectors. For example, set
u := (1/λ1)v1 + (1/λ2)v2. Then,
T(I, u, u) = λ1(1/λ1)2
v1 + λ2(1/λ2)2
v2 = u,
so u/kuk is an eigenvector. More generally, for any subset S ⊆ [k], the vector
X
i∈S
1
λi
· vi
is (proportional to) an eigenvector.
2790
Tensor Decompositions for Learning Latent Variable Models
As we now see, these additional eigenvectors can be viewed as spurious. We say a unit
vector u is a robust eigenvector of T if there exists an  > 0 such that for all θ ∈ {u0 ∈ Rn :
ku0 − uk ≤ }, repeated iteration of the map
θ̄ 7→
T(I, θ̄, θ̄)
kT(I, θ̄, θ̄)k
, (6)
starting from θ converges to u. Note that the map (6) rescales the output to have unit
Euclidean norm. Robust eigenvectors are also called attracting fixed points of (6) (see, e.g.,
Kolda and Mayo, 2011).
The following theorem implies that if T has an orthogonal decomposition as given in (4),
then the set of robust eigenvectors of T are precisely the set {v1, v2, . . . vk}, implying that
the orthogonal decomposition is unique. (For even order tensors, the uniqueness is true up
to sign-flips of the vi.)
Theorem 4.1 Let T have an orthogonal decomposition as given in (4).
1. The set of θ ∈ Rn which do not converge to some vi under repeated iteration of (6)
has measure zero.
2. The set of robust eigenvectors of T is equal to {v1, v2, . . . , vk}.
The proof of Theorem 4.1 is given in Appendix A.1, and follows readily from simple or-
thogonality considerations. Note that every vi in the orthogonal tensor decomposition is
robust, whereas for a symmetric matrix M, for almost all initial points, the map θ̄ 7→ Mθ̄
kMθ̄k
converges only to an eigenvector corresponding to the largest magnitude eigenvalue. Also,
since the tensor order is odd, the signs of the robust eigenvectors are fixed, as each −vi is
mapped to vi under (6).
4.2.2 Variational Characterization
We now discuss a variational characterization of the orthogonal decomposition. The gener-
alized Rayleigh quotient (Zhang and Golub, 2001) for a third-order tensor is
u 7→
T(u, u, u)
(u>
u)3/2
,
which can be compared to (3). For an orthogonally decomposable tensor, the following
theorem shows that a non-zero vector u ∈ Rn is an isolated local maximizer (Nocedal and
Wright, 1999) of the generalized Rayleigh quotient if and only if u = vi for some i ∈ [k].
Theorem 4.2 Let T have an orthogonal decomposition as given in (4), and consider the
optimization problem
max
u∈Rn
T(u, u, u) s.t. kuk ≤ 1.
1. The stationary points are eigenvectors of T.
2. A stationary point u is an isolated local maximizer if and only if u = vi for some
i ∈ [k].
2791
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
The proof of Theorem 4.2 is given in Appendix A.2. It is similar to local optimality analysis
for ICA methods using fourth-order cumulants (e.g., Delfosse and Loubaton, 1995; Frieze
et al., 1996).
Again, we see similar distinctions to the matrix case. In the matrix case, the only local
maximizers of the Rayleigh quotient are the eigenvectors with the largest eigenvalue (and
these maximizers take on the globally optimal value). For the case of orthogonal tensor
forms, the robust eigenvectors are precisely the isolated local maximizers.
An important implication of the two characterizations is that, for orthogonally decom-
posable tensors T, (i) the local maximizers of the objective function u 7→ T(u, u, u)/(u>
u)3/2
correspond precisely to the vectors vi in the decomposition, and (ii) these local maximizers
can be reliably identified using a simple fixed-point iteration (i.e., the tensor analogue of
the matrix power method). Moreover, a second-derivative test based on T(I, I, u) can be
employed to test for local optimality and rule out other stationary points.
4.3 Estimation via Orthogonal Tensor Decompositions
We now demonstrate how the moment tensors obtained for various latent variable models
in Section 3 can be reduced to an orthogonal form. For concreteness, we take the specific
form from the exchangeable single topic model (Theorem 3.1):
M2 =
k
X
i=1
wi µi ⊗ µi,
M3 =
k
X
i=1
wi µi ⊗ µi ⊗ µi.
(The more general case allows the weights wi in M2 to differ in M3, but for simplicity
we keep them the same in the following discussion.) We now show how to reduce these
forms to an orthogonally decomposable tensor from which the wi and µi can be recovered.
See Appendix D for a discussion as to how previous approaches (Mossel and Roch, 2006;
Anandkumar et al., 2012c,a; Hsu and Kakade, 2013) achieved this decomposition through
a certain simultaneous diagonalization method.
Throughout, we assume the following non-degeneracy condition.
Condition 4.1 (Non-degeneracy) The vectors µ1, µ2, . . . , µk ∈ Rd are linearly indepen-
dent, and the scalars w1, w2, . . . , wk > 0 are strictly positive.
Observe that Condition 4.1 implies that M2  0 is positive semidefinite and has rank k.
This is often a mild condition in applications. When this condition is not met, learning
is conjectured to be generally hard for both computational (Mossel and Roch, 2006) and
information-theoretic reasons (Moitra and Valiant, 2010). As discussed by Hsu et al. (2012b)
and Hsu and Kakade (2013), when the non-degeneracy condition does not hold, it is often
possible to combine multiple observations using tensor products to increase the rank of the
relevant matrices. Indeed, this observation has been rigorously formulated in very recent
works of Bhaskara et al. (2014) and Anderson et al. (2014) using the framework of smoothed
analysis (Spielman and Teng, 2009).
2792
Tensor Decompositions for Learning Latent Variable Models
4.3.1 The Reduction
First, let W ∈ Rd×k be a linear transformation such that
M2(W, W) = W>
M2W = I
where I is the k × k identity matrix (i.e., W whitens M2). Since M2  0, we may for
concreteness take W := UD−1/2, where U ∈ Rd×k is the matrix of orthonormal eigenvectors
of M2, and D ∈ Rk×k is the diagonal matrix of positive eigenvalues of M2. Let
µ̃i :=
√
wi W>
µi.
Observe that
M2(W, W) =
k
X
i=1
W>
(
√
wiµi)(
√
wiµi)>
W =
k
X
i=1
µ̃iµ̃>
i = I,
so the µ̃i ∈ Rk are orthonormal vectors.
Now define f
M3 := M3(W, W, W) ∈ Rk×k×k, so that
f
M3 =
k
X
i=1
wi (W>
µi)⊗3
=
k
X
i=1
1
√
wi
µ̃⊗3
i .
As the following theorem shows, the orthogonal decomposition of f
M3 can be obtained by
identifying its robust eigenvectors, upon which the original parameters wi and µi can be
recovered. For simplicity, we only state the result in terms of robust eigenvector/eigenvalue
pairs; one may also easily state everything in variational form using Theorem 4.2.
Theorem 4.3 Assume Condition 4.1 and take f
M3 as defined above.
1. The set of robust eigenvectors of f
M3 is equal to {µ̃1, µ̃2, . . . , µ̃k}.
2. The eigenvalue corresponding to the robust eigenvector µ̃i of f
M3 is equal to 1/
√
wi,
for all i ∈ [k].
3. If B ∈ Rd×k is the Moore-Penrose pseudoinverse of W>
, and (v, λ) is a robust eigen-
vector/eigenvalue pair of f
M3, then λBv = µi for some i ∈ [k].
The theorem follows by combining the above discussion with the robust eigenvector charac-
terization of Theorem 4.1. Recall that we have taken as convention that eigenvectors have
unit norm, so the µi are exactly determined from the robust eigenvector/eigenvalue pairs of
f
M3 (together with the pseudoinverse of W>
); in particular, the scale of each µi is correctly
identified (along with the corresponding wi). Relative to previous works on moment-based
estimators for latent variable models (e.g., Anandkumar et al., 2012c,a; Hsu and Kakade,
2013), Theorem 4.3 emphasizes the role of the special tensor structure, which in turn makes
transparent the applicability of methods for orthogonal tensor decomposition.
2793
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
4.3.2 Local Maximizers of (Cross Moment) Skewness
The variational characterization provides an interesting perspective on the robust eigen-
vectors for these latent variable models. Consider the exchangeable single topic models
(Theorem 3.1), and the objective function
u 7→
E[(x>
1 u)(x>
2 u)(x>
3 u)]
E[(x>
1 u)(x>
2 u)]3/2
=
M3(u, u, u)
M2(u, u)3/2
.
In this case, every local maximizer u∗ satisfies M2(I, u∗) =
√
wiµi for some i ∈ [k]. The
objective function can be interpreted as the (cross moment) skewness of the random vectors
x1, x2, x3 along direction u.
5. Tensor Power Method
In this section, we consider the tensor power method of Lathauwer et al. (2000, Remark 3)
for orthogonal tensor decomposition. We first state a simple convergence analysis for an
orthogonally decomposable tensor T.
When only an approximation T̂ to an orthogonally decomposable tensor T is available
(e.g., when empirical moments are used to estimate population moments), an orthogonal
decomposition need not exist for this perturbed tensor (unlike for the case of matrices),
and a more robust approach is required to extract the approximate decomposition. Here,
we propose such a variant in Algorithm 1 and provide a detailed perturbation analysis. We
note that alternative approaches such as simultaneous diagonalization can also be employed
(see Appendix D).
5.1 Convergence Analysis for Orthogonally Decomposable Tensors
The following lemma establishes the quadratic convergence of the tensor power method—
i.e., repeated iteration of (6)—for extracting a single component of the orthogonal decom-
position. Note that the initial vector θ0 determines which robust eigenvector will be the
convergent point. Computation of subsequent eigenvectors can be computed with deflation,
i.e., by subtracting appropriate terms from T.
Lemma 5.1 Let T ∈
N3
Rn have an orthogonal decomposition as given in (4). For a vector
θ0 ∈ Rn, suppose that the set of numbers |λ1v>
1 θ0|, |λ2v>
2 θ0|, . . . , |λkv>
k θ0| has a unique largest
element. Without loss of generality, say |λ1v>
1 θ0| is this largest value and |λ2v>
2 θ0| is the
second largest value. For t = 1, 2, . . . , let
θt :=
T(I, θt−1, θt−1)
kT(I, θt−1, θt−1)k
.
Then
kv1 − θtk2
≤

2λ2
1
k
X
i=2
λ−2
i

·
λ2v>
2 θ0
λ1v>
1 θ0
2t+1
.
That is, repeated iteration of (6) starting from θ0 converges to v1 at a quadratic rate.
2794
Tensor Decompositions for Learning Latent Variable Models
To obtain all eigenvectors, we may simply proceed iteratively using deflation, executing
the power method on T −
P
j λjv⊗3
j after having obtained robust eigenvector / eigenvalue
pairs {(vj, λj)}.
Proof Let θ0, θ1, θ2, . . . be the sequence given by θ0 := θ0 and θt := T(I, θt−1, θt−1) for
t ≥ 1. Let ci := v>
i θ0 for all i ∈ [k]. It is easy to check that (i) θt = θt/kθtk, and
(ii) θt =
Pk
i=1 λ2t−1
i c2t
i vi. (Indeed, θt+1 =
Pk
i=1 λi(v>
i θt)2vi =
Pk
i=1 λi(λ2t−1
i c2t
i )2vi =
Pk
i=1 λ2t+1−1
i c2t+1
i vi.) Then
1 − (v>
1 θt)2
= 1 −
(v>
1 θt)2
kθtk2
= 1 −
λ2t+1−2
1 c2t+1
1
Pk
i=1 λ2t+1−2
i c2t+1
i
≤
Pk
i=2 λ2t+1−2
i c2t+1
i
Pk
i=1 λ2t+1−2
i c2t+1
i
≤ λ2
1
k
X
i=2
λ−2
i ·
λ2c2
λ1c1
2t+1
.
Since λ1 > 0, we have v>
1 θt > 0 and hence kv1 − θtk2 = 2(1 − v>
1 θt) ≤ 2(1 − (v>
1 θt)2) as
required.
5.2 Perturbation Analysis of a Robust Tensor Power Method
Now we consider the case where we have an approximation T̂ to an orthogonally decom-
posable tensor T. Here, a more robust approach is required to extract an approximate
decomposition. We propose such an algorithm in Algorithm 1, and provide a detailed per-
turbation analysis. For simplicity, we assume the tensor T̂ is of size k × k × k as per the
reduction from Section 4.3. In some applications, it may be preferable to work directly with
a n × n × n tensor of rank k ≤ n (as in Lemma 5.1); our results apply in that setting with
little modification.
Algorithm 1 Robust tensor power method
input symmetric tensor T̃ ∈ Rk×k×k, number of iterations L, N.
output the estimated eigenvector/eigenvalue pair; the deflated tensor.
1: for τ = 1 to L do
2: Draw θ
(τ)
0 uniformly at random from the unit sphere in Rk.
3: for t = 1 to N do
4: Compute power iteration update
θ
(τ)
t :=
T̃(I, θ
(τ)
t−1, θ
(τ)
t−1)
kT̃(I, θ
(τ)
t−1, θ
(τ)
t−1)k
(7)
5: end for
6: end for
7: Let τ∗ := arg maxτ∈[L]{T̃(θ
(τ)
N , θ
(τ)
N , θ
(τ)
N )}.
8: Do N power iteration updates (7) starting from θ
(τ∗)
N to obtain θ̂, and set λ̂ := T̃(θ̂, θ̂, θ̂).
9: return the estimated eigenvector/eigenvalue pair (θ̂, λ̂); the deflated tensor T̃ −λ̂ θ̂⊗3.
2795
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Assume that the symmetric tensor T ∈ Rk×k×k is orthogonally decomposable, and that
T̂ = T + E, where the perturbation E ∈ Rk×k×k is a symmetric tensor with small operator
norm:
kEk := sup
kθk=1
|E(θ, θ, θ)|.
In our latent variable model applications, T̂ is the tensor formed by using empirical mo-
ments, while T is the orthogonally decomposable tensor derived from the population mo-
ments for the given model. In the context of parameter estimation (as in Section 4.3), E
must account for any error amplification throughout the reduction, such as in the whitening
step (see, e.g., Hsu and Kakade, 2013, for such an analysis).
The following theorem is similar to Wedin’s perturbation theorem for singular vectors
of matrices (Wedin, 1972) in that it bounds the error of the (approximate) decomposition
returned by Algorithm 1 on input T̂ in terms of the size of the perturbation, provided that
the perturbation is small enough.
Theorem 5.1 Let T̂ = T + E ∈ Rk×k×k, where T is a symmetric tensor with orthogonal
decomposition T =
Pk
i=1 λiv⊗3
i where each λi > 0, {v1, v2, . . . , vk} is an orthonormal basis,
and E is a symmetric tensor with operator norm kEk ≤ . Define λmin := min{λi : i ∈ [k]},
and λmax := max{λi : i ∈ [k]}. There exists universal constants C1, C2, C3 > 0 such that
the following holds. Pick any η ∈ (0, 1), and suppose
 ≤ C1 ·
λmin
k
, N ≥ C2 ·

log(k) + log log
λmax


,
and
s
ln(L/ log2(k/η))
ln(k)
· 1 −
ln(ln(L/ log2(k/η))) + C3
4 ln(L/ log2(k/η))
−
s
ln(8)
ln(L/ log2(k/η))
!
≥ 1.02 1 +
s
ln(4)
ln(k)
!
.
(Note that the condition on L holds with L = poly(k) log(1/η).) Suppose that Algorithm 1
is iteratively called k times, where the input tensor is T̂ in the first call, and in each
subsequent call, the input tensor is the deflated tensor returned by the previous call. Let
(v̂1, λ̂1), (v̂2, λ̂2), . . . , (v̂k, λ̂k) be the sequence of estimated eigenvector/eigenvalue pairs re-
turned in these k calls. With probability at least 1 − η, there exists a permutation π on [k]
such that
kvπ(j) − v̂jk ≤ 8/λπ(j), |λπ(j) − λ̂j| ≤ 5, ∀j ∈ [k],
and
T −
k
X
j=1
λ̂jv̂⊗3
j ≤ 55.
The proof of Theorem 5.1 is given in Appendix B.
One important difference from Wedin’s theorem is that this is an algorithm dependent
perturbation analysis, specific to Algorithm 1 (since the perturbed tensor need not have an
2796
Tensor Decompositions for Learning Latent Variable Models
orthogonal decomposition). Furthermore, note that Algorithm 1 uses multiple restarts to
ensure (approximate) convergence—the intuition is that by restarting at multiple points,
we eventually start at a point in which the initial contraction towards some eigenvector
dominates the error E in our tensor. The proof shows that we find such a point with high
probability within L = poly(k) trials. It should be noted that for large k, the required
bound on L is very close to linear in k.
We note that it is also possible to design a variant of Algorithm 1 that instead uses
a stopping criterion to determine if an iterate has (almost) converged to an eigenvector.
For instance, if T̃(θ, θ, θ) > max{kT̃kF /
√
2r, kT̃(I, I, θ)kF /1.05}, where kT̃kF is the tensor
Frobenius norm (vectorized Euclidean norm), and r is the expected rank of the unperturbed
tensor (r = k − # of deflation steps), then it can be shown that θ must be close to one of
the eigenvectors, provided that the perturbation is small enough. Using such a stopping
criterion can reduce the number of random restarts when a good initial point is found early
on. See Appendix C for details.
In general, it is possible, when run on a general symmetric tensor (e.g., T̂), for the
tensor power method to exhibit oscillatory behavior (Kofidis and Regalia, 2002, Example
1). This is not in conflict with Theorem 5.1, which effectively bounds the amplitude of
these oscillations; in particular, if T̂ = T + E is a tensor built from empirical moments, the
error term E (and thus the amplitude of the oscillations) can be driven down by drawing
more samples. The practical value of addressing these oscillations and perhaps stabilizing
the algorithm is an interesting direction for future research (Kolda and Mayo, 2011).
A final consideration is that for specific applications, it may be possible to use domain
knowledge to choose better initialization points. For instance, in the topic modeling appli-
cations (cf. Section 3.1), the eigenvectors are related to the topic word distributions, and
many documents may be primarily composed of words from just single topic. Therefore,
good initialization points can be derived from these single-topic documents themselves, as
these points would already be close to one of the eigenvectors.
6. Discussion
In this section, we discuss some practical and application-oriented issues related to the
tensor decomposition approach to learning latent variable models.
6.1 Practical Implementation Considerations
A number of practical concerns arise when dealing with moment matrices and tensors.
Below, we address two issues that are especially pertinent to topic modeling applica-
tions (Anandkumar et al., 2012c,a) or other settings where the observations are sparse.
6.1.1 Efficient Moment Representation for Exchangeable Models
In an exchangeable bag-of-words model, it is assumed that the words x1, x2, . . . , x` in a
document are conditionally i.i.d. given the topic h. This allows one to estimate p-th order
moments using just p words per document. The estimators obtained via Theorem 3.1 (single
topic model) and Theorem 3.5 (LDA) use only up to third-order moments, which suggests
that each document only needs to have three words.
2797
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
In practice, one should use all of the words in a document for efficient estimation of the
moments. One way to do this is to average over all `
3

· 3! ordered triples of words in a
document of length `. At first blush, this seems computationally expensive (when ` is large),
but as it turns out, the averaging can be done implicitly, as shown by Zou et al. (2013).
Let c ∈ Rd be the word count vector for a document of length `, so ci is the number of
occurrences of word i in the document, and
Pd
i=1 ci = `. Note that c is a sufficient statistic
for the document. Then, the contribution of this document to the empirical third-order
moment tensor is given by
1
`
3
 ·
1
3!
·

c ⊗ c ⊗ c + 2
d
X
i=1
ci (ei ⊗ ei ⊗ ei)
−
d
X
i=1
d
X
j=1
cicj (ei ⊗ ei ⊗ ej) −
d
X
i=1
d
X
j=1
cicj (ei ⊗ ej ⊗ ei) −
d
X
i=1
d
X
j=1
cicj (ei ⊗ ej ⊗ ej)

. (8)
It can be checked that this quantity is equal to
1
`
3
 ·
1
3!
·
X
ordered word triple (x, y, z)
ex ⊗ ey ⊗ ez
where the sum is over all ordered word triples in the document. A similar expression is
easily derived for the contribution of the document to the empirical second-order moment
matrix:
1
`
2
 ·
1
2!
·

c ⊗ c − diag(c)

. (9)
Note that the word count vector c is generally a sparse vector, so this representation allows
for efficient multiplication by the moment matrices and tensors in time linear in the size of
the document corpus (i.e., the number of non-zero entries in the term-document matrix).
6.1.2 Dimensionality Reduction
Another serious concern regarding the use of tensor forms of moments is the need to op-
erate on multidimensional arrays with Ω(d3) values (it is typically not exactly d3 due to
symmetry). When d is large (e.g., when it is the size of the vocabulary in natural language
applications), even storing a third-order tensor in memory can be prohibitive. Sparsity is
one factor that alleviates this problem. Another approach is to use efficient linear dimen-
sionality reduction. When this is combined with efficient techniques for matrix and tensor
multiplication that avoid explicitly constructing the moment matrices and tensors (such as
the procedure described above), it is possible to avoid any computational scaling more than
linear in the dimension d and the training sample size.
Consider for concreteness the tensor decomposition approach for the exchangeable single
topic model as discussed in Section 4.3. Using recent techniques for randomized linear
algebra computations (e.g., Halko et al., 2011), it is possible to efficiently approximate the
whitening matrix W ∈ Rd×k from the second-moment matrix M2 ∈ Rd×d. To do this, one
first multiplies M2 by a random matrix R ∈ Rd×k0
for some k0 ≥ k, and then computes the
top k singular vectors of the product M2R. This provides a basis U ∈ Rd×k whose span
2798
Tensor Decompositions for Learning Latent Variable Models
is approximately the range of M2. From here, an approximate SVD of U>
M2U is used to
compute the approximate whitening matrix W. Note that both matrix products M2R and
U>
M2U may be performed via implicit access to M2 by exploiting (9), so that M2 need
not be explicitly formed. With the whitening matrix W in hand, the third-moment tensor
f
M3 = M3(W, W, W) ∈ Rk×k×k can be implicitly computed via (8). For instance, the core
computation in the tensor power method θ0 := f
M3(I, θ, θ) is performed by (i) computing
η := Wθ, (ii) computing η0 := M3(I, η, η), and finally (iii) computing θ0 := W>
η0. Using the
fact that M3 is an empirical third-order moment tensor, these steps can be computed with
O(dk + N) operations, where N is the number of non-zero entries in the term-document
matrix (Zou et al., 2013).
6.2 Computational Complexity
It is interesting to consider the computational complexity of the tensor power method in the
dense setting where T ∈ Rk×k×k is orthogonally decomposable but otherwise unstructured.
Each iteration requires O(k3) operations, and assuming at most k1+δ random restarts for
extracting each eigenvector (for some small δ > 0) and O(log(k)+log log(1/)) iterations per
restart, the total running time is O(k5+δ(log(k)+log log(1/))) to extract all k eigenvectors
and eigenvalues.
An alternative approach to extracting the orthogonal decomposition of T is to reorganize
T into a matrix M ∈ Rk×k2
by flattening two of the dimensions into one. In this case, if
T =
Pk
i=1 λiv⊗3
i , then M =
Pk
i=1 λivi ⊗ vec(vi ⊗ vi). This reveals the singular value
decomposition of M (assuming the eigenvalues λ1, λ2, . . . , λk are distinct), and therefore can
be computed with O(k4) operations. Therefore it seems that the tensor power method is
less efficient than a pure matrix-based approach via singular value decomposition. However,
it should be noted that this matrix-based approach fails to recover the decomposition when
eigenvalues are repeated, and can be unstable when the gap between eigenvalues is small—
see Appendix D for more discussion.
It is worth noting that the running times differ by roughly a factor of Θ(k1+δ), which
can be accounted for by the random restarts. This gap can potentially be alleviated or
removed by using a more clever method for initialization. Moreover, using special structure
in the problem (as discussed above) can also improve the running time of the tensor power
method.
6.3 Sample Complexity Bounds
Previous work on using linear algebraic methods for estimating latent variable models cru-
cially rely on matrix perturbation analysis for deriving sample complexity bounds (Mossel
and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c,a; Hsu and Kakade, 2013).
The learning algorithms in these works are plug-in estimators that use empirical moments in
place of the population moments, and then follow algebraic manipulations that result in the
desired parameter estimates. As long as these manipulations can tolerate small perturba-
tions of the population moments, a sample complexity bound can be obtained by exploiting
the convergence of the empirical moments to the population moments via the law of large
numbers. As discussed in Appendix D, these approaches do not directly lead to practical
2799
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
algorithms due to a certain amplification of the error (a polynomial factor of k, which is
observed in practice).
Using the perturbation analysis for the tensor power method, improved sample complex-
ity bounds can be obtained for all of the examples discussed in Section 3. The underlying
analysis remains the same as in previous works (e.g., Anandkumar et al., 2012a; Hsu and
Kakade, 2013), the main difference being the accuracy of the orthogonal tensor decompo-
sition obtained via the tensor power method. Relative to the previously cited works, the
sample complexity bound will be considerably improved in its dependence on the rank pa-
rameter k, as Theorem 5.1 implies that the tensor estimation error (e.g., error in estimating
f
M3 from Section 4.3) is not amplified by any factor explicitly depending on k (there is
a requirement that the error be smaller than some factor depending on k, but this only
contributes to a lower-order term in the sample complexity bound). See Appendix D for
further discussion regarding the stability of the techniques from these previous works.
6.4 Other Perspectives
The tensor power method is simply one approach for extracting the orthogonal decomposi-
tion needed in parameter estimation. The characterizations from Section 4.2 suggest that a
number of fixed point and variational techniques may be possible (and Appendix D provides
yet another perspective based on simultaneous diagonalization). One important consider-
ation is that the model is often misspecified, and therefore approaches with more robust
guarantees (e.g., for convergence) are desirable. Our own experience with the tensor power
method (as applied to exchangeable topic modeling) is that while model misspecification
does indeed affect convergence, the results can be very reasonable even after just a dozen
or so iterations (Anandkumar et al., 2012a). Nevertheless, robustness is likely more impor-
tant in other applications, and thus the stabilization approaches (Kofidis and Regalia, 2002;
Regalia and Kofidis, 2003; Erdogan, 2009; Kolda and Mayo, 2011) may be advantageous.
Acknowledgments
We thank Boaz Barak, Dean Foster, Jon Kelner, and Greg Valiant for helpful discussions.
We are also grateful to Hanzhang Hu, Drew Bagnell, and Martial Hebert for alerting us
of an issue with Theorem 4.2 and suggesting a simple fix. This work was completed while
DH was a postdoctoral researcher at Microsoft Research New England, and partly while
AA, RG, and MT were visiting the same lab. AA is supported in part by the NSF Award
CCF-1219234, AFOSR Award FA9550-10-1-0310 and the ARO Award W911NF-12-1-0404.
Appendix A. Fixed-Point and Variational Characterizations of
Orthogonal Tensor Decompositions
We give detailed proofs of Theorems 4.1 and 4.2 in this section for completeness.
A.1 Proof of Theorem 4.1
Theorem A.1 Let T have an orthogonal decomposition as given in (4).
2800
Tensor Decompositions for Learning Latent Variable Models
1. The set of θ ∈ Rn which do not converge to some vi under repeated iteration of (6)
has measure zero.
2. The set of robust eigenvectors of T is {v1, v2, . . . , vk}.
Proof For a random choice of θ ∈ Rn (under any distribution absolutely continuous with
respect to Lebesgue measure), the values |λ1v>
1 θ|, |λ2v>
2 θ|, . . . , |λkv>
k θ| will be distinct with
probability 1. Therefore, there exists a unique largest value, say |λiv>
i θ| for some i ∈ [k],
and by Lemma 5.1, we have convergence to vi under repeated iteration of (6). Thus the
first claim holds.
We now prove the second claim. First, we show that every vi is a robust eigenvector.
Pick any i ∈ [k], and note that for a sufficiently small ball around vi, we have that for all θ
in this ball, λiv>
i θ is strictly greater than λjv>
j θ for j ∈ [k] \ {i}. Thus by Lemma 5.1, vi is
a robust eigenvector. Now we show that the vi are the only robust eigenvectors. Suppose
there exists some robust eigenvector u not equal to vi for any i ∈ [k]. Then there exists a
positive measure set around u such that all points in this set converge to u under repeated
iteration of (6). This contradicts the first claim.
A.2 Proof of Theorem 4.2
Theorem A.2 Let T have an orthogonal decomposition as given in (4), and consider the
optimization problem
max
u∈Rn
T(u, u, u) s.t. kuk ≤ 1.
1. The stationary points are eigenvectors of T.
2. A stationary point u is an isolated local maximizer if and only if u = vi for some
i ∈ [k].
Proof Consider the Lagrangian form of the corresponding constrained maximization prob-
lem over unit vectors u ∈ Rn:
L(u, λ) := T(u, u, u) −
3
2
λ(u>
u − 1).
Since
∇uL(u, λ) = ∇u
 k
X
i=1
λi(v>
i u)3
−
3
2
λ(u>
u − 1)

= 3

T(I, u, u) − λu

,
the stationary points u ∈ Rn (with kuk ≤ 1) satisfy
T(I, u, u) = λu
for some λ ∈ R, i.e., (u, λ) is an eigenvector/eigenvalue pair of T.
Now we characterize the isolated local maximizers. Observe that if u 6= 0 and T(I, u, u) =
λu for λ < 0, then T(u, u, u) < 0. Therefore u0 = (1 − δ)u for any δ ∈ (0, 1) satisfies
T(u0, u0, u0) = (1 − δ)3T(u, u, u) > T(u, u, u). So such a u cannot be a local maximizer.
2801
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Moreover, if kuk < 1 and T(I, u, u) = λu for λ > 0, then u0 = (1 + δ)u for a small enough
δ ∈ (0, 1) satisfies ku0k ≤ 1 and T(u0, u0, u0) = (1 + δ)3T(u, u, u) > T(u, u, u). Therefore a
local maximizer must have T(I, u, u) = λu for some λ ≥ 0, and kuk = 1 whenever λ > 0.
Extend {v1, v2, . . . , vk} to an orthonormal basis {v1, v2, . . . , vn} of Rn. Now pick any
stationary point u =
Pn
i=1 civi with λ := T(u, u, u) = u>
T(I, u, u). Then
λic2
i = λi(u>
vi)2
= v>
i T(I, u, u) = λv>
i u = λci ≥ 0, i ∈ [k],
and thus
∇2
uL(u, λ) = 6
k
X
i=1
λici viv>
i − 3λI = 3λ

2
X
i∈Ω
viv>
i − I

where Ω := {i ∈ [k] : ci 6= 0}. This implies that for any unit vector w ∈ Rn,
w>
∇2
uL(u, λ)w = 3λ

2
X
i∈Ω
(v>
i w)2
− 1

.
The point u is an isolated local maximum if the above quantity is strictly negative for all
unit vectors w orthogonal to u. We now consider three cases depending on the cardinality
of Ω and the sign of λ.
• Case 1: |Ω| = 1 and λ > 0. This means u = vi for some i ∈ [k] (as u = −vi implies
λ = −λi < 0). In this case,
w>
∇2
uL(u, λ)w = 3λi(2(v>
i w)2
− 1) = −3λi < 0
for all w ∈ Rn satisfying (u>
w)2 = (v>
i w)2 = 0. Hence u is an isolated local maximizer.
• Case 2: |Ω| ≥ 2 and λ > 0. Since |Ω| ≥ 2, we may pick a strict non-empty subset
S ( Ω and set
w :=
1
Z

1
ZS
X
i∈S
civi −
1
ZSc
X
i∈Ω\S
civi

where ZS :=
P
i∈S c2
i , ZSc :=
P
i∈Ω\S c2
i , and Z :=
p
1/ZS + 1/ZSc . It is easy to
check that kwk2 =
P
i∈Ω(v>
i w)2 = 1 and u>
w = 0. Consider any open neighborhood
U of u, and pick δ > 0 small enough so that ũ :=
√
1 − δ2u + δw is contained in
U. Set u0 :=
√
1 − δ2u. By Taylor’s theorem, there exists  ∈ [0, δ] such that, for
2802
Tensor Decompositions for Learning Latent Variable Models
ū := u0 + w, we have
T(ũ, ũ, ũ) = T(u0, u0, u0) + ∇uT(u, u, u)>
(ũ − u0)
u=u0
+
1
2
(ũ − u0)>
∇2
uT(u, u, u)(ũ − u0)
u=ū
= (1 − δ2
)3/2
λ + δ(1 − δ2
)λu>
w +
1
2
δ2
w>
∇2
uT(u, u, u)w
u=ū
= (1 − δ2
)3/2
λ + 0 + 3δ2
k
X
i=1
λi(v>
i (u0 + w))(v>
i w)2
= (1 − δ2
)3/2
λ + 3δ2
p
1 − δ2
k
X
i=1
λici(v>
i w)2
+ 3δ2

k
X
i=1
λi(v>
i w)3
= (1 − δ2
)3/2
λ + 3δ2
p
1 − δ2λ
X
i∈Ω
(v>
i w)2
+ 3δ2

k
X
i=1
λi(v>
i w)3
= (1 − δ2
)3/2
λ + 3δ2
p
1 − δ2λ + 3δ2

k
X
i=1
λi(v>
i w)3
=

1 −
3
2
δ2
+ O(δ4
)

λ + 3δ2
p
1 − δ2λ + 3δ2

k
X
i=1
λi(v>
i w)3
.
Since  ≤ δ, for small enough δ, the RHS is strictly greater than λ. This implies that
u is not an isolated local maximizer.
• Case 3: |Ω| = 0 or λ = 0. Note that if |Ω| = 0, then λ = 0, so we just consider λ = 0.
Consider any open neighborhood U of u, and pick j ∈ [n] and δ > 0 small enough so
that ũ :=
√
1 − δ2u + δvj is contained in U. Then
T(ũ, ũ, ũ) = (1 − δ2
)3/2
T(u, u, u) + 3λj(1 − δ2
)δc2
j + 3λi
p
1 − δ2δ2
cj + δ3
> 0 = λ
for sufficiently small δ. Thus u is not an isolated local maximizer.
From these exhaustive cases, we conclude that a stationary point u is an isolated local
maximizer if and only if u = vi for some i ∈ [k].
We are grateful to Hanzhang Hu, Drew Bagnell, and Martial Hebert for alerting us of
an issue with our original statement of Theorem 4.2 and its proof, and for suggesting a
simple fix. The original statement used the optimization constraint kuk = 1 (rather than
kuk ≤ 1), but the characterization of the decomposition with this constraint is then only
given by isolated local maximizers u with the additional constraint that T(u, u, u) > 0—that
is, there can be isolated local maximizers with T(u, u, u) ≤ 0 that are not vectors in the
decomposition. The suggested fix of Hu, Bagnell, and Herbert is to relax to kuk ≤ 1, which
eliminates isolated local maximizers with T(u, u, u) ≤ 0; this way, the characterization of
the decomposition is simply the isolated local maximizers under the relaxed constraint.
2803
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Appendix B. Analysis of Robust Power Method
In this section, we prove Theorem 5.1. The proof is structured as follows. In Appendix B.1,
we show that with high probability, at least one out of L random vectors will be a good
initializer for the tensor power iterations. An initializer is good if its projection onto an
eigenvector is noticeably larger than its projection onto other eigenvectors. We then analyze
in Appendix B.2 the convergence behavior of the tensor power iterations. Relative to the
proof of Lemma 5.1, this analysis is complicated by the tensor perturbation. We show that
there is an initial slow convergence phase (linear rate rather than quadratic), but as soon
as the projection of the iterate onto an eigenvector is large enough, it enters the quadratic
convergence regime until the perturbation dominates. Finally, we show how errors accrue
due to deflation in Appendix B.3, which is rather subtle and different from deflation with
matrix eigendecompositions. This is because when some initial set of eigenvectors and
eigenvalues are accurately recovered, the additional errors due to deflation are effectively
only lower-order terms. These three pieces are assembled in Appendix B.4 to complete the
proof of Theorem 5.1.
B.1 Initialization
Consider a set of non-negative numbers λ̃1, λ̃2, . . . , λ̃k ≥ 0. For γ ∈ (0, 1), we say a unit
vector θ0 ∈ Rk is γ-separated relative to i∗ ∈ [k] if
λ̃i∗ |θi∗,0| − max
i∈[k]\{i∗}
λ̃i|θi,0| ≥ γλ̃i|θi∗,0|
(the dependence on λ̃1, λ̃2, . . . , λ̃k is implicit).
The following lemma shows that for any constant γ, with probability at least 1 − η,
at least one out of poly(k) log(1/η) i.i.d. random vectors (uniformly distributed over the
unit sphere Sk−1) is γ-separated relative to arg maxi∈[k] λ̃i. (For small enough γ and large
enough k, the polynomial is close to linear in k.)
Lemma B.1 There exists an absolute constant c > 0 such that if positive integer L ≥ 2
satisfies
s
ln(L)
ln(k)
· 1 −
ln(ln(L)) + c
4 ln(L)
−
s
ln(8)
ln(L)
!
≥
1
1 − γ
· 1 +
s
ln(4)
ln(k)
!
, (10)
the following holds. With probability at least 1/2 over the choice of L i.i.d. random vectors
drawn uniformly distributed over the unit sphere Sk−1 in Rk, at least one of the vectors is
γ-separated relative to arg maxi∈[k] λ̃i. Moreover, with the same c, L, and for any η ∈ (0, 1),
with probability at least 1 − η over L · log2(1/η) i.i.d. uniform random unit vectors, at least
one of the vectors is γ-separated.
Proof Without loss of generality, assume arg maxi∈[k] λ̃i = 1. Consider a random matrix
Z ∈ Rk×L whose entries are independent N(0, 1) random variables; we take the j-th column
of Z to be comprised of the random variables used for the j-th random vector (before
normalization). Specifically, for the j-th random vector,
θi,0 :=
Zi,j
qPk
i0=1 Z2
i0,j
, i ∈ [n].
2804
Tensor Decompositions for Learning Latent Variable Models
It suffices to show that with probability at least 1/2, there is a column j∗ ∈ [L] such that
|Z1,j∗ | ≥
1
1 − γ
max
i∈[k]\{1}
|Zi,j∗ |.
Since maxj∈[L] |Z1,j| is a 1-Lipschitz function of L independent N(0, 1) random variables,
it follows that
Pr

max
j∈[L]
|Z1,j| − median
h
max
j∈[L]
|Z1,j|
i
>
p
2 ln(8)

≤ 1/4.
Moreover,
median
h
max
j∈[L]
|Z1,j|
i
≥ median
h
max
j∈[L]
Z1,j
i
=: m.
Observe that the cumulative distribution function of maxj∈[L] Z1,j is given by F(z) = Φ(z)L,
where Φ is the standard Gaussian CDF. Since F(m) = 1/2, it follows that m = Φ−1(2−1/L).
It can be checked that
Φ−1
(2−1/L
) ≥
p
2 ln(L) −
ln(ln(L)) + c
2
p
2 ln(L)
for some absolute constant c > 0. Also, let j∗ := arg maxj∈[L] |Z1,j|.
Now for each j ∈ [L], let |Z2:k,j| := max{|Z2,j|, |Z3,j|, . . . , |Zk,j|}. Again, since |Z2:k,j| is
a 1-Lipschitz function of k − 1 independent N(0, 1) random variables, it follows that
Pr

|Z2:k,j| > E
h
|Z2:k,j|
i
+
p
2 ln(4)

≤ 1/4.
Moreover, by a standard argument,
E
h
|Z2:k,j|
i
≤
p
2 ln(k).
Since |Z2:k,j| is independent of |Z1,j| for all j ∈ [L], it follows that the previous two displayed
inequalities also hold with j replaced by j∗.
Therefore we conclude with a union bound that with probability at least 1/2,
|Z1,j∗ | ≥
p
2 ln(L) −
ln(ln(L)) + c
2
p
2 ln(L)
−
p
2 ln(8) and |Z2:k,j∗ | ≤
p
2 ln(k) +
p
2 ln(4).
Since L satisfies (10) by assumption, in this event, the j∗-th random vector is γ-separated.
B.2 Tensor Power Iterations
Recall the update rule used in the power method. Let θt =
Pk
i=1 θi,tvi ∈ Rk be the unit
vector at time t. Then
θt+1 =
k
X
i=1
θi,t+1vi := T̃(I, θt, θt)/kT̃(I, θt, θt)k.
2805
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
In this subsection, we assume that T̃ has the form
T̃ =
k
X
i=1
λ̃iv⊗3
i + Ẽ (11)
where {v1, v2, . . . , vk} is an orthonormal basis, and, without loss of generality,
λ̃1|θ1,t| = max
i∈[k]
λ̃i|θi,t| > 0.
Also, define
λ̃min := min{λ̃i : i ∈ [k], λ̃i > 0}, λ̃max := max{λ̃i : i ∈ [k]}.
We further assume the error Ẽ is a symmetric tensor such that, for some constant p > 1,
kẼ(I, u, u)k ≤ ˜
, ∀u ∈ Sk−1
; (12)
kẼ(I, u, u)k ≤ ˜
/p, ∀u ∈ Sk−1
s.t. (u>
v1)2
≥ 1 − (3˜
/λ̃1)2
. (13)
In the next two propositions (Propositions B.1 and B.2) and next two lemmas (Lemmas B.2
and B.3), we analyze the power method iterations using T̃ at some arbitrary iterate θt using
only the property (12) of Ẽ. But throughout, the quantity ˜
 can be replaced by ˜
/p if θt
satisfies (θ>
t v1)2 ≥ 1 − (3˜
/λ̃1)2 as per property (13).
Define
Rτ :=

θ2
1,τ
1 − θ2
1,τ
1/2
, ri,τ :=
λ̃1θ1,τ
λ̃i|θi,τ |
,
γτ := 1 −
1
mini6=1 |ri,τ |
, δτ :=
˜

λ̃1θ2
1,τ
, κ :=
λ̃max
λ̃1
(14)
for τ ∈ {t, t + 1}.
Proposition B.1
min
i6=1
|ri,t| ≥
Rt
κ
, γt ≥ 1 −
κ
Rt
, θ2
1,t =
R2
t
1 + R2
t
.
Proposition B.2
ri,t+1 ≥ r2
i,t ·
1 − δt
1 + κδtr2
i,t
=
1 − δt
1
r2
i,t
+ κδt
, i ∈ [k], (15)
Rt+1 ≥ Rt ·
1 − δt
1 − γt + δtRt
≥
1 − δt
κ
R2
t
+ δt
. (16)
Proof Let θ̌t+1 := T̃(I, θt, θt), so θt+1 = θ̌t+1/kθ̌t+1k. Since θ̌i,t+1 = T̃(vi, θt, θt) =
T(vi, θt, θt) + E(vi, θt, θt), we have
θ̌i,t+1 = λ̃iθ2
i,t + E(vi, θt, θt), i ∈ [k].
2806
Tensor Decompositions for Learning Latent Variable Models
Using the triangle inequality and the fact kE(vi, θt, θt)k ≤ ˜
, we have
θ̌i,t+1 ≥ λ̃iθ2
i,t − ˜
 ≥ |θi,t| ·

λ̃i|θi,t| − ˜
/|θi,t|

(17)
and
|θ̌i,t+1| ≤ |λ̃iθ2
i,t| + ˜
 ≤ |θi,t| ·

λ̃i|θi,t| + ˜
/|θi,t|

(18)
for all i ∈ [k]. Combining (17) and (18) gives
ri,t+1 =
λ̃1θ1,t+1
λ̃i|θi,t+1|
=
λ̃1θ̌1,t+1
λ̃i|θ̌i,t+1|
≥ r2
i,t ·
1 − δt
1 + ˜

λ̃iθ2
i,t
= r2
i,t ·
1 − δt
1 + (λ̃i/λ̃1)δtr2
i,t
≥ r2
i,t ·
1 − δt
1 + κδtr2
i,t
.
Moreover, by the triangle inequality and Hölder’s inequality,
 n
X
i=2
[θ̌i,t+1]2
1/2
=
 n
X
i=2

λ̃iθ2
i,t + E(vi, θt, θt)
2
1/2
≤
 n
X
i=2
λ̃2
i θ4
i,t
1/2
+
 n
X
i=2
E(vi, θt, θt)2
1/2
≤ max
i6=1
λ̃i|θi,t|
 n
X
i=2
θ2
i,t
1/2
+ ˜

= (1 − θ2
1,t)1/2
·

max
i6=1
λ̃i|θi,t| + ˜
/(1 − θ2
1,t)1/2

. (19)
Combining (17) and (19) gives
|θ1,t+1|
(1 − θ2
1,t+1)1/2
=
|θ̌1,t+1|
Pn
i=2[θ̌i,t+1]2
1/2
≥
|θ1,t|
(1 − θ2
1,t)1/2
·
λ̃1|θ1,t| − ˜
/|θ1,t|
maxi6=1 λ̃i|θi,t| + ˜
/(1 − θ2
1,t)1/2
.
In terms of Rt+1, Rt, γt, and δt, this reads
Rt+1 ≥
1 − δt
(1 − γt)
1−θ2
1,t
θ2
1,t
1/2
+ δt
= Rt ·
1 − δt
1 − γt + δtRt
=
1 − δt
1−γt
Rt
+ δt
≥
1 − δt
κ
R2
t
+ δt
where the last inequality follows from Proposition B.1.
Lemma B.2 Fix any ρ > 1. Assume
0 ≤ δt < min
n 1
2(1 + 2κρ2)
,
1 − 1/ρ
1 + κρ
o
and γt > 2(1 + 2κρ2)δt.
1. If r2
i,t ≤ 2ρ2, then ri,t+1 ≥ |ri,t| 1 + γt
2

.
2807
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
2. If ρ2 < r2
i,t, then ri,t+1 ≥ min{r2
i,t/ρ, 1−δt−1/ρ
κδt
}.
3. γt+1 ≥ min{γt, 1 − 1/ρ}.
4. If mini6=1 r2
i,t > (ρ(1 − δt) − 1)/(κδt), then Rt+1 > 1−δt−1/ρ
κδt
· λ̃min
λ̃1
· 1
√
k
.
5. If Rt ≤ 1 + 2κρ2, then Rt+1 ≥ Rt 1 + γt
3

, θ2
1,t+1 ≥ θ2
1,t, and δt+1 ≤ δt.
Proof Consider two (overlapping) cases depending on r2
i,t.
• Case 1: r2
i,t ≤ 2ρ2. By (15) from Proposition B.2,
ri,t+1 ≥ r2
i,t ·
1 − δt
1 + κδtr2
i,t
≥ |ri,t| ·
1
1 − γt
·
1 − δt
1 + 2κρ2δt
≥ |ri,t|

1 +
γt
2

where the last inequality uses the assumption γt > 2(1+2κρ2)δt. This proves the first
claim.
• Case 2: ρ2 < r2
i,t. We split into two sub-cases. Suppose r2
i,t ≤ (ρ(1 − δt) − 1)/(κδt).
Then, by (15),
ri,t+1 ≥ r2
i,t ·
1 − δt
1 + κδtr2
i,t
≥ r2
i,t ·
1 − δt
1 + κδt
ρ(1−δt)−1
κδt
=
r2
i,t
ρ
.
Now suppose instead r2
i,t > (ρ(1 − δt) − 1)/(κδt). Then
ri,t+1 ≥
1 − δt
κδt
ρ(1−δt)−1 + κδt
=
1 − δt − 1/ρ
κδt
. (20)
Observe that if mini6=1 r2
i,t ≤ (ρ(1 − δt) − 1)/(κδt), then ri,t+1 ≥ |ri,t| for all i ∈ [k], and
hence γt+1 ≥ γt. Otherwise we have γt+1 > 1 − κδt
1−δt−1/ρ > 1 − 1/ρ. This proves the third
claim.
If mini6=1 r2
i,t > (ρ(1 − δt) − 1)/(κδt), then we may apply the inequality (20) from the
second sub-case of Case 2 above to get
Rt+1 =
1
P
i6=1(λ̃1/λ̃i)2/r2
i,t+1
1/2
>

1 − δt − 1/ρ
κδt

·
λ̃min
λ̃1
·
1
√
k
.
This proves the fourth claim.
Finally, for the last claim, if Rt ≤ 1 + 2κρ2, then by (16) from Proposition B.2 and the
assumption γt > 2(1 + 2κρ2)δt,
Rt+1 ≥ Rt ·
1 − δt
1 − γt + δtRt
≥ Rt ·
1 − γt
2(1+2κρ2)
1 − γt/2
≥ Rt

1 + γt ·
κρ2
1 + 2κρ2

≥ Rt

1 +
γt
3

.
This in turn implies that θ2
1,t+1 ≥ θ2
1,t via Proposition B.1, and thus δt+1 ≤ δt.
2808
Tensor Decompositions for Learning Latent Variable Models
Lemma B.3 Assume 0 ≤ δt < 1/2 and γt > 0. Pick any β > α > 0 such that
α
(1 + α)(1 + α2)
≥
˜

γtλ̃1
,
α
2(1 + α)(1 + β2)
≥
˜

λ̃1
.
1. If Rt ≥ 1/α, then Rt+1 ≥ 1/α.
2. If 1/α > Rt ≥ 1/β, then Rt+1 ≥ min{R2
t /(2κ), 1/α}.
Proof Observe that for any c > 0,
Rt ≥
1
c
⇔ θ2
1,t ≥
1
1 + c2
⇔ δt ≤
(1 + c2)˜

λ̃1
. (21)
Now consider the following cases depending on Rt.
• Case 1: Rt ≥ 1/α. In this case, we have
δt ≤
(1 + α2)˜

λ̃1
≤
αγt
1 + α
by (21) (with c = α) and the condition on α. Combining this with (16) from Propo-
sition B.2 gives
Rt+1 ≥
1 − δt
1−γt
Rt
+ δt
≥
1 − αγt
1+α
(1 − γt)α + αγt
1+α
=
1
α
.
• Case 2: 1/β ≤ Rt < 1/α. In this case, we have
δt ≤
(1 + β2)˜

λ̃1
≤
α
2(1 + α)
by (21) (with c = β) and the conditions on α and β. If δt ≥ 1/(2 + R2
t /κ), then (16)
implies
Rt+1 ≥
1 − δt
κ
R2
t
+ δt
≥
1 − 2δt
2δt
≥
1 − α
1+α
α
1+α
=
1
α
.
If instead δt < 1/(2 + R2
t /κ), then (16) implies
Rt+1 ≥
1 − δt
κ
R2
t
+ δt
>
1 − 1
2+R2
t /κ
κ
R2
t
+ 1
2+R2
t /κ
=
R2
t
2κ
.
2809
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
B.2.1 Approximate Recovery of a Single Eigenvector
We now state the main result regarding the approximate recovery of a single eigenvector
using the tensor power method on T̃. Here, we exploit the special properties of the error
Ẽ—both (12) and (13).
Lemma B.4 There exists a universal constant C > 0 such that the following holds. Let
i∗ := arg maxi∈[k] λ̃i|θi,0|. If
˜
 <
γ0
2(1 + 8κ)
· λ̃min · θ2
i∗,0 and N ≥ C ·

log(kκ)
γ0
+ log log
pλ̃i∗
˜


,
then after t ≥ N iterations of the tensor power method on tensor T̃ as defined in (11) and
satisfying (12) and (13), the final vector θt satisfies
θi∗,t ≥
s
1 −

3˜

pλ̃i∗
2
, kθt − vi∗ k ≤
4˜

pλ̃i∗
, |T̃(θt, θt, θt) − λ̃i∗ | ≤

27κ
 ˜

pλi∗
2
+ 2

˜

p
.
Proof Assume without loss of generality that i∗ = 1. We consider three phases: (i)
iterations before the first time t such that Rt > 1 + 2κρ2 = 1 + 8κ (using ρ := 2), (ii) the
subsequent iterations before the first time t such that Rt ≥ 1/α (where α will be defined
below), and finally (iii) the remaining iterations.
We begin by analyzing the first phase, i.e., the iterates in T1 := {t ≥ 0 : Rt ≤ 1+2κρ2 =
1 + 8κ}. Observe that the condition on ˜
 implies
δ0 =
˜

λ̃1θ2
1,0
<
γ0
2(1 + 8κ)
·
λ̃min
λ̃1
≤ min

γ0
2(1 + 2κρ2)
,
1 − 1/ρ
2(1 + 2κρ2)

,
and hence the preconditions on δt and γt of Lemma B.2 hold for t = 0. For all t ∈ T1
satisfying the preconditions, Lemma B.2 implies that δt+1 ≤ δt and γt+1 ≥ min{γt, 1−1/ρ},
so the next iteration also satisfies the preconditions. Hence by induction, the preconditions
hold for all iterations in T1. Moreover, for all i ∈ [k], we have
|ri,0| ≥
1
1 − γ0
;
and while t ∈ T1: (i) |ri,t| increases at a linear rate while r2
i,t ≤ 2ρ2, and (ii) |ri,t| increases
at a quadratic rate while ρ2 ≤ r2
i,t ≤ 1−δt−1/ρ
κδt
. (The specific rates are given, respectively,
in Lemma B.2, claims 1 and 2.) Since 1−δt−1/ρ
κδt
≤ λ̃1
2κ˜
 , it follows that mini6=1 r2
i,t ≤ 1−δt−1/ρ
κδt
for at most
2
γ0
ln
p
2ρ2
1
1−γ0

+ ln

ln λ̃1
2κ˜

ln
√
2

= O

1
γ0
+ log log
λ̃1
˜


(22)
iterations in T1. As soon as mini6=1 r2
i,t > 1−δt−1/ρ
κδt
, we have that in the next iteration,
Rt+1 >
1 − δt − 1/ρ
κδt
·
λ̃min
λ̃1
·
1
√
k
≥
7
√
k
;
2810
Tensor Decompositions for Learning Latent Variable Models
and all the while Rt is growing at a linear rate (given in Lemma B.2, claim 5). Therefore,
there are at most an additional
1 +
3
γ0
ln

1 + 8κ
7/
√
k

= O

log(kκ)
γ0

(23)
iterations in T1 over that counted in (22). Therefore, by combining the counts in (22)
and (23), we have that the number of iterations in the first phase satisfies
|T1| = O

log log
λ̃1
˜

+
log(kκ)
γ0

.
We now analyze the second phase, i.e., the iterates in T2 := {t ≥ 0 : t /
∈ T1, Rt < 1/α}.
Define
α :=
3˜

λ̃1
, β :=
1
1 + 2κρ2
=
1
1 + 8κ
.
Note that for the initial iteration t0 := min T2, we have that Rt0 ≥ 1 + 2κρ2 = 1 + 8κ = 1/β,
and by Proposition B.1, γt0 ≥ 1 − κ/(1 + 8κ) > 7/8. It can be checked that δt, γt, α,
and β satisfy the preconditions of Lemma B.3 for this initial iteration t0. For all t ∈ T2
satisfying these preconditions, Lemma B.3 implies that Rt+1 ≥ min{Rt, 1/α}, θ2
1,t+1 ≥
min{θ2
1,t, 1/(1+α2)} (via Proposition B.1), δt+1 ≤ max{δt, (1+α)2˜
/λ̃1} (using the definition
of δt), and γt+1 ≥ min{γt, 1 − ακ} (via Proposition B.1). Hence the next iteration t + 1
also satisfies the preconditions, and by induction, so do all iterations in T2. To bound the
number of iterations in T2, observe that Rt increases at a quadratic rate until Rt ≥ 1/α, so
|T2| ≤ ln

ln(1/α)
ln((1/β)/(2κ))

< ln

ln λ̃1
3˜

ln 4

= O

log log
λ̃1
˜


. (24)
Therefore the total number of iterations before Rt ≥ 1/α is
O

log(kκ)
γ0
+ log log
λ̃1
˜


.
After Rt00 ≥ 1/α (for t00 := max(T1 ∪ T2) + 1), we have
θ2
1,t00 ≥
1/α2
1 + 1/α2
≥ 1 − α2
≥ 1 −

3˜

λ̃1
2
.
Therefore, the vector θt00 satisfies the condition for property (13) of Ẽ to hold. Now we
apply Lemma B.3 using ˜
/p in place of ˜
, including in the definition of δt (which we call δt):
δt :=
˜

pλ̃1θ2
1,t
;
we also replace α and β with α and β, which we set to
α :=
3˜

pλ̃1
, β :=
3˜

λ̃1
.
2811
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
It can be checked that δt00 ∈ (0, 1/2), γt00 ≥ 1 − 3˜
κ/λ1 > 0,
α
(1 + α)(1 + α2)
≥
˜

p(1 − 3˜
κ/λ1)λ̃1
≥
˜

pγt00 λ̃1
,
α
2(1 + α)(1 + β
2
)
≥
˜

pλ̃1
.
Therefore, the preconditions of Lemma B.3 are satisfied for the initial iteration t00 in this
final phase, and by the same arguments as before, the preconditions hold for all subsequent
iterations t ≥ t00. Initially, we have Rt00 ≥ 1/α ≥ 1/β, and by Lemma B.3, we have that Rt
increases at a quadratic rate in this final phase until Rt ≥ 1/α. So the number of iterations
before Rt ≥ 1/α can be bounded as
ln

ln(1/α)
ln((1/β)/(2κ))

= ln

ln pλ̃1
3˜

ln

λ1
3˜
 · 1
2κ


≤ ln ln
pλ̃1
3˜

= O

log log
pλ̃1
˜


.
Once Rt ≥ 1/α, we have
θ2
1,t ≥ 1 −

3˜

pλ̃1
2
.
Since sign(θ1,t) = r1,t ≥ r2
1,t−1 · (1 − δt−1)/(1 + κδt−1r2
1,t−1) = (1 − δt−1)/(1 + κδt−1) > 0 by
Proposition B.2, we have θ1,t > 0. Therefore we can conclude that
kθt − v1k =
q
2(1 − θ1,t) ≤
s
2

1 −
q
1 − (3˜
/(pλ̃1))2

≤ 4˜
/(pλ̃1).
Finally,
|T̃(θt, θt, θt) − λ̃1| = λ̃1(θ3
1,t − 1) +
k
X
i=2
λ̃iθ3
i,t + Ẽ(θt, θt, θt)
≤ λ̃1|θ3
1,t − 1| +
k
X
i=2
λ̃i|θi,t|θ2
i,t + kẼ(I, θt, θt)k
≤ λ̃1 1 − θ1,t + |θ1,t(1 − θ2
1,t)|

+ max
i6=1
λ̃i|θi,t|
k
X
i=2
θ2
i,t + kẼ(I, θt, θt)k
≤ λ̃1 1 − θ1,t + |θ1,t(1 − θ2
1,t)|

+ max
i6=1
λ̃i
q
1 − θ2
1,t
k
X
i=2
θ2
i,t + kẼ(I, θt, θt)k
= λ̃1 1 − θ1,t + |θ1,t(1 − θ2
1,t)|

+ max
i6=1
λ̃i(1 − θ2
1,t)3/2
+ kẼ(I, θt, θt)k
≤ λ̃1 · 3

3˜

pλ̃1
2
+ κλ̃1 ·

3˜

pλ̃1
3
+
˜

p
≤
(27κ · (˜
/pλ̃1)2 + 2)˜

p
.
2812
Tensor Decompositions for Learning Latent Variable Models
B.3 Deflation
Lemma B.5 Fix some ˜
 ≥ 0. Let {v1, v2, . . . , vk} be an orthonormal basis for Rk, and
λ1, λ2, . . . , λk ≥ 0 with λmin := mini∈[k] λi. Also, let {v̂1, v̂2, . . . , v̂k} be a set of unit vectors
in Rk (not necessarily orthogonal), λ̂1, λ̂2, . . . , λ̂k ≥ 0 be non-negative scalars, and define
Ei := λiv⊗3
i − λ̂iv̂⊗3
i , i ∈ [k].
Pick any t ∈ [k]. If
|λ̂i − λi| ≤ ˜
,
kv̂i − vik ≤ min{
√
2, 2˜
/λi}
for all i ∈ [t], then for any unit vector u ∈ Sk−1,
t
X
i=1
Ei(I, u, u)
2
2
≤

4(5 + 11˜
/λmin)2
+ 128(1 + ˜
/λmin)2
(˜
/λmin)2

˜
2
t
X
i=1
(u>
vi)2
+ 64(1 + ˜
/λmin)2
˜
2
t
X
i=1
(˜
/λi)2
+ 2048(1 + ˜
/λmin)2
˜
2
 t
X
i=1
(˜
/λi)3
2
.
In particular, for any ∆ ∈ (0, 1), there exists a constant ∆0 > 0 (depending only on ∆) such
that ˜
 ≤ ∆0λmin/
√
k implies
t
X
i=1
Ei(I, u, u)
2
2
≤

∆ + 100
t
X
i=1
(u>
vi)2

˜
2
.
Proof For any unit vector u and i ∈ [t], the error term
Ei(I, u, u) = λi(u>
vi)2
vi − λ̂i(u>
v̂i)2
v̂i
lives in span{vi, v̂i}; this space is the same as span{vi, v̂⊥
i }, where
v̂⊥
i := v̂i − (v>
i v̂i)vi
is the projection of v̂i onto the subspace orthogonal to vi. Since kv̂i − vik2 = 2(1 − v>
i v̂i), it
follows that
ci := v>
i v̂i = 1 − kv̂i − vik2
/2 ≥ 0
(the inequality follows from the assumption kv̂i−vik ≤
√
2, which in turn implies 0 ≤ ci ≤ 1).
By the Pythagorean theorem and the above inequality for ci,
kv̂⊥
i k2
= 1 − c2
i ≤ kv̂i − vik2
.
Later, we will also need the following bound, which is easily derived from the above inequal-
ities and the triangle inequality:
|1 − c3
i | = |1 − ci + ci(1 − c2
i )| ≤ 1 − ci + |ci(1 − c2
i )| ≤ 1.5kv̂i − vik2
.
2813
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
We now express Ei(I, u, u) in terms of the coordinate system defined by vi and v̂⊥
i ,
depicted below. Define
ai := u>
vi and bi := u>
v̂⊥
i /kv̂⊥
i k

.
(Note that the part of u living in span{vi, v̂⊥
i }⊥ is irrelevant for analyzing Ei(I, u, u).) We
have
Ei(I, u, u) = λi(u>
vi)2
vi − λ̂i(u>
v̂i)2
v̂i
= λia2
i vi − λ̂i aici + kv̂⊥
i kbi
2
civi + v̂⊥
i

= λia2
i vi − λ̂i a2
i c2
i + 2kv̂⊥
i kaibici + kv̂⊥
i k2
b2
i

civi − λ̂i aici + kv̂⊥
i kbi
2
v̂⊥
i
=

(λi − λ̂ic3
i )a2
i − 2λ̂ikv̂⊥
i kaibic2
i − λ̂ikv̂⊥
i k2
b2
i ci

| {z }
=:Ai
vi − λ̂ikv̂⊥
i k aici + kv̂⊥
i kbi
2
| {z }
=:Bi
v̂⊥
i /kv̂⊥
i k

= Aivi − Bi v̂⊥
i /kv̂⊥
i k

.
The overall error can also be expressed in terms of the Ai and Bi:
t
X
i=1
Ei(I, u, u)
2
2
=
t
X
i=1
Aivi −
t
X
i=1
Bi(v̂⊥
i /kv̂⊥
i k)
2
2
≤ 2
t
X
i=1
Aivi
2
+ 2
t
X
i=1
Bi(v̂⊥
i /kv̂⊥
i k)
2
2
≤ 2
t
X
i=1
A2
i + 2
 t
X
i=1
|Bi|
2
(25)
where the first inequality uses the fact (x + y)2 ≤ 2(x2 + y2) and the triangle inequality,
and the second inequality uses the orthonormality of the vi and the triangle inequality.
It remains to bound A2
i and |Bi| in terms of |ai|, λi, and ˜
. The first term, A2
i , can be
bounded using the triangle inequality and the various bounds on |λi − λ̂i|, kv̂i − vik, kv̂⊥
i k,
and ci:
|Ai| ≤ (|λi − λ̂i|c3
i + λi|c3
i − 1|)a2
i + 2(λi + |λi − λ̂i|)kv̂⊥
i k|aibi|c2
i + (λi + |λi − λ̂i|)kv̂⊥
i k2
b2
i ci
≤ (|λi − λ̂i| + 1.5λikv̂i − vik2
+ 2(λi + |λi − λ̂i|)kv̂i − vik)|ai| + (λi + |λi − λ̂i|)kv̂i − vik2
≤ (˜
 + 7˜
2
/λi + 4˜
 + 4˜
2
/λi)|ai| + 4˜
2
/λi + ˜
3
/λ2
i
= (5 + 11˜
/λi)˜
|ai| + 4(1 + ˜
/λi)˜
2
/λi,
and therefore (via (x + y)2 ≤ 2(x2 + y2))
A2
i ≤ 2(5 + 11˜
/λi)2
˜
2
a2
i + 32(1 + ˜
/λi)2
˜
4
/λ2
i .
The second term, |Bi|, is bounded similarly:
|Bi| ≤ 2(λi + |λi − λ̂i|)kv̂⊥
i k2
(a2
i + kv̂⊥
i k2
)
≤ 2(λi + |λi − λ̂i|)kv̂i − vik2
(a2
i + kv̂i − vik2
)
≤ 8(1 + ˜
/λi)(˜
2
/λi)a2
i + 32(1 + ˜
/λi)˜
4
/λ3
i .
2814
Tensor Decompositions for Learning Latent Variable Models
Therefore, using the inequality from (25) and again (x + y)2 ≤ 2(x2 + y2),
t
X
i=1
Ei(I, u, u)
2
2
≤ 2
t
X
i=1
A2
i + 2
 t
X
i=1
|Bi|
2
≤ 4(5 + 11˜
/λmin)2
˜
2
t
X
i=1
a2
i + 64(1 + ˜
/λmin)2
˜
2
t
X
i=1
(˜
/λi)2
+ 2

8(1 + ˜
/λmin)(˜
2
/λmin)
t
X
i=1
a2
i + 32(1 + ˜
/λmin)˜

t
X
i=1
(˜
/λi)3
2
≤ 4(5 + 11˜
/λmin)2
˜
2
t
X
i=1
a2
i + 64(1 + ˜
/λmin)2
˜
2
t
X
i=1
(˜
/λi)2
+ 128(1 + ˜
/λmin)2
(˜
/λmin)2
˜
2
t
X
i=1
a2
i
+ 2048(1 + ˜
/λmin)2
˜
2
 t
X
i=1
(˜
/λi)3
2
=

4(5 + 11˜
/λmin)2
+ 128(1 + ˜
/λmin)2
(˜
/λmin)2

˜
2
t
X
i=1
a2
i
+ 64(1 + ˜
/λmin)2
˜
2
t
X
i=1
(˜
/λi)2
+ 2048(1 + ˜
/λmin)2
˜
2
 t
X
i=1
(˜
/λi)3
2
.
B.4 Proof of the Main Theorem
Theorem B.1 Let T̂ = T + E ∈ Rk×k×k, where T is a symmetric tensor with orthogonal
decomposition T =
Pk
i=1 λiv⊗3
i where each λi > 0, {v1, v2, . . . , vk} is an orthonormal basis,
and E has operator norm  := kEk. Define λmin := min{λi : i ∈ [k]}, and λmax := max{λi :
i ∈ [k]}. There exists universal constants C1, C2, C3 > 0 such that the following holds. Pick
any η ∈ (0, 1), and suppose
 ≤ C1 ·
λmin
k
, N ≥ C2 ·

log(k) + log log
λmax


,
and
s
ln(L/ log2(k/η))
ln(k)
· 1 −
ln(ln(L/ log2(k/η))) + C3
4 ln(L/ log2(k/η))
−
s
ln(8)
ln(L/ log2(k/η))
!
≥ 1.02 1 +
s
ln(4)
ln(k)
!
.
2815
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
(Note that the condition on L holds with L = poly(k) log(1/η).) Suppose that Algorithm 1
is iteratively called k times, where the input tensor is T̂ in the first call, and in each
subsequent call, the input tensor is the deflated tensor returned by the previous call. Let
(v̂1, λ̂1), (v̂2, λ̂2), . . . , (v̂k, λ̂k) be the sequence of estimated eigenvector/eigenvalue pairs re-
turned in these k calls. With probability at least 1 − η, there exists a permutation π on [k]
such that
kvπ(j) − v̂jk ≤ 8/λπ(j), |λπ(j) − λ̂j| ≤ 5, ∀j ∈ [k],
and
T −
k
X
j=1
λ̂jv̂⊗3
j ≤ 55.
Proof We prove by induction that for each i ∈ [k] (corresponding to the i-th call to
Algorithm 1), with probability at least 1 − iη/k, there exists a permutation π on [k] such
that the following assertions hold.
1. For all j ≤ i, kvπ(j) − v̂jk ≤ 8/λπ(j) and |λπ(j) − λ̂j| ≤ 12.
2. The error tensor
Ẽi+1 :=

T̂ −
X
j≤i
λ̂jv̂⊗3
j

−
X
j≥i+1
λπ(j)v⊗3
π(j) = E +
X
j≤i

λπ(j)v⊗3
π(j) − λ̂jv̂⊗3
j

satisfies
kẼi+1(I, u, u)k ≤ 56, ∀u ∈ Sk−1
; (26)
kẼi+1(I, u, u)k ≤ 2, ∀u ∈ Sk−1
s.t. ∃j ≥ i + 1  (u>
vπ(j))2
≥ 1 − (168/λπ(j))2
.
(27)
We actually take i = 0 as the base case, so we can ignore the first assertion, and just observe
that for i = 0,
Ẽ1 = T̂ −
k
X
j=1
λiv⊗3
i = E.
We have kẼ1k = kEk = , and therefore the second assertion holds.
Now fix some i ∈ [k], and assume as the inductive hypothesis that, with probability at
least 1 − (i − 1)η/k, there exists a permutation π such that two assertions above hold for
i − 1 (call this Eventi−1). The i-th call to Algorithm 1 takes as input
T̃i := T̂ −
X
j≤i−1
λ̂jv̂⊗3
j ,
which is intended to be an approximation to
Ti :=
X
j≥i
λπ(j)v⊗3
π(j).
2816
Tensor Decompositions for Learning Latent Variable Models
Observe that
T̃i − Ti = Ẽi,
which satisfies the second assertion in the inductive hypothesis. We may write Ti =
Pk
l=1 λ̃lv⊗3
l where λ̃l = λl whenever π−1(l) ≥ i, and λ̃l = 0 whenever π−1(l) ≤ i − 1. This
form is used when referring to T̃ or the λ̃i in preceding lemmas (in particular, Lemma B.1
and Lemma B.4).
By Lemma B.1, with conditional probability at least 1−η/k given Eventi−1, at least one
of θ
(τ)
0 for τ ∈ [L] is γ-separated relative to π(jmax), where jmax := arg maxj≥i λπ(j), (for
γ = 0.01; call this Event0
i; note that the application of Lemma B.1 determines C3). Therefore
Pr[Eventi−1 ∩ Event0
i] = Pr[Event0
i|Eventi−1] Pr[Eventi−1] ≥ (1 − η/k)(1 − (i − 1)η/k) ≥
1 − iη/k. It remains to show that Eventi−1 ∩ Event0
i ⊆ Eventi; so henceforth we condition
on Eventi−1 ∩ Event0
i.
Set
C1 := min

(56 · 9 · 102)−1
, (100 · 168)−1
, ∆0
from Lemma B.5 with ∆ = 1/50 . (28)
For all τ ∈ [L] such that θ
(τ)
0 is γ-separated relative to π(jmax), we have (i) |θ
(τ)
jmax,0| ≥ 1/
√
k,
and (ii) that by Lemma B.4 (using ˜
/p := 2, κ := 1, and i∗ := π(jmax), and providing C2),
|T̃i(θ
(τ)
N , θ
(τ)
N , θ
(τ)
N ) − λπ(jmax)| ≤ 5
(notice by definition that γ ≥ 1/100 implies γ0 ≥ 1 − /(1 + γ) ≥ 1/101, thus it follows
from the bounds on the other quantities that ˜
 = 2p ≤ 56C1 · λmin
k < γ0
2(1+8κ) · λ̃min · θ2
i∗,0 as
necessary). Therefore θN := θ
(τ∗)
N must satisfy
T̃i(θN , θN , θN ) = max
τ∈[L]
T̃i(θ
(τ)
N , θ
(τ)
N , θ
(τ)
N ) ≥ max
j≥i
λπ(j) − 5 = λπ(jmax) − 5.
On the other hand, by the triangle inequality,
T̃i(θN , θN , θN ) ≤
X
j≥i
λπ(j)θ3
π(j),N + |Ẽi(θN , θN , θN )|
≤
X
j≥i
λπ(j)|θπ(j),N |θ2
π(j),N + 56
≤ λπ(j∗)|θπ(j∗),N | + 56
where j∗ := arg maxj≥i λπ(j)|θπ(j),N |. Therefore
λπ(j∗)|θπ(j∗),N | ≥ λπ(jmax) − 5 − 56 ≥
4
5
λπ(jmax).
Squaring both sides and using the fact that θ2
π(j∗),N + θ2
π(j),N ≤ 1 for any j 6= j∗,
λπ(j∗)θπ(j∗),N
2
≥
16
25
λπ(jmax)θπ(j∗),N
2
+
16
25
λπ(jmax)θπ(j),N
2
≥
16
25
λπ(j∗)θπ(j∗),N
2
+
16
25
λπ(j)θπ(j),N
2
2817
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
which in turn implies
λπ(j)|θπ(j),N | ≤
3
4
λπ(j∗)|θπ(j∗),N |, j 6= j∗
.
This means that θN is (1/4)-separated relative to π(j∗). Also, observe that
|θπ(j∗),N | ≥
4
5
·
λπ(jmax)
λπ(j∗)
≥
4
5
,
λπ(jmax)
λπ(j∗)
≤
5
4
.
Therefore by Lemma B.4 (using ˜
/p := 2, γ := 1/4, and κ := 5/4), executing another N
power iterations starting from θN gives a vector θ̂ that satisfies
kθ̂ − vπ(j∗)k ≤
8
λπ(j∗)
, |λ̂ − λπ(j∗)| ≤ 5.
Since v̂i = θ̂ and λ̂i = λ̂, the first assertion of the inductive hypothesis is satisfied, as we
can modify the permutation π by swapping π(i) and π(j∗) without affecting the values of
{π(j) : j ≤ i − 1} (recall j∗ ≥ i).
We now argue that Ẽi+1 has the required properties to complete the inductive step. By
Lemma B.5 (using ˜
 := 5 and ∆ := 1/50, the latter providing one upper bound on C1 as
per (28)), we have for any unit vector u ∈ Sk−1,
X
j≤i

λπ(j)v⊗3
π(j) − λ̂jv̂⊗3
j

(I, u, u) ≤

1/50 + 100
i
X
j=1
(u>
vπ(j))2
1/2
5 ≤ 55. (29)
Therefore by the triangle inequality,
kẼi+1(I, u, u)k ≤ kE(I, u, u)k +
X
j≤i

λπ(j)v⊗3
π(j) − λ̂jv̂⊗3
j

(I, u, u) ≤ 56.
Thus the bound (26) holds.
To prove that (27) holds, pick any unit vector u ∈ Sk−1 such that there exists j0 ≥ i + 1
with (u>
vπ(j0))2 ≥ 1 − (168/λπ(j0))2. We have, via the second bound on C1 in (28) and the
corresponding assumed bound  ≤ C1 · λmin
k ,
100
i
X
j=1
(u>
vπ(j))2
≤ 100

1 − (u>
vπ(j0))2

≤ 100

168
λπ(j0)
2
≤
1
50
,
and therefore

1/50 + 100
i
X
j=1
(u>
vπ(j))2
1/2
5 ≤ (1/50 + 1/50)1/2
5 ≤ .
By the triangle inequality, we have kẼi+1(I, u, u)k ≤ 2. Therefore (27) holds, so the
second assertion of the inductive hypothesis holds. Thus Eventi−1 ∩ Event0
i ⊆ Eventi, and
Pr[Eventi] ≥ Pr[Eventi−1 ∩ Event0
i] ≥ 1 − iη/k. We conclude that by the induction principle,
2818
Tensor Decompositions for Learning Latent Variable Models
there exists a permutation π such that two assertions hold for i = k, with probability at
least 1 − η.
From the last induction step (i = k), it is also clear from (29) that kT −
Pk
j=1 λ̂jv̂⊗3
j k ≤
55 (in Eventk−1 ∩ Event0
k). This completes the proof of the theorem.
Appendix C. Variant of Robust Power Method that uses a Stopping
Condition
In this section we analyze a variant of Algorithm 1 that uses a stopping condition. The
variant is described in Algorithm 2. The key difference is that the inner for-loop is repeated
until a stopping condition is satisfied (rather than explicitly L times). The stopping condi-
tion ensures that the power iteration is converging to an eigenvector, and it will be satisfied
within poly(k) random restarts with high probability. The condition depends on one new
quantity, r, which should be set to r := k − # deflation steps so far (i.e., the first call to
Algorithm 2 uses r = k, the second call uses r = k − 1, and so on).
Algorithm 2 Robust tensor power method with stopping condition
input symmetric tensor T̃ ∈ Rk×k×k, number of iterations N, expected rank r.
output the estimated eigenvector/eigenvalue pair; the deflated tensor.
1: repeat
2: Draw θ0 uniformly at random from the unit sphere in Rk.
3: for t = 1 to N do
4: Compute power iteration update
θt :=
T̃(I, θt−1, θt−1)
kT̃(I, θt−1, θt−1)k
(30)
5: end for
6: until the following stopping condition is satisfied:
|T̃(θN , θN , θN )| ≥ max

1
2
√
r
kT̃kF ,
1
1.05
kT̃(I, I, θN )kF

.
7: Do N power iteration updates (30) starting from θN to obtain θ̂, and set λ̂ := T̃(θ̂, θ̂, θ̂).
8: return the estimated eigenvector/eigenvalue pair (θ̂, λ̂); the deflated tensor T̃ −λ̂ θ̂⊗3.
C.1 Stopping Condition Analysis
For a matrix A, we use kAkF := (
P
i,j A2
i,j)1/2 to denote its Frobenius norm. For a third-
order tensor A, we use kAkF := (
P
i kA(I, I, ei)k2
F )1/2 = (
P
i kA(I, I, vi)k2
F )1/2.
Define T̃ as before in (11):
T̃ :=
k
X
i=1
λ̃iv⊗3
i + Ẽ.
2819
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
We assume Ẽ is a symmetric tensor such that, for some constant p > 1,
kẼ(I, u, u)k ≤ ˜
, ∀u ∈ Sk−1
;
kẼ(I, u, u)k ≤ ˜
/p, ∀u ∈ Sk−1
s.t. (u>
v1)2
≥ 1 − (3˜
/λ̃1)2
;
kẼkF ≤ ˜
F .
Assume that not all λ̃i are zero, and define
λ̃min := min{λ̃i : i ∈ [k], λ̃i > 0}, λ̃max := max{λ̃i : i ∈ [k]},
` := |{i ∈ [k] : λ̃i > 0}|, λ̃avg :=

1
`
k
X
i=1
λ̃2
i
1/2
.
We show in Lemma C.1 that if the stopping condition is satisfied by a vector θ, then
it must be close to an eigenvector of T̃. Then in Lemma C.2, we show that the stopping
condition is satisfied by θN when θ0 is a good starting point (as per the conditions of
Lemma B.4).
Lemma C.1 Fix any vector θ =
Pk
i=1 θivi, and let i∗ := arg maxi∈[k] λ̃i|θi|. Assume that
` ≥ 1 and that for some α ∈ (0, 1/20) and β ≥ 2α/
√
k,
˜
 ≤ α ·
λ̃min
√
k
, ˜
F ≤
√
`
1
2
−
α
β
√
k

· λ̃avg.
If the stopping condition
|T̃(θ, θ, θ)| ≥ max

β
√
`
kT̃kF ,
1
1 + α
kT̃(I, I, θ)kF

(31)
holds, then
1. λ̃i∗ ≥ βλ̃avg/2 and λ̃i∗ |θi∗ | > 0;
2. maxi6=i∗ λ̃i|θi| ≤
√
7α · λ̃i∗ |θi∗ |;
3. θi∗ ≥ 1 − 2α.
Proof Without loss of generality, assume i∗ = 1. First, we claim that λ̃1|θ1| > 0. By the
triangle inequality,
|T̃(θ, θ, θ)| ≤
k
X
i=1
λ̃iθ3
i + |Ẽ(θ, θ, θ)| ≤
k
X
i=1
λ̃i|θi|θ2
i + ˜
 ≤ λ̃1|θ1| + ˜
.
2820
Tensor Decompositions for Learning Latent Variable Models
Moreover,
kT̃kF ≥
k
X
i=1
λ̃iv⊗3
i
F
− kẼkF
=
 k
X
j=1
k
X
i=1
λ̃iviv>
i (v>
i vj)
2
F
1/2
− kẼkF
=
 k
X
j=1
λ̃jvjv>
j
2
F
1/2
− kẼkF
=
 k
X
j=1
λ̃2
j
1/2
− kẼkF
≥
√
`λ̃avg − ˜
F .
By assumption, |T̃(θ, θ, θ)| ≥ (β/
√
`)kT̃kF , so
λ̃1|θ1| ≥ βλ̃avg −
β
√
`
˜
F − ˜
 ≥ βλ̃avg − β
1
2
−
α
β
√
k

λ̃avg −
α
√
k
λ̃min ≥
β
2
λ̃avg
where the second inequality follows from the assumptions on ˜
 and ˜
F . Since β > 0, λ̃avg > 0,
and |θ1| ≤ 1, it follows that
λ̃1 ≥
β
2
λ̃avg, λ̃1|θ1| > 0.
This proves the first claim.
Now we prove the second claim. Define M̃ := T̃(I, I, θ) =
Pk
i=1 λ̃iθiviv>
i + Ẽ(I, I, θ) (a
symmetric k × k matrix), and consider its eigenvalue decomposition
M̃ =
k
X
i=1
φiuiu>
i
where, without loss of generality, |φ1| ≥ |φ2| ≥ · · · ≥ |φk| and {u1, u2, . . . , uk} is an or-
thonormal basis. Let M :=
Pk
i=1 λ̃iθiviv>
i , so M̃ = M + Ẽ(I, I, θ). Note that the λ̃i|θi| and
|φi| are the singular values of M and M̃, respectively. We now show that the assumption
on |T̃(θ, θ, θ)| implies that almost all of the energy in M is contained in its top singular
component.
By Weyl’s theorem,
|φ1| ≤ λ̃1|θ1| + kM̃ − Mk ≤ λ̃1|θ1| + ˜
.
Next, observe that the assumption kT̃(I, I, θ)kF ≤ (1 + α)T̃(θ, θ, θ) is equivalent to (1 +
α)θ>
M̃θ ≥ kM̃kF . Therefore, using the fact that |φ1| = maxu∈Sk−1 |u>
M̃u|, the triangle
2821
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
inequality, and the fact kAkF ≤
√
kkAk for any matrix A ∈ Rk×k,
(1 + α)|φ1| ≥ (1 + α)θ>
M̃θ ≥ kM̃kF (32)
≥
k
X
i=1
λ̃iθiviv>
i
F
− Ẽ(I, I, θ) F
≥
 k
X
i=1
λ̃2
i θ2
i
1/2
−
√
kkẼ(I, I, θ)k
≥
 k
X
i=1
λ̃2
i θ2
i
1/2
−
√
k˜
.
Combining these bounds on |φ1| gives
λ̃1|θ1| + ˜
 ≥
1
1 + α
" k
X
i=1
λ̃2
i θ2
i
1/2
−
√
k˜

#
. (33)
The assumption ˜
 ≤ αλ̃min/
√
k implies that
√
k˜
 ≤ αλ̃min ≤ α
 k
X
i=1
λ̃2
i θ2
i
1/2
.
Moreover, since λ̃1|θ1| > 0 (by the first claim) and λ̃1|θ1| = maxi∈[k] λ̃i|θi|, it follows that
λ̃1|θ1| ≥ λ̃min max
i∈[k]
|θi| ≥
λ̃min
√
k
, (34)
so we also have
˜
 ≤ αλ̃1|θ1|.
Applying these bounds on ˜
 to (33), we obtain
λ̃1|θ1| ≥
1 − α
(1 + α)2
 k
X
i=1
λ̃2
i θ2
i
1/2
≥
1 − α
(1 + α)2

λ̃2
1θ2
1 + max
i6=1
λ̃2
i θ2
i
1/2
which in turn implies (for α ∈ (0, 1/20))
max
i6=1
λ̃2
i θ2
i ≤

(1 + α)4
(1 − α)2
− 1

· λ̃2
1θ2
1 ≤ 7α · λ̃2
1θ2
1.
Therefore maxi6=1 λ̃i|θi| ≤
√
7α · λ̃1|θ1|, proving the second claim.
Now we prove the final claim. This is done by (i) showing that θ has a large projection
onto u1, (ii) using an SVD perturbation argument to show that ±u1 is close to v1, and (iii)
concluding that θ has a large projection onto v1.
We begin by showing that (u>
1 θ)2 is large. Observe that from (32), we have (1+α)2φ2
1 ≥
kM̃k2
F ≥ φ2
1 + maxi6=1 φ2
i , and therefore
max
i6=1
|φi| ≤
p
2α + α2 · |φ1|.
2822
Tensor Decompositions for Learning Latent Variable Models
Moreover, by the triangle inequality,
|θ>
M̃θ| ≤
k
X
i=1
|φi|(u>
i θ)2
≤ |φ1|(u>
1 θ)2
+ max
i6=1
|φi| 1 − (u>
1 θ)2

= (u>
1 θ)2
|φ1| − max
i6=1
|φi|

+ max
i6=1
|φi|.
Using (32) once more, we have |θ>
M̃θ| ≥ kM̃kF /(1 + α) ≥ |φ1|/(1 + α), so
(u>
1 θ)2
≥
1
1+α − maxi6=1
|φi|
|φ1|
1 − maxi6=1
|φi|
|φ1|
= 1 −
α
(1 + α)

1 − maxi6=1
|φi|
|φ1|
 ≤ 1 −
α
(1 + α)(1 −
√
2α + α2)
.
Now we show that (u>
1 v1)2 is also large. By the second claim, the assumption on ˜
, and (34),
λ̃1|θ1| − max
i6=1
λ̃i|θi| > (1 −
√
7α) · λ̃1|θ1| ≥ (1 −
√
7α) · λ̃min/
√
k.
Combining this with Weyl’s theorem gives
|φ1| − max
i6=1
λ̃i|θi| ≥ λ̃1|θ1| − ˜
 − max
i6=1
λ̃i|θi| ≥ (1 − (α +
√
7α)) · λ̃min/
√
k,
so we may apply Wedin’s theorem to obtain
(u>
1 v1)2
≥ 1 −

kẼ(I, I, θ)k
|φ1| − maxi6=1 λ̃i|θi|
2
≥ 1 −

α
1 − (α +
√
7α)
2
.
It remains to show that θ1 = v>
1 θ is large. Indeed, by the triangle inequality, Cauchy-
Schwarz, and the above inequalities on (u>
1 v1)2 and (u>
1 θ)2,
|v>
1 θ| =
k
X
i=1
(u>
i v1)(u>
i θ)
≥ |u>
1 v1||u>
1 θ| −
k
X
i=2
|u>
i v1||u>
i θ|
≥ |u>
1 v1||u>
1 θ| −
 k
X
i=2
(u>
i v1)2
1/2 k
X
i=2
(u>
i θ)2
1/2
= |u>
1 v1||u>
1 θ| −

1 − (u>
i v1)2

1 − (u>
i θ)2
1/2
≥

1 −
α
(1 + α)(1 −
√
2α + α2)

1 −

α
1 − (α +
√
7α)
2!1/2
−
α
(1 + α)(1 −
√
2α + α2)
·

α
1 − (α +
√
7α)
2
!1/2
≥ 1 − 2α
2823
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
for α ∈ (0, 1/20). Moreover, by assumption we have T̃(θ, θ, θ) ≥ 0, and
T̃(θ, θ, θ) =
k
X
i=1
λ̃iθ3
i + Ẽ(θ, θ, θ)
= λ̃1θ3
1 +
k
X
i=2
λ̃iθ3
i + Ẽ(θ, θ, θ)
≤ λ̃1θ3
1 + max
i6=1
λ̃i|θi|
k
X
i=2
θ2
i + ˜

≤ λ̃1θ3
1 +
√
7αλ̃1|θ1|(1 − θ2
1) + ˜
 (by the second claim)
≤ λ̃1|θ1|3

sign(θ1) +
√
7α
(1 − 2α)2
−
√
7α +
α
(1 − 2α)3

(since |θ1| ≥ 1 − 2α)
< λ̃1|θ1|3

sign(θ1) + 1

so sign(θ1) > −1, meaning θ1 > 0. Therefore θ1 = |θ1| ≥ 1 − 2α. This proves the final
claim.
Lemma C.2 Fix α, β ∈ (0, 1). Assume λ̃i∗ = maxi∈[k] λ̃i and
˜
 ≤ min

α
5
√
k + 7
,
1 − β
7

· λ̃i∗ , ˜
F ≤
√
` ·
1 − β
2β
· λ̃i∗ .
To the conclusion of Lemma B.4, it can be added that the stopping condition (31) is satisfied
by θ = θt.
Proof Without loss of generality, assume i∗ = 1. By the triangle inequality and Cauchy-
Schwarz,
kT̃(I, I, θt)kF ≤ λ̃1|θ1,t| +
X
i6=1
λi|θi,t| + kẼ(I, I, θt)kF ≤ λ̃1|θ1,t| + λ̃1
√
k
X
i6=1
θ2
i,t
1/2
+
√
k˜

≤ λ̃1|θ1,t| +
3
√
k˜

p
+
√
k˜
.
where the last step uses the fact that θ2
1,t ≥ 1 − (3˜
/(pλ̃1))2. Moreover,
T̃(θt, θt, θt) ≥ λ̃1 −

27
 ˜

pλ1
2
+ 2

˜

p
.
Combining these two inequalities with the assumption on ˜
 implies that
T̃(θt, θt, θt) ≥
1
1 + α
kT̃(I, I, θt)kF .
2824
Tensor Decompositions for Learning Latent Variable Models
Using the definition of the tensor Frobenius norm, we have
1
√
`
kT̃kF ≤
1
√
`
k
X
i=1
λ̃iv⊗3
i
F
+
1
√
`
kẼkF = λ̃avg +
1
√
`
kẼkF ≤ λ̃avg +
1
√
`
˜
F .
Combining this with the above inequality implies
T̃(I, I, θt) ≥
β
√
`
kT̃kF .
Therefore the stopping condition (31) is satisfied.
C.2 Sketch of Analysis of Algorithm 2
The analysis of Algorithm 2 is very similar to the proof of Theorem 5.1 for Algorithm 1, so
here we just sketch the essential differences.
First, the guarantee afforded to Algorithm 2 is somewhat different than Theorem 5.1.
Specifically, it is of the following form: (i) under appropriate conditions, upon termination,
the algorithm returns an accurate decomposition, and (ii) the algorithm terminates after
poly(k) random restarts with high probability.
The conditions on  and N are the same (but for possibly different universal constants
C1, C2). In Lemma C.1 and Lemma C.2, there is reference to a condition on the Frobenius
norm of E, but we may use the inequality kEkF ≤ kkEk ≤ k so that the condition is
subsumed by the  condition.
Now we outline the differences relative to the proof of Theorem 5.1. The basic structure
of the induction argument is the same. In the induction step, we argue that (i) if the
stopping condition is satisfied, then by Lemma C.1 (with α = 0.05 and β = 1/2), we have
a vector θN such that, for some j∗ ≥ i,
1. λπ(j∗) ≥ λπ(jmax)/(4
√
k);
2. θN is (1/4)-separated relative to π(j∗);
3. θπ(j∗),N ≥ 4/5;
and (ii) the stopping condition is satisfied within poly(k) random restarts (via Lemma B.1
and Lemma C.2) with high probability. We now invoke Lemma B.4 to argue that executing
another N power iterations starting from θN gives a vector θ̂ that satisfies
kθ̂ − vπ(j∗)k ≤
8
λπ(j∗)
, |λ̂ − λπ(j∗)| ≤ 5.
The main difference here, relative to the proof of Theorem 5.1, is that we use κ := 4
√
k
(rather than κ = O(1)), but this ultimately leads to the same guarantee after taking into
consideration the condition  ≤ C1λmin/k. The remainder of the analysis is essentially the
same as the proof of Theorem 5.1.
2825
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Appendix D. Simultaneous Diagonalization for Tensor Decomposition
As discussed in the introduction, another standard approach to certain tensor decomposition
problems is to simultaneously diagonalize a collection of similar matrices obtained from the
given tensor. We now examine this approach in the context of our latent variable models,
where
M2 =
k
X
i=1
wi µi ⊗ µi
M3 =
k
X
i=1
wi µi ⊗ µi ⊗ µi.
Let V := [µ1|µ2| · · · |µk] and D(η) := diag(µ>
1 η, µ>
2 η, . . . , µ>
k η), so
M2 = V diag(w1, w2, . . . wk)V >
M3(I, I, η) = V diag(w1, w2, . . . wk)D(η)V >
Thus, the problem of determining the µi can be cast as a simultaneous diagonalization
problem: find a matrix X such that X>
M2X and X>
M3(I, I, η)X (for all η) are diagonal.
It is easy to see that if the µi are linearly independent, then the solution X>
= V † is unique
up to permutation and rescaling of the columns.
With exact moments, a simple approach is as follows. Assume for simplicity that d = k,
and define
M(η) := M3(I, I, η)M−1
2 = V D(η)V −1
.
Observe that if the diagonal entries of D(η) are distinct, then the eigenvectors of M(η) are
the columns of V (up to permutation and scaling). This criterion is satisfied almost surely
when η is chosen randomly from a continuous distribution over Rk.
The above technique (or some variant thereof) was previously used to give the efficient
learnability results, where the computational and sample complexity bounds were polyno-
mial in relevant parameters of the problem, including the rank parameter k (Mossel and
Roch, 2006; Anandkumar et al., 2012c,a; Hsu and Kakade, 2013). However, the specific
polynomial dependence on k was rather large due to the need for the diagonal entries of
D(η) to be well-separated. This is because with finite samples, M(η) is only known up
to some perturbation, and thus the sample complexity bound depends inversely in (some
polynomial of) the separation of the diagonal entries of D(η). With η drawn uniformly
at random from the unit sphere in Rk, the separation was only guaranteed to be roughly
1/k2.5 (Anandkumar et al., 2012c) (while this may be a loose estimate, the instability is ob-
served in practice). In contrast, using the tensor power method to approximately recover V
(and hence the model parameters µi and wi) requires only a mild, lower-order dependence
on k.
It should be noted, however, that the use of a single random choice of η is quite restric-
tive, and it is easy to see that a simultaneous diagonalization of M(η) for several choices
of η can be beneficial. While the uniqueness of the eigendecomposition of M(η) is only
guaranteed when the diagonal entries of D(η) are distinct, the simultaneous diagonaliza-
tion of M(η(1)), M(η(2)), . . . , M(η(m)) for vectors η(1), η(2), . . . , η(m) is unique as long as the
2826
Tensor Decompositions for Learning Latent Variable Models
columns of 




µ>
1 η(1) µ>
2 η(1) · · · µ>
k η(1)
µ>
1 η(2) µ>
2 η(2) · · · µ>
k η(2)
.
.
.
.
.
.
...
.
.
.
µ>
1 η(m) µ>
2 η(m) · · · µ>
k η(m)





are distinct (i.e., for each pair of column indices i, j, there exists a row index r such that
the (r, i)-th and (r, j)-th entries are distinct). This is a much weaker requirement for
uniqueness, and therefore may translate to an improved perturbation analysis. In fact, using
the techniques discussed in Section 4.3, we may even reduce the problem to an orthogonal
simultaneous diagonalization, which may be easier to obtain. Furthermore, a number of
robust numerical methods for (approximately) simultaneously diagonalizing collections of
matrices have been proposed and used successfully in the literature (e.g., Bunse-Gerstner
et al., 1993; Cardoso and Souloumiac, 1993; Cardoso, 1994; Cardoso and Comon, 1996;
Ziehe et al., 2004). Another alternative and a more stable approach compared to full
diagonalization is a Schur-like method which finds a unitary matrix U which simultaneously
triangularizes the respective matrices (Corless et al., 1997). It is an interesting open question
whether these techniques can yield similar improved learnability results and also enjoy the
attractive computational properties of the tensor power method.
References
D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In
Eighteenth Annual Conference on Learning Theory, pages 458–469, 2005.
E. S. Allman, C. Matias, and J. A. Rhodes. Identifiability of parameters in latent structure
models with many observed variables. The Annals of Statistics, 37(6A):3099–3132, 2009.
A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y.-K. Liu. A spectral algorithm
for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25,
2012a.
A. Anandkumar, D. Hsu, F. Huang, and S. M. Kakade. Learning mixtures of tree graphical
models. In Advances in Neural Information Processing Systems 25, 2012b.
A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models
and hidden Markov models. In Twenty-Fifth Annual Conference on Learning Theory,
volume 23, pages 33.1–33.34, 2012c.
J. Anderson, M. Belkin, N. Goyal, L. Rademacher, and J. Voss. The more, the merrier:
the blessing of dimensionality for learning large Gaussian mixtures. In Twenty-Seventh
Annual Conference on Learning Theory, 2014.
S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. The
Annals of Applied Probability, 15(1A):69–92, 2005.
S. Arora, R. Ge, and A. Moitra. Learning topic models — going beyond SVD. In Fifty-Third
IEEE Annual Symposium on Foundations of Computer Science, pages 1–10, 2012a.
2827
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
S. Arora, R. Ge, A. Moitra, and S. Sachdeva. Provable ICA with unknown Gaussian
noise, and implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems 25, 2012b.
T. Austin. On exchangeable random variables and the statistics of large graphs and hyper-
graphs. Probab. Survey, 5:80–145, 2008.
R. Bailly. Quadratic weighted automata: Spectral algorithm and likelihood maximization.
Journal of Machine Learning Research, 2011.
B. Balle and M. Mohri. Spectral learning of general weighted automata via constrained
matrix completion. In Advances in Neural Information Processing Systems 25, 2012.
B. Balle, A. Quattoni, and X. Carreras. Local loss optimization in operator models: A new
insight into spectral learning. In Twenty-Ninth International Conference on Machine
Learning, 2012.
M. Belkin and K. Sinha. Polynomial learning of distribution families. In Fifty-First Annual
IEEE Symposium on Foundations of Computer Science, pages 103–112, 2010.
A. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaraghavan. Smoothed analysis of ten-
sor decompositions. In Proceedings of the 46th Annual ACM Symposium on Theory of
Computing, 2014.
B. Boots, S. M. Siddiqi, and G. J. Gordon. Closing the learning-planning loop with predic-
tive state representations. In Proceedings of the Robotics Science and Systems Conference,
2010.
S. C. Brubaker and S. Vempala. Isotropic PCA and affine-invariant clustering. In Forty-
Ninth Annual IEEE Symposium on Foundations of Computer Science, 2008.
A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simultaneous
diagonalization. SIAM Journal on Matrix Analysis and Applications, 14(4):927–949, 1993.
J.-F. Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. blind
identification of more sources than sensors. In Acoustics, Speech, and Signal Processing,
1991. ICASSP-91., 1991 International Conference on, pages 3109–3112. IEEE, 1991.
J.-F. Cardoso. Perturbation of joint diagonalizers. Technical Report 94D027, Signal De-
partment, Télécom Paris, 1994.
J.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic
methods. In IEEE International Symposium on Circuits and Systems, pages 93–96, 1996.
J.-F. Cardoso and A. Souloumiac. Blind beamforming for non Gaussian signals. IEE
Proceedings-F, 140(6):362–370, 1993.
D. Cartwright and B. Sturmfels. The number of eigenvalues of a tensor. Linear Algebra
Appl., 438(2):942–952, 2013.
2828
Tensor Decompositions for Learning Latent Variable Models
R. B. Cattell. Parallel proportional profiles and other principles for determining the choice
of factors by rotation. Psychometrika, 9(4):267–283, 1944.
J. T. Chang. Full reconstruction of Markov models on evolutionary trees: Identifiability
and consistency. Mathematical Biosciences, 137:51–73, 1996.
K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations
and independence. In Twenty-First Annual Conference on Learning Theory, pages 9–20,
2008.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. Spectral learning of
latent-variable PCFGs. In Fiftieth Annual Meeting of the Association for Computational
Linguistics, 2012.
P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):
287–314, 1994.
P. Comon and C. Jutten. Handbook of Blind Source Separation: Independent Component
Analysis and Applications. Academic Press. Elsevier, 2010.
P. Comon, G. Golub, L.-H. Lim, and B. Mourrain. Symmetric tensors and symmetric tensor
rank. SIAM Journal on Matrix Analysis Appl., 30(3):1254–1279, 2008.
R. M. Corless, P. M. Gianni, and B. M. Trager. A reordered Schur factorization method
for zero-dimensional polynomial systems with multiple roots. In Proceedings of the 1997
International Symposium on Symbolic and Algebraic Computation, pages 133–140. ACM,
1997.
S. Dasgupta. Learning mixtures of Gaussians. In Fortieth Annual IEEE Symposium on
Foundations of Computer Science, pages 634–644, 1999.
S. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated,
spherical Gaussians. Journal of Machine Learning Research, 8(Feb):203–226, 2007.
L. De Lathauwer, J. Castaing, and J.-F. Cardoso. Fourth-order cumulant-based blind
identification of underdetermined mixtures. Signal Processing, IEEE Transactions on,
55(6):2965–2973, 2007.
N. Delfosse and P. Loubaton. Adaptive blind separation of independent sources: a deflation
approach. Signal processing, 45(1):59–83, 1995.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum-likelihood from incomplete data
via the EM algorithm. J. Royal Statist. Soc. Ser. B, 39:1–38, 1977.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. Ungar. Spectral dependency parsing
with latent variables. In Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, 2012.
M. Drton, B. Sturmfels, and S. Sullivant. Algebraic factor analysis: tetrads, pentads and
beyond. Probability Theory and Related Fields, 138(3):463–493, 2007.
2829
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
A. T. Erdogan. On the convergence of ICA algorithms with symmetric orthogonalization.
IEEE Transactions on Signal Processing, 57:2209–2221, 2009.
A. M. Frieze, M. Jerrum, and R. Kannan. Learning linear transformations. In Thirty-
Seventh Annual Symposium on Foundations of Computer Science, pages 359–368, 1996.
G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins University Press,
1996.
N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Prob-
abilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2), 2011.
R. Harshman. Foundations of the PARAFAC procedure: model and conditions for an
‘explanatory’ multi-mode factor analysis. Technical report, UCLA Working Papers in
Phonetics, 1970.
C. J. Hillar and L.-H. Lim. Most tensor problems are NP-hard. J. ACM, 60(6):45:1–45:39,
November 2013. ISSN 0004-5411. doi: 10.1145/2512329.
F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of
Mathematics and Physics, 6:164–189, 1927a.
F. L. Hitchcock. Multiple invariants and generalized rank of a p-way matrix or tensor.
Journal of Mathematics and Physics, 7:39–79, 1927b.
D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and
spectral decompositions. In Fourth Innovations in Theoretical Computer Science, 2013.
D. Hsu, S. M. Kakade, and P. Liang. Identifiability and unmixing of latent parse trees. In
Advances in Neural Information Processing Systems 25, 2012a.
D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov
models. Journal of Computer and System Sciences, 78(5):1460–1480, 2012b.
A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis.
Neural Networks, IEEE Transactions on, 10(3):626–634, 1999.
A. Hyvärinen and E. Oja. Independent component analysis: algorithms and applications.
Neural Networks, 13(4–5):411–430, 2000.
H. Jaeger. Observable operator models for discrete stochastic time series. Neural Comput.,
12(6), 2000.
A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In
Forty-second ACM Symposium on Theory of Computing, pages 553–562, 2010.
R. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models.
SIAM Journal on Computing, 38(3):1141–1156, 2008.
2830
Tensor Decompositions for Learning Latent Variable Models
E. Kofidis and P. A. Regalia. On the best rank-1 approximation of higher-order super-
symmetric tensors. SIAM Journal on Matrix Analysis and Applications, 23(3):863–884,
2002.
T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51
(3):455, 2009.
T. G. Kolda and J. R. Mayo. Shifted power method for computing tensor eigenpairs. SIAM
Journal on Matrix Analysis and Applications, 32(4):1095–1124, October 2011.
J. B. Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with
application to arithmetic complexity and statistics. Linear Algebra and Appl., 18(2):
95–138, 1977.
L. D. Lathauwer, B. D. Moor, and J. Vandewalle. On the best rank-1 and rank-
(R1, R2, ..., Rn) approximation and applications of higher-order tensors. SIAM J. Matrix
Anal. Appl., 21(4):1324–1342, 2000.
L. Le Cam. Asymptotic Methods in Statistical Decision Theory. Springer, 1986.
L.-H. Lim. Singular values and eigenvalues of tensors: a variational approach. Proceedings of
the IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive
Processing, 1:129–132, 2005.
M. Littman, R. Sutton, and S. Singh. Predictive representations of state. In Advances in
Neural Information Processing Systems 14, pages 1555–1561, 2001.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. Spectral learning for non-deterministic
dependency parsing. In Conference of the European Chapter of the Association for Com-
putational Linguistics, 2012.
J. B. MacQueen. Some methods for classification and analysis of multivariate observa-
tions. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and
Probability, volume 1, pages 281–297. University of California Press, 1967.
P. McCullagh. Tensor Methods in Statistics. Chapman and Hall, 1987.
A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In
Fifty-First Annual IEEE Symposium on Foundations of Computer Science, pages 93–102,
2010.
E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models.
Annals of Applied Probability, 16(2):583–614, 2006.
P. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU
signatures. Journal of Cryptology, 22(2):139–160, 2009.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.
P. V. Overschee and B. D. Moor. Subspace Identification of Linear Systems. Kluwer Aca-
demic Publishers, 1996.
2831
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
L. Pachter and B. Sturmfels. Algebraic Statistics for Computational Biology, volume 13.
Cambridge University Press, 2005.
A. Parikh, L. Song, and E. P. Xing. A spectral algorithm for latent tree graphical models.
In Twenty-Eighth International Conference on Machine Learning, 2011.
K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Trans-
actions of the Royal Society, London, A., page 71, 1894.
L. Qi. Eigenvalues of a real supersymmetric tensor. Journal of Symbolic Computation, 40
(6):1302–1324, 2005.
R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM
algorithm. SIAM Review, 26(2):195–239, 1984.
P. A. Regalia and E. Kofidis. Monotonic convergence of fixed-point algorithms for ICA.
IEEE Transactions on Neural Networks, 14:943–949, 2003.
S. Roch. A short proof that phylogenetic tree reconstruction by maximum likelihood is
hard. IEEE/ACM Trans. Comput. Biol. Bioinformatics, 3(1), 2006.
J. Rodu, D. P. Foster, W. Wu, and L. H. Ungar. Using regression for spectral estimation
of HMMs. In Statistical Language and Speech Processing, pages 212–223, 2013.
M. P. Schützenberger. On the definition of a family of automata. Inf. Control, 4:245–270,
1961.
S. M. Siddiqi, B. Boots, and G. J. Gordon. Reduced-rank hidden Markov models. In
Thirteenth International Conference on Artificial Intelligence and Statistics, 2010.
D. A. Spielman and S. H. Teng. Smoothed analysis: An attempt to explain the behavior of
algorithms in practice. Communications of the ACM, pages 76–84, 2009.
A. Stegeman and P. Comon. Subtracting a best rank-1 approximation may increase tensor
rank. Linear Algebra and Its Applications, 433:1276–1300, 2010.
B. Sturmfels and P. Zwiernik. Binary cumulant varieties. Ann. Comb., (17):229–250, 2013.
S. Vempala and G. Wang. A spectral algorithm for learning mixtures models. Journal of
Computer and System Sciences, 68(4):841–860, 2004.
P. Wedin. Perturbation bounds in connection with singular value decomposition. BIT
Numerical Mathematics, 12(1):99–111, 1972.
T. Zhang and G. Golub. Rank-one approximation to high order tensors. SIAM Journal on
Matrix Analysis and Applications, 23:534–550, 2001.
A. Ziehe, P. Laskov, G. Nolte, and K. R. Müller. A fast algorithm for joint diagonaliza-
tion with non-orthogonal transformations and its application to blind source separation.
Journal of Machine Learning Research, 5:777–800, 2004.
J. Zou, D. Hsu, D. Parkes, and R. P. Adams. Contrastive learning using spectral methods.
In Advances in Neural Information Processing Systems 26, 2013.
2832
