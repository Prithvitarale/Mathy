Dirichlet Process Mixtures of Generalized Mallows Models
Marina Meilă
Department of Statistics
University of Washington
Seattle, WA 98195-4322
mmp@stat.washington.edu
Harr Chen
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139-4307
harr@csail.mit.edu
Abstract
We present a Dirichlet process mixture model
over discrete incomplete rankings and study
two Gibbs sampling inference techniques for
estimating posterior clusterings. The first ap-
proach uses a slice sampling subcomponent
for estimating cluster parameters. The sec-
ond approach marginalizes out several cluster
parameters by taking advantage of approx-
imations to the conditional posteriors. We
empirically demonstrate (1) the effectiveness
of this approximation for improving conver-
gence, (2) the benefits of the Dirichlet pro-
cess model over alternative clustering tech-
niques for ranked data, and (3) the applica-
bility of the approach to exploring large real-
world ranking datasets.
1 MOTIVATION
Dirichlet process mixtures (DPM) are among the most
successful ways of modeling multimodal distributions
in a nonparametric Bayesian framework. They pro-
vide an elegant tradeoff between parameter sharing
and parameter variability between clusters, are ex-
tremely versatile due to the flexibility in choosing base
distributions, and enjoy all the other advantages of a
fully generative probabilistic model. However, the fea-
ture that makes the DPM model so useful – the fact
that it represents a fully nonparametric posterior –
also poses its greatest challenge, in that the posterior
is not computable in closed form. Therefore inference
in a DPM must be performed using approximate tech-
niques such as Monte Carlo sampling.
This paper introduces the DPM for the generalized
Mallows (GM) model, a family of distributions over
rankings. The GM has registered increasing popular-
ity in recent years, partly because of a growing interest
in ranked data and partly for its elegant computational
properties [Lebanon and Mao, 2008, Meilă et al., 2007].
However, as an exponential family model, the GM is
unimodal and thus suitable only for a limited range of
applications when applied in isolation. By incorporat-
ing it in a model hierarchy like the DPM, we can enjoy
the benefits of a fully generative multimodal model of
ranked data.
To estimate the posterior of a DPM of GMs, we present
two Gibbs sampling approaches. In the first, we explic-
itly draw all model parameters, relying on slice sam-
pling for one of the posterior distributions. Our second
approach marginalizes out several parameters by using
approximations to the conditional posteriors, acceler-
ating convergence at the cost of introducing error in
the stationary distribution.
We conduct three sets of experiments exploring the
properties of our approach. First, we compare the
two sampling approaches and find that the approxi-
mation is beneficial for improving convergence. Sec-
ond, we study the performance of the DPM of GMs in
relation to previous clustering techniques for ranked
data, showing improvements in held-out test likeli-
hood. Third, we conduct a qualitative analysis of a
large set of college admissions rankings, drawing con-
clusions that build upon observations made in previous
work.
2 GENERALIZED MALLOWS
MODELS
This section provides background on the general-
ized Mallows model, following Fligner and Verducci
[1986]. Let π denote a permutation over the set
{1, 2, 3, . . ., n}, where π(l) is the rank of element l
in π and π−1
(j) is the element at rank j. One
can uniquely determine any π by the n − 1 inte-
gers V1(π), V2(π), . . . , Vn−1(π) defined by Vj(π) =
P
l>j 1[l≺πj]. In words, Vj is the number of elements in
{j +1, . . . , n} that are ranked before j by π. It follows
that Vj takes values in {0, . . ., n − j}. Each element
Vj can be set independently in specifying a π, which is
not true of π(l) values. These Vj’s are called the code
of π.
This code can be defined w.r.t. to any reference per-
mutation σ by Vj(π | σ) ≡ Vj(πσ−1
). For any π and
σ we define s1, . . . , sn−1 to be a reciprocal form of the
code, by exchanging the places of σ and π:
sj(π | σ) = Vj(σ | π) =
X
l≻πj
1[l≺σj]. (1)
Equivalently, sj is equal to one less than the rank of
π−1
(j) in σ \ π−1
(1 : j − 1).
Based on this code representation, Fligner and Ver-
ducci [1986] introduced the following family of expo-
nential models called the generalized Mallows (GM)
models:
GMs
~
θ,σ
(π) =
e−
Pn−1
j=1 θj sj (π|σ)
ψ(~
θ)
. (2)
The GMs
distribution is parametrized by the cen-
tral permutation σ and concentration parameters ~
θ ≡
θ1:n−1 ≥ 0; ψ(~
θ) is a normalization constant that does
not depend on σ:
ψ(~
θ) =
n−1
Y
j=1
ψn−j(θj) =
n−1
Y
j=1
1 − e−(n−j+1)θj
1 − e−θj
. (3)
The GMs
model factors into a product of independent
univariate exponential models, one for each sj:
P[sj(π|σ) = k] =
e−θjk
ψn−j(θj)
. (4)
For θ1:n−1 = 0, GMs
is the uniform distribution. For
θ1:n−1 > 0, the GMs
distribution has a unique maxi-
mum at V1:n−1 = 0, i.e., at π = σ. Thus the GMs
is
centered around σ with exponential decay controlled
by ~
θ.
One can replace sj(π | σ) with Vj(π | σ) in (2), ob-
taining a GMV
with similar form to GMs
. These two
models are equivalent only when all θj are equal.
2.1 TOP-t RANKINGS
A permutation π is a top-t ranking when one only
observes the top t ranks (π−1
(1), . . . , π−1
(t)) rather
than the entire permutation. In a top-t ranking, the
codes s1:t are fixed while the remaining st+1:n−1 are
undetermined and can take any value in their respec-
tive ranges. For the GMs
model the marginals w.r.t.
s1, . . . , st for some t < n represent the probability of
a top-t ranking (π−1
(1), . . . π−1
(t)) [Fligner and Ver-
ducci, 1986]. Meilă and Bao [2008] showed that the
GMs
model for top-t rankings has sufficient statistics.
In contrast, neither of these statements hold for Vj
codes and GMV
over top-t rankings.
In the rest of this paper, we will be considering data
that consists of both full rankings and top-t rankings of
varying lengths (a full ranking is simply a top-t ranking
with t = n − 1). Thus our focus is on the GMs
model,
and GM should be understood to refer to GMs
.
2.2 SUFFICIENT STATISTICS AND
CONJUGATE PRIOR
For a given permutation π we define matrix Rj(π) as
Rj,ii′ (π) = 1[π−1(j)=i and i′6≺πi], (5)
and for a dataset π1:N of lengths t1:N we define
Rj(π1:N ) as
Rj(π1:N ) =
N
X
k=1
Rj(πk). (6)
In words, each Rj corresponds to a rank j, and element
Rj,ii′ counts how many times i was present at rank j,
minus how many of those times i′
preceded i; Rj,ii = 0
for all i, j. If the data consists of top-t rankings of
different lengths, Rj(π1:N ) will depend only on those
rankings of length at least j, and Rj(π1:N ) = 0 for
j > max(t1:N ). For datasets of varying lengths, we
will refer to max(t1:N ) as simply t.
For any top-t ranking π and complete ranking σ, we
have sj(π | σ) = Lσ(Rj(π)), where Lσ(A) denotes the
sum of the elements in the lower triangle of matrix A,
after its rows and columns are permuted by σ [Meilă
and Bao, 2008].
Matrices R1:t(π1:N ) are the sufficient statistics of the
GM for both the central permutation σ and the param-
eters ~
θ [Meilă and Bao, 2008]. The existence of finite
sufficient statistics implies that the GM will have a
conjugate prior, whose parameters are an equivalent
sample size ν > 0, and a set of equivalent sufficient
statistics of the form R0
1:t. This prior is fully described
by Meilă and Bao [2008].
In many contexts, including our present clustering
task, one desires to be uninformative w.r.t. to the cen-
tral permutation while expressing knowledge about the
parameters ~
θ. In this case, the prior has the form
P0
(σ, ~
θ ; ν, r) ∝ e−ν
P
j [θjrj +ln ψn−j (θj)]
, (7)
with r = [r1 r2 . . . rt], rj > 0 being a vector of posi-
tive parameters. This prior was used by Fligner and
Verducci [1988]. The corresponding posterior is:
P(σ, ~
θ | ν, r, π1:N )
∝ e−
P
j [(νrj +Lσ(Rj (π1:N )))θj +(ν+Nj ) ln ψn−j(θj )]
, (8)
where Nj is the number of data elements of length
at least j. The priors presented here are defined up
to a normalization constant. In general, there is no
closed-form expression for this constant.
In summary, the GM is an exponential family model
with simple sufficient statistics. Because the central
permutation is an explicit parameter, this model is
both more interpretable and tractable than other (ex-
ponential family) models over permutations. We use
it as a building block for the Dirichlet process mixture
model, which we briefly review below.
3 DIRICHLET PROCESS
MIXTURE MODELS
A Dirichlet process mixture [DPM; Antoniak, 1974] is
a generative clustering model. Generating data π1:N
from a DPM of GMs involves these steps:
G ∼ DP(α, P0
(σ, ~
θ | ν, r)),
σi, ~
θi ∼ G,
πi ∼ GM(π | σi, ~
θi).
First, a discrete distribution G over GM distributions
is sampled from the Dirichlet process prior. This prior
takes as a parameter a distribution over σ and ~
θ, in our
case the conjugate prior P0
. Next, a specific GM dis-
tribution with parameters σi, ~
θi is drawn from G. Data
point πi is finally sampled from this GM distribution.
If we sample data sequentially from this model, then
the (N + 1)th
sample will be distributed according to
σN+1, ~
θN+1 ∼
1
N + α
X
i≤N
δσi,~
θi
+
α
N + α
P0
ν,r. (9)
Hence, any finite sample will be a finite mixture
of GMs, allowing the DPM to represent ranking
data that is multimodal, with permutations clustered
around several centers. Another characterization of
the DPM is that each data point πi is associated with
a cluster label ci ∈ 1, . . . , C, and each cluster c with a
set of GM parameters σc and ~
θc.
Unlike a finite mixture, the number of clusters in the
DPM is itself a random variable. It will grow with
the size of the data in a way controlled by the concen-
tration parameter α. This makes DPM models ideal
for scenarios where the number of mixture components
is not well-defined in advance. DPMs have found ex-
tensive practical applications in areas such as topic
modeling [Teh et al., 2006], natural language process-
ing [Liang et al., 2007], vision [Sudderth et al., 2005],
and computational biology [Rasmussen et al., 2009].
Bayesian inference in the DPM model is typically con-
ducted via MCMC [Neal, 2000] or variational meth-
Algorithm Slice-Gibbs
Input Parameters ν, α, t, r1:t, T, TGibbs, TSlices, Data
π1:N of lengths t1:N
Output Samples c1:N , σc, ~
θc
Initialize c1:N , σc, ~
θc randomly
Repeat T times
1. Resample cluster assignments
For all points πi sample ci according to
P[ci = c] ∼
(
N−i,c
N+α−1 GM(πi | σc, ~
θc) if N−i,c 6= 0
α
N+α−1
(n−ti)!
n! if N−i,c = 0
If N−i,c = 0 for the sampled cluster, sample a new
σc | πi and ρc | πi according to Step 2 below
2. Resample cluster centers
For all clusters c, repeat TGibbs times
(a) Sample σc by Sample-σ-Stagewise
(b) Sample ~
θc by Sample-θ-Slice
Figure 1: Slice-Gibbs algorithm for estimating a
DPM of GMs.
ods [Blei and Jordan, 2006]. We focus on the for-
mer approach, where the goal is to produce sam-
ples drawn from the appropriate posterior distribu-
tion. In particular, if we are interested in parame-
ter estimation, our objective is to draw samples from
P(c1:N , σ1:C, ~
θ1:C | α, ν, r, π1:N ), where ci is the clus-
ter assignment of data point πi, and each cluster c has
GM parameters (σc, ~
θc).
While previous work [Neal, 2000] has made it straight-
forward to write the expression of this posterior (see
the following sections), our main challenge is in mak-
ing inference practical. Designing such methods and
making them efficient for nontrivial model sizes n and
sample sizes N is the main contribution of this paper.
4 THE SLICE-GIBBS SAMPLER
We first present a naı̈ve Gibbs sampler for estimating
a DPM of GMs, following the approach of Neal [2000].
Our main goal is to build a Gibbs Markov chain over
cluster assignments c1:N whose stationary distribution
is the desired model posterior. Taking advantage of
exchangeability, we can sample each point πi’s cluster
assignment ci as if it were the last point to be gen-
erated, i.e., conditioned on the assignments of other
data points. Assuming the cluster parameters (σc, ~
θc)
Algorithm Sample-σ-Stagewise
Input Parameters ~
θ, sufficient statistics R1:t, prior
parameter ν, optional prior parameters R0
1:t
Output Sample σ
1. Calculate matrix R =
Pt
j=1 θj(Rj +νR0
j ), or R =
Pt
j=1 θjRj if prior for σ is uninformative
2. For j = 1 : n and while R 6= 0
(a) Calculate column sums ρ1:n of R
(b) Sample σ−1
(j) = i w.p. ∝ e−ρi
(c) Set row and column σ−1
(j) of R to zero
3. Fill in remaining ranks of σ uniformly at random
with items not yet selected
Figure 2: Sample-σ-Stagewise algorithm for exactly
sampling σ from the conjugate posterior given ~
θ.
are known, this yields the following resampling update
for the cluster assignment of data point πi:
P(ci = c | c−i, σ, ~
θ) (11)
∝







N−i,c
N+α−1 GM(πi | σc, ~
θc)
if N−i,c 6= 0,
α
N+α−1
R
GM(πi | σ, ~
θ)P0
(σ, ~
θ | ν, r)dσd~
θ
if N−i,c = 0.
Here, N−i,c is the number of elements in cluster c,
excluding data point i. In many applications of the
DPM it is possible to integrate over cluster parameters
and explicitly sample only cluster assignments (known
as collapsed sampling). In the case of the GM, despite
our use of a conjugate prior the marginalization over
σ and ~
θ is analytically intractable, in part because of
the unknown normalization term. Thus for our first
sampler we resort to building a Markov chain over the
state space (c1:N , σC, ~
θC), where each variable is ex-
plicitly resampled conditioned on the other variables.
The algorithm is presented in Figure 1, while its steps
are discussed in detail below.
To sample ci | σ, ~
θ as in (11) we need to calculate the
probabilities on the right hand side. This is straight-
forward for N−i,c > 0, using (2). For N−i,c = 0 we use
the following Lemma (see Appendix for proofs).
Lemma 1 The marginal probability of a single obser-
vation is P(πi | ν, r) = (n−ti)!
n! .
Next we need σc | ~
θc, πi∈c. Let Rj = Rj(πi∈c) be the
sufficient statistics of cluster c, and Sj(σc) = Lσc (Rj).
These statistics are input to the algorithm described
by Lemma 2.
Algorithm Sample-θ-Slice
Input Parameters ν, t, r1:t, TSlices, statistics S1:t(σ)
Output Samples θ1:t
Initialize θ1:t according to previous sample
For j = 1 : t, repeat TSlices times
1. Sample u ∼ Uniform(0, P̃(θj)), where
P̃(θj) = e−(νrj +Sj (σ))θj −(ν+Nj) ln ψn−j (θj)
(10)
2. Determine slice [a, b] using step-out procedure
3. Repeatedly sample θj ∼ Uniform(a, b) until u <
P̃(θj), shrinking [a, b] with rejected samples
Figure 3: Sample-θ-Slice algorithm for slice sam-
pling ~
θ given σ.
Lemma 2 P(σ | ~
θ, ν, r, π1:N ) can be sampled exactly
by Algorithm Sample-σ-Stagewise (Figure 2).
Sampling from ~
θc | σc, πi∈c is more challenging. The
main obstacle to straightforward sampling is the un-
known normalization factor of this distribution. How-
ever, the posterior of each θ1:t is independent and uni-
modal.1
This suggests that slice sampling [Neal, 2003]
is a viable way of drawing values for ~
θc.
Lemma 3 P(~
θ | σ, ν, r, πi∈c) can be sampled using Al-
gorithm Sample-θ-Slice (Figure 3).
The structure of Algorithm Sample-θ-Slice follows
directly from Neal [2000]. The full Slice-Gibbs sam-
pler, so named for its inclusion of a slice sampler, is
presented in Figure 1. It alternates between resam-
pling cluster assignments ci of data points and cluster
parameters σc and ~
θc. Because the cluster parameters
themselves form a Gibbs chain, we take TGibbs steps
to ensure convergence; furthermore, the slice sampler
takes TSlices steps for each θj due to its serial correla-
tion. In our experiments we find that TGibbs = 10 and
TSlices = 3 are typically sufficient values.
5 THE BETA-GIBBS SAMPLER
The previous section has demonstrated the difficulty
of sampling from the conjugate posterior of a GM,
and how it can be overcome by using slice sampling
inside the Gibbs sampling step. We now present an
alternative approach in which several sampling steps
1
Only θ1:t needs to be sampled, as θt+1:n−1 does not
affect the rest of the sampling procedure.
Algorithm Sample-σ-N1
Input Top-t ranking π, prior parameters rj, ν
Output Sample σ
1. For j = 1 : t
(a) Sample Vj = k w.p. ∝ Beta(νrj + k, ν + 2)
for k = 0 : n − j
(b) Place π(j) at the (Vj + 1)th
previously unas-
signed position of σ
2. Fill the remaining ranks of σ uniformly at random
with items not in π
Figure 4: Sample-σ-N1 algorithm for approximately
sampling σ from the conjugate posterior when N = 1.
and marginalizations will be done in closed form. The
key insight is that the infinite generalized Mallows
model [Meilă and Bao, 2008] can be used to approxi-
mate some of the sampling distributions.
The first result arises from the fact that as n → ∞
the normalization constant ψj approaches the value
ψ∞(θ) = 1
1−eθ . This form of the normalization con-
stant permits several computations in closed form.
Lemma 4 [Meilă and Bao, 2008] If the number of
items n is infinite and countable then:
P(θj | σ, ν, r, π1:N ) =
Beta(e−θj
; νrj + Sj(σ), ν + Nj + 1), (12)
P(σ | ν, r, π1:N ) ∝
Qt
j=1 Beta(νrj + Sj(σ), ν + Nj + 1). (13)
In the above, (12) uses the Beta distribution, and (13)
uses the Beta function; Nj is the number of rankings
of length at least j and Sj(σ) is again Lσ(Rj(π1:N )).
For the finite case, we define an analogue to the Beta
function that arises in the marginalization of ~
θ:
B̃eta(a, b, n) ≡
Z ∞
0
e−θa

1 − e−(n+1)θ
1 − e−θ
−b+1
dθ.
(14)
Using this representation it can be easily verified that
for finite n,
P(σ | ν, r, π1:N )
∝
Qt
j=1 B̃eta(νrj + Sj(σ), ν + Nj + 1, n − j). (15)
Note that as n → ∞, B̃eta(a, b, n) → Beta(a, b),
which will form the core of our approximation. We
can now show the following Lemmas (see Appendix
for proofs).
Algorithm Beta-Gibbs
Input Parameters ν, α, t, r1:t, T, TGibbs, TSlices, Data
π1:N of lengths t1:N
Output Samples c1:N , σc, ~
θc
Initialize c1:N , σc, ~
θc randomly
Repeat T times
1. Resample cluster assignments
For all points πi sample ci according to
P[ci = c] ∼





N−i,c
N+α−1
Qt
j=1
Beta(sj (πi|σc)+νrj +Sj (σc),ν+Nc,j+2)
Beta(νrj +Sj (σc),ν+Nc,j +1)
if N−i,c 6= 0
α
N+α−1
(n−ti)!
n! if N−i,c = 0
If N−i,c = 0 for the sampled cluster, sample a new
σc|πi by Sample-σ-N1
2. Resample cluster centers
For all clusters c, if Nc > 1 repeat TGibbs times
(a) Sample σc | ~
θc by Sample-σ-Stagewise
(b) Sample ~
θc | σc using (12) from Lemma 4
If Nc = 1 sample σc by Sample-σ-N1
Figure 5: Beta-Gibbs algorithm for estimating a
DPM of GMs.
Lemma 5
n
X
s=0
B̃eta(s + a, N + 1, n) = B̃eta(a, N, n). (16)
This result also holds as n → ∞.
Lemma 6 Marginalizing over ~
θ for a single π yields:
P(π | σ, ν, r, πi∈c) = (17)
t
Y
j=1
B̃eta(sj(π | σ) + νrj + Sj(σ), ν + Nj + 2, n − j)
B̃eta(νrj + Sj(σ), ν + Nj + 1, n − j)
.
Lemma 7 P(σ | ν, r, π) (i.e., when N = 1) can
be sampled approximately by Algorithm Sample-σ-
N1 (Figure 4).
Lemmas 6 and 7, together with Lemmas 1 and 2, allow
us to (approximately) marginalize out the continuous ~
θ
parameters for much of the sampling. This algorithm,
called Beta-Gibbs because of the extensive use of the
Beta function, is given in Figure 5. The algorithm
approximates B̃eta(a, b, n) with the easily computable
Beta(a, b) for sampling P(~
θc | σc), P(ci | σc) when
0 50 100 150 200 250
10
−4
10
−2
10
0
iterations
VI
distance
0 50 100 150 200 250
10
−4
10
−2
10
0
iterations
VI
distance
0 50 100 150 200 250
10
−4
10
−2
10
0
iterations
VI
distance
0 50 100 150 200 250
10
−4
10
−2
10
0
iterations
VI
distance
Beta−Gibbs
Slice−Gibbs
0
0 0
0
Dataset 1
Dataset 3
Dataset 2
Dataset 4
Figure 6: Performance of Slice-Gibbs and Beta-
Gibbs on four artificial datasets, averaged over ten
replicates. Each plot displays VI distance to the true
data labeling. Lower is better.
N−i,c > 0, and P(σc | π) when Nc = 1. For the
resampling of σc for non-singleton clusters, we resort
to an inner Gibbs sampler, setting TGibbs to 10 as with
Slice-Gibbs.
6 EMPIRICAL COMPARISON OF
SLICE-GIBBS AND BETA-GIBBS
The purpose of introducing the Beta-Gibbs sampler
was (1) to make the resampling of the parameters more
efficient, and (2) more importantly, to reduce variance
and accelerate convergence to the stationary distribu-
tion, which is a typical effect of marginalizing over cer-
tain parameters. We now verify how well we succeeded
by running experiments on four artificial datasets un-
der varying conditions. For each experiment, we gen-
erate 500 points from each of 10 clusters for a total of
N = 5000 samples. Each cluster’s points are generated
from a GM with true σ∗
and ~
θ∗
given below. To ensure
that the dataset is not too easily separable, each σ∗
is
drawn from the conjugate posterior of σ, conditioned
on 100 permutations drawn randomly from a GM with
~
θ = 0.7.
Dataset n t ~
θ∗
1 20 10 θ∗
i = 1
2 20 19 θ∗
i = 1
3 20 10 θ∗
i = 1.5 − (i − 1) × 0.1
4 20 19 θ∗
i = 1.5 − (i − 1) × 0.05
We measure the Variation of Information (VI) dis-
tance [Meilă, 2007] between the sampled and true clus-
terings at each iteration. We average over ten runs for
each dataset, initializing randomly with 20 clusters.
Priors α, ν, and r1:t are all set to one.
Figure 6 shows the results of this experiment. In ev-
ery case, Beta-Gibbs converges to the true clustering
much more rapidly than Slice-Gibbs. Furthermore
n = 10 n = 20
2 4 6 8 10
−0.15
−0.1
−0.05
0
0.05
2 4 6 8 10
−20
−15
−10
−5
0
5
x 10
−3
n = 50
2 4 6 8 10
−6
−4
−2
0
2
x 10
−4
4
8
16
32
Figure 7: Relative error (1 − Beta(a, b)/B̃eta(a, b, n))
as a function of a, the equivalent number of inversions,
for various values of b, the equivalent sample size plus
two, for n = 10, 20, 50.
we note that each iteration of Slice-Gibbs is typically
slower than Beta-Gibbs, due to the additional itera-
tions of slice sampling. Interestingly, the comparison
does not change when complete rankings are observed
(datasets 2 and 4), where the Beta approximation of
B̃eta is poorest. These results support the use of the
Beta-Gibbs approximation for estimating a DPM of
GMs in the general case.
We also assessed the quality of our Beta(a, b) approx-
imations w.r.t. the correct values B̃eta(a, b, n) in Fig-
ure 7. The approximation will be more accurate for
larger n, so we consider the values n = 10, 20, 50 and
ν = 1. The relative error is largest for small b and
large a. This would occur in very small clusters, and
worsens when consensus worsens, when the parameter
prior is more diffuse (i.e., with large rj), and when j is
higher. The effect is to overestimate the θj’s for higher
ranks and the probability of assigning points to small
clusters.
7 COMPARISON TO RELATED
WORK
Modeling of multimodal ranking data has been at-
tempted in a variety of paradigms. For instance, top-
t ranked data representing votes in Irish elections has
been modeled as mixtures of Plackett-Luce and Benter
models by Gormley and Murphy [2008a] and Gormley
and Murphy [2008b], respectively. Busse et al. [2007]
developed an EM algorithm for estimating finite mix-
tures of GM models for top-t rankings.
Of the nonparametric methods, the most flexible and
theoretically principled is the Kernel Density Estima-
tor (KDE) of Lebanon and Mao [2008], in which the
kernel is the GM model with θj = θ, and where the
data is partial rankings of a given type. One of the
main algorithmic contributions of Lebanon and Mao is
the tractable evaluation of the kernel, which includes
summation over entire cosets of super-exponential car-
dinalities. Meilă and Bao [2008] introduced the expo-
nential blurring mean-shift (EBMS) clustering algo-
rithm, which also uses the GM model with all-equal
θj’s as the kernel. The algorithm features a heuris-
tic method for estimating the kernel width θ and does
not need a stopping rule, so it requires no outside pa-
rameters. Finally, Guiver and Snelson [2009] present
an example of elegant Bayesian estimation where in-
tractable inference in the Plackett-Luce model is ap-
proximated efficiently.
The most relevant comparisons to this work are with
the nonparametric approaches of EBMS [Meilă and
Bao, 2008] and KDE [Lebanon and Mao, 2008]. The
former is not a generative model, and has various para-
metric limitations: the kernel width θ is represented by
a single parameter and the output rankings are trun-
cated at some user-set standard length. The latter
model is generative, and applies to any type of par-
tial rankings, which is beyond the scope of our current
model. On the other hand, KDE has only one parame-
ter for kernel width θ, just like EBMS, and this param-
eter must be manually set. This is a limitation since
in many instances of real-world data higher ranks are
more concentrated around the mean than lower ranks.
7.1 EXPERIMENTS
We now conduct experimental comparisons of DPM,
EBMS, and KDE on both artificial and real data. For
the former, the data is generated from a fixed mixture
model with K = 3 mixture components, all having the
same single parameter θ1:t = 1.2
We set n = 12, t = 5,
and varied N from 100 to 10000. We fit each of the
three models to this data and calculate log-likelihood
on a held-out test set. For DPM and EBMS, we also
calculate the quality of the obtained clustering using
VI distance [Meilă, 2007]. To give an idea of the best
achievable performance, we use the same criteria to
evaluate the true model. For KDE the kernel width
is set to the true θ. These conditions are the most
favorable for the competing alternatives to the DPM.3
2
An experiment with K = 30 clusters produces similar
results, though then EBMS does not scale well to large n.
3
The chosen kernel width is not provably optimal for
KDE, as the optimal kernel width varies with the sample
size. However, we have tested the KDE model under a
wide range of sample sizes, and it is very likely that θ = 1
is near optimal for at least one of these.
50 150 400 1100 2980 8100 22030
−8
−7
−6
−5
−4
−3
−2
N
log−likelihood
true
DPMM
EBMS
KDE
50 150 400 1100 2980 8100 22030
−0.5
0
0.5
1
1.5
N
VI
distance
DPMM train
EBMS train
DPMM test
EBMS test
true test
Figure 8: Performance of DPM, EBMS, and KDE on a
mixture of K = 3 Mallows models, with n = 12, t = 5,
and training sample sizes N = 100, . . ., 10000, aver-
aged over 10 replicates. The test set size is 3000. Top:
test set log-likelihood, higher is better; bottom: VI dis-
tance to true data labeling, lower is better. EBMS was
too slow for the larger N’s.
The results are shown in Figure 8. One sees that
DPM, even though it has more parameters than neces-
sary to explain the data, clearly performs better than
EBMS and KDE in terms of likelihood, being almost
equal to the true model. The σ and θ estimates are also
centered on the true values (not shown). On VI dis-
tance, the heuristic EBMS performs surprisingly well
on the training data, occasionally surpassing DPM,
but produces poor clusters on the test data. The VI
of the true model shows that this is a case of well-
separated mixtures, but not a trivial one.
We run a similar comparison on the
Jester dataset [Goldberg et al., 2001], which
consists of joke preferences for a large group of people.
Of the 100 available jokes we restrict the data to the
Rank Cluster 1 (8.1%) Cluster 2 (6.2%) Cluster 3 (6.0%) Cluster 4 (5.8%) Cluster 5 (5.7%)
1 Business (Dublin) Comp Appl (Dublin) Business (Limerick) Engineering (Dublin) Comp Appl (Dublin)
2 Commerce (Dublin) Comp Sys (Limerick) Commerce (Galway) Engineering (Galway) Engineering (Dublin)
3 Business (Dublin) Software Dev (Limerick) Commerce (Cork) Civil Eng (Galway) Engineering (Dublin)
4 Marketing (Dublin) Comp Appl (Cork) Business (Waterford) Engineering (Dublin) Comp Sci (Dublin)
5 Marketing (Dublin) Appl Comp (Waterford) Humanities (Galway) Elec Eng (Limerick) Comp Sci (Dublin)
6 Humanities (Dublin) Comp Network (Carlow) Humanities (Cork) Mech Eng (Limerick) Science (Dublin)
7 Bus & Econ (Dublin) Software Dev (Cork) Admin (Limerick) Engineering (Dublin) Engineering (Dublin)
8 Bus & Legal (Dublin) Software (Athlone) Business (Dublin) Elec Eng (Galway) Comp Sci (Dublin)
9 Acct & Fin (Dublin) Comp Sci (Maynooth) Business (Dublin) Info Tech (Limerick) Computing (Dublin)
10 Acct & HR (Dublin) Info Tech (Limerick) Comp Sys (Limerick) Civil Eng (Cork) Comp Sci (Maynooth)
Business, Dublin Comp Sci, ex-Dublin Business, ex-Dublin Engineering Comp Sci, Dublin
Table 1: Top courses of the five largest clusters found in a representative run of the DPM over college admissions
data. Proportions of the data assigned to each cluster is shown next to the cluster number. We list course names
and school locations, and summarize the theme of each cluster.
50 150 400 1100 2980 8100
−16
−15
−14
−13
−12
−11
−10
−9
−8
N
log−likelihood
DPMM
KDE0.03
KDE0.1
KDE0.4
KDE1
Figure 9: Test set log-likelihood of DPM and KDE on
the Jester data, with n = 70, t = 5 and training
sample sizes N = 100, 1000, 3000, averaged over 10
replicates. Higher is better. The test set size is 3000.
Further reducing the kernel width for KDE leads to
results almost identical to the width 0.03 case.
n = 70 most frequently rated, and to the top t = 5
rankings. As Figure 9 shows, we again observe that
DPM outperforms KDE over all kernel widths tried.
DPM finds between 4 and 9 clusters in 10 trials, with
θj in the range (0.03, 0.06) for all j’s and N’s.
8 ANALYSIS OF COLLEGE
COURSE RANKINGS
We also conduct an analysis of Irish third-level col-
lege applications, where prospective students rank up
to ten preferred academic courses across a number of
schools [Gormley and Murphy, 2006]. In combination
with examination scores, this data is used by the Cen-
tral Applications Office (http://www.cao.ie/) to de-
termine placements into third-level degree programs.
The dataset consists of N = 53757 students in the year
2000 selecting from n = 533 courses and ranking up
to t = 10 of them. To facilitate a comparison with
previous work [Gormley and Murphy, 2006], we set α
and ν to 100 so as to induce a finer-grained cluster-
0 20 40 60
10
0
10
1
10
2
10
3
10
4
cluster
size
Figure 10: Cluster size distribution for a representative
run of the college applications data, N = 53757. Out
of 177 clusters, 114 were singletons, 35 had at least 54
points (0.1%), and 27 had at least 538 points (1%).
ing, running four samplers to 500 iterations each. The
four runs yield between 23 and 27 substantial clusters
(those with at least 1% of the data points), with similar
central permutations recurring across runs. Figure 10
illustrates the sizes of the induced clusters.
Table 1 displays the top-10 courses from the central
permutations of the largest clusters of a representa-
tive run. The results show clear thematic consistency
in the top ranked courses by vocation and/or loca-
tion, concordant in particular with Gormley and Mur-
phy’s observation of the “frequent distinction between
sets of applicants who apply for degrees of a simi-
lar discipline but are deemed separate on the basis
of whether or not the institutions to which they ap-
ply are in Dublin.” Notably, their analysis revealed
distinct clusters of computer science preferences, one
for Dublin-based schools and one with regional varia-
tion. We additionally find a clear separation between
Dublin-based business programs and outside business
programs, a phenomenon that was observed by Gorm-
ley and Murphy but not explicitly identified by their
clustering.
We can also interpret the posterior samples of ~
θ to
gain insight into data separation by rank, which is an
1 2 3 4 5 6 7 8 9 10
0
0.5
1
1.5
2
2.5
3
rank
θ
j
All Clusters
Small Clusters
Large Clusters
Figure 11: Average of θj weighted by cluster size as
a function of rank j for the college application data,
replicated across four runs. The decreasing trend sup-
ports the intuition that top course preferences are bet-
ter separated than less-desired choices.
advantage of using the GM for modeling clusters com-
pared to the Plackett-Luce model. We compute an
average of each θj, weighted by cluster size, across the
four runs. We also perform this analysis for only large
and only small clusters, thresholding at a size of 5%
of the data points (splitting the data points roughly
equally into large and small). Figure 11 presents these
averages. The clearly decreasing overall trend rein-
forces the intuition that top-ranked choices tend to be
more coherent and distinctive than later entries in the
top-10 ranking. Furthermore, we find that small clus-
ters tend to diverge less at top ranks than large clus-
ters, but this trend reverses around the fourth rank. A
qualitative examination of the data suggests that this
may be because small clusters tend to correspond to
more specialized interests with fewer relevant courses
(e.g., courses at one specific smaller school), leaving
fewer choices for the top ranks but allowing for greater
divergence later on.4
9 DISCUSSION
We introduced nonparametric Bayesian DPMs on
ranked data domains with top-t rankings of variable
lengths. Our inference algorithms are able to run on
substantial dataset sizes and large n’s.
We leveraged a combination of statistical and compu-
tational insights in developing our techniques. Statisti-
4
In fact, the average ranking length t for data points
in small clusters is shorter than for large clusters: 6.15
compared to 6.63.
cally, the rich combinatorial structure of the parameter
space allowed us to perform explicit marginalizations
and normalizations in special cases. Computationally,
we exploited the special structure of the Rj sufficient
statistics and of the Lσ operator, thereby eliminating
n from the most intensive computations. While the
faster Beta-Gibbs algorithm uses approximate pos-
teriors, we have verified empirically the quality of that
approximation and the advantages it yields to conver-
gence.
Our algorithm works with informative priors as well,
with only a minor modification (replacing Sample-σ-
N1). One avenue of future work is to explore other
sampling schemes than the one described by Neal
[2000], such as split-merge algorithms [Jain and Neal,
2007].
Acknowledgments
Harr Chen is supported by a National Science Foun-
dation Graduate Fellowship.
Appendix
Proof of Lemma 1 The marginal of a single π of
length t is
Z1(π) =
X
σ
Z ∞
0
GMs
(πi; σ, ~
θ)P0
(σ, ~
θ; ν, r)d~
θ.
(18)
By Lemma 6, the integral is equal to
1
n!
t
Y
j=1
B̃eta(νrj + sj(π | σ), ν + 2, n − j)
B̃eta(νrj, ν + 1, n − j)
. (19)
Note that sj(π | σ) = Vj(σ | π), where Vj(σ | π)
should be read as “the rank in σ of item j of π” and
is therefore well defined for j = 1 : t.
Any configuration of Vj’s uniquely determines a subset
of the positions in σ, and the Vj’s can take any value in
their admissible range when σ ranges over all infinite
permutations. Thus, sj(π | σ) ranges from 0 to n −
j, and consequently the summation over σ commutes
with the product over j. For every configuration of
s1:t, there will be (n − t)! different permutations with
that configuration. It follows that:
X
σ
t
Y
j=1
B̃eta(νrj + sj(π | σ), ν + 2, n − j)
= (n − t)!
t
Y
j=1
n−j
X
sj =0
B̃eta(νrj + sj, ν + 2, n − j)
= (n − t)!
t
Y
j=1
B̃eta(νrj, ν + 1, n − j).
The last equality is obtained from Lemma 5. Hence,
Z1(π) = (n − t)!/n!.
Proof of Lemma 2 From Meilă and Bao [2008], for
any given θ,
P(σ | θ, π1:N , ν, r)
∝ e−
Pt
j=1[θj(Lσ(Rj (π1:N ))+νrj )+(N+ν) ln ψn−j (θj)]
∝ e−
Pt
j=1 θj Lσ(Rj (π1:n))
= e−Lσ(R)
.
We use a key observation of Meilă et al. [2007], which
is that for a distribution over permutations like the one
above, the first rank of σ is distributed proportionally
to the column sums of R, the second rank is distributed
proportionally to the column sums of R after deleting
row and column σ−1
(1), etc. Hence, the ranks of σ
can be sampled sequentially by
P(σ−1
(1) = k) ∝ e−
P
i Rik
, (20)
. . .
P(σ−1
(j) = k) ∝ e−
P
i6∈σ−1(1:j−1) Rik
. (21)
Proof of Lemma 3 This follows from Neal [2003].
Proof of Lemma 5
n
X
s=0
B̃eta(s + a, b, n)
=
Z ∞
0
n
X
s=0
e−θ(s+a)

1 − e−(n+1)θj
1 − e−θj
−b+1
dθ
=
Z ∞
0
e−θa 1 − e−(n+1)θj
1 − e−θj

1 − e−(n+1)θj
1 − e−θj
−b+1
dθ
= B̃eta(a, b − 1, n).
Proof of Lemma 6 This follows by direct calculus.
Proof of Lemma 7 The crucial observation here is
the same as in Lemma 1: since N = 1, Lσ(Rj(π)) =
sj(π | σ) = Vj(σ | π) by (1). As a consequence, the
posterior of σ is a product of multinomials, one for
each j = 1 : t:
P[Vj = v] ∝ B̃eta(νrj + v, ν + 2, n − j). (22)
We approximate B̃eta(a, b, n) by Beta(a, b). After Vj
is sampled, to construct σ one places π−1
(j) in the
V th
j available position in σ. (See Meilă et al. [2007] for
the detailed proof of this procedure.) The remaining
n−t positions are filled uniformly at random from the
items not in π.
References
C. E. Antoniak. Mixtures of Dirichlet processes with appli-
cations to Bayesian nonparametric problems. Ann Stat,
2(6):1152–1174, 1974.
D. M. Blei and M. I. Jordan. Variational inference for
Dirichlet process mixtures. Bayes Anal, 1(1):121–144,
2006.
L. M. Busse, P. Orbanz, and J. Bühmann. Cluster analysis
of heterogeneous rank data. In Proceedings of ICML,
2007.
M. A. Fligner and J. S. Verducci. Distance based ranking
models. J Roy Stat Soc B Met, 48(3):359–369, 1986.
M. A. Fligner and J. S. Verducci. Multistage ranking mod-
els. J Am Stat Assoc, 83(403):892–901, 1988.
K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigen-
taste: A constant time collaborative filtering algorithm.
Inform Retrieval, 4(2):133–151, 2001.
I. C. Gormley and T. B. Murphy. Analysis of Irish third-
level college applications data. J Roy Stat Soc A Sta,
169(2):361–379, 2006.
I. C. Gormley and T. B. Murphy. Exploring voting blocs
within the Irish electorate: a mixture modeling ap-
proach. J Am Stat Assoc, 103(483):1014–1027, 2008a.
I. C. Gormley and T. B. Murphy. A mixture of experts
model for rank data with applications in election studies.
Ann Appl Stat, 2(4):1452–1477, 2008b.
J. Guiver and E. Snelson. Bayesian estimation for Plackett-
Luce ranking models. In Proceedings of ICML, 2009.
S. Jain and R. M. Neal. Splitting and merging compo-
nents of a nonconjugate Dirichlet process mixture model.
Bayes Anal, 2(3):445–472, 2007.
G. Lebanon and Y. Mao. Non-parametric modeling of par-
tially ranked data. J Mach Learn Res, 9:2401–2429,
2008.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. The in-
finite PCFG using hierarchical Dirichlet processes. In
Proceedings of EMNLP, 2007.
M. Meilă. Comparing clusterings—an information based
distance. J Multivariate Anal, 98:873–895, 2007.
M. Meilă and L. Bao. Estimation and clustering with infi-
nite rankings. In Proceedings of UAI, 2008.
M. Meilă, K. Phadnis, A. Patterson, and J. Bilmes. Con-
sensus ranking under the exponential model. In Proceed-
ings of UAI, 2007.
R. M. Neal. Markov chain sampling methods for Dirichlet
process mixture models. J Comput Graph Stat, 9(2):
249–265, 2000.
R. M. Neal. Slice sampling. Ann Stat, 31(3):705–767, 2003.
C. E. Rasmussen, B. J. de la Cruz, Z. Ghahramani, and
D. L. Wild. Modeling and visualizing uncertainty in
gene expression clusters using Dirichlet process mix-
tures. IEEE/ACM T Comput Bi, 6(4):615–628, 2009.
E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S.
Willsky. Describing visual scenes using transformed
dirichlet processes. In Advances in NIPS, 2005.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
Hierarchical Dirichlet processes. J Am Stat Assoc, 101
(476):1566–1581, 2006.
