TakingaHINT:LeveragingExplanationstoMakeVisionandLanguageModelsMoreGroundedRamprasaathR.Selvaraju1StefanLee1,4YilinShen2HongxiaJin2ShaliniGhosh2LarryHeck2DhruvBatra1,3DeviParikh1,31GeorgiaInstituteofTechnology,2SamsungResearch,3FacebookAIResearch,4OregonStateUniversity{ramprs,steflee,dbatra,parikh}@gatech.edu{yilin.shen,hongxia.jin,shalini.ghosh,larry.h}@samsung.comAbstractManyvisionandlanguagemodelssufferfrompoorvi-sualgrounding–oftenfallingbackoneasy-to-learnlan-guagepriorsratherthanbasingtheirdecisionsonvisualconceptsintheimage.Inthiswork,weproposeagenericapproachcalledHumanImportance-awareNetworkTuning(HINT)thateffectivelyleverageshumandemonstrationstoimprovevisualgrounding.HINTencouragesdeepnetworkstobesensitivetothesameinputregionsashumans.Ourapproachoptimizesthealignmentbetweenhumanattentionmapsandgradient-basednetworkimportances–ensuringthatmodelslearnnotjusttolookatbutratherrelyonvi-sualconceptsthathumansfoundrelevantforataskwhenmakingpredictions.WeapplyHINTtoVisualQuestionAn-sweringandImageCaptioningtasks,outperformingtopap-proachesonsplitsthatpenalizeover-relianceonlanguagepriors(VQA-CPandrobustcaptioning)usinghumanatten-tiondemonstrationsforjust6%ofthetrainingdata.1.IntroductionManypopularandwell-performingmodelsformulti-modal,vision-and-languagetasksexhibitpoorvisualgrounding–failingtoappropriatelyassociatewordsorphraseswiththeimageregionstheydenoteandrelyingin-steadonsuperficiallinguisticcorrelations[2,1,36,10,12].Forexample,answeringthequestion‘Whatcolorarethebananas?’withyellowregardlessoftheirripenessevidentintheimage.Whenchallengedwithdatasetsthatpenal-izerelianceonthesesortofbiases[2,10],state-of-the-artmodelsdemonstratesignificantdropsinperformancede-spitetherebeingnochangetothesetofvisualandlinguisticconceptsaboutwhichmodelsmustreason.Inadditiontothesediagnosticdatasets,anotherpow-erfulclassoftoolsforobservingthisshortcominghasbeengradient-basedexplanationtechniques[27,35,26,24]whichallowresearcherstoexaminewhichportionsoftheFigure1:Ourapproach,HINT,alignsvisualexplanationsforoutputdecisionsofapretrainedmodelwithspatialinputregionsdeemedimportantbyhumanannotators–forcingmodelstobasetheirdecisionsonthesesameregionandreducingmodelbias.inputmodelsrelyonwhenmakingdecisions.Applica-tionofthesetechniqueshasshownthatvision-and-languagemodelsoftenfocusonseeminglyirrelevantimageregionsthatdiffersignificantlyfromwherehumansubjectsfixatewhenaskedtoperformthesametasks[7,24]–e.g.focusingonaproducestandratherthanthebananasinourexample.Whilesomewhatdissatisfying,thesefindingsarenoten-tirelysurprising–afterall,standardtrainingprotocolsdonotprovideanyguidanceforvisualgrounding.Instead,modelsaretrainedoninput-outputpairsandmustresolvegroundingfromco-occurrences–achallengingtask,espe-ciallyinthepresenceofmoredirectandeasiertolearncor-relationsinlanguage.Considerourpreviousexampleques-tion,thewords‘color’,‘banana’,and‘yellow’aregivenasdiscretetokensthatwilltriviallymatchineveryoccurrencewhentheseunderlyingconceptsarereferenced.Incontrast,actuallygroundingthisquestionrequiresdealingwithallvi-sualvariationsofbananasandlearningthecommonfeatureofthingsdescribedas‘yellow’.Toaddressthis,weexploreifgivingasmallhintintheformofhumanattentiondemon-strationscanhelpimprovegroundingandreliability.Forthedominantparadigmofvision-and-languagemod-elsthatcomputeanexplicitquestion-guidedattentionoverimageregions[25,30,13,32,19,3],aseeminglystraight-1arXiv:1902.03751v2forwardsolutionistoprovideexplicitgroundingsupervi-sion–trainingmodelstoattendtotheappropriateimageregions.Whilepriorwork[21,16]hasshownthisapproachresultsinmorehuman-likeattentionmaps,ourexperimentsshowittobeineffectiveatreducinglanguagebias.Cru-cially,attentionmechanismsarebottom-upprocessesthatfeedfinalclassificationmodelssuchthatevenwhenattend-ingtoappropriateregions,modelscanignorevisualcon-tentinfavoroflanguagebias.Inresponse,weintroduceageneric,second-orderapproachthatinsteadalignsgradient-basedexplanationswithhumanattention.Ourapproach,whichwecallHumanImportance-awareNetworkTuning(HINT),enforcesarankinglossbetweenhumanannotationsofinputimportanceandgradient-basedexplanationsproducedbyadeepnetwork–updatingmodelparametersviaagradient-of-gradientstep.Importantly,thisconstrainsmodelstonotonlylookatthecorrectregionsbuttoalsobesensitivetothecontentpresenttherewhenmakingpredictions.WhileweexperimentwithHINTinthecontextofvision-and-languageproblems,theapproachitselfisgeneralandcanbeappliedtofocusmodeldecisionsonspecificinputsinanycontext.WeapplyHINTtotwotasks–VisualQuestionAnswer-ing(VQA)[5]andimagecaptioning[14]–andfindourap-proachsignificantlyimprovesvisualgrounding.Withhu-manimportancesupervisionforonly6%ofthetrainingset,ourHINT’edmodelimprovesthestate-of-the-artby8percentagepointsonthechallengingdatasetVQAUnderChangingPriors(VQA-CP)[2],whichisdesignedtotestvisualgrounding.InbothVQAandImageCaptioning,weseesignificantlyimprovedcorrelationsbetweenhumanat-tentionandvisualexplanationsforHINTtrainedmodels,showingthatmodelslearntomakedecisionsusingsimilarevidenceashumans(evenonnewimages).WeperformhumanstudieswhichshowthathumansperceivemodelstrainedusingHINTtobemorereasonableandtrustworthy.Contributions.Tosummarizeourcontributions,we•introduceHumanImportance-awareNetworkTuning(HINT),ageneralapproachforconstrainingthesensitiv-ityofdeepnetworkstospecificinputregionsanddemon-strateitresultsinsignificantlyimprovedvisualground-ingfortwovisionandlanguagetasks,•setanewstate-of-the-artonthebias-sensitiveVQAUn-derChangingPriors(VQA-CP)dataset[2],and•conductstudiesshowingthathumansfindHINTedmod-elsmoretrustworthythanstandardmodels.2.RelatedWorkModelInterpretability.Therehasbeensignificantre-centinterestinbuildingmachinelearningmodelsthataretransparentandinterpretableintheirdecisionmakingpro-cess.Fordeepnetworks,severalworksproposeexplana-tionsbasedoninternalstatesofthenetwork[34,11,37,24].MostrelatedtoourworkistheapproachofSelvarajuetal.[24]whichcomputesneuronimportanceaspartofavisualexplanation.Inthiswork,weenforcethattheseimportancescoresalignwithimportancesprovidedbydomainexperts.VisionandLanguageTasks.ImageCaptioning[15]andVisualQuestionAnswering(VQA)[5]haveemergedastwoofthemostwidelystudiedvision-and-languageproblems.Theimagecaptioningtaskrequiresgeneratingnaturallan-guagedescriptionsofimagecontentsandtheVQAtaskrequiresansweringfree-fromquestionsaboutimages.Inboth,modelsmustlearntoassociateimagecontentwithnat-uralfree-formtext.Consequentially,attentionbasedmod-elsthatexplicitlyreasonaboutimage-textcorrespondenceshavebecomethedominantparadigm[25,30,13,32,19,3];however,thereisgrowingevidencethateventheseatten-tionalmodelsstilllatchontolanguagebiases[2,36,4].Recently,Agrawaletal.[2]introducedanovel,bias-sensitivedatasetsplitfortheVQAtask.Thissplit,calledVQAUnderChangingPriors(VQA-CP),isconstructedsuchthattheanswerdistributionsdiffersignificantlybe-tweentrainingandtest.Assuch,modelsthatmemorizelanguageassociationsintraininginsteadofactuallyground-ingtheiranswersinimagecontentwillperformpoorlyonthetestset.LikewiseLuetal.[19]introducearobustcap-tioningsplitoftheCOCOcaptioningdataset[15]inwhichthedistributionofco-occurringobjectsdifferssignificantlybetweentrainingandtest.Weusethesedatasetsplitstoevaluatetheimpactofourmethodonvisualgrounding.DebiasingVisionandLanguageModels.Anumberofrecentworkshaveaimedtoreducetheeffectoflanguagebiasinvisionandlanguagemodels.Hendricksetal.[4]studythegenerationofgender-specificwordsinimagecaptioning–showingthatmod-elsHumanAttentionforVQA.Dasetal.[7]collectedhu-manattentionmapsforasubsetoftheVQAdataset[5].Givenaquestionandablurryimage,humanswereaskedtointeractivelydeblurregionsintheimageuntiltheycouldconfidentlyanswer.Inthiswork,weutilizethesemaps,enforcingthegradient-basedvisualexplanationsofmodeldecisionstocloselymatchthehumanattention.Supervisingmodelattention.Liuetal.[16]andQiaoetal.[21]applyhumanattentionsupervisiontoFigure2:OurHumanImportance-awareNetworkTuning(HINT)approach:Givenanimageandaquestionlike“Didhehittheball?”,wepassthemthroughtheBottom-upTop-downarchitectureshownintheleft.FortheNotethatnetworkimportancesαaregradientsofthescorewithrespecttoproposalembeddings.ThustheyareafunctionofalltheintermediateparametersofthenetworkrangingfromthemodelattentionlayerweightsIsthistherightsizedskateboardforhim?GT:YesGrad-CAMfor‘Yes’Grad-CAMfor‘Yes’Pred:NoPred:YesBeforeHINTAfterHINT(a)Whatcolorarethesigns?GT:RedandWhiteBeforeHINTAfterHINTGrad-CAMfor‘RedandWhite’Pred:RedGrad-CAMfor‘RedandWhite’Pred:RedandWhite(b)Isthisbabysuckingonapacifier?GT:YesGrad-CAMfor‘Yes’Grad-CAMfor‘Yes’Pred:NoPred:Yes(c)Pred:NoPred:YesIsthisatouristfriendlyarea?GT:YesGrad-CAMfor‘Yes’Grad-CAMfor‘Yes’Pred:NoPred:Yes(d)Figure3:QualitativecomparisonofmodelsonvalidationsetbeforeandafterapplyingHINT.Foreachexample,theleftcolumnshowstheinputimagealongwiththequestionandtheground-truth(GT)answerfromtheVQA-CPvalsplit.Inthemiddlecolumn,forthebasemodelweshowtheexplanationvisualizationfortheGTansweralongwiththemodel’sanswer.SimilarlyweshowtheexplanationsandpredictedanswerfortheHINTedmodelsinthethirdcolumn.WeseethattheHINTedmodellooksatmoreappropriateregionsandanswersmoreaccurately.Forexample,fortheexamplein(a),thebasemodelonlylooksattheboy,andafterweapplyHINT,itlooksatboththeboyandtheskateboardinordertoanswer‘Yes’.AfterapplyingHINT,themodelalsochangesitsanswerfrom‘No’to‘Yes’.Morequalitativeexamplescanbefoundinthesupplementarymaterial.ModelVQA-CPtestVQAv2valOverallYes/NoNumberOtherOverallYes/NoNumberOtherSAN[32]24.9638.3511.1421.7452.4170.0639.2847.84UpDn[3]39.4945.2111.:9642.9862.8580.8942.7854.44GVQA[2]†31.3057.9913.6822.1448.2472.0331.1734.65UpDn+Attn.Align39.3743.0211.8945.0063.2480.9942.5555.22UpDn+AdvReg[22]†41.1765.4915.4835.4862.7579.8442.3555.16UpDn+HINT(ours)46.7367.2710.6145.8863.3881.1842.9955.56Table1:Resultsoncompositional(VQA-CP)andstandardsplit(VQAv2).Weseethatourapproach(HINT)getsasignificantboostofover7%fromthebaseUpDnmodelonVQA-CPandminorgainsonVQAv2.TheAttn.AlignbaselineseessimilargainsonVQAv2,butfailstoimprovegroundingonVQA-CP.NotethatforVQAv2,duringHINTfinetuningweapplytheVQAcrossentropylossevenforthesampleswithouthumanattentionannotation.†resultstakenfromcorrespondingpapers.importancewithrespecttomodeldecisions,ensuringthathumanspecifiedregionsareactuallyusedbythenetwork–resultinginamodelthatisrightfortherightreasons.Varyingtheamountofhumanattentionsupervision.TheplottotherightshowsperformancefordifferentamountsofHumanAttentionmapsforVQA-CP.Notethatthex-axisgoesfromusingnoHINTsupervisiontousingalltheHumanattentionmapsduringtraining,whichamountsto6%oftheVQAv2data.Notethatwithhumanattentionsupervisionforjust1.5%oftheVQAdataset,ourapproachachievesa5%improvementinperformance.Qualitativeexamples.Fig.6showsqualitativeexamplesshowingtheeffectofapplyingHINTtotheBottom-upTop-downVQAmodel.Fig.6(b)showsanimageandaques-tion,‘Whatcolorarethesigns?’,thebasemodelanswers“Red”whichispartiallycorrect,butitfailstogroundthean-swercorrectly.TheHINTedmodelnotonlyanswers“RedandWhite”correctlybutalsolooksattheredstopsignandthewhitestreetsign.5.2.HINTforImageCaptioningOurimplementationoftheBottom-upTop-downcap-tioningmodelinPytorch[20]achievesaCIDEr[28]scoreof1.06onthestandardsplitand0.90ontherobustsplit.UponapplyingHINTtothebasemodeltrainedonthero-bustsplit,weobtainaCIDErscoreof0.92,animprovementof0.02overthebasemodel.Forthemodeltrainedonthestandardsplit,performancedropsAcloseupofaforkanorangeanappleandanonionappleorangeBeforeHINTAfterHINTforkforkappleorange(a)AsmalldoglayingonabednexttoalaptopcomputerdogdogbedbedlaptoplaptopBeforeHINTAfterHINT(b)(c)(d)Figure4:QualitativecomparisonofcaptioningmodelsonvalidationsetbeforeandafterapplyingHINT.Foreachexample,theleftcolumnshowstheinputimagealongwiththeground-truthcaptionfromtheCOCOrobustsplit.Inthemiddlecolumn,forthebasemodelweshowtheexplanationvisualizationforthevisualwordmentionedbelow.SimilarlyweshowtheexplanationsfortheHINTedmodelsinthethirdcolumn.WeseethattheHINTedmodellooksatmoreappropriateregions.Forexamplein(a)notehowtheHINTedmodelcorrectlylocalizesthefork,appleandtheorangewhengeneratingthecorrespondingvisualwords,butthebasemodelfailstodoso.Interestinglythemodelisabletogroundeventheshadowofacatin(f)!InsidebathroomwithalargeclockfaceonthemirrorWhengeneratingtheword:ClockClockClockFigure5:AMTinterfaceforevaluatingthebaselinecaptioningmodelandourHINTedmodel.HINTedmodeloutperformsbase-AppendicesA.QualitativeexamplesInFig.6weshowexamplesapplyingHINTfortheBottom-upTop-downVQAmodel.Theleftcolumnshowstheinputimagealongwiththequestionandtheground-truth(GT)answerfromtheVQA-CPvalsplit.Inthemiddlecolumn,forthebasemodelweshowtheexplanationvisualizationfortheGTansweralongwiththemodel’sanswer.Similarlyweshowtheexplanationsandpre-dictedanswerfortheHINTedmodelsinthethirdcolumn.WeseethattheHINTedmodelnotonlylooksatmoreappropriateregionscomparedtothebasemodels.Fig.6(a)showsanimageandaquestion,“Isthepersonscream-ing”.Notonlydoesthebasemodelanswer“no”incorrectly,butitalsocannotlocalizetherightanswer–looksjustatthebearfor“yes”.TheHINTedmodelanswers“yes”correctlyandlooksatboththebearandthefaceoftheperson.Fortheimage6(b)withquestion“Doesthebuildinghaveaclockonit?”,thebasemodelincorrectlyanswersno,whereastheHINTedmodelnotonlyas-nwers‘yes’correctly,italsolocalizestheclockonthebuilding.ThebottomrowshowstwoexampleswhereHINThelpswithlo-calizingtherightanswer,althoughtheanswersfromboththemod-els(basemodelandHINTedmodel)areincorrect.InFig.7weshowqualitativeexamplesshowingtheeffectofapplyingHINTontheTop-downBottom-up[3]captioningmodeltrainedontheRobustsplitoftheCOCOdataset.Theleftcolumnshowstheinputimagealongwiththeground-truthcaptionfromtheCOCOrobustsplit.Inthemiddlecolumn,forthebasemodelweshowtheexplanationvisualizationforthevisualwordmen-tionedbelow.SimilarlyweshowtheexplanationsfortheHINTedmodelsinthethirdcolumn.WeseethattheHINTedmodellooksatmoreappropriateregionswhengeneratingthementionedvisualword(belowthevisualization).Forexample,fortheinputimageinFig.7(a)and(b),thebasemodelonlyplacesalittleimportanceonthefacewhilegeneratingtheword‘guy’,whereastheHINTedmodelcorrectlylooksatthefaceoftheperson.Similarlywhengenerating‘ties’theHINTedmodellooksatthewholetieregioncomparedtothebasemodel.Similarlyfortheimagesin(e)and(f),theHINTedmodellooksmorecorrectlyatthespoonsforthevisualword‘spoon’andsink,fortheword‘sink’.References[1]AishwaryaAgrawal,DhruvBatra,andDeviParikh.Ana-lyzingthebehaviorofvisualquestionansweringmodels.InEMNLP,2016.1[2]AishwaryaAgrawal,DhruvBatra,DeviParikh,andAnirud-dhaKembhavi.Dontjustassume;lookandanswer:Over-comingpriorsforvisualquestionanswering.InIEEECon-ferenceonComputerVisionandPatternRecognition,2018.1,2,5,6[3]PeterAnderson,XiaodongHe,ChrisBuehler,DamienTeney,MarkJohnson,StephenGould,andLeiZhang.Bottom-upandtop-downattentionforimagecaptioningandvisualquestionanswering.InCVPR,2018.1,2,3,6,9[4]LisaAnneHendricks,KayleeBurns,KateSaenko,TrevorDarrell,andAnnaRohrbach.Womenalsosnowboard:Over-comingbiasincaptioningmodels.2018.2[5]StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZitnick,andDeviParikh.VQA:VisualQuestionAnswering.2015.2,3[6]KyunghyunCho,BartVanMerriënboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,HolgerSchwenk,andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation.arXivpreprintarXiv:1406.1078,2014.3[7]AbhishekDas,HarshAgrawal,C.LawrenceZitnick,DeviParikh,andDhruvBatra.HumanAttentioninVisualQues-tionAnswering:DoHumansandDeepNetworksLookattheSameRegions?2016.1,3,8[8]HaoFang,SaurabhGupta,ForrestIandola,RupeshKSri-vastava,LiDeng,PiotrDollár,JianfengGao,XiaodongHe,MargaretMitchell,JohnCPlatt,etal.FromCaptionstoVi-sualConceptsandBack.2015.3[9]RossGirshick.Fastr-cnn.InProceedingsoftheIEEEinter-nationalconferenceoncomputervision,pages1440–1448,2015.3[10]YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBa-tra,andDeviParikh.Makingthevinvqamatter:Elevatingtheroleofimageunderstandinginvisualquestionanswer-ing.InCVPR,2017.1,5[11]YashGoyal,AkritMohapatra,DeviParikh,andDhruvBa-tra.Interpretingvisualquestionansweringmodels.CoRR,abs/1608.08974,2016.2[12]JustinJohnson,BharathHariharan,LaurensvanderMaaten,LiFei-Fei,CLawrenceZitnick,andRossGirshick.Clevr:Adiagnosticdatasetforcompositionallanguageandelemen-taryvisualreasoning.2017.1[13]VahidKazemiandAliElqursh.Show,ask,attend,andanswer:Astrongbaselineforvisualquestionanswering.CoRR,abs/1704.03162,2017.1,2[14]T.Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-manan,P.Dollár,andC.L.Zitnick.MicrosoftCOCO:Com-monObjectsinContext.2014.2,4[15]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,andCLawrenceZitnick.Microsoftcoco:Commonobjectsincontext.InECCV.2014.2[16]ChenxiLiu,JunhuaMao,FeiSha,andAlanLYuille.Atten-tioncorrectnessinneuralimagecaptioning.InAAAI,2017.2,3[17]JiasenLu,CaimingXiong,DeviParikh,andRichardSocher.Knowingwhentolook:Adaptiveattentionviaavisualsen-tinelforimagecaptioning.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),volume6,page2,2017.3[18]JiasenLu,JianweiYang,DhruvBatra,andDeviParikh.Hierarchicalquestion-image(a)(b)(c)(d)(e)(f)(g)(h)(i)(j)(k)(l)(m)(n)Figure6:QualitativecomparisonofmodelsbeforeandafterapplyingHINT.Theleftcolumnshowstheinputimagealongwiththe(a)(b)(c)(d)(e)(f)(g)(h)(i)(j)(k)(l)(m)(n)Figure7:QualitativecomparisonofTop-downBottom-upcaptioningmodelbeforeandafterapplyingHINT.Theleftcolumnshowstheinputimageing.InAAAI,2018.2,3[22]SainandanRamakrishnan,AishwaryaAgrawal,andStefanLee.Overcominglanguagepriorsinvisualquestionanswer-ingwithadversarialregularization.InNeuralInformationProcessingSystems(NIPS),2018.2,5,6