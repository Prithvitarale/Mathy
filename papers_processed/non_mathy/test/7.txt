MultimodalExplanations:JustifyingDecisionsandPointingtotheEvidenceDongHukPark1,LisaAnneHendricks1,ZeynepAkata2,3,AnnaRohrbach1,3,BerntSchiele3,TrevorDarrell1,nototheritemsintheimage,suchasthebread.WeproposeVQAandactivityrecognitionastestbedsforstudyingexplanationsbecausetheyarechallengingandim-portantvisualtaskswhichhaveinterestingpropertiesforDatasetSplit#Imgs#Q/APairs#UniqueQ.#UniqueA.#Expl.(Avg.#w)Expl.VocabSize#Comple.Pairs#VisualAnn.VQA-XTrain248762945912942114731536(8.56)124126050–Val143114598132464377Q:Whatisthepersondoing?A:SkiingQ:Whatistheboydoing?A:SkateboardingQ:Whatgamearetheyplaying?A:Baseball(a)VQA-XActivity:MowingLawnActivity:Planting,PottingActivity:Bicycling,Mountain(b)ACT-XFigure3:Humanannotatedvisualexplanations.Ontheleft:exampleannotationscollectedonVQA-Xdataset.Ontheright:ExampleannotationscollectedonACT-Xdataset.Thevisualevidencethatjustifiestheanswerissegmentedinyellow.VQA-HATVQA-XQ:Whatisthepersonholding?A:SkiPolesVQA-HATVQA-XQ:Aretherestreetlights?A:YesFigure4:HumanvisualannotationsfromVQA-HATandVQA-X.Weaggregatealltheannotationsineachimageandnormalizethemtocreateaprobabilitydistribution.Thedistributionisthenvisualizedovertheimageasaheatmap.classlabelforACT-X).Theyareaskedtosegmentobjectsand/orregionsthatmostprominentlyjustifytheanswer.Foreachdatasetwerandomlysampleimagesfromthetestsplit,andforeachimagewecollect3annotations.Someexam-plescanbeseeninFigure3.ComparingwithVQA-HAT.Athoroughcomparisonbe-tweenourdatasetandVQA-HATdatasetfrom[11]iscur-rentlynotviablebecausethetwodatasetshavedifferentsplitsandtheoverlapissmall.However,wepresentquali-tativecomparisoninFigure4.Inthefirstrow,ourVQA-XannotationhasafinergranularitysinceitsegmentsouttheobjectsininterestmoreaccuratelythantheVQA-HATan-notation.Inthesecondrow,ourannotationcontainslessex-traneousinformationthantheVQA-HATannotation.SincetheVQA-HATannotationsarecollectedbyhavinghumans“unblur”theimages,theyaremorelikelytointroducenoisewhenirrelevantregionsareuncovered.4.PointingandJustificationModel(PJ-X)Thegoalofourworkistoimplementamultimodalex-planationsystemthatjustifiesadecisionwithnaturallan-guageandpointstotheevidence.WedeliberatelydesignourPointingandJustificationModel(PJ-X)toallowtrain-ingthesetwotasks.Specificallywewanttorelyonnat-urallanguagejustificationsandtheclassificationlabelsastheonlysupervision.Wedesignthemodeltolearnhowtopointinalatentway.Forthepointingwerelyonanatten-tionmechanism[4]whichallowsthemodeltofocusonaspatialsubsetofthevisualrepresentation.Wefirstpredicttheanswergivenanimageandquestionusingtheansweringmodel.Thengiventheanswer,ques-tion,andimage,wegeneratevisualandtextualexplanationswiththemultimodalexplanationmodel.AnoverviewofourmodelispresentedinFigure5.Answeringmodel.Invisualquestionansweringthegoalistopredictananswergivenaquestionandanimage.Forac-tivityrecognitionwedonothaveanexplicitquestion.Thus,weignorethequestionwhichisequivalenttosettingthequestionrepresentationtofQ(Q)=1,avectorofones.WebaseouransweringmodelontheoverallarchitecturefromtheMCBmodel[14],butreplacetheMCBunitwithasimplerelement-wisemultiplicationtopoolmultimodalfeatures.Wefoundthatthisleadstosimilarperformance,butmuchfastertraining(seesupplementalmaterial).Indetail,weextractspatialimagefeaturesfI(I,n,m)fromthelastconvolutionallayerofCNNyesLSTMIsthisahealthymeal?SoftmaxBecause…itcontainsavarietyofvegetablesonthetable.FCEmbedConvI+QI+Q+ALSTMAnsweringModelMultimodalExplanation5.ExperimentsInthissection,afterdetailingtheexperimentalsetup,wepresentquantitativeresultsonablationsdonefortextualjus-tificationandvisualpointingtasks,anddiscusstheirimpli-cations.Additionally,weprovideandanalyzequalitativeresultsforbothtasks.5.1.ExperimentalSetupHere,wedetailourexperimentalsetupintermsofmodeltraining,hyperparametersettings,andevaluationmetrics.Modeltrainingandhyperparameters.ForVQA,thean-sweringmodelofPJ-Xispre-trainedontheVQAv2train-ingset[16].Wethenfreezeorfinetunetheweightsoftheansweringmodelwhentrainingthemultimodalexpla-nationmodelontextualannotationsastheVQA-XdatasetissignificantlysmallerthantheoriginalVQAtrainingset.Foractivityrecognition,answeringandexplanationcompo-nentsofPJ-Xaretrainedjointly.ThespatialfeaturesizeofPJ-XisN=M=14.ForVQA,welimittheanswerspacetothe3000mostfrequentlyoccurringanswersonthetrain-ingset(i.e.|Y|=3000)whereasforactivityrecognition,|Y|=397.Wesettheanswerembeddingsizeasd=300forbothtasks.Evaluationmetrics.Weevaluateourtextualjustifica-tionsw.r.tBLEU-4[24],METEOR[5],ROUGE[20],CIDEr[32]andSPICE[1]metrics,whichmeasurethede-greeofsimilaritybetweengeneratedandgroundtruthsen-tences.Wealsoincludehumanevaluationsinceautomaticmetricsdonotalwaysreflecthumanpreference.Weran-domlychoose1000datapointseachfromthetestsplitsofVQA-XandACT-Xdatasets,wherethemodelpredictsthecorrectanswer,andthenforeachdatapointask3humansubjectstojudgewhetherageneratedexplanationisbetterthan,worsethan,orequivalenttothegroundtruthexplana-tion(wenotethathumanjudgesdonotknowwhatexpla-nationisgroundtruthandtheorderofsentencesisrandom-ized).Wereportthepercentageofgeneratedexplanationswhichareequivalenttoorbetterthangroundtruthhumanexplanations,whenatleast2outof3humanjudgesagree.Forvisualpointingtask,weuseEarthMover’sDis-tance(EMD)[28]whichmeasuresthedistancebetweentwoprobabilitydistributionsoveraregion.Weusethecodefrom[25]tocomputeEMD.WealsoreportonRankCorre-lationwhichwasusedin[11].ForcomputingRankCorre-lation,wefollow[11]wherewescalethegeneratedatten-tionmapandthehumanground-truthannotationsfromtheVQA-X/ACT-X/VQA-HATdatasetsto14×14,rankthepixelvalues,andthencomputecorrelationbetweenthesetworankedlists.5.2.TextualJustificationWeablatePJ-XandcomparewithrelatedapproachesonourVQA-XandACT-Xdatasetsthroughautomaticandhu-manevaluationsforthegeneratedGT-ansTrain-Att.VQA-XACT-XCondi-ingforAutomaticevaluationHumanAutomaticevaluationHumanApproachtioningDataExpl.BMRCSevalBMRCSeval[17]YesDesc.No––––––12.915.939.012.412.017.4OursonDescriptionsYesDesc.Yes6.112.826.436.212.134.56.912.928.320.37.322.9Oursw/oAttentionYesExpl.No18.017.642.466.314.340.116.917.042.033.310.6TheactivityisA:MowingLawn…becauseheiskneelinginthegrassnexttoalawnmower.…becauseheispushingalawnmoweroveragrassylawn.TheactivityisA:MountainBiking…becauseheisridingabicycledownamountainpathinamountainousarea.…becauseheiswearingacyclinguniformandridingabicycledowntheroad.A:MowingLawnA:RoadBikingFigure7:ACT-Xqualitativeresults:ForeachimageQ:Doestheguylookhappy?GT:No…becausehehasasmileonhisface.…becausethewallsarecrackedanddirty.TheactivityisGT:Cello,Sitting…becausesheissittingonachairinfrontofamicrophoneandstrummingaguitar.…becauseheisstandinginalivingroomandpushingavacuumcleaner.Pred:YesQ:Doesthisroomneedtoberenovated?GT:NoPred:YesPred:Guitar,SittingGT:PaintingInsideHousePred:VacuumingFigure9:Visualandtextualexplanationsgeneratedbyourmodelconditionedonincorrectpredictions.nosetheperformanceofanAIsystem.ComplementaryExplanations.Multimodalexplanationscansupportdifferenttasksorsupporteachother.Interest-ingly,inFigure8,wepresentsomeexampleswherevisualpointingismoreinsightfulthantextualjustification,andviceversa.LookingattheleftexampleinFigure8,itisratherdifficulttoexplain“leaning”withlanguageandthemodelresortstogeneratingacorrect,yetuninsightfulsen-tence.However,theconceptiseasilyconveyedwhenlook-ingatthevisualpointingresult.Incontrast,therightex-ampleshowstheopposite.Lookingatonlysomepatchesoftheskypresentedbythevisualpointingresultdoesnotnecessarilyconfirmifthesceneiscloudyornot,whileitisalsounclearifattendingtotheentireregionoftheskyisadesiredbehavior.Yet,thetextualjustificationsuccinctlycapturestherationale.Theseexamplesclearlydemonstratethevalueofgeneratingmultimodalexplanations.DiagnosticExplanations.Weevaluateanauxiliarytaskwherehumanshavetoguesswhetherthesystemcorrectlyorincorrectlyansweredthequestion.Thepredictedanswerisnotshown;onlyimage,question,correctanswer,andtex-tual/visualexplanations.Thesetcontains50%correctlyan-sweredquestions.Wecompareourmodelagainstthemod-elsusedforablationsinTable2.Table4indicatesthatex-planationsarebetterthannoexplanationsandourmodelismorehelpfulthanmodelstrainedondescriptionsandalsomodelstrainedtogeneratetextualexplanationsonly.6.ConclusionAsasteptowardsexplainableAImodels,weproposedmultimodalexplanationsforreal-worldtasks.OurmodelisVQA-XACT-XWithoutexplanation57.5%51.5%OursonDescriptions66.5%72.5%Oursw/oAttention61.5%76.5%Ours70.0%80.5%Table4:Accuracyofhumansjudgingifthemodelpredictedcorrectly.thefirsttobecapableofprovidingnaturallanguagejusti-ficationsofdecisionsaswellaspointingtotheevidenceinanimage.Wehavecollectedtwonovelexplanationdatasetsthroughcrowdsourcingforvisualquestionansweringandactivityrecognition,i.e.VQA-XandACT-X.Wequantita-tivelydemonstratedthatlearningtopointhelpsachievehighReferences[1]P.Anderson,B.Fernando,M.Johnson,andS.Gould.Spice:Semanticpropositionalimagecaptionevalua-tion.InProceedingsoftheEuropeanConferenceonComputerVision(ECCV),2016.6[2]M.Andriluka,L.Pishchulin,sociationforComputationalLinguistics(ACL),pages311–318,2002.6[25]O.PeleandM.Werman.Fastandrobustearthmover’sdistances.In2009IEEE12thInternationalCon-ferenceonComputerVision,pages460–467.IEEE,September2009.6[26]L.Pishchulin,M.Andriluka,andB.Schiele.Fine-grainedactivityrecognitionwithholisticandposebasedfeatures.InProceedingsoftheGermanCon-feenceonPatternRecognition(GCPR),pages678–689.Springer,2014.3[27]S.Reed,Z.Akata,S.Mohan,S.Tenka,B.Schiele,andH.Lee.Learningwhatandwheretodraw.InAdvancesinNeuralInformationProcessingSystems(NIPS),2016.3[28]Y.Rubner,C.Tomasi,andL.J.Guibas.Ametricfordistributionswithapplicationstoimagedatabases.InProceedingsoftheIEEEInternationalConferenceonComputerVision(ICCV),1998.6[29]K.J.Shih,S.Singh,andD.Hoiem.Wheretolook:Focusregionsforvisualquestionanswering.InPro-ceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016.3[30]E.H.ShortliffeandB.G.Buchanan.Amodelofinex-actreasoninginmedicine.Mathematicalbiosciences,23(3):351–379,1975.2[31]M.VanLent,W.Fisher,andM.Mancuso.Anex-plainableartificialintelligencesystemforsmall-unittacticalbehavior.InNCAI,2004.2[32]R.Vedantam,C.LawrenceZitnick,andD.