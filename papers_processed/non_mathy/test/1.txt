TheChallengeofCraftingIntelligibleIntelligenceDanielS.Weld1,21PaulG.AllenSchoolofComputerScience&EngineeringUniversityofWashingtonSeattle,WashingtonGaganBansal12MicrosoftResearchRedmond,WashingtonABSTRACTSinceFigure2:ApproachesforcraftingintelligibleAI.Sectionnumbersindicatewhereeachaspectisdiscussed.Thissurveyhighlightskeyapproachesandchallengesforbuild-ingintelligibleintelligence.Section2characterizesintelligibilityandexplainswhyitHumanErrorsAIErrorsAISpecificErrorsFigure3:Thedashedblueshapeindicatesthespaceofpossi-blemistakeshumanscanmake.TheredshapedenotestheAI’smistakes;itssmallersizeindicatesaFigure4:ApartofFigure1from[4]showing3(of56total)componentsforaGA2Mmodel,whichwastrainedtopredictapatient’sriskofdyingfrompneumonia.Thetwolinegraphsdepictthecontributionofindividualfeaturestorisk:a)patient’sage,andb)booleanvariableasthma.They-axisdenotesitscontribution(logodds)topredictedrisk.Theheatmap,c,visualizesthecontributionduetopairwiseinteractionsbetweenageandcancerrate.CaruanaetAlternatively,adomainexpertcouldgrouptermssemanticallytofacilitateperusal.However,whenthenumberoffeaturesgrowsintothemillions—whichoccurswhendealingwithclassifiersovertext,audio,imageandvideodata2L.A.Hendricks,Z.Akata,M.Rohrbach,J.Donahue,B.Schiele,T.DarrellImageRelevanceClassRelevanceImageDescriptionVisualExplanationClassDefinitionFig.1.Ourproposedmodelgeneratesvisualexplanations.Visualexplanationsarebothimagerelevantandclassrelevant.Incontrast,imagedescriptionsareimagerelevant,butnotnecessarilyclassrelevant,andclassdefinitionsareclassrelevantbutnotnec-essarilyimagerelevant.Inthevisualexplanationsabove,classdiscriminativevisualfeaturesthatarealsopresentintheimagearediscussed.showninFigure1,explanationsaredistinctfromdescriptions,whichprovideasentencebasedonlyonvisualinformation,anddefinitions,whichprovideasentencebasedonlyonclassinformation.Unlikedescriptionsanddefinitions,visualexplanationsdetailwhyacertaincategoryisappropriateforagivenimagewhileonlymentioningimagerelevantfeatures.Asanexample,letusconsideranimageclassificationsystemthatpredictsacertainimagebelongstotheclass“westerngrebe”(Figure1,top).Astandardcaptioningsystemmightprovideadescriptionsuchas“Thisisalargebirdwithawhiteneckandblackbackinthewater.”However,asthisdescriptiondoesnotmentiondiscriminativefeatures,itcouldalsobeappliedtoa“laysanalbatross”(Figure1,bottom).Incontrast,weproposetoprovideexplanations,suchas“Thisisawesterngrebebecausethisbirdhasalongwhiteneck,pointyyellowbeak,andaredeye.”Theexplanationincludesthe“redeye”property,e.g.,whencrucialfordistinguishingbetween“westerngrebe”and“laysanalbatross”.Assuch,oursystemexplainswhythepredictedcategoryisthemostappropriatefortheimage.WeoutlineourapproachinFigure2.Weconditionlanguagegenerationonbothanimageandapredictedclasslabelwhichallowsustogenerateclass-specificsentences.Unlikeothercaptionmodels,whichconditiononvisualfea-turesfromanetworkpre-trainedonImageNet[6],ourmodelalsoincludesafine-grainedrecognitionpipelinetoproducestrongimagefeatures[3].Likemanycontemporarydescriptionmodels[7,8,9,10,11],ourmodellearnstogenerateasequenceofwordsusinganLSTM[12].However,wedesignanovellossfunctionwhichencouragesgeneratedsentencestoincludeclassdiscriminativeinforma-tion.Onechallengeindesigningalosstooptimizeforclassspecificityisthatclassspecificityisaglobalsentenceproperty:e.g.,whereasasentence“Thisisanallblackbirdwithabrightredeye”isclassspecifictoa“BronzedCowbird”,wordsandphrasesinthesentence,suchas“black”or“redeye”arelessclassdiscriminativeontheirown.Ourproposedgenerationlossenforcesthatgener-atedsequencesfulfillacertainglobalproperty,suchascategoryspecificity.Ourfinaloutputisasampledsentence,sowebackpropagatethediscriminativelossLaysanAlbatrossDescription:ClassDefinition:andwhitebelly.VisualExplanation:neckandblackback..Ourproposedmodelgeneratesvisualexplanations.Virelevantandclassrelevant.Incontrast,imagedescritnecessarilyclassrelevant,andclassdefinitionsarecyimagerelevant.Inthevisualexplanationsabove,esthatarealsopresentintheimagearediscussed.inFigure1,explanationsaredistinctfromdesctencebasedonlyonvisualinformation,anddefincebasedonlyonclassinformation.Unlikedesclexplanationsdetailwhyacertaincategoryisappronlymentioningimagerelevantfeatures.Asanageclassificationsystemthatpredictsacertainimerngrebe”(Figure1,top).Astandardcaptioningptionsuchas“Thisisalargebirdwithawhitene.”However,asthisdescriptiondoesnotmentiondalsobeappliedtoa“laysanalbatross”(Figure1,LaysanAlbatrossDescription:Thisisalargebirdwithawhiteneckandablackbeakinthewater.ClassDefinition:TheLaysanAlbatrossisalargeseabirdwithaFigure7:AnexampleofaninteractiveexplanatorydialogforgaininginsightintoaDOG/FISHimageclassifier.(Forillustration,thequestionsandanswersareshowninEnglishlanguagetext,butouruseofa‘dialog’isforillustrationonly.AninteractiveGUI,e.g.,buildingontheideasofKrauseetal.[20],wouldlikelybeabetterrealization.)actionsbytheuser.Thismatchesresultsfrompsychologyliterature,summarizedinSection2,andhighlightsGrice’smaxims,especiallythosepertainingtoquantityandrelation.ItalsobuildsonLimandDey’sworkinubiquitouscomputing,whichinvestigatedthekindsofquestionsuserswishedtoaskaboutcomplex,context-awareapplications[24].Weenvisionaninteractiveexplanationsystemthatsupportsmanydifferentfollow-upanddrill-downactionafterpresentingauserwithaninitialexplanation:•Redirectingtheanswerbychangingthefoil:“Sure,butwhydidn’tyoupredictclassC?”•Askingformoredetail(i.e.,amorecomplexexplanatorymodel),perhapswhilerestrictingtheexplanationtoasubregionoffeaturespace:“I’monlyconcernedaboutwomenoverage50...”•Askingforadecision’srationale:“Whatmadeyoubelievethis?”Towhichthesystemmightrespondbydisplayingthelabeledtrainingexamplesthatweremostinfluentialinreachingthatdecision,e.g.,onesidentifiedbyinfluencefunctions[19]ornearestneighbormethods.•Querythemodel’ssensitivitybyaskingwhatminimalperturba-tionstocertainfeatureswouldleadtoadifferentoutput.•Changingthevocabularybyadding(orremoving)afeatureintheexplanatorymodel,eitherfromapredefinedset,byusingmethodsfrommachineteaching,orwithconceptactivationvectors[17].•Perturbingtheinputexampletoseetheeffectonbothpredictionandexplanation.Inadditiontoaidingunderstandingofthemodel(directlytestingacounterfactual),thisactionenablesanaffecteduserwhowantstocontesttheinitialprediction:“Butofficer,oneofthosepriorDUIswasoverturned...?”•Adjustingthemodel:Basedonnewunderstanding,theusermaywishtocorrectthemodel.Here,weexpecttobuildontoolsforinteractivemachinelearning[1]andexplanatorydebug-ging[20,21],whichhaveexploredinteractionsforaddingnewtrainingexamples,correctingerroneouslabelsinexistingdata,specifyingnewfeatures,andmodifyingshapefunctions.Asmentionedintheprevioussection,itmaybechallengingtomapuseradjustments,thataremadeinreferencetoanexplana-torymodel,backintotheoriginal,inscrutablemodel.Tomaketheseideasconcrete,Figure7presentsapossibledialogasausertriestounderstandtherobustnessofadeepneuraldog/fishclassifierbuiltatopInceptionv3[39].Asthefigureshows:1)Thecomputercorrectlypredictsthattheimagedepictsafish.2)Theuserrequestsanexplanation,whichisprovidedusingLIME[33].3)Theuser,concernedthattheclassifierispayingmoreattentiontothebackgroundthantothefishitself,askstoseethetrainingdatathatinfluencedtheclassifier;thenearestneighborsarecomputedusinginfluencefunctions[19].Whilethereareanemonesinthoseimages,italsoseemsthatthesystemisrecognizingaclownfish.4)Togainconfidence,theusereditstheinputimagetoremovethebackground,resubmitsittotheclassifierandcheckstheexplanation.7EXPLAININGCOMBINATORIALSEARCHMostoftheprecedingdiscussionhasfocusedonintelligiblemachinelearning,whichisjustonetypeofartificialintelligence.However,thesameissuesalsoconfrontsystemsbasedondeep-lookaheadsearch.Whilemanyplanningalgorithmshavestrongtheoreticalproperties,suchassoundness,theysearchoveractionmodelsthatincludetheirownassumptions.Furthermore,goalspecificationsarelikewiseincomplete[29].Iftheseunspokenassumptionsareincorrect,thenaformallycorrectplanmaystillbedisastrous.Consideraplanningalgorithmthathasgeneratedasequenceofactionsforaremote,mobilerobot.Iftheplanisshortwithamoderatenumberofactions,thentheproblemmaybeinherentlyin-telligible,andahumancouldeasilyspotaproblem.However,largersearchspacescouldbecognitivelyoverwhelming.Inthesecases,localexplanationsofferasimplificationtechniquethatishelpful,justasitwaswhenexplainingmachinelearning.Thevocabularyissueislikewisecrucial:howdoesonesuccinctlyandabstractlysummarizeacompletesearchsubtree?Dependingonthechoiceofexplanatoryfoil,differentanswersareappropriate[8].Sreedharanetal.describeanalgorithmforgeneratingtheminimalexplanationthatpatchesauser’spartialunderstandingofadomain[37].Workonmixed-initiativeplanning[7]hasdemonstratedtheimportanceofsupportinginteractivedialogwithaplanningsystem.SincemanyAIsystems,e.g.,AlphaGo[35],combinedeepsearchandmachinelearning,additionalchallengeswillresultfromtheneedtoexplaininteractionsbetweencombinatoricsandlearnedmodels.8FINALTHOUGHTSInordertotrustdeployedAIsystems,wemustintelligible.IntelligibilitywillhelpusspotAIthatmakesmistakesduetodistributionaldriftorincompleterepresentationsofgoalsandfeatures.Intelligibilitywillalsofacilitatecontrolbyhumansinincreasinglycommoncollaborativehuman/AIteams.Furthermore,intelligibilitywillhelphumanslearnfromAI.Finally,therearelegalreasonstowantintelligibleAI,includingtheEuropeanGDPRandagrowingneedtoassignliabilitywhenAIerrs.Dependingonthecomplexityofthemodelsinvolved,twoap-proachestoenhancingunderstandingmaybeappropriate:1)usinganinherentlyinterpretablemodel,or2)adoptinganinscrutablycomplexmodelandgeneratingposthocexplanationsbymappingittoasimpler,explanatorymodelthroughacombinationofcurryingandlocalapproximation.Whenlearningamodeloveramediumnumberofhuman-interpretablefeatures,