Don’tTaketheEasyWayOut:EnsembleBasedMethodsforAvoidingKnownDatasetBiasesChristopherClark∗,MarkYatskar†,LukeZettlemoyer∗∗PaulG.AllenSchoolofCSE,UniversityofWashingtonWhatcoloristhegrass?Bias-OnlyModelRobustModelEnsembleDataGradientsBrownYellowGoldGreenBlueGrayOtherp(answer|model)BrownYellowGoldGreenBlueGrayOtherp(answer|ensemble)BrownYellowGoldGreenBlueGrayOtherp(answer|bias)TrainingLossTrainingPredictionEvaluationFigure1:AnexampleofapplyingourmethodtoaVisualQuestionAnswering(VQA)task.Weassumepredictinggreenforthegivenquestionisalmostalwayscorrectonthetrainingdata.Topreventamodelfromlearningthisbias,wefirsttrainabias-onlymodelthatonlyusesthequestionasinput,andthentrainarobustmodelinanensemblewiththebias-onlymodel.Sincethebias-onlymodelwillhavealreadycapturedthetargetpattern,therobustmodelhasnoincentivetolearnit,andRecentworkhasfocusedonbiasesthatcomefromignoringpartsoftheinput(e.g.,guessingtheanswertoaquestionbeforeseeingtheevi-dence).Solutionsincludegenerativeobjectivestoforcemodelstounderstandalltheinput(LewisandFan,2019),carefullydesignedmodelarchi-tecture(Agrawaletal.,2018;Zhangetal.,2016),oradversarialremovalofclass-indicativefeaturesfrommodel’sinternalrepresentations(Ramakrish-nanetal.,2018;Zhangetal.,2018a;Belinkovetal.,2019;GrandandBelinkov,2019).Incontrast,weconsiderbiasesbeyondpartial-inputcases(Fengetal.,2019),andshowourmethodissuperioronVQA-CP.Concurrently,Heetal.(2019)alsosuggestedusingaproduct-of-expertsensembletotrainunbiasedmodels,butwecon-siderawidervarietyofensemblingapproachesandtestonadditionaldomains.Arelatedtaskispreventingmodelsfromus-ingparticularproblematicdatasetfeatures,whichisoftenstudiedfromtheperspectiveoffair-ness(Zhaoetal.,2017;Burnsetal.,2018).Apopularapproachistouseanadversarytoremoveinformationaboutatargetfeature,oftengenderorethnicity,fromamodel’sinternalrepresenta-tions(EdwardsandStorkey,2016;Wangetal.,2018;Kimetal.,2019).Incontrast,thebiasesweconsiderarerelatedtofeaturesthatareessentialtotheoveralltask,sotheycannotsimplybeignored.Evaluatingmodelsonout-of-domainexamplesbuiltbyapplyingminorperturbationstoexist-ingexampleshasalsobeenthesubjectofrecentstudy(Szegedyetal.,2014;BelinkovandBisk,2018;CarliniandWagner,2018;Glockneretal.,2018).Thedomainshiftsweconsiderinvolvelargerchangestotheinputdistribution,builttoun-coverhigher-levelflawsinexistingmodels.3MethodsThissectiondescribesthetwostagesofourmethod,(1)buildingabias-onlymodeland(2)us-ingittotrainarobustmodelthroughensembling.3.1TrainingaBias-OnlyModelThegoalofthefirststageistobuildamodelthatperformswellontrainingdata,butislikelytoper-formverypoorlyontheout-of-domaintestset.Sinceweassumewedonothaveaccesstoex-amplesfromthetestset,wemustapplya-prioriknowledgetomeetthisgoal.Themoststraightforwardapproachistoiden-tifyasetoffeaturesthatarecorrelatedwiththeclasslabelduringtraining,butareknowntobeun-correlatedoranticorrelatedwiththelabelonthetestset,andthentrainaclassifieronthosefea-tures.2Forexample,ourVQA-CP(Agrawaletal.,2018)bias-onlymodel(seeSection5.2)usesthequestiontypeasinput,becausethecorrelationsbe-tweenquestiontypesandanswersisverydifferentinthetrainsetthanthetestset(e.g.,2isacommonanswerto“Howmany...”questionsonthetrainset,butisrareforsuchquestionsonthetestset).However,abenefitofourmethodisthatthebiascanbemodelledusinganykindofpredictor,giv-ingusawaytocapturemorecomplexintuitions.Forexample,onSQuADourbias-onlymodelop-eratesonaviewoftheinputbuiltfromTF-IDFscores(seeSection5.4),andonourchangingpriorTriviaQAdatasetourbias-onlymodelmakesuseofapre-trainednamedentityrecognition(NER)tagger(seeSection5.5).3.2TrainingaRobustModelThisstagetrainsarobustmodelthatavoidsusingthemethodlearnedbythebias-onlymodel.3.2.1ProblemDefinitionWeassumentrainingexampleshx1,x2,...,xni,eachofwhichhasanintegerlabelyi,whereyi∈{1,2,...,C}andCisthenumberofclasses.Weadditionallyassumeapre-trainedbias-onlypre-dictor,h,whereh(x3.2.3BiasProductOursimplestensembleisaproductofex-perts(Hinton,2002):ˆpi=softmax(log(pi)+log(bi))Equivalently,ˆpi∝pi◦bi,where◦iselement-wisemultiplication.ProbabilisticJustification:Foragivenex-ample,x,letxbbethebiasoftheexample.Thatis,itisthefeatureswewilluseinourbias-onlymodel.Letx−bbeaviewoftheTaskDatasetDomainShiftBias-OnlyModelMainModelNLISyntheticMNLISyntheticindicatorfeaturesarerandomizedIndicatorfeaturesCo-AttentionVQAVQA-CPv2.0Correlationsbetweenquestion-typesandanswersarealteredQuestion-typeBottomUpTopDownNLIHANSSentencepairsalwayscontainthesamewordsSharedwordfeaturesBERT&Co-AttentionQAAdv.SQuADDistractorsentencesareaddedtothecontextTF-IDFsentenceselectorModifiedBiDAFQATriviaQA-CPQuestionsaskaboutdifferentkindsofentitiesNERanswerdetectorModifiedBiDAFTable1:Summaryofthetimethetokencorrespondstotheexample’slabel(i.e.,“0”iftheclassis“entailment”,“1”iftheclassiscontradiction,ect.).Intheout-of-domaintestset,thetokenisselectedrandomly.Excluder:ThesameasIndicator,butwitha3%chancetheaddedtokencorrespondstotheexample’slabel,meaningthetokencanusuallybeusedtoeliminateoneofthethreeoutputclasses.Dependent:Intheprevioustwosettings,theaddedbiasisindependentoftheexamplegiventheDebiasingMethodIndicatorExcluderDependentAcc.w/BiasAcc.w/BiasAcc.w/BiasNone69.3686.4968.0683.5663.2387.90Reweight75.4482.7470.3683.2969.8185.50BiasProduct76.2781.3277.3380.4171.8584.98Learned-Mixin76.29Isthisa....?NoQuestionTypeBiasAnswerHigherBiasWeightLowerBiasWeightHowmany….?2Howmanyanimals?[2]G=5.61G+=5.89Howmanybirds?[17]G=0.17G+=1.95Isthisablackbear?[No]G=4.65G+=5.96Isthisaphotoorpainting?[Painting]WhiteWhatcoloristhe….?G=0.87G+=4.32Whatcoloristhedoor?[White]Whatcoloristhetenniscourt?[Purple]G=0.00G+=0.48PizzaWhatDebiasingMethodTF-IDFFilteredTF-IDFAddSentAddSentOneDevAddSentAddSentOneDevNone42.5453.9180.6142.5453.9180.61Reweight41.5553.0680.5942.7453.8380.51BiasProduct47.1757.7478.6344.4155.7378.22Learned-Mixin42.25ReferencesAishwaryaAgrawal,DhruvBatra,DeviParikh,andAniruddhaKembhavi.2018.Don’tJustAssume;LookandAnswer:OvercomingPriorsforVisualQuestionAnswering.InCVPR.AnkeshAnand,EugeneBelilovsky,KyleKastner,HugoLarochelle,andAaronCourville.2018.BlindfoldBaselinesforEmbodiedQA.ComputingResearchRepository,arXiv:1811.05013.Version1.PeterAnderson,XiaodongHe,ChrisBuehler,DamienTeney,MarkJohnson,StephenGould,andLeiZhang.2018.Bottom-UpandTop-DownAttentionforImageCaptioningandVisualQuestionAnswer-ing.InCVPR.StanislawAntol,AishwaryaRobinJiaandPercyLiang.2017.AdversarialEx-amplesforEvaluatingReadingComprehensionSys-tems.InEMNLP.MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer.2017.TriviaQA:ALargeScaleDis-tantlySupervisedDatasetExperimentPenaltySyntheticIndicator0.01SyntheticExcluder0.005SyntheticDependent0.005VQA-CP-0.36HANSRecurrent0.03HANSBERT0.03Adver.SQuADTF-IDFFiltered2.0Adver.SQuADTF-IDF2.0TriviaQA-CPLocation0.4TriviaQA-CPPerson0.2Table7:Entropypenaltyweightforthelearned-mixin+Hensembleonallourexperiments.BCo-AttentionNLIModelThemodelweuseforNLIisbasedonESIM(Chenetal.,2017).Ithasthefollow-ingstages:Embed:EmbedthewordsusingacharacterModelDebiasingMethodMNLICorrect:EntailmentCorrect:Non-entailmentLexicalSubseq.ConstLexicalSubseq.ConstCo-AttentionNone78.7397.8399.6797.281.373.683.68Reweight77.0380.1077.8473.7615.6834.2735.44BiasProduct76.6377.89MethodAccuracyLocationPersonPrecisionRecallF1PrecisionRecallF1Patterns72.8498.3070.6182.1999.1233.4350.00Yago88.5695.8785.3190.2894.7080.0086.73Yago+Patterns91.7395.4493.8894.6594.7085.37