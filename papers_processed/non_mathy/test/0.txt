IstheMostAccurateAItheBestTeammate?OptimizingAIforTeamworkGaganBansal1BesmiraNushi2EceKamar2EricHorvitz2DanielS.Weld1,31UniversityofWashington2MicrosoftResearch3AllenInstituteforAIAbstractAIpractitionerstypicallystrivetodevelopthemostaccuratesystems,makinganimplicitassumptionthattheAIsystemwillfunctionautonomously.However,inpractice,AIsystemsoftenareusedtoprovideadvicetopeopleindomainsrangingfromcriminaljusticeandfinancetohealthcare.InsuchAI-adviseddecisionmaking,humansandmachinesformateam,wherethehumanisresponsibleformakingfinaldecisions.ButisthemostaccurateAIthebestteammate?Weargue“No”—predictableperformancemaybeworthaslightsacri-ficeinAIaccuracy.Instead,wearguethatAIsystemsshouldbetrainedinahuman-centeredmanner,directlyoptimizedforteamperformance.Westudythisproposalforaspecifictypeofhuman-AIteaming,wherethehumanoverseerchoosestoeitheraccepttheAIrecommendationorsolvethetaskthem-selves.Tooptimizetheteamperformanceforthissettingwemaximizetheteam’sexpectedutility,expressedintermsofthequalityofthefinaldecision,costofverifying,andindi-vidualaccuraciesofpeopleandmachines.Ourexperimentswithlinearandnon-linearmodelsonreal-world,high-stakesdatasetsshowthatthemostaccuracyAImaynotleadtohigh-estteamperformanceandshowthebenefitofmodelingteam-workduringtrainingthroughimprovementsinexpectedteamutilityacrossdatasets,consideringparameterssuchashumanskillandthecosttasks,suchasabilitytocomplementandcoordinatewithone’spartner.Similarly,creatinghigh-performinghuman-AIteamsmayrequiretrainingAIsystemsthatexhibitad-ditionalhuman-centeredproperties,e.g.,facilitatingappro-priatelevelsoftrustanddelegation.Implicitly,thisisthemotivationbehindmuchworkinintelligibleAI,includingeffortsaimedatenhancingtheunderstandabilityofcom-plexAIinference(Horvitzetal.1986),interpretabilityofmachine-learnedmodels(Caruanaetal.2015;WeldandBansal2019),andperformingpost-hocexplanationsMeta-decision/DecisionCorrectIncorrectAccept[A]1−βSolve[S]1−λ−β−λTable1:Utilityasafunctionofmeta-decisionanddecision.mendationasthefinaldecision.Incontrast,inSolve,theuserignorestherecommendationandcomputesthefinaldecisionthemselves.Letmdenotethefunctionthatmapsaninputinstanceandrecommendationtoameta-decisioninM={Accept,Solve}.Further,Udenotestheutil-ityfunction,whichdependsonthehumanmeta-decisionandfinaldecisiond(Figure1).Asaresult,theoptimalclassifierh∗wouldmaximizetheteam’sexpectedutility:h∗=argmaxhEx,y[U(m,d)](1)2.Mistakesarecostly:AcorrectdecisionresultsDataset#FeaturesSizeFrac.Pos.Scenario12100000.43Moons2100000.50German2410000.30Fico3998610.52Recidivism1361720.46MIMIC714211390.13Table2:NumberoffeaturesLoglossExpectedUtilityLossClassifierDatasetAccuracyExpectedUtil.Emp.Util.∆Accuracy∆ExpectedUtil.∆Emp.Util.LinearFico0.7290.4870.575-0.2470.013-0.075German0.7540.5290.594-0.0150-0.019MIMIC0.8810.6940.8-0.0040.066-0.035Moons0.8850.6870.79-0.020.079-0.006recidivism0.6690.4850.52-0.170.015-0.02Scenario10.8580.5240.593-0.1650.1020.061MLPFico0.7250.4720.574-0.2440.028-0.074German0.7520.530.618-0.036-0.027-0.056MIMIC0.8810.7190.799-0.0010.049-0.029Moons10.9440.98900.0490.006Recidivism0.6740.4670.521-0.1680.033-0.021Scenario110.8260.854-0.10.080.057Table3:Comparisonofaccuracy,expectedandempiricalteamutilitiesofclassifiersoptimizedforlog-loss(withacheckpointonaccuracy)andexpectedteamutility(withacheckpointonexpectedutility)usingAdamforλ=0.5,a=1.0,β=1.0.Observationsaveragedover50train/testsplits.∆indicatesdifferencewithrespecttolog-loss.Classifiertrainedtooptimizeexpectedteamutilityachieveshigherexpectedutilityatthecostofautomationaccuracy.However,wenoticeamismatchbetweenexpectedandempiricalutilities–empiricalutilitydecreasedeventhoughexpectedutilityincreased.DatasetExpectedUtilLLEmp.UtilLL∆ExpectedUtil(A)∆Emp.Util(B)∆∗Emp.Util(C)Fico-2d0.4750.5110.025-0.011-0.004German-2d0.5140.60.076-0.004-0.016MIMIC-2d0.6410.7720.121-0.0090.005Moons0.7670.8130.016-0.0060.034Recidivism-2d0.4780.5180.022-0.0170.007Scenario10.7070.7150.0450.0690.068Table4:Testperformanceoflinearclassifierthatoptimizeslog-lossandteamutilityusingbrute-forceoptimizationontwo-dimensionaldomains.Whileweobserveconsistentimprovementsintheteam’sexpectedutility(columnmarkedA)acrossdomains,improvementsinexpectedutilitydidnottranslatetoimprovementsinempiricalutility(valuesincolumnmarkedBarenegative),indicatingamismatchbetweentheexpectedandempiricalmetricsofteamutilities.Atthesametime,exhaustivesearchshowsexistenceoflinearclassifierswithhigherempiricalutility(columnmarkedC).Valueswereaveragedoverfiveseeds.ObservationsincolumnConFico-2dandGerman-2dwerenegativeontestsetduetoover-fitting.dimensionalversionsofourdataset(byselectingtwotopmostinformativefeatures)andtrainedlinearclassifiersus-ingexhaustivesearch,whichbydefinitioncannotgetstuckinlocalminimas.Weagainfoundapersistenceofthemis-matchbetweenexpectedandempiricalutilities(Table4).Inaddition,wealsonoticedthatthereexistclassifierswithhigherempiricalutilityiftheexhaustivesearchmaximizesdirectlyforempiricalutility(columnCinTable4),whichfurtherdemonstratestheexistenceofthemismatch.2Theseresultsprovideevidencethatthechallengewithachievingcomparableincreasesinempiricalutilitytothoseinexpectedutilityisnotonlyduetooptimizationissues(e.g.,localminimasandplateausduetoflatnessoftheex-pectedutilitycurveintheSolveregion).Thereexistsafun-damentalMLchallengeofloss-metricmismatch,whichwasprominentinoursetup.Intherestofthesection,wepresentfurtheranalysesofimprovementsinthenormativedecisionmakingmetricofexpectedutility,whichasdescribedearlier,isusefulindecision-makingunderuncertainty.RQ2:WhilethemetricsinTable3(changeinaccuracyandutility)provideaglobalunderstandingoftheclassifierbe-2Notethatdirectlyoptimizingforempiricalutilityisnoteffec-tiveviastochasticoptimization.havior,hereweattempttounderstandhowtheseimprove-mentswereachievedandwhetherthebehaviorofthenewmodelsisconsistentwiththeoriginalintuition.Figure4dis-playsthedifferenceinbehavior(averagedover50seeds)betweentheclassifiersproducedbylog-lossandtheonethatmaximizesteamutilityontheScenario1andMIMICdataset.Specifically,asshowninFigure4,wevisualizeandcomparethefollowingbehaviorsofthetwoclassifiers:V1.Calibrationusingreliabilitycurves,whichcomparesystemconfidenceanditstrueaccuracy.Aperfectlycal-ibratedsystem,forexample,willbe80%accurateonre-gionsthatis80%confident.However,inpractice,systemsmaybeover-orunder-confident.V2.Distributionsofconfidenceinpredictions.Forexam-ple,inFigure4,thenewclassifiermakesmorehigh-confidencepredictionsthanthemostaccurateclassifier.V3.Densityofsystemaccuracyasfunctionofconfidenceintruelabel.Thus,theareaunderthiscurveindicatesthesystem’stotalaccuracy.NoteFigure4:Behavioroflinearclassifiersthatoptimizelog-lossandexpectedteamutilityontheScenario1andMIMICdatasets(observationsaveragedover50runs).ThelattermakesfewerpredictionsintheSolveregionagainthatforextremelyhigh-stakedecisions,automationorAIrecommendationmaynotalwaysprovidevalue.4DiscussionandFutureWorkImplicationsforcomplexhuman-AIteamsWhilewein-vestigatedasimplifiedhuman-AIteamwork(asdefinedinSection2),oursetupallowsextensionstomorecomplexteamandusers.Forexample,onecanrelaxourassump-tionthatusersarerationalbymodifyingthehuman-policyinEquation4,sothatwhenthepredictionconfidenceisgreaterthanthethreshold,theuserchoosesAcceptwithprobabil-ityp<1,insteadof1.0.Here,1−pdenotestheproba-bilityoftheuserbeingirrational—assessedfromhistori-caldata,ifavailable.Similarly,inmorecomplexsituationsusersmaymakeAcceptandSolvedecisionsusingasystems.Sinceexplainabilitydoesnotguaranteeimprove-mentsincollaboration(Bansaletal.2020),thereisneedtobringcollaborationasanobjectivetoeverystepofsystemdevelopment,startingfromthetrainingobjective.EthicalStatementAbroadercontributionofthisworkistorethinkhowMLmodelsaredefinedandoptimizedwhentheyaredeployedinhuman-AIcollaborationscenarios,e.g.,forsupportinghu-mandecisionmakinginhigh-stakesareas(includinghealth-careandcriminaljustice)whereAIsystemsalreadyinflu-enceuserdecisionswithimportantconsequencesforindi-vidualsandsociety.SincemostAIsystemsareoptimizedautomationperformance,moreresearchisneededtocre-ateeffectiveadvisorysystemsbyintegratingteam-centeredconsiderationsintheformalmachineryofoptimizationusedtobuildandexecutetheseAIsystems.Weexaminedoneap-proachtoraisingtheexpectedvalueofAI-aidedhumande-cisionmakingbyconsideringteamworkintheoptimizationobjective.Beyondthedirectuseofthemethodsforoptimizinghuman-AIteamwork,themethodscanbevaluableforbuild-inginsightsonteaming.Forexample,resultsshowedthatthereexistregionsinthespaceofcollaborationparameterswhereautomatedassistance,orevenprovidinganAIrecom-mender,maynotmakesense–whenthecostofmistakeswashighandhumandecisionsaresufficientlyaccurate,thealgo-rithmalwayshandsovercontroltohumans,discouragingtheneedforalgorithmicsupport.SuchananalysishighlightstheimportanceofcarefullyquestioningandevaluatingwhetherAIdeploymentisbeneficialfromateamperspective.Amorerigorousevaluationrequiresrobustandonlineestimationofcostsanduserbehaviortoensurethatthetrainingandreal-worldobjectivesalign.Whilewedidnotaddresstheprob-lemofestimatingandupdatingsuchparameters,wewishtobringattentiontothefactthatproblemssuchasunderes-timationofcosts(oroverestimationofrewards)maystillleadtohigh-costmistakesevenwhenfollowingtheopti-mizationapproachweproposedinthispaper.Wehopethatadvancesininterdisciplinaryresearchonmeasuringtheim-pactandcostsinsocio-technicalsystemswillfurtherinformdecisionsanddesignsabouttheroleandbehaviorofAIinhuman-AIteamworkinfuturework.Finally,werecognizethatsignificantethicalissuesareraisedbythenatureofhumanoversightandagencyoverAIinoursimplifiedhuman-AIteaming.Ourformulationofuserpolicyassumedthat,whentheAIsystemisconfident,theusercompletelytrustsAIinferencesandforgoesfurtherhumandeliberation.Suchapolicycanleadtoinappropri-atetransfersofresponsibilityinrealisticsettings.Evenifthemodelisconfidentandhasbeenhistoricallycorrect,humanswillstillneedtostaycognizantofthepotentialforpoorlycharacterizedandunexpectedmodesofAIfailure,e.g.,duetodistributionalshiftsorchangesintheinfluencesoflatentvariableswithchangesincontextorworkload.Thus,inreal-worldsettings,thepolicythatwestudiedcanbedangerous.InusesofAI,wherehigh-confidencerecommendationsaretypicallytrustedandthereisapracticeoflittleornohumandeliberationaboutthevalidityofautomatedoutput,humanoverseersofAIshouldbeawareoftheirrelianceandtheirneedtotakefullaccountabilityforoutcomeslinkedtotheinferences.7AcknowledgmentsThismaterialisbaseduponresearchinitiallyperformedduringGaganBansal’ssummerinternshipatMicrosoftRe-search,withcontinuingsupportbyONRgrantN00014-18-1-2193,theUniversityofWashingtonWRF/CableProfes-sorship,andtheAllenInstituteforArtificialIntelligence(AI2).TheauthorsthankZeyuanAllen-Zhu,RichCaru-ana,BryanWilder,andtheanonymousreviewersforhelpfulcomments.ReferencesBach,F.R.;Heckerman,D.;andHorvitz,E.2006.Consid-eringcostasymmetryinlearningclassifiers.JMLR.Bansal,G.;Nushi,B.;Kamar,E.;Lasecki,W.S.;Weld,D.S.;andHorvitz,E.2019a.BeyondAccuracy:TheRoleofMentalModelsinHuman-AITeamPerformance.InHCOMP.Bansal,G.;Nushi,B.;Kamar,E.;Weld,D.S.;Lasecki,W.S.;andHorvitz,E.2019b.Updatesinhuman-aiteams:Understandingandaddressingtheperformance/compatibil-itytradeoff.InAAAI.Bansal,G.;Wu,T.;Zhou,J.;Fok,R.;Nushi,B.;Kamar,E.;Ribeiro,M.T.;andWeld,D.S.2020.DoestheWholeEx-ceeditsParts?TheEffectofExplanationsonComplemen-taryTeamPerformance.ArXivURLhttps://arxiv.org/abs/2006.14779.Beach,B.H.1975.Expertjudgmentaboutuncertainty:Bayesiandecisionmakinginrealisticsettings.Organiza-tionalBehaviorandHumanPerformance14(1):10–59.Burges,C.J.1998.Atutorialonsupportvectormachinesforpatternrecognition.DataMiningandKnowledgeDiscovery2(2):121–167.Caruana,R.;Lou,Y.;Gehrke,Hendrycks,D.;andGimpel,K.2017.Abaselinefordetect-ingmisclassifiedandout-of-distributionexamplesinneuralnetworks.ICLR.Hendrycks,D.;Mazeika,M.;andDietterich,T.G.2018.Deepanomalydetectionwithoutlier