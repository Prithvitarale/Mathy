STEM: A Stochastic Two-Sided Momentum Algorithm
Achieving Near-Optimal Sample and Communication
Complexities for Federated Learning
Prashant Khanduriâˆ—â€ 
, Pranay Sharma
, Haibo Yangâˆ—
, Mingyi Hongâ€ 
, Jia Liuâˆ—
,
Ketan Rajawatâ€¡
, and Pramod K. Varshney
âˆ—
Department of Electrical and Computer Engineering,
The Ohio State University, OH, USA
â€ 
Department of Electrical and Computer Engineering,
University of Minnesota, MN, USA

Department of Electrical Engineering and Computer Science,
Syracuse University, NY, USA
â€¡
Department of Electrical Engineering,
Indian Institute of Technology Kanpur, India
Email: khand095@umn.edu, psharm04@syr.edu, yang.5952@buckeyemail.osu.edu
mhong@umn.edu, liu@ece.osu.edu, ketan@iitk.ac.in, varshney@syr.edu
Abstract
Federated Learning (FL) refers to the paradigm where multiple worker nodes (WNs) build a joint model
by using local data. Despite extensive research, for a generic non-convex FL problem, it is not clear, how to
choose the WNsâ€™ and the serverâ€™s update directions, the minibatch sizes, and the local update frequency, so
that the WNs use the minimum number of samples and communication rounds to achieve the desired solution.
This work addresses the above question and considers a class of stochastic algorithms where the WNs perform
a few local updates before communication. We show that when both the WNâ€™s and the serverâ€™s directions
are chosen based on a stochastic momentum estimator, the algorithm requires OÌƒ(âˆ’3/2
) samples and OÌƒ(âˆ’1
)
communication rounds to compute an -stationary solution. To the best of our knowledge, this is the first FL
algorithm that achieves such near-optimal sample and communication complexities simultaneously. Further,
we show that there is a trade-off curve between local update frequencies and local minibatch sizes, on which
the above sample and communication complexities can be maintained. Finally, we show that for the classical
FedAvg (a.k.a. Local SGD, which is a momentum-less special case of the STEM), a similar trade-off curve
exists, albeit with worse sample and communication complexities. Our insights on this trade-off provides
guidelines for choosing the four important design elements for FL algorithms, the update frequency, directions,
and minibatch sizes to achieve the best performance.
1 Introduction
In Federated Learning (FL), multiple worker nodes (WNs) collaborate with the goal of learning a joint model,
by only using local data. Therefore it has become popular for machine learning problems where datasets
are massively distributed [1]. In FL, the data is often collected at or off-loaded to multiple WNs which in
collaboration with a server node (SN) jointly aim to learn a centralized model [2,3]. The local WNs share the
computational load and since the data is local to each WN, FL also provides some level of data privacy [4]. A
1
arXiv:2106.10435v1
[cs.LG]
19
Jun
2021
(a) Communication complexity. (b) Minibatch sizes vs Local Updates.
Figure 1: The 3D surface in (a) plots the communication complexity of the proposed STEM for different minibatch sizes
and number of local updates. The surface is generated such that each point represents STEM with a particular choice of
(b, I), so that it requires OÌƒ(âˆ’3/2
) samples to achieve -stationarity, but a . Plot (b) shows the optimal trade off between
the minibatch sizes and the number of local updates at each WN (i.e., achieving the lowest communication and sample
complexities). Both plots are generated for an accuracy of  = 10âˆ’3
and all the constants dependent on system parameters
(variance of stochastic gradients, heterogeneity parameter, optimality gap, Lipschitz constants, etc.) are assumed to be
1. Fed STEM is a special case of STEM where O(1) minibatch is used; Minibatch STEM is a special case of STEM where
O(1) local updates are used.
classical distributed optimization problem that K WNs aim to solve:
min
xâˆˆRd

f(x) :=
1
K
K
X
k=1
f(k)
(x) :=
1
K
K
X
k=1
EÎ¾(k)âˆ¼D(k) [f(k)
(x; Î¾(k)
)]

. (1)
where f(k) : Rd â†’ R denotes the smooth (possibly non-convex) objective function and Î¾(k) âˆ¼ D(k) represents
the sample/s drawn from distribution D(k) at the kth WN with k âˆˆ [K]. When the distributions D(k) are
different across the WNs, it is referred to as the heterogeneous data setting.
The optimization performance of non-convex FL algorithms is typically measured by the total number of
samples accessed (cf. Definition 2.2) and the total rounds of communication (cf. Definition 2.3) required by each
WN to achieve an -stationary solution (cf. Definition 2.1). To minimize the sample and the communication
complexities, FL algorithms rely on the following four key design elements: (i) the WNsâ€™ local model update
directions, (ii) Minibatch size to compute each local direction, (iii) the number of local updates before WNs
share their parameters, and (iv) the SNâ€™s update direction. How to find effective FL algorithms by (optimally)
designing these parameters has received significant research interest recently.
Contributions. The main contributions of this work are listed below:
1) We propose the Stochastic Two-Sided Momentum (STEM) algorithm, that utilizes a momentum-assisted
stochastic gradient directions for both the WNs and SN updates. We show that there exists an optimal trade
off between the minibatch sizes and local update frequency, such that on the trade-off curve STEM requires
OÌƒ(âˆ’3/2)1 samples and OÌƒ(âˆ’1) communication rounds to reach an -stationary solution; see Figure 1 for an
illustration. These complexity results are the best achievable for first-order stochastic FL algorithms (under
certain assumptions, cf. Assumption 1); see [5â€“8] and [9,10], as well as Remark 1 of this paper for discussions
regarding optimality. To the best of our knowledge, STEM is the first algorithm which â€“ (i) simultaneously
achieves the optimal sample and communication complexities for FL and (ii) can optimally trade off the
minibatch sizes and the local update frequency.
1
The notation OÌƒ(Â·) hides the logarithmic factors.
2
Algorithm Work Sample Communication Minibatch (b) Local Updates (I) /round
FedAvg
[12] / [14]
O(âˆ’2
)
O(âˆ’3/2
) O(1) O(âˆ’1/2
)
[15]/ [16] O(âˆ’2
) O(1) O(1)
this work O(âˆ’3/2
) O âˆ’
2(1âˆ’Î½)
(4âˆ’Î½)

O âˆ’ 3Î½
2(4âˆ’Î½)

SCAFFOLDâˆ—
[15] O(âˆ’2
) O(âˆ’2
) O(1) O(1)
FedPD/FedProxâ€¡
[9]/ [10] O(âˆ’2
) O(âˆ’1
) O(1) O(âˆ’1
)
MIMEâ€ 
/FedGLOMO [17]/ [18] O(âˆ’3/2
) O(âˆ’3/2
) O(1) O(1)
STEM
O âˆ’
3(1âˆ’Î½)
2(3âˆ’Î½)

O âˆ’ Î½
(3âˆ’Î½)

Fed STEM O(1) O(âˆ’1/2
)
Minibatch STEMâˆ—
this work OÌƒ(âˆ’3/2
) OÌƒ(âˆ’1
)
O(âˆ’1/2
) O(1)
Table 1: Comparison of FedAvg and STEM with different FL algorithms for various choices of the minibatch sizes (b)
and the number of per node local updates between two rounds of communication (I).

Î½ âˆˆ [0, 1] trades off b and I; Î½ = 1 (resp. Î½ = 0) uses multiple (resp. O(1)) local updates and O(1) (resp. multiple)
samples. Fed STEM and Minibatch STEM are two variants of the proposed STEM.
â€¡
The data heterogeneity assumption is weaker than Assumption 2 (please see [9] for details).
â€ 
Requires bounded Hessian dissimilarity to model data heterogeneity across WNs.
âˆ—
Guarantees for Minibatch STEM with I = 1 and SCAFFOLD are independent of the data heterogeneity.
2) A momentum-less special case of our STEM result further reveals some interesting insights of the classical
FedAvg algorithm (a.k.a. the Local SGD) [11â€“13]. Specifically, we show that for FedAvg, there also exists a
trade-off between the minibatch sizes and the local update frequency, on which it requires O(âˆ’2) samples and
O(âˆ’3/2) communication rounds to achieve an -stationary solution.
Collectively, our insights on the trade-offs provide practical guidelines for choosing different design elements
for FL algorithms.
Related Works. FL algorithms were first proposed in the form of FedAvg [11], where the local update
directions at each WN were chosen to be the SGD updates. Earlier works analyzed these algorithms in
the homogeneous data setting [19â€“25], while many recent studies have focused on designing new algorithms
to deal with heterogeneous data settings, as well as problems where the local loss functions are non-convex
[9,10,12â€“16,18,26â€“32]. In [12], the authors showed that Parallel Restarted SGD (Local SGD or FedAvg [11])
achieves linear speed up while requiring O(âˆ’2) samples and O(âˆ’3/2) rounds of communication to reach
an -stationary solution. In [14], a Momentum SGD was proposed, which achieved the same sample and
communication complexities as Parallel Restarted SGD [12], without requiring that the second moments of the
gradients be bounded. Further, it was shown that under the homogeneous data setting, the communication
complexity can be improved to O(âˆ’1) while maintaining the same sample complexity. The works in [15, 16]
conducted tighter analysis for FedAvg with partial WN participation with O(1) local updates and batch sizes.
Their analysis showed that FedAvgâ€™s sample and communication complexities are both O(âˆ’2). Additionally,
SCAFFOLD was proposed in [15], which utilized variance reduction based local update directions [33] to
achieve the same sample and communication complexities as FedAvg. Similarly, VRL-SGD proposed in [29] also
utilized variance reduction and showed improved communication complexity of O(âˆ’1), while requiring the same
computations as FedAvg. Importantly, both SCAFFOLD and VRL-SGDâ€™s guarantees were independent of the
data heterogeneity. The FedProx proposed in [10] used a penalty based method to improve the communication
complexity of FedAvg (i.e., the Parallel Restarted and Momentum SGD [12, 14]) to O(âˆ’1). FedProx used
a gradient similarity assumption to model data heterogeneity which can be stringent for many practical
applications. This assumption was relaxed by FedPD proposed in [9].
Recently, the works [17, 18] proposed to utilize hybrid momentum gradient estimators [7, 8]. The MIME
algorithm [17] matched the optimal sample complexity (under certain smoothness assumptions) of O(âˆ’3/2)
of the centralized non-convex stochastic optimization algorithms [5â€“8]. Similarly, Fed-GLOMO [18] achieved
3
the same sample complexity while employing compression to further reduce communication. Both MIME and
Fed-GLOMO required O(âˆ’3/2) communication rounds to achieve an -stationary solution. Please see Table 1
for a summary of the above discussion.
The comparison of Local SGD (FedAvg) to Minibatch SGD for convex and strongly convex problems with
homogeneous data setting was first conducted in [19] and later extended to heterogeneous setting in [13]. It
was shown that Minibatch SGD almost always dominates the Local SGD. In contrast, it was shown in [24] that
Local SGD dominates Minibatch SGD in terms of generalization performance. Although existing FL results
are rich, but they are somehow ad hoc and there is a lack of principled understanding of the algorithms. We
note that the proposed STEM algorithmic framework provides a theoretical framework that unifies all existing
FL results on sample and communication complexities.
Notations. The expected value of a random variable X is denoted by E[X] and its expectation conditioned
on an Event A is denoted as E[X|Event A]. We denote by R (and Rd) the real line (and the d-dimensional
Euclidean space). The set of natural numbers is denoted by N. Given a positive integer K âˆˆ N, we denote
[K] , {1, 2, . . . , K}. Notation k Â· k denotes the `2-norm and hÂ·, Â·i the Euclidean inner product. For a discrete
set B, |B| denotes the cardinality of the set.
2 Preliminaries
Before we proceed to the the algorithms, we make the following assumptions about problem (1).
Assumption 1 (Sample Gradient Lipschitz Smoothness). The stochastic functions f(k)(Â·, Î¾(k)) with Î¾(k) âˆ¼ D(k)
for all k âˆˆ [K], satisfy the mean squared smoothness property, i.e, we have
Ekâˆ‡f(k)
(x; Î¾(k)
) âˆ’ âˆ‡f(k)
(y; Î¾(k)
)k2
â‰¤ L2
kx âˆ’ yk2
for all x, y âˆˆ Rd
.
Assumption 2 (Unbiased gradient and Variance Bounds). (i) Unbiased Gradient. The stochastic gradients
computed at each WN are unbiased
E[âˆ‡f(k)
(x; Î¾(k)
)] = âˆ‡f(k)
(x), âˆ€ Î¾(k)
âˆ¼ D(k)
, âˆ€ k âˆˆ [K].
(ii) Intra- and inter- node Variance Bound. The following bounds hold:
Ekâˆ‡f(k)
(x; Î¾(k)
) âˆ’ âˆ‡f(k)
(x)k2
â‰¤ Ïƒ2
, kâˆ‡f(k)
(x) âˆ’ âˆ‡f(`)
(x)k2
â‰¤ Î¶2
, âˆ€ Î¾(k)
âˆ¼ D(k)
, âˆ€k, ` âˆˆ [K].
Note that Assumption 1 is stronger than directly assuming f(k)â€™s are Lipschitz smooth (which we will refer
to as the averaged gradient Lipschitz smooth condition), but it is still a rather standard assumption in SGD
analysis. For example it has been used in analyzing centralized SGD algorithms such as SPIDER [5], SNVRG [6],
STORM [7] (and many others) as well as in FL algorithms such as MIME [17] and Fed-GLOMO [18]. The
second relation in Assumption 2-(ii) quantifies the data heterogeneity, and we call Î¶ > 0 as the heterogeneity
parameter. This is a typical assumption required to evaluate the performance of FL algorithms. If data
distributions across individual WNs are identical, i.e., D(k) = D(`) for all k, ` âˆˆ [K] then we have Î¶ = 0.
Next, we define the -stationary solution for non-convex optimization problems, as well as quantify the
computation and communication complexities to achieve an -stationary point.
Definition 2.1 (-Stationary Point). A point x is called -stationary if kâˆ‡f(x)k2 â‰¤ . Moreover, a stochastic
algorithm is said to achieve an -stationary point in t iterations if E[kâˆ‡f(xt)k2] â‰¤ , where the expectation is
over the stochasticity of the algorithm until time instant t.
Definition 2.2 (Sample complexity). We assume an Incremental First-order Oracle (IFO) framework [34],
where, given a sample Î¾(k) âˆ¼ D(k) at the kth node and iterate x, the oracle returns (f(k)(x; Î¾(k)), âˆ‡f(k)(x; Î¾(k))).
Each access to the oracle is counted as a single IFO operation. We measure the sample (and computational)
complexity in terms of the total number of calls to the IFO by all WNs to achieve an -stationary point given
in Definition 2.1.
4
Definition 2.3 (Communication complexity). We define a communication round as a one back-and-forth
sharing of parameters between the WNs and the SN. Then the communication complexity is defined to be the
total number of communication rounds between any WN and the SN required to achieve an -stationary point
given in Definition 2.1.
3 The STEM algorithm and the trade-off analysis
In this section, we discuss the proposed algorithm and present the main results. The key in the algorithm
design is to carefully balance all the four design elements mentioned in Sec. 1, so that sufficient and useful
progress can be made between two rounds of communication.
Let us discuss the key steps of STEM, listed in Algorithm 1. In Step 10, each node locally updates its model
parameters using the local direction dk
t , computed by using b stochastic gradients at two consecutive iterates
x
(k)
t+1 and x
(k)
t . After every I local steps, the WNs share their current local models {x
(k)
t+1}K
k=1 and directions
{d
(k)
t+1}K
k=1 with the SN. The SN aggregates these quantities, and performs a server-side momentum step, before
returning xÌ„t+1 and Â¯
dt+1 to all the WNs. Because both the WNs and the SN perform momentum based updates,
we call the algorithm a stochastic two-sided momentum algorithm. The key parameters are: b the minibatch
size, I the local update steps between two communication rounds, {Î·t} the stepsizes, and {at} the momentum
parameters.
One key technical innovation of our algorithm design is to identify the most suitable way to incorporate
momentum based directions in FL algorithms. Although the momentum-based gradient estimator itself is not
new and has been used in the literature before (see e.g., in [7,8] and [17,18] to improve the sample complexities
of centralized and decentralized stochastic optimization problems, respectively), it is by no means clear if and
how it can contribute to improve the communication complexity of FL algorithms. We show that in the FL
setting, the local directions together with the local models have to be aggregated by the SN so to avoid being
influenced too much by the local data. More importantly, besides the WNs, the SN also needs to perform
updates using the (aggregated) momentum directions. Finally, such two-sided momentum updates have to be
done carefully with the correct choice of minibatch size b, and the local update frequency I. Overall, it is the
judicious choice of all these design elements that results in the optimal sample and communication complexities.
Next, we present the convergence guarantees of the STEM algorithm.
3.1 Main results: convergence guarantees for STEM
In this section, we analyze the performance of STEM. We first present our main result, and then provide
discussions about a few parameter choices. In the next subsection, we discuss a special case of STEM related
to the classical FedAvg and minibatch SGD algorithms.
Theorem 3.1. Under the Assumptions 1 and 2, suppose the stepsize sequence is chosen as:
Î·t =
ÎºÌ„
(wt + Ïƒ2t)1/3
, (2)
where we define :
ÎºÌ„ =
(bK)2/3Ïƒ2/3
L
, wt = max

2Ïƒ2
, 4096L3
I3
ÎºÌ„3
âˆ’ Ïƒ2
t,
c3ÎºÌ„3
4096L3I3

.
Further, let us set c = 64L2
bK + Ïƒ2
24ÎºÌ„3LI
= L2

64
bK + 1
24(bK)2I

, and set the initial batch size as B = bI; set the
local updates I and minibatch size b as follows:
I = O (T/K2
)Î½/3

, b = O (T/K2
)1/2âˆ’Î½/2

(3)
where Î½ satisfies Î½ âˆˆ [0, 1]. Then for STEM the following holds:
5
Algorithm 1 The Stochastic Two-Sided Momentum (STEM) Algorithm
1: Input: Parameters: c > 0, the number of local updates I, batch size b, stepsizes {Î·t}.
2: Initialize: Iterate x
(k)
1 = xÌ„1 = 1
K
PK
k=1 x
(k)
1 , descent direction d
(k)
1 = Â¯
d1 = 1
K
PK
k=1 d
(k)
1 with d
(k)
1 =
1
B
P
Î¾
(k)
1 âˆˆB
(k)
1
âˆ‡f(k)(x
(k)
1 ; Î¾
(k)
1 ) and |B
(k)
1 | = B for k âˆˆ [K].
3: Perform: x
(k)
2 = xk
1 âˆ’ Î·1d
(k)
1 , âˆ€ k âˆˆ [K]
4: for t = 1 to T do
5: for k = 1 to K do # at the WN
6: d
(k)
t+1 =
1
b
X
Î¾
(k)
t+1âˆˆB
(k)
t+1
âˆ‡f(k)
(x
(k)
t+1; Î¾
(k)
t+1)+(1âˆ’at+1)

d
(k)
t âˆ’
1
b
X
Î¾
(k)
t+1âˆˆB
(k)
t+1
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t+1)

with |B
(k)
t+1| = b, at+1 =cÎ·2
t
7: if t mod I = 0 then # at the SN
8: d
(k)
t+1 = Â¯
dt+1 := 1
K
PK
k=1 d
(k)
t+1
9: x
(k)
t+2 := xÌ„t+1 âˆ’ Î·t+1
Â¯
dt+1 = 1
K
PK
k=1 x
(k)
t+1 âˆ’ Î·t+1
Â¯
dt+1 # server-side momentum step
10: else x
(k)
t+2 = x
(k)
t+1 âˆ’ Î·t+1d
(k)
t+1 # worker-side momentum step
11: end if
12: end for
13: end for
14: Return: xÌ„a chosen uniformly randomly from {xÌ„t}T
t=1
(i) For xÌ„a chosen according to Algorithm 1, we have:
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
K2Î½/3T1âˆ’Î½/3

+ OÌƒ

Ïƒ2
K2Î½/3T1âˆ’Î½/3

+ OÌƒ

Î¶2
K2Î½/3T1âˆ’Î½/3

. (4)
(ii) For any Î½ âˆˆ [0, 1], we have
Sample Complexity: The sample complexity of STEM is OÌƒ(âˆ’3/2). This implies that each WN requires at
most OÌƒ(Kâˆ’1âˆ’3/2) gradient computations, thereby achieving linear speedup with the number of WNs present
in the network.
Communication Complexity: The communication complexity of STEM is OÌƒ(âˆ’1).
The proof of this result is relegated to the Supplemental Material. A few remarks are in order.
Remark 1 (Near-Optimal sample and communication complexities). Theorem 3.1 suggests that when I and b
are selected appropriately, then STEM achieves OÌƒ(âˆ’3/2) and OÌƒ(âˆ’1) sample and communication complexities.
Taking them separately, these complexity bounds are the best achievable by the existing FL algorithms (upto
logarithmic factors regardless of sample or batch Lipschitz smooth assumption) [35]; see Table 1. We note
that the O(âˆ’3/2) complexity is the best possible that can be achieved by centralized SGD with the sample
Lipschitz gradient assumption; see [5]. On the other hand, the O(âˆ’1) complexity bound is also likely to be the
optimal, since in [9] the authors showed that even when the local steps use a class of (deterministic) first-order
algorithms, O(âˆ’1) is the best achievable communication complexity. The only difference is that [9] does not
explicitly assume the intra-node variance bound (i.e., the second relation in Assumption 2-(ii)). We leave the
precise characterization of the communication lower bound with intra-node variance as future work.
Remark 2 (The Optimal Batch Sizes and Local Updates Trade-off). The parameter Î½ âˆˆ [0, 1] is used to balance
the local minibatch sizes b, and the number of local updates I. Eqs. in (3) suggest that when Î½ increases
from 0 to 1, b decreases and I increases. Specifically, if Î½ = 1, then b is only O(1) but I = O(T1/3/K2/3). In
this case, each WN chooses a small minibatch while executing multiple local updates, and STEM resembles a
FedAvg (a.k.a. Local SGD) algorithm but with double-sided momentum update directions, and is referred to
as Fed STEM. In contrast, if Î½ = 0, then b = O(T1/2/K) but I is only O(1). In this case, each WN chooses
6
a large batch size while executing only a few, or even one, local updates, and STEM resembles the Minibatch
SGD, but again with different update directions, and is referred to as Minibatch STEM. Such a trade-off can be
seen in Fig. 1(b). Due to space limitation, these two special cases will be precisely stated in the supplementary
materials as corollaries of Theorem 3.1.
Remark 3 (The Sub-Optimal Batch Sizes and Local Updates Trade-off). From our proof (Theorem A.10
included in the supplemental material), we can see that STEM requires OÌƒ max

(bÂ·I)âˆ’1, Kâˆ’1âˆ’3/2

samples
and OÌƒ max

âˆ’1, (b Â· I)âˆ’1Kâˆ’1âˆ’3/2

and communication rounds. According to the above expressions, if b Â· I
increases beyond O(Kâˆ’1âˆ’1/2), then the sample complexity will increase from the optimal OÌƒ(âˆ’3/2); otherwise,
the optimal sample complexity OÌƒ(âˆ’3/2) is maintained. On the other hand, if bÂ·I decreases beyond O(Kâˆ’1âˆ’1/2),
the communication complexity increases from OÌƒ(âˆ’1). For instance, if we choose b = O(1) and I = O(1) the
communication complexity becomes OÌƒ(âˆ’3/2). This trade-off is illustrated in Figure 1(a), where we maintain
the optimal sample complexity, while changing b and I to generate the trade-off surface.
Remark 4 (Data Heterogeneity). The term OÌƒ

Î¶2
K2Î½/3T1âˆ’Î½/3

in the gradient bound (4) captures the effect of the
heterogeneity of data across WNs, where Î¶ is the parameter characterizing the intra-node variance and has been
defined in Assumption 2-(ii). Highly heterogeneous data with large Î¶2 can adversely impact the performance
of STEM. Note that such a dependency on Î¶ also appears in other existing FL algorithms, such as [9,14,18].
However, there is one special case of STEM that does not depend on the parameter Î¶. This is the case where
I = 1, i.e., the minibatch SGD counterpart of STEM where only a single local iteration is performed between
two communication rounds. We have the following corollary.
Corollary 1 (Minibatch STEM). Under Assumptions 1 and 2 , and choose the algorithm parameters as
in Theorem 3.1. At each WN, choose I = 1, b = (T/K2
)1/2
, and the initial batch size B = b Â· I. Then
STEM satisfies:
(i) For xÌ„a chosen according to Algorithm 1, we have
Ekâˆ‡f(xÌ„a)k2
= O
f(xÌ„1) âˆ’ fâˆ—
T

+ OÌƒ
Ïƒ2
T

.
(ii) Minibatch STEM achieves OÌƒ(âˆ’3/2) sample and OÌƒ(âˆ’1) communication complexity.
Next, we show that FedAvg also exhibits a trade-off similar to that of STEM but with worse sample and
communication complexities.
3.2 Special cases: The FedAvg algorithm
We briefly discuss another interesting special case of STEM, where the local momentum update is replaced
by the conventional SGD (i.e., at = 1, âˆ€ t), while the server does not perform the momentum update (i.e.,
Â¯
dt = 0, âˆ€ t). This is essentially the classical FedAvg algorithm, just that it balances the number of local updates
I and the minibatch size b. We show that this algorithm also exhibits a trade-off between b and I and on the
trade-off curve it achieves O(âˆ’2) sample complexity and O(âˆ’3/2) communication complexity.
Theorem 3.2 (The FedAvg Algorithm). Under Assumptions 1 and 2, suppose the stepsize is chosen as:
Î· =
q
bK
T ; Let us set:
I = O (T/K3
)Î½/4

, b = O (T/K3
)1/3âˆ’Î½/3

(5)
where Î½ âˆˆ [0, 1] is a constant. Then for FedAvg with T â‰¥ 81L2I2bK, the following holds
(i) For xÌ„a chosen according to Algorithm 2, we have
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
KÎ½/2T2/3âˆ’Î½/6

+ O

Ïƒ2
KÎ½/2T2/3âˆ’Î½/6

+ O

Î¶2
KÎ½/2T2/3âˆ’Î½/6

.
7
Algorithm 2 The FedAvg Algorithm
1: Input: {Î·t}T
t=0; I, the # of local updates per communication rounds; b, the minibatch sizes.
2: for t = 1 to T do
3: for k = 1 to K do
4: d
(k)
t = 1
b
P
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)(x
(k)
t ; Î¾
(k)
t ) with |B
(k)
t | = b
5: x
(k)
t+1 = x
(k)
t âˆ’ Î·td
(k)
t
6: if t mod I = 0 then
7: x
(k)
t+1 = xÌ„t+1 = 1
K
PK
k=1 x
(k)
t+1
8: end if
9: end for
10: end for
11: Return: xÌ„a chosen uniformly randomly from {xÌ„t}T
t=1
(ii) For any choice of Î½ âˆˆ [0, 1] we have:
Sample Complexity: The sample complexity of FedAvg is O(âˆ’2). This implies that each WN requires at
most O(Kâˆ’1âˆ’2) gradient computations, thereby achieving linear speedup with the number of WNs in the
network.
Communication Complexity: The communication complexity of FedAvg is O(âˆ’3/2).
Note that the requirement on T being lower bounded is only relevant for theoretical purposes, a similar
requirement was also imposed in [14] to prove convergence. Again, the parameter Î½ âˆˆ [0, 1] in the statement
of Theorem 3.2 balances I and b at each WN while maintaining state-of-the-art sample and communication
complexities; please see Table 1 for a comparison of those bounds with existing FedAvg bounds. For Î½ = 1,
FedAvg (cf. Theorem 3.2) reduces to FedAvg proposed in [12,14] and for Î½ = 0, the algorithm can be viewed
as a large batch FedAvg with constant local updates [15, 16]. Note that similar to STEM, it is known that
for I = 1, the Minibatch SGDâ€™s performance is independent of the heterogeneity parameter, Î¶ [13]. We also
point out that if Algorithm 1 uses Nesterovâ€™s or Polyakâ€™s momentum [14] at local WNs instead of the recursive
momentum estimator we get the same guarantees as in Theorem 3.2.
In summary, this section established that once the WNâ€™s and the SNâ€™s update directions (SGD in FedAvg
and momentum based directions in STEM) are fixed, there exists a sequence of optimal choices of the number
of local updates I, and the batch sizes b, which guarantees the best possible sample and communication
complexities for the particular algorithm. The trade-off analysis presented in this section provides some useful
guidelines for how to best select b and I in practice. Our subsequent numerical results will also verify that if b
or I are not chosen judiciously, then the practical performance of the algorithms can degrade significantly.
4 Numerical results
In this section, we validate the proposed STEM algorithm and compare its performance with the de facto
standard FedAvg [11] and recently proposed SCAFFOLD [15]. The goal of our experiments are three-fold: (1)
To show that STEM performs on par, if not better, compared to other algorithms in both moderate and high
heterogeneity settings, (2) there are multiple ways to reach the desired solution accuracy, one can either choose
a large batch size and perform only a few local updates or select a smaller batch size and perform multiple
local updates, and finally, (3) if the local updates and the batch sizes are not chosen appropriately, the WNs
might need to perform excessive computations to achieve the desired solution accuracy, thereby slowing down
convergence.
Data and Parameter Settings: We compare the algorithms for image classification tasks on CIFAR-10 and
MNIST data sets with 100 WNs in the network. For both CIFAR-10 and MNIST, each WN implements a
two-hidden-layer convolutional neural network (CNN) architecture followed by three linear layers for CIFAR-10
8
Figure 2: Training loss and testing accuracy for classification on CIFAR-10 dataset against the number of
communication rounds for moderate heterogeneity setting with b = 8 and I = 61.
Figure 3: Training loss and testing accuracy for classification on CIFAR-10 dataset against the number of
communication rounds for moderate heterogeneity setting with b = 64 and I = 7.
and two for MNIST. All the experiments are implemented on a single NVIDIA Quadro RTX 5000 GPU. We
consider two settings, one with moderate heterogeneity and the other with high heterogeneity. For both settings,
the data is partitioned into disjoint sets among the WNs. In the moderate heterogeneity setting, the WNs have
access to partitioned data from all the classes but for the high heterogeneity setting the data is partitioned
such that each WN can access data from only a subset (5 out of 10 classes) of classes. For CIFAR-10 (resp.
MNIST), each WN has access to 490 (resp. 540) samples for training and 90 (resp. 80) samples for testing
purposes.
For STEM, we set wt = 1, c = cÌ„/ÎºÌ„2 and tune for ÎºÌ„ and cÌ„ in the range ÎºÌ„ âˆˆ [0.01, 0.5] and cÌ„ âˆˆ [1, 10],
respectively (cf. Theorem 3.1). We note that for small batch sizes ÎºÌ„ âˆˆ [0.01, 0.1], whereas for larger batch sizes
ÎºÌ„ âˆˆ [0.3, 0.5] perform well. We diminish Î·t according to (2) in each epoch2. For SCAFFOLD and FedAvg,
the stepsize choices of 0.1 and 0.01 perform well for large and smaller batch sizes, respectively. We use cross
entropy as the loss function and evaluate the algorithm performance under a few settings discussed next.
Discussion: In Figures 2 and 3, we compare the training and testing performance of STEM with FedAvg and
SCAFFOLD for CIFAR-10 dataset under moderate heterogeneity setting. For Figure 2, we choose b = 8 and
I = 61, whereas for Figure 3, we choose b = 64 and I = 7. We first note that for both cases STEM performs
2
We define epoch as a single pass over the whole data.
9
Figure 4: Training loss and testing accuracy for classification on CIFAR-10 dataset against the number of
communication rounds for high heterogeneity setting with b = 8 and I = 61.
Figure 5: Training loss and the testing accuracy for classification on MNIST data set against the number of
samples accessed at each WN for high heterogeneity setting with b = 8.
better than FedAvg and SCAFFOLD. Moreover, observe that for both settings, small batches with multiple
local updates (Figure 2) and large batches with few local updates (Figure 3), the algorithms converge with
approximately similar performance, corroborating the theoretical analysis (see Discussion in Section 1). Next,
in Figure 4 we evaluate the performance of the proposed algorithms on CIFAR-10 with high heterogeneity
setting for b = 8 and I = 61. We note that STEM outperforms FedAvg and SCAFFOLD in this setting as well.
Finally, with the next set of experiments we emphasize the importance of choosing b and I carefully. In Figure
5, we compare the training and testing performance of the algorithms against the number of samples accessed
at each WN for the classification task on MNIST dataset with high heterogeneity. We fix b = 8 and conduct
experiments under two settings, one with I = 67, and the other with I = 536 local updates at each WN. Note
that although a large number of local updates might lead to fewer communication rounds but it can make the
sample complexity extremely high as is demonstrated by Figure 5. For example, Figure 5 shows that to reach
testing accuracy of 96 âˆ’ 97% with I = 67, STEM requires approximately 5000 âˆ’ 6000 samples, in contrast with
I = 536 it requires more than 25000 samples at each WN. Similar behavior can be observed if we fix I > 1
and increase the local batch sizes. This implies not choosing the local updates and the batch sizes judiciously
might lead to increased sample complexity.
10
5 Conclusion
In this work, we proposed a novel algorithm STEM, for distributed stochastic non-convex optimization with
applications to FL. We showed that STEM reaches an -stationary point with OÌƒ(âˆ’3/2) sample complexity
while achieving linear speed-up with the number of WNs. Moreover, the algorithm achieves a communication
complexity of OÌƒ(âˆ’1). We established a (optimal) trade-off that allows interpolation between varying choices
of local updates and the batch sizes at each WN while maintaining (near optimal) sample and communication
complexities. We showed that FedAvg (a.k.a LocalSGD) also exhibits a similar trade-off while achieving worse
complexities. Our results provide guidelines to carefully choose the update frequency, directions, and minibatch
sizes to achieve the best performance. The future directions of this work include developing lower bounds on
communication complexity that establishes the tightness of the analysis conducted in this work.
11
References
[1] J. KonecÌŒnyÌ€, H. B. McMahan, D. Ramage, and P. RichtaÌrik, â€œFederated optimization: Distributed machine
learning for on-device intelligence,â€ arXiv preprint arXiv:1610.02527, 2016.
[2] M. Li, D. G. Andersen, A. J. Smola, and K. Yu, â€œCommunication efficient distributed machine learning
with the parameter server,â€ in Advances in Neural Information Processing Systems 27, Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2014, pp.
19â€“27.
[3] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang
et al., â€œLarge scale distributed deep networks,â€ in Advances in neural information processing systems,
2012, pp. 1223â€“1231.
[4] T. LeÌauteÌ and B. Faltings, â€œProtecting privacy through distributed computation in multi-agent decision
making,â€ Journal of Artificial Intelligence Research, vol. 47, pp. 649â€“695, 2013.
[5] C. Fang, C. J. Li, Z. Lin, and T. Zhang, â€œSpider: Near-optimal non-convex optimization via stochastic
path-integrated differential estimator,â€ in Advances in Neural Information Processing Systems, 2018, pp.
689â€“699.
[6] D. Zhou, P. Xu, and Q. Gu, â€œStochastic nested variance reduction for nonconvex optimization,â€ arXiv
preprint arXiv:1806.07811, 2018.
[7] A. Cutkosky and F. Orabona, â€œMomentum-based variance reduction in non-convex SGD,â€ in Advances in
Neural Information Processing Systems 32. Curran Associates, Inc., 2019, pp. 15 236â€“15 245.
[8] Q. Tran-Dinh, N. H. Pham, D. T. Phan, and L. M. Nguyen, â€œHybrid stochastic gradient descent algorithms
for stochastic nonconvex optimization,â€ arXiv preprint arXiv:1905.05920, 2019.
[9] X. Zhang, M. Hong, S. Dhople, W. Yin, and Y. Liu, â€œFedpd: A federated learning framework with optimal
rates and adaptivity to non-iid data,â€ 2020.
[10] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, â€œFederated optimization in
heterogeneous networks,â€ arXiv preprint arXiv:1812.06127, 2018.
[11] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, â€œCommunication-efficient learning
of deep networks from decentralized data,â€ in Artificial Intelligence and Statistics. PMLR, 2017, pp.
1273â€“1282.
[12] H. Yu, S. Yang, and S. Zhu, â€œParallel restarted sgd with faster convergence and less communication:
Demystifying why model averaging works for deep learning,â€ 2018.
[13] B. Woodworth, K. K. Patel, and N. Srebro, â€œMinibatch vs local sgd for heterogeneous distributed learning,â€
2020.
[14] H. Yu, R. Jin, and S. Yang, â€œOn the linear speedup analysis of communication efficient momentum sgd
for distributed non-convex optimization,â€ 2019.
[15] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh, â€œScaffold: Stochastic controlled
averaging for federated learning,â€ in International Conference on Machine Learning. PMLR, 2020, pp.
5132â€“5143.
[16] H. Yang, M. Fang, and J. Liu, â€œAchieving linear speedup with partial worker participation in non-iid
federated learning,â€ 2021.
12
[17] S. P. Karimireddy, M. Jaggi, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and A. T. Suresh, â€œMime:
Mimicking centralized stochastic algorithms in federated learning,â€ arXiv preprint arXiv:2008.03606, 2020.
[18] R. Das, A. Hashemi, S. Sanghavi, and I. S. Dhillon, â€œImproved convergence rates for non-convex federated
learning with compression,â€ arXiv preprint arXiv:2012.04061, 2020.
[19] B. Woodworth, K. K. Patel, S. U. Stich, Z. Dai, B. Bullins, H. B. McMahan, O. Shamir, and N. Srebro,
â€œIs local sgd better than minibatch sgd?â€ arXiv preprint arXiv:2002.07839, 2020.
[20] H. Yu and R. Jin, â€œOn the computation and communication complexity of parallel sgd with dynamic batch
sizes for stochastic non-convex optimization,â€ in International Conference on Machine Learning. PMLR,
2019, pp. 7174â€“7183.
[21] J. Wang and G. Joshi, â€œCooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms,â€ 2018.
[22] A. Khaled, K. Mishchenko, and P. RichtaÌrik, â€œBetter communication complexity for local sgd,â€ arXiv,
2019.
[23] S. U. Stich, â€œLocal sgd converges fast and communicates little,â€ arXiv preprint arXiv:1805.09767, 2018.
[24] T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi, â€œDonâ€™t use large mini-batches, use local sgd,â€ 2018.
[25] F. Zhou and G. Cong, â€œOn the convergence properties of a k-step averaging stochastic gradient
descent algorithm for nonconvex optimization,â€ in Proceedings of the Twenty-Seventh International Joint
Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence
Organization, 7 2018, pp. 3219â€“3227. [Online]. Available: https://doi.org/10.24963/ijcai.2018/447
[26] F. Sattler, S. Wiedemann, K.-R. MuÌˆller, and W. Samek, â€œRobust and communication-efficient federated
learning from non-iid data,â€ IEEE transactions on neural networks and learning systems, vol. 31, no. 9,
pp. 3400â€“3413, 2019.
[27] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, â€œFederated learning with non-iid data,â€ arXiv
preprint arXiv:1806.00582, 2018.
[28] J. Wang, V. Tantia, N. Ballas, and M. Rabbat, â€œSlowmo: Improving communication-efficient distributed
sgd with slow momentum,â€ arXiv preprint arXiv:1910.00643, 2019.
[29] X. Liang, S. Shen, J. Liu, Z. Pan, E. Chen, and Y. Cheng, â€œVariance reduced local sgd with lower
communication complexity,â€ arXiv preprint arXiv:1912.12844, 2019.
[30] P. Sharma, P. Khanduri, S. Bulusu, K. Rajawat, and P. K. Varshney, â€œParallel restarted SPIDER â€“
communication efficient distributed nonconvex optimization with optimal computation complexity,â€ arXiv
preprint arXiv:1912.06036, 2019.
[31] S. J. Reddi, S. Kale, and S. Kumar, â€œOn the convergence of adam and beyond,â€ arXiv preprint
arXiv:1904.09237, 2019.
[32] A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. Stich, â€œA unified theory of decentralized sgd with
changing topology and local updates,â€ in International Conference on Machine Learning. PMLR, 2020,
pp. 5381â€“5393.
[33] R. Johnson and T. Zhang, â€œAccelerating stochastic gradient descent using predictive variance reduction,â€
in Advances in Neural Information Processing Systems 26. Curran Associates, Inc., 2013, pp. 315â€“323.
13
[34] L. Bottou, F. E. Curtis, and J. Nocedal, â€œOptimization methods for large-scale machine learning,â€ SIAM
Review, vol. 60, no. 2, pp. 223â€“311, 2018.
[35] Y. Drori and O. Shamir, â€œThe complexity of finding stationary points with stochastic gradient descent,â€
in International Conference on Machine Learning. PMLR, 2020, pp. 2658â€“2667.
14
Appendix
The organization of the Appendix is given below. In Appendix A, we present the proof of the convergence
guarantees for STEM (Algorithm 1) stated in Section 3.1. In Appendix B, we present the proof of the
convergence guarantees associated with FedAvg (Algorithm 2) stated in Section 3.2. Finally, in Appendix
C we state some useful lemmas utilized throughout the proofs.
A Proofs of Convergence Guarantees for STEM
In this section we present the proofs for the convergence of STEM. First, we present some preliminary lemmas
to be utilized throughout the proof. For readerâ€™s convenience here we restate the steps of the Algorithm 1 in
Algorithm 3.
A.1 Preliminary lemmas
Lemma A.1. Define eÌ„t := Â¯
dt âˆ’ 1
K
PK
k=1 âˆ‡f(k)(x
(k)
t ), then the iterates generated according to Algorithm 3 satisfy
E
"
(1 âˆ’ at)eÌ„tâˆ’1,
1
K
K
X
k=1
1
b
X
Î¾
(k)
t âˆˆB
(k)
t

âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 #
= 0,
where the expectation is w.r.t. the stochasticity of the algorithm.
Proof. Note that, given the filtration
Ft = Ïƒ(x
(k)
1 , x
(k)
2 , . . . , x
(k)
t for all k âˆˆ [K]),
the gradient error term, eÌ„tâˆ’1, is fixed. The only randomness in the left hand side of the statement of the Lemma
is with respect to Î¾
(k)
t , for all k âˆˆ [K]. This implies that we have
E
"
(1 âˆ’ at)eÌ„tâˆ’1,
1
K
K
X
k=1
1
b
X
Î¾
(k)
t âˆˆB
(k)
t

âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 #
= E
"
(1 âˆ’ at)eÌ„tâˆ’1,
1
K
K
X
k=1
E

1
b
X
Î¾
(k)
t âˆˆB
(k)
t

âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)

Ft
#
.
The result then follows from the fact that Î¾
(k)
t is chosen uniformly randomly at each k âˆˆ [K], and we have from
(Assumption 2) that: E

âˆ‡f(k)(x
(k)
t ; Î¾
(k)
t )

= âˆ‡f(k)(x
(k)
t ). This implies we have
E

1
b
X
Î¾
(k)
t âˆˆB
(k)
t
h
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
i
Ft

= 0
for all k âˆˆ [K].
Therefore, the lemma is proved.
15
Lemma A.2. For k, ` âˆˆ [K] with k 6= `, the iterates generated according to Algorithm 3 satisfy
E
" X
Î¾
(k)
t âˆˆB
(k)
t
h
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
i
,
X
Î¾
(`)
t âˆˆB
(`)
t
h
âˆ‡f(`)
(x
(`)
t ; Î¾
(`)
t ) âˆ’ âˆ‡f(`)
(x
(`)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(`)
(x
(`)
tâˆ’1; Î¾
(`)
t ) âˆ’ âˆ‡f(`)
(x
(`)
tâˆ’1)
i#
= 0
Proof. Again note from the fact that conditioned on Ft the batches B
(k)
t and B
(`)
t for all k, ` âˆˆ [K] with k 6= `
across WNs are chosen independently of each other. Therefore, we have
E
" X
Î¾
(k)
t âˆˆB
(k)
t
h
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
i
,
X
Î¾
(`)
t âˆˆB
(`)
t
h
âˆ‡f(`)
(x
(`)
t ; Î¾
(`)
t ) âˆ’ âˆ‡f(`)
(x
(`)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(`)
(x
(`)
tâˆ’1; Î¾
(`)
t ) âˆ’ âˆ‡f(`)
(x
(`)
tâˆ’1)
i#
= E
"
E
 X
Î¾
(k)
t âˆˆB
(k)
t
h
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
i
Ft

,
E
 X
Î¾
(`)
t âˆˆB
(`)
t
h
âˆ‡f(`)
(x
(`)
t ; Î¾
(`)
t ) âˆ’ âˆ‡f(`)
(x
(`)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(`)
(x
(`)
tâˆ’1; Î¾
(`)
t ) âˆ’ âˆ‡f(`)
(x
(`)
tâˆ’1)
i
Ft
#
.
The result then follows from the fact that Î¾
(k)
t is chosen uniformly randomly across k âˆˆ [K] and we have from
the unbiased gradient Assumption 2 that: E

âˆ‡f(k)(x
(k)
t ; Î¾
(k)
t )

= âˆ‡f(k)(x
(k)
t ). This implies we have
E
 X
Î¾
(k)
t âˆˆB
(k)
t
h
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
i
Ft

= 0
for all k âˆˆ [K].
Therefore, the lemma is proved.
Lemma A.3. For eÌ„1 := Â¯
d1 âˆ’ 1
K
PK
k=1 âˆ‡f(k)(x
(k)
1 ) where Â¯
d1 chosen according to Algorithm 3, we have:
EkeÌ„1k2
â‰¤
Ïƒ2
KB
.
16
Proof. Using the definition of eÌ„1 we have:
EkeÌ„1k2
= E Â¯
d1 âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
1 )
2
(a)
= E
1
K
K
X
k=1
1
B
X
Î¾
(k)
1 âˆˆB
(k)
1
âˆ‡f(k)
(x
(k)
1 ; Î¾
(k)
1 ) âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
1 )
2
(b)
=
1
K2B2
K
X
k=1
E
X
Î¾
(k)
1 âˆˆB
(k)
1
âˆ‡f(k)
(x
(k)
1 ; Î¾
(k)
1 ) âˆ’ âˆ‡f(k)
(x
(k)
1 )
 2
(c)
=
1
K2B2
K
X
k=1
X
Î¾
(k)
1 âˆˆB
(k)
1
E âˆ‡f(k)
(x
(k)
1 ; Î¾
(k)
1 ) âˆ’ âˆ‡f(k)
(x
(k)
1 )
2
(d)
â‰¤
Ïƒ2
KB
.
where (a) follows from the definition of Â¯
d1 in Algorithm 1 and (b) follows from the following: From Assumption
2, given Ft we have: E

âˆ‡f(k)(x
(k)
1 ; Î¾
(k)
1 )

= âˆ‡f(k)(x
(k)
1 ), for all k âˆˆ [K]. Moreover, given Ft the samples Î¾
(k)
1
and Î¾
(`)
1 at the kth and the `th WNs are chosen uniformly randomly, and independent of each other for all
k, ` âˆˆ [K] and k 6= `.
E
 X
Î¾
(k)
1 âˆˆB
(k)
1
âˆ‡f(k)
(x
(k)
1 ; Î¾
(k)
1 ) âˆ’ âˆ‡f(k)
(x
(k)
1 )

,
X
Î¾
(`)
1 âˆˆB
(`)
1
âˆ‡f(`)
(x
(`)
1 ; Î¾
(`)
1 ) âˆ’ âˆ‡f(`)
(xÌ„1)

#
= E
 X
Î¾
(k)
1 âˆˆB
(k)
1
E

âˆ‡f(k)
(x
(k)
1 ; Î¾
(k)
1 ) âˆ’ âˆ‡f(k)
(x
(k)
1 ) Ft

| {z }
=0
,
X
Î¾
(`)
1 âˆˆB
(`)
1
E

âˆ‡f(`)
(x
(`)
1 ; Î¾
(`)
1 ) âˆ’ âˆ‡f(`)
(x
(`)
1 ) Ft

| {z }
=0

= 0.
The equality (c) follows from the fact that the samples Î¾
(k)
1 âˆˆ B
(k)
1 for all k âˆˆ [K] are chosen independently
of each other. Then we conclude (c) from an argument similar to that of (b). Finally, (d) results from the
intra-node variance bound given in Assumption 2.
Hence, the lemma is proved.
Next, using the preliminary lemmas developed in this section we prove the main results of the work.
A.2 Proof of Main Results: STEM
In this section, we utilize the results developed in earlier sections to derive the main result of the paper presented
in Section 3.1. Throughout the section we assume Assumptions 1 and 2 to hold. Before proceeding, we first
define some notations.
We define tÌ„s := sI +1 with s âˆˆ [S]. Note from Algorithm 3 that at (sÃ—I)th iteration, i.e., when t mod I = 0,
the descent directions, {d
(k)
t }K
k=1, corresponding to t = (tÌ„s)th time instant are shared with the SN. At the same
time instant, the iterates, {x
(k)
t }K
k=1 are also shared and the SN performs the â€œserver side momentum stepâ€ (cf.
Step 9 of Algorithm 3).
A.2.1 Proof of Descent Lemma
In the first step, we bound the error accumulation via the iterates generated by Algorithm 3.
17
Algorithm 3 The Stochastic Two-Sided Momentum (STEM) Algorithm
1: Input: Parameters: c > 0, the number of local updates I, batch size b, stepsizes {Î·t}.
2: Initialize: Iterate x
(k)
1 = xÌ„1 = 1
K
PK
k=1 x
(k)
1 , descent direction d
(k)
1 = Â¯
d1 = 1
K
PK
k=1 d
(k)
1 with d
(k)
1 =
1
B
P
Î¾
(k)
1 âˆˆB
(k)
1
âˆ‡f(k)(x
(k)
1 ; Î¾
(k)
1 ) and |B
(k)
1 | = B for k âˆˆ [K].
3: Perform: x
(k)
2 = xk
1 âˆ’ Î·1d
(k)
1 , âˆ€ k âˆˆ [K]
4: for t = 1 to T do
5: for k = 1 to K do # at the WN
6: d
(k)
t+1 =
1
b
X
Î¾
(k)
t+1âˆˆB
(k)
t+1
âˆ‡f(k)
(x
(k)
t+1; Î¾
(k)
t+1)+(1âˆ’at+1)

d
(k)
t âˆ’
1
b
X
Î¾
(k)
t+1âˆˆB
(k)
t+1
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t+1)

with |B
(k)
t+1| = b, at+1 =cÎ·2
t
7: if t mod I = 0 then # at the SN
8: d
(k)
t+1 = Â¯
dt+1 := 1
K
PK
k=1 d
(k)
t+1
9: x
(k)
t+2 := xÌ„t+1 âˆ’ Î·t+1
Â¯
dt+1 = 1
K
PK
k=1 x
(k)
t+1 âˆ’ Î·t+1
Â¯
dt+1 # server-side momentum step
10: else x
(k)
t+2 = x
(k)
t+1 âˆ’ Î·t+1d
(k)
t+1 # worker-side momentum step
11: end if
12: end for
13: end for
14: Return: xÌ„a chosen uniformly randomly from {xÌ„t}T
t=1
Lemma A.4 (Error Accumulation from Iterates). For each t âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] and s âˆˆ [S], the iterates x
(k)
t for
each k âˆˆ [K] generated from Algorithm 3 satisfy:
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤ (I âˆ’ 1)
t
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
,
where the expectation is w.r.t the stochasticity of the algorithm.
Proof. Note from Algorithm 3 and the definition of tÌ„s that at t = tÌ„sâˆ’1 with s âˆˆ [S], x
(k)
t = xÌ„t, for all k. This
implies
K
X
k=1
kx
(k)
tÌ„sâˆ’1
âˆ’ xÌ„tÌ„sâˆ’1
k2
= 0.
Therefore, the statement of the lemma holds trivially. Moreover, for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1], with s âˆˆ [S], we have
from Algorithm 3: x
(k)
t = x
(k)
tâˆ’1 âˆ’ Î·tâˆ’1d
(k)
tâˆ’1, this implies that:
x
(k)
t = x
(k)
tÌ„sâˆ’1
âˆ’
tâˆ’1
X
`=tÌ„sâˆ’1
Î·`d
(k)
` and xÌ„t = xÌ„tÌ„sâˆ’1
âˆ’
tâˆ’1
X
`=tÌ„sâˆ’1
Î·`
Â¯
d`.
18
This implies that for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1], with s âˆˆ [S] we have
K
X
k=1
kx
(k)
t âˆ’ xÌ„tk2
=
K
X
k=1
x
(k)
tÌ„sâˆ’1
âˆ’ xÌ„tÌ„sâˆ’1
âˆ’
 tâˆ’1
X
`=tÌ„sâˆ’1
Î·`d
(k)
` âˆ’
tâˆ’1
X
`=tÌ„sâˆ’1
Î·`
Â¯
d`
 2
(a)
=
K
X
k=1
tâˆ’1
X
`=tÌ„sâˆ’1
Î·`d
(k)
` âˆ’ Î·`
Â¯
d`
 2
(b)
â‰¤ (I âˆ’ 1)
tâˆ’1
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
kd
(k)
` âˆ’ Â¯
d`k2
â‰¤ (I âˆ’ 1)
t
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
kd
(k)
` âˆ’ Â¯
d`k2
,
where the equality (a) follows from the fact that x
(k)
tÌ„sâˆ’1
= xÌ„tÌ„sâˆ’1
and inequality (b) uses the Lemma C.3 along
with the fact that we have d
(k)
t = Â¯
dt for t = tÌ„sâˆ’1.
Taking expectation on both sides yields the statement of the lemma.
Next, we utilize Lemma A.4 along with the smoothness of the function f(Â·) (Assumption 1) to show descent
in the objective function value at consecutive iterates.
Lemma A.5 (Descent Lemma). With eÌ„t := Â¯
dt âˆ’ 1
K
PK
k=1 âˆ‡f(k)(x
(k)
t ), for all t âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] and s âˆˆ [S], the
iterates generated by Algorithm 3 satisfy:
Ef(xÌ„t+1) â‰¤ Ef(xÌ„t) âˆ’

Î·t
2
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+ Î·tEkeÌ„tk2
+
Î·tL2(I âˆ’ 1)
K
t
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
,
where the expectation is w.r.t the stochasticity of the algorithm.
Proof. Using the smoothness of f (Assumption 1) we have:
f(xÌ„t+1) â‰¤ f(xÌ„t) + hâˆ‡f(xÌ„t), xÌ„t+1 âˆ’ xÌ„ti +
L
2
kxÌ„t+1 âˆ’ xÌ„tk2
(a)
= f(xÌ„t) âˆ’ Î·thâˆ‡f(xÌ„t), Â¯
dti +
Î·2
t L
2
k Â¯
dtk2
(b)
= f(xÌ„t) âˆ’ Î·tk Â¯
dtk2
+ Î·th Â¯
dt âˆ’ âˆ‡f(xÌ„t), Â¯
dti +
Î·2
t L
2
k Â¯
dtk2
(c)
= f(xÌ„t) âˆ’

Î·t
2
âˆ’
Î·2
t L
2

k Â¯
dtk2
âˆ’
Î·t
2
kâˆ‡f(xÌ„t)k2
+
Î·t
2
k Â¯
dt âˆ’ âˆ‡f(xÌ„t)k2
(d)
â‰¤ f(xÌ„t) âˆ’

Î·t
2
âˆ’
Î·2
t L
2

k Â¯
dtk2
âˆ’
Î·t
2
kâˆ‡f(xÌ„t)k2
+ Î·t
Â¯
dt âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
+ Î·t
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t ) âˆ’ âˆ‡f(k)
(xÌ„t)
 2
, (6)
where equality (a) follows from the iterate update given in Step 10 of Algorithm 3, (b) results by adding and
subtracting Â¯
dt to âˆ‡f(xÌ„t) in the inner product term and using the linearity of the inner product, (c) follows
19
from the relation hx, yi = 1
2kxk2 + 1
2kyk2 âˆ’ 1
2kxâˆ’yk2, finally inequality (d) results from adding and subtracting
1
K
PK
k=1 âˆ‡f(k)(x
(k)
t ) in the last term of (c) and using Lemma C.3.
Taking expectation on both sides and considering the last term of (6), we have
E
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t ) âˆ’ âˆ‡f(k)
(xÌ„t)
 2
â‰¤
1
K
K
X
k=1
E âˆ‡f(k)
(x
(k)
t ) âˆ’ âˆ‡f(k)
(xÌ„t)
2
â‰¤
L2
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
, (7)
where the first inequality follows from Lemma C.3, and the second follows from the L-smoothness of f(k)(Â·)
(Assumption 1).
Substituting (7) in (6) and using the definition eÌ„t := Â¯
dt âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t ) we get:
Ef(xÌ„t+1) â‰¤ Ef(xÌ„t) âˆ’

Î·t
2
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+ Î·tEkeÌ„tk2
+
Î·tL2
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
. (8)
Finally, using Lemma A.4 to bound the last term of (8), we get:
Ef(xÌ„t+1) â‰¤ Ef(xÌ„t) âˆ’

Î·t
2
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+ Î·tEkeÌ„tk2
+
Î·tL2(I âˆ’ 1)
K
t
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
.
Hence, the lemma is proved.
Lemma A.5 shows that the expected descent in the function f depends on the magnitude of the expected
gradient error term eÌ„t, and the expected gradient drift across WNs, i.e., Ekd
(k)
` âˆ’ Â¯
d`k2. This implies that to
ensure sufficient descent we need to control the gradient error and the gradient drift across WNs. We achieve
this by carefully designing the number of local updates, I, at each WN, and the batch-sizes b (and initial batch
size B), that each WN uses to compute the descent direction.
Next, we present the error contraction lemma which analyzes how the term EkeÌ„tk2 contracts across time.
A.2.2 Proof of Gradient Error Contraction
Lemma A.6 (Gradient Error Contraction). Define eÌ„t := Â¯
dt âˆ’ 1
K
PK
k=1 âˆ‡f(k)(x
(k)
t ), then for every t âˆˆ [T] the
iterates generated by Algorithm 3 satisfy
EkeÌ„t+1k2
â‰¤ (1 âˆ’ at+1)2
EkeÌ„tk2
+
8(1 âˆ’ at+1)2L2
bK2
(I âˆ’ 1)
I
Î·2
t
K
X
k=1
E d
(k)
t âˆ’ Â¯
dt
2
+
4(1 âˆ’ at+1)2L2Î·2
t
bK
Ek Â¯
dtk2
+
2a2
t+1Ïƒ2
bK
,
where the expectation is w.r.t the stochasticity of the algorithm.
20
Proof. Consider the error term keÌ„tk2 as
EkeÌ„tk2
= E Â¯
dt âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
(a)
= E
1
K
K
X
k=1
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) + (1 âˆ’ at)

Â¯
dtâˆ’1 âˆ’
1
K
K
X
k=1
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t )

âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
(b)
= E
1
K
K
X
k=1
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
 
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)

+ (1 âˆ’ at)eÌ„tâˆ’1
2
,
where (a) follows from the definition of descent direction given in Step 6 of Algorithm 3; (b) follows by adding
and subtracting (1 âˆ’ at) 1
K
PK
k=1 âˆ‡f(k)(x
(k)
tâˆ’1) and using the definition of eÌ„tâˆ’1. Further simplifying the above
expression, we get
EkeÌ„tk2 (c)
= (1 âˆ’ at)2
EkeÌ„tâˆ’1k2
+
1
b2K2
E
K
X
k=1
X
Î¾
(k)
t âˆˆB
(k)
t
h 
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 i 2
(d)
= (1 âˆ’ at)2
EkeÌ„tâˆ’1k2
+
1
b2K2
K
X
k=1
E
X
Î¾
(k)
t âˆˆB
(k)
t
h
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
i 2
,
(e)
= (1 âˆ’ at)2
EkeÌ„tâˆ’1k2
+
1
b2K2
K
X
k=1
X
Î¾
(k)
t âˆˆB
(k)
t
E

âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 2
, (9)
where (c) results from expanding the norm using inner product and noting that the cross terms are zero in
expectation from Lemma A.1; (d) follows from expanding the norm using the inner products across k âˆˆ [K]
and noting that the cross term is zero in expectation from Lemma A.2; finally, (e) results from expanding the
norm using the inner product across samples used to compute the minibatch gradients and the inner product is
zero since at each node k âˆˆ [K], the samples in the minibatch, Î¾
(k)
t âˆˆ B
(k)
t , are sampled independently of each
other.
21
Now considering the 2nd term of (9) above, we have
E âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ (1 âˆ’ at) âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 2
= E (1 âˆ’ at)

âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )

âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)

+ at âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )
 2
(a)
â‰¤ 2(1 âˆ’ at)2
E âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t )

âˆ’ âˆ‡f(k)
(x
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 2
+ 2a2
t E âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
t )
2
(b)
â‰¤ 2(1 âˆ’ at)2
E âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t )
2
+ 2a2
t Ïƒ2
(c)
â‰¤ 2(1 âˆ’ at)2
L2
Ekx
(k)
t âˆ’ x
(k)
tâˆ’1k2
+ 2a2
t Ïƒ2
(d)
â‰¤ 2(1 âˆ’ at)2
L2
Î·2
tâˆ’1Ekd
(k)
tâˆ’1k2
+ 2a2
t Ïƒ2
(e)
â‰¤ 8(1 âˆ’ at)2
L2 (I âˆ’ 1)
I
Î·2
tâˆ’1Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+ 4(1 âˆ’ at)2
L2
Î·2
tâˆ’1Ek Â¯
dtâˆ’1k2
+ 2a2
t Ïƒ2
, (10)
where (a) follows from Lemma C.3; (b) results from use of Assumption 2 and mean variance inequality: For a
random variable Z we have EkZ âˆ’ E[Z]k2 â‰¤ EkZk2; (c) follows from the Lipschitz continuity of the gradient
given in Assumption 1; (d) results from the iterate update equation given in Step 10 of Algorithm 3; finally,
(e) uses the fact that: (i) for I = 1 we have d
(k)
t = Â¯
dt for all t âˆˆ [T] and (ii) for I â‰¥ 2 we use Lemma C.3 and
the fact that (I âˆ’ 1)/I â‰¥ 1/2.
Substituting (10) in (9) we get:
EkeÌ„tk2
â‰¤ (1 âˆ’ at)2
EkeÌ„tâˆ’1k2
+
8(1 âˆ’ at)2L2
bK2
(I âˆ’ 1)
I
Î·2
tâˆ’1
K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+
4(1 âˆ’ at)2L2Î·2
tâˆ’1
bK
Ek Â¯
dtâˆ’1k2
+
2a2
t Ïƒ2
bK
.
Finally, the lemma is proved by replacing t by t + 1.
Lemma A.6 shows that the gradient error contracts in each iteration. Next, we first define a potential
function and then utilize Lemmas A.5 and A.6 to show descent in the potential function.
A.2.3 Descent in Potential Function
We define the potential function as a linear combination of the objective function and the gradient estimation
error: eÌ„t := Â¯
dt âˆ’ 1
K
PK
k=1 âˆ‡f(k)(x
(k)
t )
Î¦t := f(xÌ„t) +
bK
64L2
keÌ„tk2
Î·tâˆ’1
. (11)
Next, we characterize the descent in the potential function.
Lemma A.7 (Potential Function Descent). For tÌ„ âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] and for Î·t â‰¤ 1
16LI we have
E[Î¦tÌ„+1 âˆ’ Î¦tÌ„sâˆ’1
] â‰¤ âˆ’
tÌ„
X
t=tÌ„sâˆ’1

7Î·t
16
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
tÌ„
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
Ïƒ2c2
32L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t
+
33
256K
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
where the expectation is w.r.t the stochasticity of the algorithm.
22
Proof. To get the descent on the the potential function, we first consider the term:
EkeÌ„t+1k2
Î·t
âˆ’
EkeÌ„tk2
Î·tâˆ’1
.
Using Lemma A.6 we get
EkeÌ„t+1k2
Î·t
âˆ’
EkeÌ„tk2
Î·tâˆ’1
â‰¤

(1 âˆ’ at+1)2
Î·t
âˆ’
1
Î·tâˆ’1

EkeÌ„tk2
+
8(1 âˆ’ at+1)2L2
bK2
(I âˆ’ 1)
I
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
+
4(1 âˆ’ at+1)2L2Î·t
bK
Ek Â¯
dtk2
+
2a2
t+1Ïƒ2
Î·tbK
(a)
â‰¤ Î·âˆ’1
t âˆ’ Î·âˆ’1
tâˆ’1 âˆ’ cÎ·t

EkeÌ„tk2
+
8L2
bK2
(I âˆ’ 1)
I
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
+
4L2Î·t
bK
Ek Â¯
dtk2
+
2Ïƒ2c2Î·3
t
bK
, (12)
where inequality (a) utilizes the fact that (1 âˆ’ at)2 â‰¤ 1 âˆ’ at â‰¤ 1 for all t âˆˆ [T].
Let us consider Î·âˆ’1
t âˆ’ Î·âˆ’1
tâˆ’1 in the first term of the inequality in (12) and using the definition of the stepsize
Î·t from Theorem 3.1, we have
Î·âˆ’1
t âˆ’ Î·âˆ’1
tâˆ’1 =
(wt + Ïƒ2t)1/3
ÎºÌ„
âˆ’
(wtâˆ’1 + Ïƒ2(t âˆ’ 1))1/3
ÎºÌ„
(a)
â‰¤
(wt + Ïƒ2t)1/3
ÎºÌ„
âˆ’
(wt + Ïƒ2(t âˆ’ 1))1/3
ÎºÌ„
(b)
â‰¤
Ïƒ2
3ÎºÌ„(wt + Ïƒ2(t âˆ’ 1))2/3
(c)
â‰¤
22/3Ïƒ2ÎºÌ„2
3ÎºÌ„3(wt + Ïƒ2t)2/3
(d)
=
22/3Ïƒ2
3ÎºÌ„3
Î·2
t
(e)
â‰¤
Ïƒ2
24ÎºÌ„3LI
Î·t, (13)
where inequality (a) follows from the fact that we choose wt â‰¤ wtâˆ’1 (see definition of wt in Theorem 3.1), (b)
results from the concavity of x1/3 as:
(x + y)1/3
âˆ’ x1/3
â‰¤
y
3x2/3
.
In inequality (c), we have used the fact that wt â‰¥ 2Ïƒ2, finally, (d) and (e) utilize the definition of Î·t and the
fact that Î·t â‰¤ 1
16LI for all t âˆˆ [T], respectively.
Now combining the first term of inequality in (12) with (13) and choosing c =
64L2
bK
+
Ïƒ2
24ÎºÌ„3LI
we get:
Î·âˆ’1
t âˆ’ Î·âˆ’1
tâˆ’1 âˆ’ cÎ·t â‰¤ âˆ’
64L2
bK
Î·t.
Therefore, we have from (12):
EkeÌ„t+1k2
Î·t
âˆ’
EkeÌ„tk2
Î·tâˆ’1
â‰¤ âˆ’
64L2Î·t
bK
EkeÌ„tk2
+
8L2
bK2
(I âˆ’ 1)
I
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
+
4L2Î·t
bK
Ek Â¯
dtk2
+
2Ïƒ2c2Î·3
t
bK
bK
64L2

EkeÌ„t+1k2
Î·t
âˆ’
EkeÌ„tk2
Î·tâˆ’1

â‰¤ âˆ’Î·tEkeÌ„tk2
+
1
8K
(I âˆ’ 1)
I
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
+
Î·t
16
Ek Â¯
dtk2
+
Ïƒ2c2Î·3
t
32L2
.
23
Finally, using Lemma A.5 and the definition of potential function given in (11), using the above we get the
descent in the potential function for any t âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] with s âˆˆ [S] as:
E[Î¦t+1 âˆ’ Î¦t] â‰¤ âˆ’

7Î·t
16
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
Î·tL2(I âˆ’ 1)
K
t
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
+
1
8K
(I âˆ’ 1)
I
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
+
Ïƒ2c2Î·3
t
32L2
.
Summing the above over t = tÌ„sâˆ’1 to tÌ„ for tÌ„ âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1], we get:
E[Î¦tÌ„+1 âˆ’ Î¦tÌ„sâˆ’1
] â‰¤ âˆ’
tÌ„
X
t=tÌ„sâˆ’1

7Î·t
16
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
tÌ„
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
Ïƒ2c2
32L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t
+
L2(I âˆ’ 1)
K
tÌ„
X
t=tÌ„sâˆ’1
Î·t
t
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
+
1
8K
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤ âˆ’
tÌ„
X
t=tÌ„sâˆ’1

7Î·t
16
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
tÌ„
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
Ïƒ2c2
32L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t
+
L2(I âˆ’ 1)
K
 tÌ„
X
t=tÌ„sâˆ’1
Î·t
 tÌ„
X
`=tÌ„sâˆ’1
Î·2
`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2

+
1
8K
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
.
Finally, using the fact that we have: Î·t â‰¤ 1
16LI for all t âˆˆ [T], we get:
E[Î¦tÌ„+1 âˆ’ Î¦tÌ„sâˆ’1
] â‰¤ âˆ’
tÌ„
X
t=tÌ„sâˆ’1

7Î·t
16
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
tÌ„
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
Ïƒ2c2
32L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t
+
L2(I âˆ’ 1)
K

I Ã—
1
16LI
Ã—
1
16LI
 tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
+
1
8K
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
= âˆ’
tÌ„
X
t=tÌ„sâˆ’1

7Î·t
16
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
tÌ„
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
Ïƒ2c2
32L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t
+
33
256K
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
.
Therefore, the lemma is proved.
Multiple local updates at each WN on heterogeneous data can cause the local descent directions to drift
away from each other. Next, we bound this error accumulated via gradient drift across WNs.
24
A.2.4 Accumulated Gradient Consensus Error
We first upper bound the gradient consensus error given by term
PK
k=1 Ekd
(k)
t âˆ’ Â¯
dtk2.
Lemma A.8 (Gradient Consensus Error). For every t âˆˆ [T] and some Î² > 0 we have
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤

(1 âˆ’ at)2
(1 + Î²) + 4L2

1 +
1
Î²

Î·2
tâˆ’1
 K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+ 4KL2

1 +
1
Î²

Î·2
tâˆ’1Ek Â¯
dtâˆ’1k2
+
4KÏƒ2
b

1 +
1
Î²

a2
t + 8KÎ¶2

1 +
1
Î²

a2
t
+32L2

1 +
1
Î²

(I âˆ’ 1)a2
t
tâˆ’1
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
.
where the expectation is w.r.t. the stochasticity of the algorithm.
Proof. Using the definition of the descent direction d
(k)
t from Algorithm 3 we have
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
=
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) + (1 âˆ’ at) d
(k)
tâˆ’1 âˆ’
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t )

âˆ’

1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
t ; Î¾
(j)
t ) + (1 âˆ’ at) Â¯
dtâˆ’1 âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )

 2
=
K
X
k=1
E (1 âˆ’ at) d
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1

+
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
t ; Î¾
(j)
t )
âˆ’ (1 âˆ’ at)

1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
 2
(a)
â‰¤ (1 + Î²)(1 âˆ’ at)2
K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+

1 +
1
Î²
 K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
t ; Î¾
(j)
t )
âˆ’ (1 âˆ’ at)

1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
 2
(14)
where inequality (a) follows from the Youngâ€™s inequality for some Î² > 0. Now considering the second term in
25
(14), we get
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
t ; Î¾
(j)
t )
âˆ’ (1 âˆ’ at)

1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
 2
=
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
t ; Î¾
(j)
t )
âˆ’

1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )

+ at

1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
 2
(a)
â‰¤ 2
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
t ; Î¾
(j)
t )
âˆ’

1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
 2
+ 2a2
t
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
2
(b)
â‰¤ 2
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t )
 2
+ 2a2
t
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
2
(c)
â‰¤ 2
K
X
k=1
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
E âˆ‡f(k)
(x
(k)
t ; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t )
2
+ 2a2
t
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
2
(d)
â‰¤ 2L2
K
X
k=1
Ekx
(k)
t âˆ’ x
(k)
tâˆ’1k2
+ 2a2
t
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
2
,
(15)
where inequality (a) above follows from Lemma C.3, (b) follows from Lemma C.1, inequality (c) again uses
Lemma C.3 and (d) follows from the Lipschitz-smoothness of the individual functions f(k) given in Assumption
1.
26
Next, we consider the second term in (15) above, we have
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t )
2
(a)
=
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)

âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t ) âˆ’ âˆ‡f(j)
(x
(j)
tâˆ’1)

+ âˆ‡f(k)
(x
(k)
tâˆ’1) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
tâˆ’1)
2
(b)
â‰¤ 2
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)

âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
t âˆˆB
(j)
t
âˆ‡f(j)
(x
(j)
tâˆ’1; Î¾
(j)
t ) âˆ’ âˆ‡f(j)
(x
(j)
tâˆ’1)
 2
+ 2
K
X
k=1
E âˆ‡f(k)
(x
(k)
tâˆ’1) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
tâˆ’1)
2
(c)
â‰¤ 2
K
X
k=1
E
1
b
X
Î¾
(k)
t âˆˆB
(k)
t
âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 2
+ 2
K
X
k=1
E âˆ‡f(k)
(x
(k)
tâˆ’1) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
tâˆ’1)
2
(d)
â‰¤ 2
K
X
k=1
1
b2
X
Î¾
(k)
t âˆˆB
(k)
t
E âˆ‡f(k)
(x
(k)
tâˆ’1; Î¾
(k)
t ) âˆ’ âˆ‡f(k)
(x
(k)
tâˆ’1)
 2
+ 4
K
X
k=1
E âˆ‡f(k)
(xÌ„tâˆ’1) âˆ’ âˆ‡f(xÌ„tâˆ’1)
2
+ 8
K
X
k=1
E âˆ‡f(k)
(x
(k)
tâˆ’1) âˆ’ âˆ‡f(k)
(xÌ„tâˆ’1)
2
+ 8
K
X
k=1
E âˆ‡f(xÌ„tâˆ’1) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
tâˆ’1)
2
(e)
â‰¤
2KÏƒ2
b
+ 4
K
X
k=1
1
K
K
X
j=1
Ekâˆ‡f(k)
(xÌ„tâˆ’1) âˆ’ âˆ‡f(j)
(xÌ„tâˆ’1)k2
+ 16L2
K
X
k=1
Ekx
(k)
tâˆ’1 âˆ’ xÌ„tâˆ’1k2
(g)
â‰¤
2KÏƒ2
b
+ 4KÎ¶2
+ 16L2
K
X
k=1
Ekx
(k)
tâˆ’1 âˆ’ xÌ„tâˆ’1k2
, (16)
where equality (a) follows from adding and subtracting âˆ‡f(k)(x
(k)
tâˆ’1) and 1
K
PK
j=1 âˆ‡f(j)(x
(j)
tâˆ’1) inside the norm;
inequality (b) uses Lemma C.3; inequality (c) results from the use of Lemma C.1; inequality (d) expands the sum
of the first term using inner products and utilizes the fact that the cross product terms are zero in expectation.
This follows from the fact that conditioned on Ft we have E[âˆ‡f(k)(x
(k)
t ; Î¾
(k)
t )] = âˆ‡f(k)(x
(k)
t ) for all k âˆˆ [K] and
t âˆˆ [T]; inequality (e) utilizes intra-node variance Bound given in Assumption 2, Lemma C.3 and Lipschitz
smoothness Assumption 1; finally, (g) results from the inter-node variance bound stated in Assumption 2.
27
Finally, substituting (16) and (15) in (14), we get
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤ (1 âˆ’ at)2
(1 + Î²)
K
X
k=1
E d
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1
2
+ 2L2

1 +
1
Î²
 K
X
k=1
Ekx
(k)
t âˆ’ x
(k)
tâˆ’1k2
+
4KÏƒ2
b

1 +
1
Î²

a2
t + 8KÎ¶2

1 +
1
Î²

a2
t + 32L2

1 +
1
Î²

a2
t
K
X
k=1
Ekx
(k)
tâˆ’1 âˆ’ xÌ„tâˆ’1k2
(a)
â‰¤ (1 âˆ’ at)2
(1 + Î²)
K
X
k=1
E d
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1
2
+ 2L2

1 +
1
Î²

Î·2
tâˆ’1
K
X
k=1
Ekd
(k)
tâˆ’1k2
+
4KÏƒ2
b

1 +
1
Î²

a2
t + 8KÎ¶2

1 +
1
Î²

a2
t
+ 32L2

1 +
1
Î²

(I âˆ’ 1)a2
t
tâˆ’1
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
(b)
â‰¤ (1 âˆ’ at)2
(1 + Î²)
K
X
k=1
E d
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1
2
+ 4L2

1 +
1
Î²

Î·2
tâˆ’1
K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+ 4L2

1 +
1
Î²

Î·2
tâˆ’1
K
X
k=1
Ek Â¯
dtâˆ’1k2
+
4KÏƒ2
b

1 +
1
Î²

a2
t + 8KÎ¶2

1 +
1
Î²

a2
t
+ 32L2

1 +
1
Î²

(I âˆ’ 1)a2
t
tâˆ’1
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
=

(1 âˆ’ at)2
(1 + Î²) + 4L2

1 +
1
Î²

Î·2
tâˆ’1
 K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+ 4KL2

1 +
1
Î²

Î·2
tâˆ’1Ek Â¯
dtâˆ’1k2
+
4KÏƒ2
b

1 +
1
Î²

a2
t + 8KÎ¶2

1 +
1
Î²

a2
t
+ 32L2

1 +
1
Î²

(I âˆ’ 1)a2
t
tâˆ’1
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
,
where inequality (a) follows from the iterate update given in Step 10 of Algorithm 3 and the application Lemma
A.4; and inequality (b) results from the use of Lemma C.3.
Next, we utilize the above Lemma A.8 to bound the accumulated gradient consensus error in the potential
functionâ€™s descent derived in Lemma A.7.
Lemma A.9 (Accumulated Gradient Consensus Error). For tÌ„ âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] with s âˆˆ [S] we have
33
256K
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤
tÌ„
X
t=tÌ„sâˆ’1
Î·t
64
Ek Â¯
dtk2
+
Ïƒ2c2
64bL2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t +
Î¶2c2
32L2
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t .
Proof. First, from the statement of Lemma A.8, considering the coefficient of first term on the right hand side
28
of the expression, we have:
(1 âˆ’ at)2
(1 + Î²) + 4L2

1 +
1
Î²

Î·2
tâˆ’1
(a)
â‰¤ 1 + Î² + 4L2

1 +
1
Î²

Î·2
tâˆ’1
(b)
â‰¤ 1 +
1
I
+ 4L2
(I + 1)Î·2
tâˆ’1
(c)
â‰¤ 1 +
1
I
+
I + 1
64I2
(d)
â‰¤ 1 +
33
32I
,
where inequality (a) uses the fact that (1 âˆ’ at)2 â‰¤ 1; the second inequality (b) follows from taking Î² = 1/I,
inequality (c) uses the bound Î·t â‰¤ 1/16LI for all t âˆˆ [T]. Finally, the last inequality (d) results by using the
fact that we have I + 1 â‰¤ 2I. Substituting in the statement of Lemma A.8 above, we get
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤

1 +
33
32I
 K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+ 4KL2

1 +
1
Î²

Î·2
tâˆ’1Ek Â¯
dtâˆ’1k2
+
4KÏƒ2
b

1 +
1
Î²

a2
t
+ 8KÎ¶2

1 +
1
Î²

a2
t +32L2

1 +
1
Î²

(I âˆ’ 1)a2
t
tâˆ’1
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
.
(a)
â‰¤

1 +
33
32I
 K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+ 8KL2
IÎ·2
tâˆ’1Ek Â¯
dtâˆ’1k2
+
8KIÏƒ2
b
c2
Î·4
tâˆ’1
+ 16KIÎ¶2
c2
Î·4
tâˆ’1+64L2
I2
c2
Î·4
tâˆ’1
tâˆ’1
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
(b)
â‰¤

1 +
33
32I
 K
X
k=1
Ekd
(k)
tâˆ’1 âˆ’ Â¯
dtâˆ’1k2
+
KL
2
Î·tâˆ’1Ek Â¯
dtâˆ’1k2
+
KÏƒ2c2
2bL
Î·3
tâˆ’1
+
KÎ¶2c2
L
Î·3
tâˆ’1+ 64L2
I2
c2
Î·4
tâˆ’1
tâˆ’1
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
(17)
where (a) follows from using Î² = 1/I, the fact that I + 1 â‰¤ 2I and the definition of at from Algorithm 3.
Note form Algorithm 3 that we have d
(k)
t = Â¯
dt for t = tÌ„sâˆ’1 with s âˆˆ [S]. This implies that for t = tÌ„sâˆ’1 with
29
s âˆˆ [S], we have,
PK
k=1 kd
(k)
t âˆ’ Â¯
dtk2 = 0. Applying (17) above recursively for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1] we get:
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤
KL
2
tâˆ’1
X
`=tÌ„sâˆ’1

1 +
33
32I
tâˆ’1âˆ’`
Î·`Ek Â¯
d`k2
+
KÏƒ2c2
2bL
tâˆ’1
X
`=tÌ„sâˆ’1

1 +
33
32I
tâˆ’1âˆ’`
Î·3
`
+
KÎ¶2c2
L
tâˆ’1
X
`=tÌ„sâˆ’1

1 +
3
2I
tâˆ’1âˆ’`
Î·3
` + 64L2
I2
c2
tâˆ’1
X
`=tÌ„sâˆ’1

1 +
33
32I
tâˆ’1âˆ’`
Î·4
`
`
X
Â¯
`=tÌ„sâˆ’1
Î·2
Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
(a)
â‰¤
KL
2

1 +
33
32I
I t
X
`=tÌ„sâˆ’1
Î·`Ek Â¯
d`k2
+
KÏƒ2c2
2bL

1 +
33
32I
I t
X
`=tÌ„sâˆ’1
Î·3
`
+
KÎ¶2c2
L

1 +
33
32I
I t
X
`=tÌ„sâˆ’1
Î·3
` + 64L2
I3
c2

1
16LI
5
1 +
33
32I
I t
X
Â¯
`=tÌ„sâˆ’1
Î·Â¯
`
K
X
k=1
Ekd
(k)
Â¯
`
âˆ’ Â¯
dÂ¯
`k2
(b)
â‰¤
3KL
2
t
X
`=tÌ„sâˆ’1
Î·`Ek Â¯
d`k2
+
3KÏƒ2c2
2bL
t
X
`=tÌ„sâˆ’1
Î·3
` +
3KÎ¶2c2
L
t
X
`=tÌ„sâˆ’1
Î·3
`
+ 192L2
I3
c2

1
16LI
5 t
X
`=tÌ„sâˆ’1
Î·`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
, (18)
where inequality (a) follows from the fact that 1 + 33/32I > 1 and t âˆ’ 1 âˆ’ ` â‰¤ I for t âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] and
` âˆˆ [tÌ„sâˆ’1, t] and inequality (b) follows from the fact that (1+33/32I)I â‰¤ e33/32 < 3 and Î·t â‰¤ 1
16LI for all t âˆˆ [T].
Next, multiplying both sides of (18) by Î·t and summing over t = tÌ„sâˆ’1 to tÌ„ for tÌ„ âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] with s âˆˆ [S]
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤
3KL
2
tÌ„
X
t=tÌ„sâˆ’1
Î·t
t
X
`=tÌ„sâˆ’1
Î·`Ek Â¯
d`k2
+
3KÏƒ2c2
2bL
tÌ„
X
t=tÌ„sâˆ’1
Î·t
t
X
`=tÌ„sâˆ’1
Î·3
`
+
3KÎ¶2c2
L
tÌ„
X
t=tÌ„sâˆ’1
Î·t
t
X
`=tÌ„sâˆ’1
Î·3
` + 192L2
I3
c2

1
16LI
5 tÌ„
X
t=tÌ„sâˆ’1
Î·t
t
X
`=tÌ„sâˆ’1
Î·`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
(a)
â‰¤
3KL
2
 tÌ„
X
t=tÌ„sâˆ’1
Î·t
 tÌ„
X
`=tÌ„sâˆ’1
Î·`Ek Â¯
d`k2
+
3KÏƒ2c2
2bL
 tÌ„
X
t=tÌ„sâˆ’1
Î·t
 tÌ„
X
`=tÌ„sâˆ’1
Î·3
`
+
3KÎ¶2c2
L
 tÌ„
X
t=tÌ„sâˆ’1
Î·t
 tÌ„
X
`=tÌ„sâˆ’1
Î·3
` + 192L2
I3
c2

1
16LI
5 tÌ„
X
t=tÌ„sâˆ’1
Î·t
 tÌ„
X
`=tÌ„sâˆ’1
Î·`
K
X
k=1
Ekd
(k)
` âˆ’ Â¯
d`k2
(b)
â‰¤
3K
32
tÌ„
X
t=tÌ„sâˆ’1
Î·tEk Â¯
dtk2
+
3KÏƒ2c2
32bL2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t +
3KÎ¶2c2
16L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t + 192L2
I4
c2

1
16LI
6 tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
where inequality (a) uses the fact that t âˆˆ [tÌ„sâˆ’1, tÌ„] and (b) follows from the fact that we have Î·t â‰¤ 1/16LI for
all t âˆˆ [T]. Rearranging the terms we get

1 âˆ’ 192L2
I4
c2

1
16LI
6 tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤
3K
32
tÌ„
X
t=tÌ„sâˆ’1
Î·tEk Â¯
dtk2
+
3KÏƒ2c2
32bL2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t +
3KÎ¶2c2
16L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t
30
using the fact that c â‰¤ 128L2/bK, b â‰¥ 1, K â‰¥ 1 and I â‰¥ 1, we have
h
1 âˆ’ 192L2I4c2

1
16LI
6i
â‰¥ 4
5, therefore,
we get
33
256K
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·t
K
X
k=1
Ekd
(k)
t âˆ’ Â¯
dtk2
â‰¤
tÌ„
X
t=tÌ„sâˆ’1
Î·t
64
Ek Â¯
dtk2
+
Ïƒ2c2
64bL2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t +
Î¶2c2
32L2
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t .
Hence, the lemma is proved.
A.2.5 Proof of Theorem 3.1
Next, to prove Theorem 3.1 we first prove an intermediate theorem by utilizing Lemmas A.9 and A.7 derived
above.
Theorem A.10. Choosing the parameters as
(i) ÎºÌ„ =
(bK)2/3Ïƒ2/3
L
,
(ii) c =
64L2
bK
+
Ïƒ2
24ÎºÌ„3LI
(i)
= L2

64
bK
+
1
24(bK)2I

â‰¤
128L2
bK
,
(iii) We choose {wt}T
t=0 as
wt = max

2Ïƒ2
, 4096L3
I3
ÎºÌ„3
âˆ’ Ïƒ2
t,
c3ÎºÌ„3
4096L3I3
 (i)(ii)
â‰¤ Ïƒ2
max

2, 4096I3
(bK)2
âˆ’ t,
512
bKI3

.
Moreover, for any number of local updates, I â‰¥ 1, batch sizes, b â‰¥ 1, and initial batch size, B â‰¥ 1, computed at
individual WNs, STEM satisfies:
Ekâˆ‡f(xÌ„a)k2
â‰¤

32LI
T
+
2L
(bK)2/3T2/3

(f(xÌ„1) âˆ’ fâˆ—
) +

8bI2
BT
+
bI
2(bK)2/3BT2/3

Ïƒ2
+

2562I
T
+
642
(bK)2/3T2/3

Ïƒ2
log(T + 1) +

2562I
T
+
642
(bK)2/3T2/3

Î¶2 (I âˆ’ 1)
I
log(T + 1).
Proof. Substituting the gradient consensus error derived in Lemma A.9 into the Potential function descent
derived in Lemma A.7, we can write the descent of potential function for tÌ„ âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] with s âˆˆ [S] as:
E[Î¦tÌ„+1 âˆ’ Î¦tÌ„sâˆ’1
] â‰¤ âˆ’
tÌ„
X
t=tÌ„sâˆ’1

27Î·t
64
âˆ’
Î·2
t L
2

Ek Â¯
dtk2
âˆ’
tÌ„
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
c2Ïƒ2
32L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t +
c2Ïƒ2
64bL2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t +
c2Î¶2
32L2
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t
(a)
â‰¤ âˆ’
tÌ„
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
3c2Ïƒ2
64L2
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t +
c2Î¶2
32L2
(I âˆ’ 1)
I
tÌ„
X
t=tÌ„sâˆ’1
Î·3
t .
where (a) follows from the fact that Î·t â‰¤ 1
16LI for all t âˆˆ [T] and b â‰¥ 1. Taking tÌ„ = tÌ„s âˆ’ 1 = sI, the above
expression can be written as:
E[Î¦tÌ„s
âˆ’ Î¦tÌ„sâˆ’1
] â‰¤ âˆ’
tÌ„sâˆ’1
X
t=tÌ„sâˆ’1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
3c2Ïƒ2
64L2
tÌ„sâˆ’1
X
t=tÌ„sâˆ’1
Î·3
t +
c2Î¶2
32L2
(I âˆ’ 1)
I
tÌ„sâˆ’1
X
t=tÌ„sâˆ’1
Î·3
t .
31
Summing over all the restarts, i.e, s âˆˆ [S], we get:
E[Î¦tÌ„S
âˆ’ Î¦tÌ„0
]â‰¤ âˆ’
tÌ„Sâˆ’1
X
t=tÌ„0
Î·t
2
Ekâˆ‡f(xÌ„t)k2
+
3c2Ïƒ2
64L2
tÌ„Sâˆ’1
X
t=tÌ„0
Î·3
t +
c2Î¶2
32L2
(I âˆ’ 1)
I
tÌ„Sâˆ’1
X
t=tÌ„0
Î·3
t .
Assuming that T = SI, then from the definition of tÌ„s that tÌ„0 = 1 and tÌ„S = SI + 1 = T + 1, we get
T
X
t=1
Î·t
2
Ekâˆ‡f(xÌ„t)k2
â‰¤ E[Î¦1 âˆ’ Î¦T+1] +
3c2Ïƒ2
64L2
T
X
t=1
Î·3
t +
c2Î¶2
32L2
(I âˆ’ 1)
I
T
X
t=1
Î·3
t
(a)
â‰¤ f(xÌ„1) âˆ’ fâˆ—
+
bK
64L2
EkeÌ„1k2
Î·0
+
3c2Ïƒ2
64L2
T
X
t=1
Î·3
t +
c2Î¶2
32L2
(I âˆ’ 1)
I
T
X
t=1
Î·3
t
(b)
â‰¤ f(xÌ„1) âˆ’ fâˆ—
+
Ïƒ2
64L2
b
BÎ·0
+
3c2Ïƒ2
64L2
T
X
t=1
Î·3
t +
c2Î¶2
32L2
(I âˆ’ 1)
I
T
X
t=1
Î·3
t . (19)
where (a) follows from the fact that fâˆ— â‰¤ Î¦T+1 and (b) results from application of Lemma A.3.
First, let us consider the last term of the (19) above, we have from the definition of the stepsize Î·t
T
X
t=1
Î·3
t =
T
X
t=1
ÎºÌ„3
wt + Ïƒ2t
(a)
â‰¤
T
X
t=1
ÎºÌ„3
Ïƒ2 + Ïƒ2t
=
ÎºÌ„3
Ïƒ2
T
X
t=1
1
1 + t
(b)
â‰¤
ÎºÌ„3
Ïƒ2
ln(T + 1). (20)
where inequality (a) above follows from the fact that we have wt â‰¥ 2Ïƒ2 > Ïƒ2 and inequality (b) follows from
the application of Lemma C.2.
Substituting (20) in (19), dividing both sides by T and using the fact that Î·t is non-increasing in t we have
1
T
T
X
t=1
Ekâˆ‡f(xÌ„t)k2
â‰¤
2(f(xÌ„1) âˆ’ fâˆ—)
Î·T T
+
1
Î·T T
Ïƒ2
32L2
b
BÎ·0
+
1
Î·T T
3c2ÎºÌ„3
32L2
log(T + 1)
+
1
Î·T T
c2ÎºÌ„3
16L2
Î¶2
Ïƒ2
(I âˆ’ 1)
I
log(T + 1)
(a)
â‰¤
2(f(xÌ„1) âˆ’ fâˆ—)
Î·T T
+
1
Î·T T
Ïƒ2
32L2
b
BÎ·0
+
1
Î·T T
c2ÎºÌ„3
4L2
log(T + 1)
+
1
Î·T T
c2ÎºÌ„3
4L2
Î¶2
Ïƒ2
(I âˆ’ 1)
I
log(T + 1). (21)
where (a) above utilizes the fact that 1/16 < 3/32 < 1/4.
Now considering each term of (21) above separately and using the definition of Î·t =
ÎºÌ„
(wt + Ïƒ2t)1/3
we get
from the coefficient of the first term:
1
Î·T T
=
(wT + Ïƒ2T)1/3
ÎºÌ„T
(a)
â‰¤
w
1/3
T
ÎºÌ„T
+
Ïƒ2/3
ÎºÌ„T2/3
(b)
â‰¤
16LI
T
+
L
(bK)2/3T2/3
. (22)
32
where inequality (a) follows from identity (x + y)1/3 â‰¤ x1/3 + y1/3 and inequality (b) follows from the definition
of ÎºÌ„ and wT
wT = max

2Ïƒ2
, 4096L3
I3
ÎºÌ„3
âˆ’ Ïƒ2
T,
c3ÎºÌ„3
4096L3I3

â‰¤ Ïƒ2
max

2, 4096I3
(bK)2
âˆ’ T,
512
bKI3

,
where we used 4096L3
I3
ÎºÌ„3
> 4096L3
I3
ÎºÌ„3
âˆ’Ïƒ2
T â‰¥ max

2Ïƒ2
,
c3ÎºÌ„3
4096L3I3

. Note that this choice of wT captures
the worst case guarantees for STEM.
Now, let us consider the second term of (21), we have from the definition of Î·0 and Î·T
1
Î·T T
Ïƒ2
32L2
b
BÎ·0
â‰¤

16LI
T
+
L
(bK)2/3T2/3

Ã—
Ïƒ2
32L2
Ã—
bw
1/3
0
BÎºÌ„
(a)
â‰¤

16LI
T
+
L
(bK)2/3T2/3

Ã—
Ïƒ2
32L2
Ã—
16LIb
B
(b)
â‰¤
8bI2
BT
Ïƒ2
+
bI
(bK)2/3BT2/3
Ïƒ2
2
. (23)
where inequality (a) follows from the identity (x + y)1/3 â‰¤ x1/3 + y1/3 and (b) follows from the definition of ÎºÌ„
and using w0 â‰¤ 4096L3I3ÎºÌ„3 and wT â‰¤ 4096L3I3ÎºÌ„3 (Similar to the approach in (22) this choice of w0 and wT
capture the worst case convergence guarantees for STEM.)
Finally, considering the term 1
Î·T T
c2ÎºÌ„3
4L2 common to the last two terms in (21) above, we have from the
definition of the stepsize, Î·t,
1
Î·T T
c2ÎºÌ„3
4L2
â‰¤

16LI
T
+
L
(bK)2/3T2/3

Ã—

128L2
bK
2
Ã—
(bK)2Ïƒ2
L3
Ã—
1
4L2
(a)
â‰¤ 2562
Ïƒ2 I
T
+ 642
Ïƒ2 1
(bK)2/3T2/3
. (24)
where inequality (a) follows from the identity (x + y)1/3 â‰¤ x1/3 + y1/3 and (b) again uses wT â‰¤ 4096L3I3ÎºÌ„3
along with the definition of ÎºÌ„ and c.
Finally, substituting the bounds obtained in (22), (23) and (24) into (21), we get
Ekâˆ‡f(xÌ„a)k2
â‰¤

32LI
T
+
2L
(bK)2/3T2/3

(f(xÌ„1) âˆ’ fâˆ—
) +

8bI2
BT
+
bI
2(bK)2/3BT2/3

Ïƒ2
+

2562I
T
+
642
(bK)2/3T2/3

Ïƒ2
log(T + 1) +

2562I
T
+
642
(bK)2/3T2/3

Î¶2 (I âˆ’ 1)
I
log(T + 1).
Hence, the theorem is proved.
Next, using Theorem A.10 we prove Theorem 3.1.
Theorem A.11 (Theorem 3.1: Trade-off: Local Updates vs Batch Sizes). With the parameters chosen
according to Theorem A.10 and for any Î½ âˆˆ [0, 1] at each WN we set the total number of local updates as
I = O (T/K2)Î½/3

, batch size, b = O (T/K2)1/2âˆ’Î½/2

, and the initial batch size, B = bI. Then STEM satisfies:
(i) We have:
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
K2Î½/3T1âˆ’Î½/3

+ OÌƒ

Ïƒ2
K2Î½/3T1âˆ’Î½/3

+ OÌƒ

(I âˆ’ 1)
I
Ã—
Î¶2
K2Î½/3T1âˆ’Î½/3

.
33
(ii) Sample Complexity: To achieve an -stationary point STEM requires at most O(âˆ’3/2) gradient computations.
This implies that each WN requires at most O(Kâˆ’1âˆ’3/2) gradient computations, thereby achieving linear
speedup with the number of WNs present in the network.
(iii) Communication Complexity: To achieve an -stationary point STEM requires at most O(âˆ’1) communication
rounds.
Proof. The proof of statement (i) follows from the statement of Theorem A.10 and substituting the values of
parameters B, I and b in the expression. First, replacing B = bI in the statement of Theorem A.10 yields
Ekâˆ‡f(xÌ„a)k2
â‰¤

32LI
T
+
2L
(bK)2/3T2/3

(f(xÌ„1) âˆ’ fâˆ—
) +

8I
T
+
1
2(bK)2/3T2/3

Ïƒ2
+

2562I
T
+
642
(bK)2/3T2/3

Ïƒ2
log(T + 1) +

2562I
T
+
642
(bK)2/3T2/3

Î¶2 (I âˆ’ 1)
I
log(T + 1).
Then using the fact that I = O (T/K2)Î½/3

and b = O (T/K2)1/2âˆ’Î½/2

yields the expression of statement (i).
Next, we compute the computation and communication complexity of the algorithm.
â€¢ Sample Complexity [Theorem A.11(ii)]: From the statement of Theorem A.11(i), total iterations required
to achieve an -stationary point are:
OÌƒ

1
K2Î½/3T1âˆ’Î½/3

=  â‡’ T = OÌƒ

1
K2Î½/(3âˆ’Î½)3/(3âˆ’Î½)

. (25)
In each iteration, each WN computes 2b stochastic gradients, therefore, the total gradient computations at
each WN are 2bT. Using b = O (T/K2)1/2âˆ’Î½/2

, we get the total gradient computations required at each
WN as:
bT = OÌƒ

T3/2âˆ’Î½/2
K1âˆ’Î½

(25)
= OÌƒ

1
K3/2

This implies that the sample complexity is OÌƒ(âˆ’3/2).
â€¢ Communication Complexity [Theorem A.11(iii)]: The total rounds of communication to achieve an -stationary
point are T/I, with I = O (T/K2)Î½/3

and T given in (25), therefore, we have the communication
complexity as:
T
I
= OÌƒ T1âˆ’Î½/3
K2Î½/3
 (25)
= OÌƒ

1


.
Hence, the theorem is proved.
Corollary 2 (FedSTEM: Local Updates). With the choice of parameters given in Theorem A.10. At each WN,
setting constant batch size, b â‰¥ 1, number of local updates, I = (T/b2K2)1/3, and the initial batch size, B = bI.
Then STEM satisfies the following:
(i) We have:
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
(bK)2/3T2/3

+ OÌƒ

Ïƒ2
(bK)2/3T2/3

+ OÌƒ

Î¶2
(bK)2/3T2/3

.
(ii) Sample Complexity: To achieve an -stationary point FedSTEM requires at most OÌƒ(âˆ’3/2) gradient computations
while achieving linear speedup with the number of WNs.
34
(iii) Communication Complexity: To achieve an -stationary point FedSTEM requires at most OÌƒ(âˆ’1) communication
rounds.
Proof. The proof of statement (i) follows from substituting the values of the parameters b, I and B as defined
in the statement of the Corollary in the statement of Theorem A.10.
Next, we compute the sample and communication complexity of the algorithm.
â€¢ Sample Complexity: From the statement of Corollary 2(i), total iterations, T, required to achieve an
-stationary point are:
OÌƒ

1
(bK)2/3T2/3

=  â‡’ T = OÌƒ

1
bK3/2

. (26)
At each iteration the algorithm computes 2b stochastic gradients. Therefore, the total number of gradient
computations required at each WN are of the order of 2bT, which is OÌƒ(Kâˆ’1âˆ’3/2). Therefore, the sample
complexity of the algorithm is OÌƒ(âˆ’3/2).
â€¢ Communication Complexity: Total rounds of communication to achieve an -stationary point is T/I,
therefore we have from the choice of I that
T
I
= OÌƒ (bK)2/3
T2/3
 (26)
= OÌƒ

1


.
Hence, the corollary is proved.
An alternate design choice for the algorithm is to design large batch-size gradients and communicate more
often. The next corollary captures this idea.
Corollary 3 (Corollary 1: Minibatch STEM). With the choice of parameters given in Theorem A.10. At each
WN, choosing the number of local updates, I = 1, the batch size, b = T1/2
/K, and the initial batch size, B = bI.
Then STEM satisfies:
(i) We have:
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
T

+ OÌƒ

Ïƒ2
T

.
(ii) Sample Complexity: To achieve an -stationary point Minibatch STEM requires at most OÌƒ(âˆ’3/2) gradient
computations while achieving linear speedup with the number of WNs.
(iii) Communication Complexity: To achieve an -stationary point Minibatch STEM requires at most OÌƒ(âˆ’1)
communication rounds.
Proof. The proof of statement (i) follows from substituting the values of the parameters b, I and B given in
the statement of the Corollary in the statement of Theorem A.10.
Next, we compute the sample and communication complexity of the algorithm.
â€¢ Sample Complexity: From the statement of Corollary 3(i), total iterations, T, required to achieve an
-stationary point are:
OÌƒ

I
T

=  â‡’ T = OÌƒ

I


. (27)
35
In each iteration, each WN computes 2b stochastic gradients, therefore, the total gradient computations at
each WN are 2bT. Using the fact that b = O

T1/2
I3/2K

. The total gradients computed at each WN to reach
an -stationary point are:
OÌƒ

I

Ã—
I1/2
1/2I3/2K

= OÌƒ

1
K3/2

.
Therefore, the communication complexity if OÌƒ(âˆ’3/2).
â€¢ Communication Complexity: The total rounds of communication required to reach an -stationary point
are T/I, therefore we have
T
I
(27)
= OÌƒ

1


.
Hence, the corollary is proved.
B Proofs of Convergence Guarantees for FedAvg
In this section, we present the proofs for the FedAvg algorithm. Before stating the proofs in detail we first
present some preliminaries lemmas which shall be used for proving the main results of the paper. We first fix
some notations:
We define tÌ„s := sI +1 with s âˆˆ [S]. Note from Algorithm 2 that at (sÃ—I)th iteration, i.e., when t mod I = 0,
the iterates, {x
(k)
t }K
k=1 corresponding to t = (tÌ„s)th time instant are shared with the SN. We define the filtration
Ft as the sigma algebra generated by iterates x
(k)
1 , x
(k)
2 , . . . , x
(k)
t as
Ft = Ïƒ(x
(k)
1 , x
(k)
2 , . . . , x
(k)
t , for all k âˆˆ [K]).
Also, throughout the section we assume Assumptions 1 and 2 to hold. Next, we present the proof of Theorem
3.2. The proof follows in few steps which are discussed next.
B.1 Proof of Main Results: FedAvg
Lemma B.1. For Â¯
dt := 1
K
PK
k=1 d
(k)
t where d
(k)
t for all k âˆˆ [K] and t âˆˆ [T] is chosen according to Algorithm 2,
we have:
E Â¯
dt âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
â‰¤
Ïƒ2
bK
,
where the expectation is w.r.t the stochasticity of the the algorithm.
Proof. The proof follows from the same argument as in Lemma A.3.
Next, we bound the error accumulated via the iterates generated by the local updates of Algorithm 2.
Lemma B.2 (Error Accumulation from Iterates). For the choice of stepsize Î· â‰¤ 1
9LI , the iterates x
(k)
t for each
k âˆˆ [K] generated from Algorithm 2 satisfy:
T
X
t=1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤ 3Î·2
(I âˆ’ 1)Ïƒ2
T + 5Î·2
(I âˆ’ 1)2
Î¶2
T,
where the expectation is w.r.t the stochasticity of the algorithm.
36
Proof. Note from Algorithm 2 and the definition of tÌ„s that at t = tÌ„sâˆ’1 with s âˆˆ [S], x
(k)
t = xÌ„t, for all k. This
implies
1
K
K
X
k=1
kx
(k)
tÌ„sâˆ’1
âˆ’ xÌ„tÌ„sâˆ’1
k2
= 0.
Therefore, the statement of the lemma holds trivially. Moreover, for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1], with s âˆˆ [S], we have
from Algorithm 2: x
(k)
t = x
(k)
tâˆ’1 âˆ’ Î·d
(k)
tâˆ’1, this implies that:
x
(k)
t = x
(k)
tÌ„sâˆ’1
âˆ’
tâˆ’1
X
`=tÌ„sâˆ’1
Î·d
(k)
` and xÌ„t = xÌ„tÌ„sâˆ’1
âˆ’
tâˆ’1
X
`=tÌ„sâˆ’1
Î· Â¯
d`.
This implies that for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1], with s âˆˆ [S] we have
1
K
K
X
k=1
kx
(k)
t âˆ’ xÌ„tk2
=
1
K
K
X
k=1
x
(k)
tÌ„sâˆ’1
âˆ’ xÌ„tÌ„sâˆ’1
âˆ’
 tâˆ’1
X
`=tÌ„sâˆ’1
Î·d
(k)
` âˆ’
tâˆ’1
X
`=tÌ„sâˆ’1
Î· Â¯
d`
 2
(a)
=
Î·2
K
K
X
k=1
tâˆ’1
X
`=tÌ„sâˆ’1
d
(k)
` âˆ’ Â¯
d`
 2
(b)
=
Î·2
K
K
X
k=1
tâˆ’1
X
`=tÌ„sâˆ’1

1
b
X
Î¾
(k)
` âˆˆB
(k)
`
âˆ‡f(k)
(x
(k)
` ; Î¾
(k)
` ) âˆ’
1
K
K
X
j=1
1
b
X
Î¾
(j)
` âˆˆB
(j)
`
âˆ‡f(j)
(x
(j)
` ; Î¾
(j)
` )
 2
(c)
â‰¤
2Î·2
K
K
X
k=1
tâˆ’1
X
`=tÌ„sâˆ’1

1
b
X
Î¾
(k)
` âˆˆB
(k)
`
âˆ‡f(k)
(x
(k)
` ; Î¾
(k)
` ) âˆ’ âˆ‡f(k)
(x
(k)
` )

âˆ’
1
K
K
X
j=1

1
b
X
Î¾
(j)
` âˆˆB
(j)
`
âˆ‡f(j)
(x
(j)
` ; Î¾
(j)
` ) âˆ’ âˆ‡f(j)
(x
(j)
` )
 2
+
2Î·2
K
K
X
k=1
tâˆ’1
X
`=tÌ„sâˆ’1

âˆ‡f(k)
(x
(k)
` ) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
` )
 2
(d)
â‰¤
2Î·2
K
K
X
k=1
tâˆ’1
X
`=tÌ„sâˆ’1

1
b
X
Î¾
(k)
` âˆˆB
(k)
`
âˆ‡f(k)
(x
(k)
` ; Î¾
(k)
` ) âˆ’ âˆ‡f(k)
(x
(k)
` )
 2
+
2Î·2
K
K
X
k=1
tâˆ’1
X
`=tÌ„sâˆ’1

âˆ‡f(k)
(x
(k)
` ) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
` )
 2
, (28)
where the equality (a) follows from the fact that x
(k)
tÌ„sâˆ’1
= xÌ„tÌ„sâˆ’1
for t = tÌ„sâˆ’1; (b) results from the definition
of the stochastic gradient employed by FedAvg in Algorithm 2; (c) uses Lemma C.3 and (d) follows from the
application of Lemma C.1.
Taking expectation on both sides and let us next consider each term of (28) above separately, we have for
37
any k âˆˆ [K] from the first term of (28) above
E
tâˆ’1
X
`=tÌ„sâˆ’1

1
b
X
Î¾
(k)
` âˆˆB
(k)
`
âˆ‡f(k)
(x
(k)
` ; Î¾
(k)
` ) âˆ’ âˆ‡f(k)
(x
(k)
` )
 2
(a)
=
tâˆ’1
X
`=tÌ„sâˆ’1
E
1
b
X
Î¾
(k)
` âˆˆB
(k)
`
âˆ‡f(k)
(x
(k)
` ; Î¾
(k)
` ) âˆ’ âˆ‡f(k)
(x
(k)
` )
2
(b)
=
tâˆ’1
X
`=tÌ„sâˆ’1
1
b2
X
Î¾
(k)
` âˆˆB
(k)
`
E âˆ‡f(k)
(x
(k)
` ; Î¾
(k)
` ) âˆ’ âˆ‡f(k)
(x
(k)
` )
2
(c)
â‰¤
(I âˆ’ 1)
b
Ïƒ2
(d)
â‰¤ (I âˆ’ 1)Ïƒ2
, (29)
where (a) results from the fact that E
h
1
b
P
Î¾
(k)
` âˆˆB
(k)
`
âˆ‡f(k)(x
(k)
` ; Î¾
(k)
` )âˆ’âˆ‡f(k)(x
(k)
` ) FÂ¯
`
i
= 0 for any Â¯
` < `; (b) uses
the fact that E

âˆ‡f(k)(x
(k)
` ; Î¾
(k)
` ) âˆ’ âˆ‡f(k)(x
(k)
` ) âˆ‡f(k)(x
(k)
` ; Î¶
(k)
` ) âˆ’ âˆ‡f(k)(x
(k)
` )

= 0 for samples Î¾
(k)
` , Î¶
(k)
` âˆ¼ D(k)
chosen independent; (c) utilizes intra-node variance bound in Assumption 2(ii) and the fact that (tâˆ’1)âˆ’tÌ„sâˆ’1 â‰¤
I âˆ’ 1 for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1]; and finally, (d) uses the fact that b â‰¥ 1.
Next, we consider the second term of (28) for any k âˆˆ [K], we have
K
X
k=1
E
tâˆ’1
X
`=tÌ„sâˆ’1

âˆ‡f(k)
(x
(k)
` ) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
` )
 2
(a)
â‰¤ (I âˆ’ 1)
tâˆ’1
X
`=tÌ„sâˆ’1
K
X
k=1
E âˆ‡f(k)
(x
(k)
` ) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(x
(j)
` )
2
(b)
â‰¤ (I âˆ’ 1)
tâˆ’1
X
`=tÌ„sâˆ’1

4
K
X
k=1
E âˆ‡f(k)
(x
(k)
` ) âˆ’ âˆ‡f(k)
(xÌ„`)
2
+ 4
K
X
k=1
E âˆ‡f(xÌ„`) âˆ’
1
K
K
X
j=1
âˆ‡f(x
(j)
` )
2
+ 2
K
X
k=1
E âˆ‡f(k)
(xÌ„`) âˆ’ âˆ‡f(xÌ„`)
2

(c)
â‰¤ (I âˆ’ 1)
tâˆ’1
X
`=tÌ„sâˆ’1

8L2
K
X
k=1
E x
(k)
` âˆ’ xÌ„`
2
+ 2
K
X
k=1
E âˆ‡f(k)
(xÌ„`) âˆ’
1
K
K
X
j=1
âˆ‡f(j)
(xÌ„`)
2
(d)
â‰¤ 8L2
(I âˆ’ 1)
tâˆ’1
X
`=tÌ„sâˆ’1
K
X
k=1
E x
(k)
` âˆ’ xÌ„`
2
+ 2K(I âˆ’ 1)2
Î¶2
, (30)
where (a) utilizes the fact that (t âˆ’ 1) âˆ’ tÌ„sâˆ’1 â‰¤ I âˆ’ 1 for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1]; (b) results from the application
of Lemma C.3; (c) follows from Assumption 1; and (d) utilizes the inter-node variance Assumption 2 and the
fact that (t âˆ’ 1) âˆ’ tÌ„sâˆ’1 â‰¤ I âˆ’ 1 for t âˆˆ [tÌ„sâˆ’1 + 1, tÌ„s âˆ’ 1].
Substituting (29) and (30) in (28) and taking expectation on both sides we get
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤ 2Î·2
(I âˆ’ 1)Ïƒ2
+ 4Î·2
(I âˆ’ 1)2
Î¶2
+ 16L2
(I âˆ’ 1)Î·2
tâˆ’1
X
`=tÌ„sâˆ’1
1
K
k
X
k=1
Ekx
(k)
` âˆ’ xÌ„`k2
.
38
Summing both sides from t = tÌ„sâˆ’1 to tÌ„s âˆ’ 1, we get
tÌ„sâˆ’1
X
t=tÌ„sâˆ’1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤ 2Î·2
(I âˆ’ 1)Ïƒ2
I + 4Î·2
(I âˆ’ 1)2
Î¶2
I + 16L2
(I âˆ’ 1)Î·2
tÌ„sâˆ’1
X
t=tÌ„sâˆ’1
tâˆ’1
X
`=tÌ„sâˆ’1
1
K
K
X
k=1
Ekx
(k)
` âˆ’ xÌ„`k2
(a)
â‰¤ 2Î·2
(I âˆ’ 1)Ïƒ2
I + 4Î·2
(I âˆ’ 1)2
Î¶2
I + 16L2
(I âˆ’ 1)Î·2
tÌ„sâˆ’1
X
t=tÌ„sâˆ’1
tÌ„sâˆ’1
X
`=tÌ„sâˆ’1
1
K
K
X
k=1
Ekx
(k)
` âˆ’ xÌ„`k2
(b)
â‰¤ 2Î·2
(I âˆ’ 1)Ïƒ2
I + 4Î·2
(I âˆ’ 1)2
Î¶2
I + 16L2
(I âˆ’ 1)Î·2
I
tÌ„sâˆ’1
X
t=tÌ„sâˆ’1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
,
where (a) uses that fact that t â‰¤ tÌ„s âˆ’ 1; (b) results from ts âˆ’ tsâˆ’1 â‰¤ I for all s âˆˆ [S]. Finally, summing over
s âˆˆ [S] and using T = SI we get
T
X
t=1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤ 2Î·2
(I âˆ’ 1)Ïƒ2
T + 4Î·2
(I âˆ’ 1)2
Î¶2
T + 16L2
I2
Î·2
T
X
t=1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
.
Rearranging the terms, we get
(1 âˆ’ 16L2
I2
Î·2
)
T
X
t=1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤ 2Î·2
(I âˆ’ 1)Ïƒ2
T + 4Î·2
(I âˆ’ 1)2
Î¶2
T.
Finally, using the fact that Î· â‰¤ 1
9LI we have 1 âˆ’ 16L2I2Î·2 â‰¥ 4/5. Multiplying, both sides by 5/4 we get
T
X
t=1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤ 3Î·2
(I âˆ’ 1)Ïƒ2
T + 5Î·2
(I âˆ’ 1)2
Î¶2
T.
Therefore, the lemma is proved.
Lemma B.3 (Descent Lemma). For all t âˆˆ [tÌ„sâˆ’1, tÌ„s âˆ’ 1] and s âˆˆ [S], with the choice of stepsizes Î· â‰¤ 1
9LI , the
iterates generated by Algorithm 2 satisfy:
Ef(xÌ„t+1) â‰¤ Ef(xÌ„t) âˆ’
Î·
2
Ekâˆ‡f(xÌ„t)k2
+
Î·L2
2K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
+
Î·2L
bK
Ïƒ2
,
where the expectation is w.r.t the stochasticity of the algorithm.
39
Proof. Using the smoothness of f (Assumption 1) we have:
E[f(xÌ„t+1)] â‰¤ E
h
f(xÌ„t) + hâˆ‡f(xÌ„t), xÌ„t+1 âˆ’ xÌ„ti +
L
2
kxÌ„t+1 âˆ’ xÌ„tk2
i
(a)
= E
h
f(xÌ„t) âˆ’ Î·hâˆ‡f(xÌ„t), Â¯
dti +
Î·2L
2
k Â¯
dtk2
i
(b)
= E
h
f(xÌ„t) âˆ’ Î·
D
âˆ‡f(xÌ„t),
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
E
+
Î·2L
2
k Â¯
dtk2
i
(c)
= E

f(xÌ„t) âˆ’
Î·
2
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
âˆ’
Î·
2
kâˆ‡f(xÌ„t)k2
+
Î·
2
âˆ‡f(xÌ„t) âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
+ Î·2
L Â¯
dt âˆ’
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
+ Î·2
L
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2

(d)
â‰¤ E

f(xÌ„t) âˆ’
Î·
2
âˆ’ Î·2
L
 1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t )
2
âˆ’
Î·
2
kâˆ‡f(xÌ„t)k2
+
Î·L2
2K
K
X
k=1
kx
(k)
t âˆ’ xÌ„tk2
+
Î·2L
bK
Ïƒ2

(e)
â‰¤ E

f(xÌ„t) âˆ’
Î·
2
kâˆ‡f(xÌ„t)k2
+
Î·L2
2K
K
X
k=1
kx
(k)
t âˆ’ xÌ„tk2
+
Î·2L
bK
Ïƒ2

,
where equality (a) follows from the iterate update given in Step 5 of Algorithm 2; (b) results from the fact that
we have E[âˆ‡f(k)(x
(k)
t ; Î¾
(k)
t )|Ft] = âˆ‡f(k)(x
(k)
t ); (c) uses ha, bi = 1
2[kak2 + kbk2 âˆ’ ka âˆ’ bk2] and Lemma C.3; (d)
results from (31) below and Lemma B.1; and (e) results from the stepsize choice of Î· â‰¤ 1
9LI .
E
1
K
K
X
k=1
âˆ‡f(k)
(x
(k)
t ) âˆ’ âˆ‡f(k)
(xÌ„t)
 2
â‰¤
1
K
K
X
k=1
E âˆ‡f(k)
(x
(k)
t ) âˆ’ âˆ‡f(k)
(xÌ„t)
2
â‰¤
L2
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
, (31)
where the first inequality follows from Lemma C.3, and the second results from Assumption 1.
Hence, the lemma is proved.
B.1.1 Proof of Theorem 3.2
The proof of Theorem 3.2 follows by replacing the choices of b and I given in (5) in the following result.
Theorem B.4. Under Assumptions 1 and 2, with stepsize Î· =
q
bk
T . Then for T â‰¥ 81L2I2bK with any choice
of minibatch sizes, b â‰¥ 1, and number of local updates, I â‰¥ 1, the iterates generated from Algorithm 2 satisfy
Ekâˆ‡f(xÌ„a)k2
â‰¤
2(f(xÌ„t)) âˆ’ fâˆ—)
(bk)1/2T1/2
+
2L
(bk)1/2T1/2
Ïƒ2
+
3L2bK(I âˆ’ 1)
T
Ïƒ2
+
5L2bK(I âˆ’ 1)2
T
Î¶2
.
Proof. Summing the result of Lemma B.3 for t = [T] and multiplying both sides by 2/Î·T we get
1
T
T
X
t=1
Ekâˆ‡f(xÌ„t)k2
â‰¤
2(f(xÌ„t) âˆ’ f(xÌ„t+1))
Î·T
+
2Î·L
bK
Ïƒ2
+
L2
T
T
X
t=1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
â‰¤
2(f(xÌ„t) âˆ’ fâˆ—)
Î·T
+
2Î·L
bK
Ïƒ2
+
L2
T
T
X
t=1
1
K
K
X
k=1
Ekx
(k)
t âˆ’ xÌ„tk2
40
where the second inequality uses f(xÌ„tâˆ’1) â‰¥ fâˆ—. Next, using Lemma B.2 we get
1
T
T
X
t=1
Ekâˆ‡f(xÌ„t)k2
â‰¤
2(f(xÌ„t) âˆ’ fâˆ—)
Î·T
+
2Î·L
bK
Ïƒ2
+ 3L2
Î·2
(I âˆ’ 1)Ïƒ2
+ 5L2
Î·2
(I âˆ’ 1)2
Î¶2
.
Finally, using the definition of xÌ„a from Algorithm 2 and the choice of Î· =
q
bK
T , we get
Ekâˆ‡f(xÌ„a)k2
â‰¤
2(f(xÌ„t) âˆ’ fâˆ—)
(bK)1/2T1/2
+
2L
(bK)1/2T1/2
Ïƒ2
+
3L2bK(I âˆ’ 1)
T
Ïƒ2
+
5L2bK(I âˆ’ 1)2
T
Î¶2
.
Therefore, we have the theorem.
Finally, substituting the choice of I and b given in (5) we get the statement of Theorem 3.2. Next two
remarks characterize the behavior of FedAvg for two extreme choices of I and b.
Remark 5 (FedAvg: multiple local updates). Choosing Î½ = 1 in Theorem 3.2 implies I = (T/b3K3)1/4 and
b = O(1), we have
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
K1/2T1/2

+ O

Ïƒ2
K1/2T1/2

+ O

Î¶2
K1/2T1/2

,
while the sample and communication complexities are still O(âˆ’2) and O(âˆ’3/2), respectively. Note that these
are the same guarantees for FedAvg analyzed in [14,20].
Remark 6 (FedAvg: large batch). Choosing Î½ = 0 in Theorem 3.2 implies I = O(1) > 1 (we allow multiple
local updates, i.e. I > 1) and b = (T/I4K3)1/3, then we have
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
T2/3

+ O

Ïƒ2
T2/3

+ O

Î¶2
T2/3

.
while the sample and communication complexities are again O(âˆ’2) and O(âˆ’3/2), respectively.
Minibatch SGD: When the parameters are shared after each local update, for such case we have I = 1
and for the choice of b = O(T/K) we have:
Ekâˆ‡f(xÌ„a)k2
= O

f(xÌ„1) âˆ’ fâˆ—
T

+ O

Ïƒ2
T

.
This implies that the sample and communication complexitiess are O(âˆ’2) and O(âˆ’1). Again, this result is
independent of the heterogeniety parameter Î¶ (cf. Assumption 2) as the algorithm for I = 1 is essentially a
centralized algorithm.
C Useful lemmas
Lemma C.1. For a finite sequence x(k) âˆˆ Rd for k âˆˆ [K] define xÌ„ := 1
K
PK
k=1 x(k), we then have
K
X
k=1
kx(k)
âˆ’ xÌ„k2
â‰¤
K
X
k=1
kx(k)
k2
.
41
Proof. Using the notation x =
h
(x(1))
T
, (x(2))
T
, . . . , (x(K))
T
iT
âˆˆ RKd, denoting Id âˆˆ RdÃ—d and IKd âˆˆ RKdÃ—Kd
as identity matrices and representing 1 âˆˆ RK as the vector of all ones. We rewrite the left hand side of the
statement as
K
X
k=1
kx(k)
âˆ’ xÌ„k2
= x âˆ’

I âŠ—
11T
K

x
2
=

IKd âˆ’

Id âŠ—
11T
K

x
2
(a)
â‰¤ kxk2
=
K
X
k=1
kx(k)
k2
,
where (a) follows from the fact that the induced matrix norm IKd âˆ’

Id âŠ— 11T
K

â‰¤ 1.
Lemma C.2 (From [7]). Let a0 > 0 and a1, a2, . . . , aT â‰¥ 0. We have
T
X
t=1
at
a0 +
Pt
i=t ai
â‰¤ ln

1 +
Pt
i=1 ai
a0

.
Lemma C.3. For X1, X2, . . . , Xn âˆˆ Rd, we have
kX1 + X2 + . . . + Xnk2
â‰¤ nkX1k2
+ nkX2k2
+ . . . + nkXnk2
.
42
