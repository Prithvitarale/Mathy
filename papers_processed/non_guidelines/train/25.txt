A comparative study of neural network techniques
for automatic software vulnerability detection
Gaigai Tang, Lianxiao Meng,
Huiqiang Wang*
School of computer science
and technology,
Harbin Engineering University,
Harbin, China
Shuangyin Ren, Qiang Wang, Lin Yang*
National Key Laboratory of Science and
Technology on Information System Security,
Institute of System Engineering,
Chinese Academy of Military Science,
Beijing, China
Weipeng Cao
College of Computer Science
and Software Engineering,
Shenzhen University,
Shenzhen, China
Abstract—Software vulnerabilities are usually caused by
design flaws or implementation errors, which could be exploited
to cause damage to the security of the system. At present,
the most commonly used method for detecting software vul-
nerabilities is static analysis. Most of the related technologies
work based on rules or code similarity (source code level) and
rely on manually defined vulnerability features. However, these
rules and vulnerability features are difficult to be defined and
designed accurately, which makes static analysis face many
challenges in practical applications. To alleviate this problem,
some researchers have proposed to use neural networks that
have the ability of automatic feature extraction to improve
the intelligence of detection. However, there are many types
of neural networks, and different data preprocessing methods
will have a significant impact on model performance. It is
a great challenge for engineers and researchers to choose a
proper neural network and data preprocessing method for
a given problem. To solve this problem, we have conducted
extensive experiments to test the performance of the two most
typical neural networks (i.e., Bi-LSTM and RVFL) with the
two most classical data preprocessing methods (i.e., the vector
representation and the program symbolization methods) on
software vulnerability detection problems and obtained a series
of interesting research conclusions, which can provide valuable
guidelines for researchers and engineers. Specifically, we found
that 1) the training speed of RVFL is always faster than Bi-
LSTM, but the prediction accuracy of Bi-LSTM model is higher
than RVFL; 2) using doc2vec for vector representation can
make the model have faster training speed and generalization
ability than using word2vec; and 3) multi-level symbolization
is helpful to improve the precision of neural network models.
Keywords-software vulnerability, neural network, machine
learning, Bi-LSTM, RVFL
I. INTRODUCTION
As the functions of modern software systems become
more and more complicated, software vulnerabilities caused
by design flaws and implementation errors become an in-
evitable problem in engineering. According to the statistics
released by the Common Vulnerabilities and Exposures
(CVE) [1] and National Vulnerability Database (NVD) [2],
the number of software vulnerabilities has increased from
1600 to nearly 100000 since 1999 [3]. These vulnerabilities
pose a serious threat to the security of existing software
systems.
Most of the classical vulnerability detection techniques
are driven by rules [4],[5],[6],[7],[8],[9] and code similarity
metrics [10],[11], which are generated by experts experience.
Although rule-based detection techniques can be applied to
vulnerability detection in specific scenarios, the performance
of these methods depends heavily on the experience of
developers. Generally, it is very difficult to describe the
features of software vulnerabilities accurately, which leads
to the corresponding detection rules are also difficult to be
defined accurately and completely.
To alleviate the above problems, neural network-based
automatic vulnerability detection technology (source code
level) has been proposed and shows great potential in related
issues [12],[13],[14],[15],[16]. Neural networks can auto-
matically extract complex features from input data, avoiding
the problems of high cost, instability, and incompleteness
of manually constructing features and empirically defining
rules. A representative work is the VulDeePecker system
proposed in [15], which is the first effort to use neural
network techniques to solve software vulnerability detection
problems. Specifically, the VulDeePecker first extracts the
code gadget, which is a form of source code fragments
related to the vulnerability, and then symbolizes them to
get the corresponding symbolic data. After that, it uses
word2vec [17] to transform the symbolic data into the
vector representation and then applies the vectors to the
model training of the Bidirectional Long Short-term Mem-
ory network (Bi-LSTM) [18] to get the detection model.
The authors have used several experiments to prove that
VulDeePecker is superior to other classical vulnerability
detection techniques in terms of the accuracy of vulnerability
detection.
The success of VulDeePecker shows that it is feasible to
use neural networks to detect software vulnerabilities, but
there are still some shortcomings in that work, as follows.
• VulDeePecker only tried the Bi-LSTM with the itera-
tive training mechanism to train the detection model,
but this type of neural network often requires a long
training time and will face many obstacles in practical
applications.
• VulDeePecker used word2vec to vectorize the software
codes and then used the generated vector (variable
length) as the input of the neural network. This method
sometimes requires us to perform additional work on
arXiv:2104.14978v1
[cs.SE]
29
Apr
2021
the preprocessing of the vector (e.g., padding zeros),
which will cause the dimension of the vector to be
very high and affect the training efficiency of the
model. Moreover, this method may also lose important
semantic information about the source codes and affect
the effectiveness of the vulnerability detection model.
• VulDeePecker used two types of symbolization to do
symbolic representation, but it is still unclear which one
is the most suitable for neural networks.
The above unsolved problems inspired us to choose
two completely different neural networks and different data
preprocessing methods to model software vulnerability de-
tection problems in this study. Specifically, we selected
the Bi-LSTM with the iterative training mechanism and
the Random Vector Functional Link network (RVFL) [19]
with the non-iterative training mechanism to train on the
vulnerability data set to get corresponding detection models,
and then compared the precision and training efficiency of
the models. We found that the precision of the Bi-LSTM
model is always higher than that of the RVFL model, but
the training efficiency of the RVFL is always higher than that
of the Bi-LSTM. Moreover, we have implemented a multi-
level symbolization method for symbolic representation and
proposed the use of doc2vec [20] for vector representation.
In details, we first perform three symbolizations to obtain
the symbolic representations of the source codes related
to the vulnerability. The advantage of using three levels
of symbolization is that one can significantly reduce the
noise introduced by irrelevant information in vulnerable
codes. Then we use the doc2vec to automatically transform
the symbolic representations of the source codes to their
vector representations. Our study found that doc2vec is more
suitable for modeling this problem than word2vec used in
[15], because it can not only convert source codes with
arbitrary length into a fixed-length feature representation, but
also better grasp the semantic information. These advantages
are helpful to improve the precision and training efficiency of
the model. These experimental results can provide valuable
guidance for researchers and engineers to choose appropriate
neural networks and data preprocessing methods for software
vulnerability detection problems.
The rest of this paper is organized as follows. Section
II discusses the work related to the automatic detection of
software vulnerability. Section III describes the details of
the automatic software vulnerability detection system using
neural network techniques. In Section IV, we give details of
our experimental setting, results, and corresponding analysis.
The conclusions and future works are discussed in Section
V.
II. RELATED WORK
Traditional machine learning techniques such as Deci-
sion Tree [21] and Support Vector Machine [22], mainly
extract the vulnerability features from the pre-classified
vulnerabilities. However, the vulnerability detection patterns
generated with this type of features are usually tailored for
specific vulnerabilities. In [23], both simple text features
(e.g., character count, character diversity, maximum nesting
depth and complex text features (e.g., character n-grams,
word n-grams and suffix trees) are extracted from the source
codes and analyzed by using the naive Bayes classifier. The
experimental results show that simple features performed
unexpectedly better compared to the complex features.
Neural network based techniques are able to learn vulner-
ability features automatically. In [15], the authors present
a vulnerability detection system VulDeePecker based on
deep learning. VulDeePecker obtains the samples by first
extracting code gadgets (i.e., snippets of codes that are
semantically related to the vulnerability) from the buggy
programs and then transforming into the vector represen-
tations. The learning algorithm is based on the Long Short-
Term Memory (LSTM). Experimental evaluations show that
VulDeePecker outperforms the state-of-the-art vulnerability
detection systems in terms of both accuracy and efficiency.
[13] has implemented various machine learning models for
the detection of bugs that can lead to security vulnerabilities
in C/C++ code. They use features derived from the build
process and the source code. [14] has developed a vulner-
ability detection tool based on deep feature representation
learning that can directly interpret the parsed source codes.
The source codes are firstly transformed into tokens and
then embedded as vectors for both convolutional neural
networks (CNNs) and recurrent neural networks(RNNs).
Experimental results show that the tool results in high
accuracy. [24] has proposed a systematic framework for
using deep learning to detect vulnerabilities that dubbed
syntax-based, semantics-based and vector representations
(SySeVR). SySeVR can accommodate syntax and semantic
information pertinent to vulnerabilities. The source codes are
successively represented by syntax-based, semantics-based
and vector representations. Experiments show that SySeVR
is able to detect several vulnerabilities that are not reported in
the National Vulnerability Database. [16] performs a quanti-
tative evaluation of the impacts of different factors (e.g., data
dependency and control dependency) on the effectiveness of
neural network based vulnerability detection techniques. In
[12], the authors propose a tool called VuRLE for automatic
detection and repair of vulnerabilities. Experiments show
that VuRLE results in a large improvement on the accuracy
of detection and the number of repaired vulnerabilities.
There are also some vulnerability detection techniques
that make use of manually-defined features [4], [5], [6], [7],
[8], [9], or code similarity metrics [10],[11]. However, there
are several drawbacks. First, defining vulnerability features
is error-prone and costs a lot of manual labor. Second,
the features can hardly be complete and usually contain
only partial information about the vulnerabilites, which may
lead to a high rate of false-positive and false-negative [15].
Moreover, code similarity based approaches are limited to
detecting vulnerabilities caused by code clone.
III. THE METHODOLOGY
We show in Figure. 1 an overview of the source code
level automatic software vulnerability detection system using
Figure 1. Overview of the automatic software vulnerability detection
system using neural networks
neural network techniques. It first creates a symbolic form
of the code gadget by using multilevel symbolization. The
code gadgets are divided into two parts. One is for training
the detection model and the other one is for vulnerability
detection. Then it transforms the symbolic form into the
vector representation with a low dimension by using the tool
doc2vec. Finally, it applies two neural network techniques
namely the Bi-LSTM and the RVFL to train the detection
model. In the subsequent sections, we present the details of
the main components of the system.
A. Symbolic representation
Code gadget is a fragment of the codes that are
closely relevant to the vulnerability. By using symbolization,
code gadget can be further transformed into a symbolic rep-
resentation. The benefit of symbolic representation is that it
can effectively reduce the time cost of vector representation
precess since it can condense the size of the corpus, which
is used for training the vector representation tool. In sym-
bolization, vulnerability features in each code gadget such
as local variables or user-defined functions are transformed
into certain short and fixed symbolic presentations, where the
same features are mapped to the same symbolic presentation.
In this work, we deploy three symbolization types sorted by
priority.
• Function calls symbolization (F): user-defined func-
tion names are symbolically represented as FN. This
symbolization type is set to the first priority because
vulnerability is usually caused by improper utilization
of library/API function calls. Symbolization on user-
defined functions can improve the Signal-Noise Ratio
(SNR) of the library function in vulnerability informa-
tion.
• Variable symbolization (V): variable names including
parameters and local variables are symbolically repre-
sented as VN.
• Data type symbolization (D): data types of variable and
user-defined function are symbolically represented as
TN. It has the least priority since usually many data
types are not related to vulnerability information.
The symbol N mentioned above in symbolization is a
number representing the sequence of the first occurrence of
the feature. Besides, all the symbolization types will reserve
the keywords of C/C++ language.
Table I
MULTILEVEL SYMBOLIZATION
Symbolization level Symbolization group
Level 1 F
Level 2 F+V;F+D
Level 3 F+V+D
We build a multilevel symbolization mechanism according
to the priority of a single symbolization shown in Table. I.
Level 2 includes two symbolization groups F+V and F+D.
This is because symbolization V and D may have different
effects on improving the SNR of vulnerability information
with different data-sets.
We take Sample 0 below as an example and explain
how the symbolization works. We choose the symbolization
group F+V in Level 2. From Sample 0 we can observe that
there are 2 user-defined functions, 5 variables and 2 data
types.
• In Level 1, the two user-defined functions are symbol-
ically represented as F1 and F2.
• In Level 2, the five variable names are symbolically
represented as V i, i ∈ [1, 5].
• In Level 3, the two data types are symbolically repre-
sented as T1 and T2.
As a result, with the three levels of symbolization, Sam-
ple 0 is gradually simplified to a generalized symbolic
representation, which may effectively characterize different
manifestations of the same vulnerability.
Sample 0
1: static void goodG2B()
2: list < char∗ > dataList;
3: goodG2BSink(dataList);
4: void goodG2BSink(list < char∗ > List)
5: char ∗ data = List.back();
6: if(sscanf(data, ”&d”, &n) == 1))
Level 1 F
1: static void F1()
2: list < char∗ > dataList;
3: F2(dataList);
4: void F2(list < char∗ > List)
5: char ∗ data = List.back();
6: if(sscanf(data, ”&d”, &n) == 1))
Level 2 F+V
1: static void F1()
2: list < char∗ > V 1;
3: F2(V 1);
4: voidF2(list < char∗ > V 2)
5: char ∗ V 3 = V 2.back();
6: if(sscanf(V 3, V 4, &V 5) == 1))
Level 3 F+V+D
1: T1 F1()
2: list < T2∗ > V 1;
3: F2(V 1);
4: void F2(list < T2∗ > V 2)
5: T2 ∗ V 3 = V 2.back();
6: if(sscanf(V 3, V 4, &V 5) == 1))
B. Vector representation
Since the input type accepted by neural networks is a
vector, it still needs to be converted to the vector repre-
sentation after symbolizing the source codes. Currently, the
most popular vectorization methods are word2vec [25] and
doc2vec [26].
word2vec is a distributed representation method for
words. Compared with the one-hot representation, a high-
dimensional and sparse representation method, word2vec
can get low-dimensional and dense vector representation,
which is conducive to the improvement of training efficiency
and accuracy of the model, so it has been widely used in
vulnerability detection scenarios recently [13], [15], [16].
However, word2vec is not suitable for vectorizing sentences
or documents because it ignores the influence of the word
order on the information of sentence or document.
doc2vec was proposed in [26], where the authors pro-
posed the unsupervised algorithm Paragraph V ector that
can learn fixed-length feature representations from variable-
length pieces of texts, ranging from sentences to documents.
The Paragraph V ector can memorize the missing content
in the current context or the topic of the paragraph, so it can
extract global features better than word2vec.
The input vector of neural networks is required to be
fixed length. word2vec converts words to vector representa-
tions in a one-to-one fashion, therefore the length of the
converted vector representation varies with the length of
the text. To satisfy the requirement of neural networks,
the vector representations generated by word2vec need to
be further processed to get the corresponding fixed-length
version. Different from word2vec, doc2vec can directly
get fixed-length vector representations from pieces of texts
with arbitrary length. As mentioned above, doc2vec can also
extract more semantic information from the context of the
text than word2vec. Therefore, one can infer that doc2vec
is more suitable than word2vec in the scenarios with rich
semantic information.
C. Neural network model
Here we introduce two types of neural networks, that is,
Recurrent Neural Network (RNN) [27] using the iterative
training mechanism and RVFL [28] using the non-iterative
training mechanism.
RNN is a typical neural network for processing sequence
data [27][29]. One of its biggest advantages is the ability
to mine the context information of the data. However, since
the original RNN can only mine the text information in a
Figure 2. A typical network structure of Bi-LSTM
forward way, while the vulnerability information usually re-
lates to both the preceding and backward parts of the codes,
it can be inferred that the original RNN may not be suitable
for dealing with the vulnerability detection problems. For
this reason, we choose the improved version of RNN, that
is, Bi-LSTM, which can not only express the long-term
dependency information in the input data but also can bi-
directionally mine the context information of the data (i.e.,
forward and backward). These advantages make Bi-LSTM
very suitable for processing software vulnerability detection
data (source code level). Note that all parameters of the Bi-
LSTM are iteratively fine-tuned using methods like gradient
descent. A typical network structure of Bi-LSTM is shown
in Figure. 2.
Different from Bi-LSTM, RVFL is a special type of
feed-forward neural networks with the non-iterative training
mechanism. RVFL was proposed by Pao YH et al. in
the 1990s [28], which randomly assigns values to some
parameters according to certain rules and keeps these param-
eters be frozen throughout the training process, while other
parameters are calculated by the least square method. This
training mechanism can bring RVFL much faster training
speed than traditional neural networks with the iterative
training mechanism on some tasks with a relatively small
data scale. In recent years, RVFL and its variants have been
widely concerned and applied in many scenarios [30]. The
network structure of the RVFL is shown in Figure. 3. For
the technical details of RVFL, please refer to [31].
IV. EXPERIMENT AND EVALUATION
Our aim is to quantitatively evaluate the performances of
the neural network based automatic software vulnerability
detection techniques. To be specific, we investigate the
following questions in the experiments.
• Question1 (Q1): How different neural network models
perform on vulnerability detection on the source code
level?
• Question2 (Q2): How different vector representation
methods affect the performances of neural networks?
Specifically, does doc2vec outperform word2vec for
the task of vulnerability detection?
Figure 3. A typical RVFL with a single hidden layer network structure
• Question3 (Q3): What are the effects of different sym-
bolization on the performances of neural networks?
A. Experiment setting and implementation
1) Data-set: We include the following three data-sets
from [15] in our experiments. Each sample is a source code
file with known vulnerabilities. In Table. II we show the
number of samples (i.e., source code files) in each data-set.
• BE-ALL: samples with Buffer Error vulnerabilities
(CWE-119) and ALL library/API function calls.
• RM-ALL: samples with Resource Management error
vulnerabilities (CWE-399) and ALL library/API func-
tion calls.
• HY-ALL: sample with HYbrid buffer error vulnerabil-
ities (CWE-119) and resource management error vul-
nerabilities (CWE-399) and ALL library/API function
calls.
Table II
NUMBER OF SAMPLES IN EACH DATA-SET
Data-set Code Gadgets Vulnerable
code gadgets
Not vulnerable
code gadgets
BE-ALL 39753 10440 29313
RM-ALL 21885 7285 14600
HY-ALL 61638 17725 43913
Each data-set is partitioned into two parts with a propor-
tion of 80% and 20%, where the larger part is for training
and the other one is for detection. All samples in the data-set
are in the form of code gadgets with ground truth labels.
2) Parameters setting for neural networks: In our exper-
iments, we used two types of neural networks for model
training: Bi-LSTM and RVFL. For both, there is only one
hidden layer in the network structure. In Bi-LSTM, hidden
layer neurons perform two-time computation operations (i.e.,
forward and backward), while in RVFL they only perform
a one-time computation operation. We considered a one-
time computation operation of the neurons as a unit of
measurement. To make a fair comparison, we set the number
of hidden layer neurons of RVFL to twice that of Bi-LSTM,
which guarantees the structural complexity of the two neural
networks is essentially the same. We have implemented the
CPU versions of Bi-LSTM and RVFL, and all the models
are trained in the PC environment with CPU. For Bi-LSTM,
the batch size, dropout rate, the number of epochs, and the
number of the hidden layer neurons were set to 64, 0.5, 2,
and 60, respectively, and the RMSProp was chosen as the
optimizer. For RVFL, the number of the hidden layer neurons
was set to 120 and the Sigmoid function was chosen as the
activation function. The input weights and the hidden biases
of RVFL were generated randomly from (-1, 1) and (0, 1)
under a uniform distribution, respectively.
B. Results and evaluation
1) Evaluation metrics: In our experiment, we used the
same indexes as [32] to evaluate the effectiveness of the
vulnerability detection model, that is, False Positive Rate
(FPR), False Negative Rate (FNR), Precision (P), and F1-
measure (F1). The value range of these four indicators is [0,
1]. For FPR and FNR, the closer their values are to 0, the
better the performance of the model; for other indicators,
the closer their values are to 1, the better the performance
of the model.
The quality of vector representation can be evaluated by
CosineDistance (cosine) between vectors in the vector
space, which can be calculated by the following formula.
The range of the cosine value is [0,1]. The closer the value
is to 1, the more similar the two vectors are.
cosine(A, B) =
A · B
||A||2||B||2
where A and B refer to vectors.
In our experiments, we have the following three configu-
rations:
• word2vec with Bi-LSTM (w+B), which was used by
VulDeePecker;
• doc2vec with Bi-LSTM (d+B);
• doc2vec with RVFL (d+R).
2) Results for Q1: Regarding the impacts of different
neural network models on the performances of vulnerability
detection. We evaluate the precision of the above three
configurations on all data-sets, while we evaluate the effi-
ciency of configuration d+B and d+R on the largest data-
set HY-ALL. In the experiments, all the three data-sets are
preprocessed with the symbolization group F+V.
Table III
EFFECT OF DIFFERENT VECTOR REPRESENTATION TOOLS ON
VULNERABILITY DETECTION PRECISION
Data-set Pattern FPR(%) FNR(%) P(%) F1(%)
BE-ALL
w+B 2.9 18.0 91.7 86.6
d+B 3.9 16.9 88.1 85.5
d+R 6.7 32.5 85.1 74.3
RM-ALL
w+B 2.8 4.7 94.6 95.0
d+B 3.8 9.7 91.9 91.1
d+R 3.1 20.0 90.5 86.5
HY-ALL
w+B 5.1 16.1 86.9 85.4
d+B 3.3 16.2 91.0 87.2
d+R 7.7 29.7 85.0 75.6
As illustrated in Table. III, the configurations d+B and
w+B outperform d+R on the metrics P and F1 for all the
three data-sets. In terms of FPR and FNR, the configurations
w+B and d+B have a smaller sum of FPR and FNR than
d+R. The general result is that the configurations with Bi-
LSTM perform better than the ones with RFVL on the
precision of vulnerability detection. This can be explained
by the fact that Bi-LSTM is able to express the long-term
dependency information in the input, while RVFL is based
on forward neural network, which is slightly inferior to Bi-
LSTM in context processing.
Table IV
EFFECT OF DIFFERENT NEURAL NETWORK MODELS ON VULNERABILITY
DETECTION EFFICIENCY
Pattern Training
code gadgets
Detection
code gadgets
Training
Time(s)
Detection
Time(s)
w+B 48744 12894 36372.2 156.2
d+B 49310 12328 1123.5 3.2
d+R 49310 12328 4.62 0.16
From Table. IV, we can observe that the configuration
w+B takes the longest time for training and detection on
the data-set HY-ALL, while the configuration d+B takes
less than 1/30 of the time needed by w+B. It’s because
that configuration w+B in [15] outputs vectors with a long
dimension of 2500, which creates a higher computation
complexity for the neural networks. Moreover, compared
with the configuration d+B, configuration d+R further im-
proves the efficiency of training and detection to the level
of a few seconds. This can be explained by the fact that
the non-iterative training mechanism of RVFL reduces the
computation of parameters, however with a sacrifice of
precision.
We can conclude that the configuration with Bi-LSTM
achieves a higher precision, while the configuration with
RVFL is more effective. Bi-LSTM results in a high accuracy
for detecting vulnerabilities because of its advantage of
context processing, while RVFL results in a high speed for
training model because of its non-iterative training mecha-
nism. In the practice, we usually need to make a trade-off
between the prediction accuracy and efficiency of the model.
Specifically, if the speed of model training takes precedence
over its accuracy, then RVFL is recommended; otherwise,
Bi-LSTM is recommended.
3) Results for Q2: To answer the second question, we
evaluate the accuracy of the two vector representation meth-
ods doc2vec and word2vec. We implement experiments
with the samples shown in Sample 1, Sample 2 , Sample
3 and Sample 4. Sample 2 and Sample 4 are labeled as
’vulnerable’ while the Sample 1 and Sample 3 are not.
All samples are from the data-set BE-ALL and HY-ALL.
We evaluate vector representations by using the similarity
measure cosine. Both samples are preprocessed with sym-
bolization group of F+V. The vector dimension of word2vec
is set to be 2500, where the vector dimension of one word is
set to 50 and the number of words to represent a paragraph
is set to be 50. The vector dimension of doc2vec is set to
be 250. The different dimension settings of word2vec and
doc2vec is because that if both dimensions are set to be the
same (e.g. 250), then for word2vec the vector dimension of
one word will be 5, or the number of words to represent
a paragraph will be 5, which may have a great impact on
the effectiveness of vector representation. As a result, the
comparison of the accuracy of word2vec and doc2vec is
carried out under the condition that they both use a proper
dimension of vector representation.
Sample 1
1: data = (char∗)malloc(100 ∗ sizeof(char));
2: goodG2BSource(data);
3: void goodG2BSource(char ∗ &data)
4: memset(data,0
A0
, 50 − 1);
5: data[50 − 1] =0
\00
;
6: char dest[50] = ””;
7: strcpy(dest, data);
Sample 2
1: char ∗ data;
2: data = (char∗)malloc(100 ∗ sizeof(char));
3: if(5 == 5)
4: memset(data,0
A0
, 100 − 1);
5: data[100 − 1] =0
\00
;
6: char dest[50] = ””;
7: strcpy(dest, data);
Sample 3
1: data = −1;
2: char inputBuffer[CHAR ARRAY SIZE] =0000
;
3: if(fgets(inputBuffer, CHAR ARRAY SIZE,
stdin)! = NULL)
Sample 4
1: char inputBuffer[CHAR ARRAY SIZE] =0000
;
2: if(fgets(inputBuffer, CHAR ARRAY SIZE,
stdin)! = NULL)
3: data = atoi(inputBuffer);
From Table. V, we observe that for Sample1 and Sam-
ple2 word2vec outputs vectors with a higher cosine than
doc2vec , while for Sample3 and Sample4 it outputs a op-
posite outcome. Obviously, the similarity between Sample1
and Sample2 is not as high as the result of word2vec, but
it is close to the result of doc2vec. While the similarity
between Sample3 and Sample4 is closer to the result of
doc2vec, proving that doc2vec can generate a more accurate
vector representation with lower dimension than word2vec.
It can be explained by the fact that as noted in [15],
in order to obtain a fixed length of vector representation,
vectors generated from word2vec should be padded with
Table V
EFFECTIVENESS OF WORD2VEC COMPARE WITH DOC2VEC
Sample Tool Vector
dimension
Cosine(BE-
ALL)
Cosine(HY-
ALL)
1&2
word2vec 2500 0.832 0.828
doc2vec 250 0.642 0.639
3&4
word2vec 2500 0.482 0.514
doc2vec 250 0.586 0.578
zeros, which may cause the loss of semantic information
of the samples. Moreover, it is understood that vectors with
low dimensions can speed up the training process of the
neural network model. This is also justified by the results in
Table. III, where the configurations with doc2vec show better
results than the ones with word2vec due to the benefits
gained from the vector representation of doc2vec.
4) Results for Q3: Given that fact that the configuration
d + B has the highest precision of vulnerability detection,
we take the configuration d + B as the baseline to discuss
whether symbolization can further improve the accuracy. We
implement experiments with all the three data-sets. For each
data-set, we apply the symbolization level from 1 to 3 for
preprocessing.
Table VI
EFFECTIVENESS OF SYMBOLIZATION LEVELS ON PRECISION
Data-set Symbolization
group
FPR(%) FNR(%) P(%) F1(%)
BE-ALL
F 2.9 13.8 91.2 88.6
F+V 3.9 16.9 88.1 85.5
F+D 3.2 15.4 90.4 87.4
F+V+D 3.5 15.5 89.4 86.9
RM-ALL
F 2.8 7.8 94.1 93.1
F+V 3.8 9.7 91.9 91.1
F+D 2.6 6.5 94.5 94.0
F+V+D 3.4 9.1 92.7 91.8
HY-ALL
F 3.2 12.2 92.0 89.8
F+V 3.3 16.2 91.0 87.2
F+D 3.2 14.4 91.7 88.5
F+V+D 3.3 14.0 91.1 88.5
Table. VI summarizes the results of how different levels
of symbolization can affect the precision.
From the perspective of data-set types, different sym-
bolization levels have a bigger impact on the precision of
vulnerability detection with smaller data-sets, which show
the maximum precision deviation of 3.1% in BE-ALL and
2.6% in RM-ALL. However, in large data-set HY-ALL,
this phenomenon is much weakened, which is shown as
0.9%. This may be because since the richness of data-
sets improves the generalization ability of the detection
model itself, and the impact of symbolization is gradually
reduced. From the perspective of symbolization levels, the
configuration d + B with the symbolization level of 1 shows a
better and more stable performance than other symbolization
levels, while the symbolization level of 2 results an unstable
performance, and the symbolization level of 3 outcomes the
worst performance. The main reason is that with a high
level of symbolization we may lose some key vulnerability
information in the source codes. Moreover, it should be
mentioned that symbolization groups of F +D outperforms
than symbolization groups of F + V with all data-sets, it
may due to there are many contents related to data type in
the source codes, after symbolizing them can better capture
the vulnerability information.
V. CONCLUSIONS
In this paper, we conducted a comprehensive experiment
to analyze the impact of the types of neural networks and
different data preprocessing methods on solving software
vulnerability detection problems. Our experimental results
show that the RVFL outperforms Bi-LSTM on the efficiency
of vulnerability detection, while Bi-LSTM performs better
on the precision. Moreover, vector representation using
doc2vec and an appropriate level of symbolization can
effectively improve the accuracy of vulnerability detection.
These experimental conclusions will provide researchers and
engineers with guidelines when choosing neural networks
and data preprocessing methods for vulnerability detection.
In the future, we will study the combination of Bi LSTM
and RVFL to design a new neural network and use it for
source code level vulnerability detection. It is expected that
the new method will meet real-life engineering needs in
terms of efficiency and precision.
ACKNOWLEDGMENT
This research is partially supported by the vulnerabil-
ity analysis technology for UAV communication protocol
project of National Key Laboratory of Science and Technol-
ogy on Information System Security, the Natural Science
Foundation of China (No. 61872104), and the Opening
Project of Shanghai Trusted Industrial Control Platform
(TICPSH202003008-ZC).
REFERENCES
[1] Common vulnerabilities and exposures. In https://cve.mitre.org.
[2] National vulnerability database. In https://nvd.nist.gov/.
[3] Tiantian Ji, Yue Wu, Chang Wang, and et al. The coming era of al-
phahacking?: A survey of automatic software vulnerability detection,
exploitation and patching techniques. In Third IEEE International
Conference on Data Science in Cyberspace, DSC 2018, Guangzhou,
China, June 18-21, 2018, pages 53–60, 2018.
[4] FlawFinder. In http://www.dwheeler.com/flawfinder.
[5] Rough Audit Tool. In https://code.google.com/archive/p/rough-
auditing-tool-for-security/.
[6] John Viega, J. T. Bloch, Y. Kohno, and Gary McGraw. ITS4: A static
vulnerability scanner for C and C++ code. In 16th Annual Computer
Security Applications Conference (ACSAC 2000), 11-15 December
2000, New Orleans, Louisiana, USA, page 257, 2000.
[7] Checkmarx. In https://www.checkmarx.com/.
[8] Coverity. In https://scan.coverity.com/.
[9] HP Fortify. In https://www.hpfod.com/.
[10] Seulbae Kim, Seunghoon Woo, Heejo Lee, and Hakjoo Oh. VUDDY:
A scalable approach for vulnerable code clone discovery. In 2017
IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA,
USA, May 22-26, 2017, pages 595–614, 2017.
[11] Zhen Li, Deqing Zou, Shouhuai Xu, and et al. Vulpecker: an
automated vulnerability detection system based on code similarity
analysis. In Proceedings of the 32nd Annual Conference on Com-
puter Security Applications, ACSAC 2016, Los Angeles, CA, USA,
December 5-9, 2016, pages 201–213, 2016.
[12] Siqi Ma, Ferdian Thung, David Lo, and et al. Vurle: Automatic
vulnerability detection and repair by learning from examples. In
Computer Security - ESORICS 2017 - 22nd European Symposium
on Research in Computer Security, Oslo, Norway, September 11-15,
2017, Proceedings, Part II, pages 229–246, 2017.
[13] Jacob A. Harer, Louis Y. Kim, Rebecca L. Russell, and et al.
Automated software vulnerability detection with machine learning.
CoRR, abs/1803.04497, 2018.
[14] Rebecca L. Russell, Louis Y. Kim, Lei H. Hamilton, and et al. Auto-
mated vulnerability detection in source code using deep representation
learning. In 17th IEEE International Conference on Machine Learning
and Applications, ICMLA 2018, Orlando, FL, USA, December 17-20,
2018, pages 757–762, 2018.
[15] Zhen Li, Deqing Zou, Shouhuai Xu, and et al. Vuldeepecker: A deep
learning-based system for vulnerability detection. In 25th Annual
Network and Distributed System Security Symposium, NDSS 2018,
San Diego, California, USA, February 18-21, 2018, 2018.
[16] Zhen Li, Deqing Zou, Jing Tang, and et al. A comparative study
of deep learning-based vulnerability detection system. IEEE Access,
7:103184–103197, 2019.
[17] Word2vec. In https://nvd.nist.gov/.
[18] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and
Yoshua Bengio. On the properties of neural machine translation:
Encoder-decoder approaches. In Proceedings of SSST@EMNLP 2014,
Eighth Workshop on Syntax, Semantics and Structure in Statistical
Translation, Doha, Qatar, 25 October 2014, pages 103–111, 2014.
[19] Boris Igelnik and Yoh-Han Pao. Stochastic choice of basis functions
in adaptive function approximation and the functional-link net. IEEE
Trans. Neural Networks, 6(6):1320–1329, 1995.
[20] Doc2vec. In https://radimrehurek.com/gensim/models/doc2vec.html.
[21] J. Ross Quinlan. Induction of decision trees. Machine Learning,
1(1):81–106, 1986.
[22] Corinna Cortes and Vladimir Vapnik. Support-vector networks.
Machine Learning, 20(3):273–297, 1995.
[23] Boris Chernis and Rakesh M. Verma. Machine learning methods
for software vulnerability detection. In Proceedings of the Fourth
ACM International Workshop on Security and Privacy Analytics,
IWSPA@CODASPY 2018, Tempe, AZ, USA, March 19-21, 2018, pages
31–39, 2018.
[24] Zhen Li, Deqing Zou, Shouhuai Xu, and et al. Sysevr: A framework
for using deep learning to detect software vulnerabilities. CoRR,
abs/1807.06756, 2018.
[25] Lior Wolf, Yair Hanani, Kfir Bar, and Nachum Dershowitz. Joint
word2vec networks for bilingual semantic representations. Int. J.
Comput. Linguistics Appl., 5(1):27–42, 2014.
[26] Quoc V. Le and Tomas Mikolov. Distributed representations of
sentences and documents. In Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Beijing, China, 21-26
June 2014, pages 1188–1196, 2014.
[27] Jinsong Su, Zhixing Tan, and Deyi Xiong et al. Lattice-based
recurrent neural network encoders for neural machine translation.
In Proceedings of the Thirty-First AAAI Conference on Artificial
Intelligence, February 4-9, 2017, San Francisco, California, USA,
pages 3302–3308, 2017.
[28] Yoh-Han Pao and Yoshiyasu Takefuji. Functional-link net computing:
Theory, system architecture, and functionalities. IEEE Computer,
25(5):76–79, 1992.
[29] Mike Schuster and Kuldip K. Paliwal. Bidirectional recurrent neural
networks. IEEE Trans. Signal Processing, 45(11):2673–2681, 1997.
[30] Weipeng Cao, Xizhao Wang, Zhong Ming, and Jinzhu Gao. A review
on neural networks with random weights. Neurocomputing, 275:278–
287, 2018.
[31] Weipeng Cao, Jinzhu Gao, Zhong Ming, Shubin Cai, and Hua Zheng.
Impact of probability distribution selection on RVFL performance.
In Meikang Qiu, editor, Smart Computing and Communication -
Second International Conference, SmartCom 2017, Shenzhen, China,
December 10-12, 2017, Proceedings, volume 10699 of Lecture Notes
in Computer Science, pages 114–124. Springer, 2017.
[32] Marcus Pendleton, Richard Garcia-Lebron, Jin-Hee Cho, and
Shouhuai Xu. A survey on systems security metrics. ACM Comput.
Surv., 49(4):62:1–62:35, 2017.
