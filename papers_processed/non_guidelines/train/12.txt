ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks
Qilong Wang1
, Banggu Wu1
, Pengfei Zhu1
, Peihua Li2
, Wangmeng Zuo3
, Qinghua Hu1,∗
1
Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University, China
2
Dalian University of Technology, China 3
Harbin Institute of Technology, China
Abstract
Recently, channel attention mechanism has demon-
strated to offer great potential in improving the perfor-
mance of deep convolutional neural networks (CNNs).
However, most existing methods dedicate to developing
more sophisticated attention modules for achieving better
performance, which inevitably increase model complexity.
To overcome the paradox of performance and complexity
trade-off, this paper proposes an Efficient Channel Atten-
tion (ECA) module, which only involves a handful of param-
eters while bringing clear performance gain. By dissecting
the channel attention module in SENet, we empirically show
avoiding dimensionality reduction is important for learning
channel attention, and appropriate cross-channel interac-
tion can preserve performance while significantly decreas-
ing model complexity. Therefore, we propose a local cross-
channel interaction strategy without dimensionality reduc-
tion, which can be efficiently implemented via 1D convolu-
tion. Furthermore, we develop a method to adaptively select
kernel size of 1D convolution, determining coverage of lo-
cal cross-channel interaction. The proposed ECA module
is efficient yet effective, e.g., the parameters and computa-
tions of our modules against backbone of ResNet50 are 80
vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respec-
tively, and the performance boost is more than 2% in terms
of Top-1 accuracy. We extensively evaluate our ECA mod-
ule on image classification, object detection and instance
segmentation with backbones of ResNets and MobileNetV2.
The experimental results show our module is more efficient
while performing favorably against its counterparts.
1. Introduction
Deep convolutional neural networks (CNNs) have been
widely used in computer vision community, and have
∗Qinghua Hu is the corresponding author.
Email: {qlwang, wubanggu, huqinghua}@tju.edu.cn. The work was sup-
ported by the National Natural Science Foundation of China (Grant No.
61806140, 61876127, 61925602, 61971086, U19A2073, 61732011), Ma-
jor Scientific Research Project of Zhejiang Lab (2019DB0ZX01). Q. Wang
was supported by National Postdoctoral Program for Innovative Talents.
Figure 1. Comparison of various attention modules (i.e.,
SENet [14], CBAM [33], A2
-Nets [4] and ECA-Net) using
ResNets [11] as backbone models in terms of classification ac-
curacy, network parameters and FLOPs, indicated by radiuses of
circles. Note that our ECA-Net obtains higher accuracy while hav-
ing less model complexity.
achieved great progress in a broad range of tasks, e.g., im-
age classification, object detection and semantic segmenta-
tion. Starting from the groundbreaking AlexNet [17], many
researches are continuously investigated to further improve
the performance of deep CNNs [29, 30, 11, 15, 19, 20, 32].
Recently, incorporation of channel attention into convolu-
tion blocks has attracted a lot of interests, showing great po-
tential in performance improvement [14, 33, 13, 4, 9, 18, 7].
One of the representative methods is squeeze-and-excitation
networks (SENet) [14], which learns channel attention for
each convolution block, bringing clear performance gain for
various deep CNN architectures.
Following the setting of squeeze (i.e., feature ag-
gregation) and excitation (i.e., feature recalibration) in
SENet [14], some researches improve SE block by cap-
turing more sophisticated channel-wise dependencies [33,
4, 9, 7] or by combining with additional spatial atten-
tion [33, 13, 7]. Although these methods have achieved
arXiv:1910.03151v4
[cs.CV]
7
Apr
2020
GAP
×
: element-wise product
Adaptive Selection of
Kernel Size:
5
k =

1 1 C
 

1 1 C
 

( )
k C

=
W
H
C
W
H
C

～
Figure 2. Diagram of our efficient channel attention (ECA) mod-
ule. Given the aggregated features obtained by global average
pooling (GAP), ECA generates channel weights by performing a
fast 1D convolution of size k, where k is adaptively determined
via a mapping of channel dimension C.
higher accuracy, they often bring higher model complex-
ity and suffer from heavier computational burden. Different
from the aforementioned methods that achieve better per-
formance at the cost of higher model complexity, this paper
focuses instead on a question: Can one learn effective chan-
nel attention in a more efficient way?
To answer this question, we first revisit the channel at-
tention module in SENet. Specifically, given the input fea-
tures, SE block first employs a global average pooling for
each channel independently, then two fully-connected (FC)
layers with non-linearity followed by a Sigmoid function
are used to generate channel weights. The two FC layers
are designed to capture non-linear cross-channel interac-
tion, which involve dimensionality reduction for controlling
model complexity. Although this strategy is widely used in
subsequent channel attention modules [33, 13, 9], our em-
pirical studies show dimensionality reduction brings side ef-
fect on channel attention prediction, and it is inefficient and
unnecessary to capture dependencies across all channels.
Therefore, this paper proposes an Efficient Channel At-
tention (ECA) module for deep CNNs, which avoids dimen-
sionality reduction and captures cross-channel interaction in
an efficient way. As illustrated in Figure 2, after channel-
wise global average pooling without dimensionality reduc-
tion, our ECA captures local cross-channel interaction by
considering every channel and its k neighbors. Such method
is proven to guarantee both efficiency and effectiveness.
Note that our ECA can be efficiently implemented by fast
1D convolution of size k, where kernel size k represents the
coverage of local cross-channel interaction, i.e., how many
neighbors participate in attention prediction of one channel.
To avoid manual tuning of k via cross-validation, we de-
velop a method to adaptively determine k, where coverage
of interaction (i.e., kernel size k) is proportional to channel
dimension. As shown in Figure 1 and Table 3, as opposed
to the backbone models [11], deep CNNs with our ECA
Model No DR Cross-channel Interaction Lightweight
SENet [14] ×
√
–
CBAM [33] ×
√
×
GE-θ− [13]
√
×
√
GE-θ [13]
√
× ×
GE-θ+ [13] ×
√
×
A2-Net [4] ×
√
×
GSoP-Net [9] ×
√
×
ECA-Net (Ours)
√ √ √
Table 1. Comparison of existing attention modules in terms of
whether no channel dimensionality reduction (No DR), cross-
channel interaction and less parameters than SE (indicated by
lightweight) or not.
module (called ECA-Net) introduce very few additional pa-
rameters and negligible computations, while bringing no-
table performance gain. For example, for ResNet-50 with
24.37M parameters and 3.86 GFLOPs, the additional pa-
rameters and computations of ECA-Net50 are 80 and 4.7e-
4 GFLOPs, respectively; meanwhile, ECA-Net50 outper-
forms ResNet-50 by 2.28% in terms of Top-1 accuracy.
Table 1 summarizes existing attention modules in terms
of whether channel dimensionality reduction (DR), cross-
channel interaction and lightweight model, where we can
see that our ECA module learn effective channel attention
by avoiding channel dimensionality reduction while captur-
ing cross-channel interaction in an extremely lightweight
way. To evaluate our method, we conduct experiments on
ImageNet-1K [6] and MS COCO [23] in a variety of tasks
using different deep CNN architectures.
The contributions of this paper are summarized as fol-
lows. (1) We dissect the SE block and empirically demon-
strate avoiding dimensionality reduction and appropriate
cross-channel interaction are important to learn effective
and efficient channel attention, respectively. (2) Based on
above analysis, we make an attempt to develop an extremely
lightweight channel attention module for deep CNNs by
proposing an Efficient Channel Attention (ECA), which in-
creases little model complexity while bringing clear im-
provement. (3) The experimental results on ImageNet-1K
and MS COCO demonstrate our method has lower model
complexity than state-of-the-arts while achieving very com-
petitive performance.
2. Related Work
Attention mechanism has proven to be a potential means
to enhance deep CNNs. SE-Net [14] presents for the first
time an effective mechanism to learn channel attention and
achieves promising performance. Subsequently, develop-
ment of attention modules can be roughly divided into two
directions: (1) enhancement of feature aggregation; (2)
combination of channel and spatial attentions. Specifically,
CBAM [33] employs both average and max pooling to ag-
gregate features. GSoP [9] introduces a second-order pool-
ing for more effective feature aggregation. GE [13] explores
spatial extension using a depth-wise convolution [5] to ag-
gregate features. CBAM [33] and scSE [27] compute spa-
tial attention using a 2D convolution of kernel size k × k,
then combine it with channel attention. Sharing similar phi-
losophy with Non-Local (NL) neural networks [32], GC-
Net [2] develops a simplified NL network and integrates
with the SE block, resulting in a lightweight module to
model long-range dependency. Double Attention Networks
(A2
-Nets) [4] introduces a novel relation function for NL
blocks for image or video recognition. Dual Attention Net-
work (DAN) [7] simultaneously considers NL-based chan-
nel and spatial attentions for semantic segmentation. How-
ever, most above NL-based attention modules can only be
used in a single or a few convolution blocks due to their high
model complexity. Obviously, all of the above methods fo-
cus on developing sophisticated attention modules for better
performance. Different from them, our ECA aims at learn-
ing effective channel attention with low model complexity.
Our work is also related to efficient convolutions, which
are designed for lightweight CNNs. Two widely used effi-
cient convolutions are group convolutions [37, 34, 16] and
depth-wise separable convolutions [5, 28, 38, 24]. As given
in Table 2, although these efficient convolutions involve less
parameters, they show little effectiveness in attention mod-
ule. Our ECA module aims at capturing local cross-channel
interaction, which shares some similarities with channel lo-
cal convolutions [36] and channel-wise convolutions [8];
different from them, our method investigates a 1D convolu-
tion with adaptive kernel size to replace FC layers in chan-
nel attention module. Comparing with group and depth-
wise separable convolutions, our method achieves better
performance with lower model complexity.
3. Proposed Method
In this section, we first revisit the channel attention mod-
ule in SENet [14] (i.e., SE block). Then, we make a em-
pirical diagnosis of SE block by analyzing effects of di-
mensionality reduction and cross-channel interaction. This
motivates us to propose our ECA module. In addition, we
develop a method to adaptively determine parameter of our
ECA, and finally show how to adopt it for deep CNNs.
3.1. Revisiting Channel Attention in SE Block
Let the output of one convolution block be X ∈
RW ×H×C
, where W, H and C are width, height and chan-
nel dimension (i.e., number of filters). Accordingly, the
weights of channels in SE block can be computed as
ω = σ(f{W1,W2}(g(X))), (1)
where g(X) = 1
W H
PW,H
i=1,j=1 Xij is channel-wise global
average pooling (GAP) and σ is a Sigmoid function. Let
Methods Attention #.Param. Top-1 Top-5
Vanilla N/A 0 75.20 92.25
SE σ(f{W1,W2}(y)) 2 × C2/r 76.71 93.38
SE-Var1 σ(y) 0 76.00 92.90
SE-Var2 σ(w y) C 77.07 93.31
SE-Var3 σ(Wy) C2 77.42 93.64
SE-GC1 σ(GC16(y)) C2/16 76.95 93.47
SE-GC2 σ(GCC/16(y)) 16 × C 76.98 93.31
SE-GC3 σ(GCC/8(y)) 8 × C 76.96 93.38
ECA-NS σ(ω) with Eq. (7) k × C 77.35 93.61
ECA (Ours) σ(C1Dk(y)) k = 3 77.43 93.65
Table 2. Comparison of various channel attention modules using
ResNet-50 as backbone model on ImageNet. #.Param. indicates
number of parameters of the channel attention module; indicates
element-wise product; GC and C1D indicate group convolutions
and 1D convolution, respectively; k is kernel size of C1D.
y = g(X), f{W1,W2} takes the form
f{W1,W2}(y) = W2ReLU(W1y), (2)
where ReLU indicates the Rectified Linear Unit [25]. To
avoid high model complexity, sizes of W1 and W2 are
set to C × (C
r ) and (C
r ) × C, respectively. We can see
that f{W1,W2} involves all parameters of channel attention
block. While dimensionality reduction in Eq. (2) can reduce
model complexity, it destroys the direct correspondence be-
tween channel and its weight. For example, one single FC
layer predicts weight of each channel using a linear com-
bination of all channels. But Eq. (2) first projects chan-
nel features into a low-dimensional space and then maps
them back, making correspondence between channel and
its weight be indirect.
3.2. Efficient Channel Attention (ECA) Module
After revisiting SE block, we conduct empirical com-
parisons for analyzing effects of channel dimensionality re-
duction and cross-channel interaction on channel attention
learning. According to these analyses, we propose our effi-
cient channel attention (ECA) module.
3.2.1 Avoiding Dimensionality Reduction
As discussed above, dimensionality reduction in Eq. (2)
makes correspondence between channel and its weight be
indirect. To verify its effect, we compare the original SE
block with its three variants (i.e., SE-Var1, SE-Var2 and SE-
Var3), all of which do not perform dimensionality reduc-
tion. As presented in Table 2, SE-Var1 with no parameter
is still superior to the original network, indicating channel
attention has ability to improve performance of deep CNNs.
Meanwhile, SE-Var2 learns the weight of each channel in-
dependently, which is slightly superior to SE block while
involving less parameters. It may suggest that channel and
its weight needs a direct correspondence while avoiding di-
mensionality reduction is more important than considera-
tion of nonlinear channel dependencies. Additionally, SE-
Var3 employing one single FC layer performs better than
two FC layers with dimensionality reduction in SE block.
All of above results clearly demonstrate avoiding dimen-
sionality reduction is helpful to learn effective channel at-
tention. Therefore, we develop our ECA module without
channel dimensionality reduction.
3.2.2 Local Cross-Channel Interaction
Given the aggregated feature y ∈ RC
without dimensional-
ity reduction, channel attention can be learned by
ω = σ(Wy), (3)
where W is a C × C parameter matrix. In particular, for
SE-Var2 and SE-Var3 we have
W =



















Wvar2 =



w1,1
· · · 0
.
.
.
...
.
.
.
0 . . . wC,C


 ,
Wvar3 =



w1,1
· · · w1,C
.
.
.
...
.
.
.
w1,C
. . . wC,C


 ,
(4)
where Wvar2 for SE-Var2 is a diagonal matrix, involving C
parameters; Wvar3 for SE-Var3 is a full matrix, involving
C×C parameters. As shown in Eq. (4), the key difference is
that SE-Var3 considers cross-channel interaction while SE-
Var2 does not, and consequently SE-Var3 achieves better
performance. This result indicates that cross-channel inter-
action is beneficial to learn channel attention. However, SE-
Var3 requires a mass of parameters, leading to high model
complexity, especially for large channel numbers.
A possible compromise between SE-Var2 and SE-Var3
is extension of Wvar2 to a block diagonal matrix, i.e.,
WG =



W1
G · · · 0
.
.
.
...
.
.
.
0 . . . WG
G


 , (5)
where Eq. (5) divides channel into G groups each of which
includes C/G channels, and learns channel attention in each
group independently, which captures cross-channel interac-
tion in a local manner. Accordingly, it involves C2
/G pa-
rameters. From perspective of convolution, SE-Var2, SE-
Var3 and Eq. (5) can be regarded as a depth-wise separa-
ble convolution, a FC layer and group convolutions, respec-
tively. Here, SE block with group convolutions (SE-GC) is
indicated by σ(GCG(y)) = σ(WGy). However, as shown
in [24], excessive group convolutions will increase memory
access cost and so decrease computational efficiency. Fur-
thermore, as shown in Table 2, SE-GC with varying groups
bring no gain over SE-Var2, indicating it is not an effective
scheme to capture local cross-channel interaction. The rea-
son may be that SE-GC completely discards dependences
among different groups.
In this paper, we explore another method to capture local
cross-channel interaction, aiming at guaranteeing both ef-
ficiency and effectiveness. Specifically, we employ a band
matrix Wk to learn channel attention, and Wk has





w1,1 · · · w1,k 0 0 · · · · · · 0
0 w2,2 · · · w2,k+1 0 · · · · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
...
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · wC,C−k+1 · · · wC,C





.
(6)
Clearly, Wk in Eq. (6) involves k × C parameters, which
is usually less than those of Eq. (5). Furthermore, Eq. (6)
avoids complete independence among different groups in
Eq. (5). As compared in Table 2, the method in Eq. (6)
(namely ECA-NS) outperforms SE-GC of Eq. (5). As for
Eq. (6), the weight of yi is calculated by only considering
interaction between yi and its k neighbors, i.e.,
ωi = σ
 k
X
j=1
wj
i yj
i

, yj
i ∈ Ωk
i , (7)
where Ωk
i indicates the set of k adjacent channels of yi.
A more efficient way is to make all channels share the
same learning parameters, i.e.,
ωi = σ
 k
X
j=1
wj
yj
i

, yj
i ∈ Ωk
i . (8)
Note that such strategy can be readily implemented by a fast
1D convolution with kernel size of k, i.e.,
ω = σ(C1Dk(y)), (9)
where C1D indicates 1D convolution. Here, the method in
Eq. (9) is called by efficient channel attention (ECA) mod-
ule, which only involves k parameters. As presented in Ta-
ble 2, our ECA module with k = 3 achieves similar results
with SE-var3 while having much lower model complexity,
which guarantees both efficiency and effectiveness by ap-
propriately capturing local cross-channel interaction.
3.2.3 Coverage of Local Cross-Channel Interaction
Since our ECA module (9) aims at appropriately capturing
local cross-channel interaction, so the coverage of interac-
tion (i.e., kernel size k of 1D convolution) needs to be de-
termined. The optimized coverage of interaction could be
tuned manually for convolution blocks with different chan-
nel numbers in various CNN architectures. However, man-
ual tuning via cross-validation will cost a lot of comput-
ing resources. Group convolutions have been successfully
Figure 3. PyTorch code of our ECA module.
adopted to improve CNN architectures [37, 34, 16], where
high-dimensional (low-dimensional) channels involve long
range (short range) convolutions given the fixed number of
groups. Sharing the similar philosophy, it is reasonable that
the coverage of interaction (i.e., kernel size k of 1D con-
volution) is proportional to channel dimension C. In other
words, there may exist a mapping φ between k and C:
C = φ(k). (10)
The simplest mapping is a linear function, i.e., φ(k) =
γ ∗ k − b. However, the relations characterized by linear
function are too limited. On the other hand, it is well known
that channel dimension C (i.e., number of filters) usually is
set to power of 2. Therefore, we introduce a possible solu-
tion by extending the linear function φ(k) = γ ∗ k − b to a
non-linear one, i.e.,
C = φ(k) = 2(γ∗k−b)
. (11)
Then, given channel dimension C, kernel size k can be
adaptively determined by
k = ψ(C) =
log2(C)
γ
+
b
γ odd
, (12)
where |t|odd indicates the nearest odd number of t. In this
paper, we set γ and b to 2 and 1 throughout all the experi-
ments, respectively. Clearly, through the mapping ψ, high-
dimensional channels have longer range interaction while
low-dimensional ones undergo shorter range interaction by
using a non-linear mapping.
3.3. ECA Module for Deep CNNs
Figure 2 illustrates the overview of our ECA module. Af-
ter aggregating convolution features using GAP without di-
mensionality reduction, ECA module first adaptively deter-
mines kernel size k, and then performs 1D convolution fol-
lowed by a Sigmoid function to learn channel attention. For
applying our ECA to deep CNNs, we replace SE block by
our ECA module following the same configuration in [14].
The resulting networks are named by ECA-Net. Figure 3
gives PyTorch code of our ECA.
4. Experiments
In this section, we evaluate the proposed method
on large-scale image classification, object detection and
instance segmentation using ImageNet [6] and MS
COCO [23], respectively. Specifically, we first assess the
effect of kernel size on our ECA module, and compare with
state-of-the-art counterparts on ImageNet. Then, we ver-
ify the effectiveness of our ECA-Net on MS COCO using
Faster R-CNN [26], Mask R-CNN [10] and RetinaNet [22].
4.1. Implementation Details
To evaluate our ECA-Net on ImageNet classification, we
employ four widely used CNNs as backbone models, in-
cluding ResNet-50 [11], ResNet-101 [11], ResNet-512 [11]
and MobileNetV2 [28]. For training ResNets with our
ECA, we adopt exactly the same data augmentation and
hyper-parameter settings in [11, 14]. Specifically, the input
images are randomly cropped to 224×224 with random hor-
izontal flipping. The parameters of networks are optimized
by stochastic gradient descent (SGD) with weight decay of
1e-4, momentum of 0.9 and mini-batch size of 256. All
models are trained within 100 epochs by setting the initial
learning rate to 0.1, which is decreased by a factor of 10
per 30 epochs. For training MobileNetV2 with our ECA,
we follow the settings in [28], where networks are trained
within 400 epochs using SGD with weight decay of 4e-5,
momentum of 0.9 and mini-batch size of 96. The initial
learning rate is set to 0.045, and is decreased by a linear
decay rate of 0.98. For testing on the validation set, the
shorter side of an input image is first resized to 256 and a
center crop of 224 × 224 is used for evaluation. All models
are implemented by PyTorch toolkit1
.
We further evaluate our method on MS COCO using
Faster R-CNN [26], Mask R-CNN [10] and RetinaNet [22],
where ResNet-50 and ResNet-101 along with FPN [21] are
used as backbone models. We implement all detectors by
using MMDetection toolkit [3] and employ the default set-
tings. Specifically, the shorter side of input images are re-
sized to 800, then all models are optimized using SGD with
weight decay of 1e-4, momentum of 0.9 and mini-batch size
of 8 (4 GPUs with 2 images per GPU). The learning rate
is initialized to 0.01 and is decreased by a factor of 10 af-
ter 8 and 11 epochs, respectively. We train all detectors
within 12 epochs on train2017 of COCO and report the re-
sults on val2017 for comparison. All programs run on a
PC equipped with four RTX 2080Ti GPUs and an Intel(R)
1https://github.com/BangguWu/ECANet
Method Backbone Models #.Param. FLOPs Training Inference Top-1 Top-5
ResNet [11]
ResNet-50
24.37M 3.86G 1024 FPS 1855 FPS 75.20 92.52
SENet [14] 26.77M 3.87G 759 FPS 1620 FPS 76.71 93.38
CBAM [33] 26.77M 3.87G 472 FPS 1213 FPS 77.34 93.69
A2-Nets [4]† 33.00M 6.50G N/A N/A 77.00 93.50
GCNet [2] 28.08M 3.87G N/A N/A 77.70 93.66
GSoP-Net1 [9] 28.05M 6.18G 596 FPS 1383 FPS 77.68 93.98
AA-Net [1]†,♦ 25.80M 4.15G N/A N/A 77.70 93.80
ECA-Net (Ours) 24.37M 3.86G 785 FPS 1805 FPS 77.48 93.68
ResNet [11]
ResNet-101
42.49M 7.34G 386 FPS 1174 FPS 76.83 93.48
SENet [14] 47.01M 7.35G 367 FPS 1044 FPS 77.62 93.93
CBAM [33] 47.01M 7.35G 270 FPS 635 FPS 78.49 94.31
AA-Net [1]†,♦ 45.40M 8.05G N/A N/A 78.70 94.40
ECA-Net (Ours) 42.49M 7.35G 380 FPS 1089 FPS 78.65 94.34
ResNet [11]
ResNet-152
57.40M 10.82G 281 FPS 815 FPS 77.58 93.66
SENet [14] 63.68M 10.85G 268 FPS 761 FPS 78.43 94.27
ECA-Net (Ours) 57.40M 10.83G 279 FPS 785 FPS 78.92 94.55
MobileNetV2 [28]
MobileNetV2
3.34M 319.4M 711 FPS 2086 FPS 71.64 90.20
SENet 3.40M 320.1M 671 FPS 2000 FPS 72.42 90.67
ECA-Net (Ours) 3.34M 319.9M 676 FPS 2010 FPS 72.56 90.81
Table 3. Comparison of different attention methods on ImageNet in terms of network parameters (#.Param.), floating point operations per
second (FLOPs), training or inference speed (frame per second, FPS), and Top-1/Top-5 accuracy (in %). †: Since the source code and
models of A2
-Nets and AA-Net are publicly unavailable, we do not compare their running time. ♦: AA-Net is trained with Inception data
augmentation and different setting of learning rates.
Xeon Silver 4112 CPU@2.60GHz.
4.2. Image Classification on ImageNet-1K
Here, we first assess the effect of kernel size on our ECA
module and verify the effectiveness of our method to adap-
tively determine kernel size, then we compare with state-
of-the-art counterparts and CNN models using ResNet-50,
ResNet-101, ResNet-152 and MobileNetV2.
4.2.1 Effect of Kernel Size (k) on ECA Module
As shown in Eq. (9), our ECA module involves a parame-
ter k, i.e., kernel size of 1D convolution. In this part, we
evaluate its effect on our ECA module and validate the ef-
fectiveness of our method for adaptive selection of kernel
size. To this end, we employ ResNet-50 and ResNet-101
as backbone models, and train them with our ECA module
by setting k be from 3 to 9. The results are illustrated in
Figure 4, from it we have the following observations.
Firstly, when k is fixed in all convolution blocks, ECA
module obtains the best results at k = 9 and k = 5 for
ResNet-50 and ResNet-101, respectively. Since ResNet-
101 has more intermediate layers that dominate perfor-
mance of ResNet-101, it may prefer to small kernel size.
Besides, these results show that different deep CNNs have
various optimal k, and k has a clear effect on performance
of ECA-Net. Furthermore, accuracy fluctuations (∼0.5%)
of ResNet-101 are larger than those (∼0.15%) of ResNet-
50, and we conjecture the reason is that the deeper net-
3 5 7 9
Number of k
76.8
77.0
77.2
77.4
77.6
77.8
78.0
78.2
78.4
78.6
78.8
Top-1
accuracy
SENet50
SENet101
ECA-Net50
ECA-Net101
ECA50-Adaptive
ECA101-Adaptive
Figure 4. Results of our ECA module with various numbers of k
using ResNet-50 and ResNet-101 as backbone models. Here, we
also give the results of ECA module with adaptive selection of
kernel size and compare with SENet as baseline.
works are more sensitive to the fixed kernel size than the
shallower ones. Additionally, kernel size that is adaptively
determined by Eq. (12) usually outperforms the fixed ones,
while it can avoid manual tuning of parameter k via cross-
validation. Above results demonstrate the effectiveness of
our adaptive kernel size selection in attaining better and sta-
ble results. Finally, ECA module with various numbers of
k consistently outperform SE block, verifying that avoid-
ing dimensionality reduction and local cross-channel inter-
action have positive effects on learning channel attention.
4.2.2 Comparisons Using Different Deep CNNs
ResNet-50 We compare our ECA module with several
state-of-the-art attention methods using ResNet-50 on Im-
ageNet, including SENet [14], CBAM [33], A2
-Nets [4],
AA-Net [1], GSoP-Net1 [9] and GCNet [2]. The evalua-
tion metrics include both efficiency (i.e., network param-
eters, floating point operations per second (FLOPs) and
training/inference speed) and effectiveness (i.e., Top-1/Top-
5 accuracy). For comparison, we duplicate the results of
ResNet and SENet from [14], and report the results of other
compared methods in their original papers. To test train-
ing/inference speed of various models, we employ publicly
available models of the compared CNNs, and run them on
the same computing platform. The results are given in Ta-
ble 3, where we can see that our ECA-Net shares almost the
same model complexity (i.e., network parameters, FLOPs
and speed) with the original ResNet-50, while achieving
2.28% gains in Top-1 accuracy. Comparing with state-of-
the-art counterparts (i.e., SENet, CBAM, A2
-Nets, AA-Net,
GSoP-Net1 and GCNet), ECA-Net obtains better or com-
petitive results while benefiting lower model complexity.
ResNet-101 Using ResNet-101 as backbone model, we
compare our ECA-Net with SENet [14], CBAM [33] and
AA-Net [1]. From Table 3 we can see that ECA-Net out-
performs the original ResNet-101 by 1.8% with almost
the same model complexity. Sharing the same tendency
on ResNet-50, ECA-Net is superior to SENet and CBAM
while it is very competitive to AA-Net with lower model
complexity. Note that AA-Net is trained with Inception data
augmentation and different setting of learning rates.
ResNet-152 Using ResNet-152 as backbone model, we
compare our ECA-Net with SENet [14]. From Table 3 we
can see that ECA-Net improves the original ResNet-152
over about 1.3% in terms of Top-1 accuracy with almost
the same model complexity. Comparing with SENet, ECA-
Net achieves 0.5% gain in terms of Top-1 with lower model
complexity. The results with respect to ResNet-50, ResNet-
101 and ResNet-152 demonstrate the effectiveness of our
ECA module on the widely used ResNet architectures.
MobileNetV2 Besides ResNet architectures, we also verify
the effectiveness of our ECA module on lightweight CNN
architectures. To this end, we employ MobileNetV2 [28]
as backbone model and compare our ECA module with SE
block. In particular, we integrate SE block and ECA mod-
ule in convolution layer before residual connection lying in
each ’bottleneck’ of MobileNetV2, and parameter r of SE
block is set to 8. All models are trained using exactly the
same settings. The results in Table 3 show our ECA-Net im-
proves the original MobileNetV2 and SENet by about 0.9%
and 0.14% in terms of Top-1 accuracy, respectively. Fur-
thermore, our ECA-Net has smaller model size and faster
training/inference speed than SENet. Above results verify
the efficiency and effectiveness of our ECA module again.
CNN Models #.Param. FLOPs Top-1 Top-5
ResNet-200 74.45M 14.10G 78.20 94.00
Inception-v3 25.90M 5.36G 77.45 93.56
ResNeXt-101 46.66M 7.53G 78.80 94.40
DenseNet-264 (k=32) 31.79M 5.52G 77.85 93.78
DenseNet-161 (k=48) 27.35M 7.34G 77.65 93.80
ECA-Net50 (Ours) 24.37M 3.86G 77.48 93.68
ECA-Net101 (Ours) 42.49M 7.35G 78.65 94.34
Table 4. Comparisons with state-of-the-art CNNs on ImageNet.
4.2.3 Comparisons with Other CNN Models
At the end of this part, we compare our ECA-Net50 and
ECA-Net101 with other state-of-the-art CNN models, in-
cluding ResNet-200 [12], Inception-v3 [31], ResNeXt [34],
DenseNet [15]. These CNN models have deeper and wider
architectures, and their results all are copied from the orig-
inal papers. As presented in Table 4, ECA-Net101 outper-
forms ResNet-200, indicating that our ECA-Net can im-
prove the performance of deep CNNs using much less com-
putational cost. Meanwhile, our ECA-Net101 is very com-
petitive to ResNeXt-101, while the latter one employs more
convolution filters and expensive group convolutions. In ad-
dition, ECA-Net50 is comparable to DenseNet-264 (k=32),
DenseNet-161 (k=48) and Inception-v3, but it has lower
model complexity. All above results demonstrate that our
ECA-Net performs favorably against state-of-the-art CNNs
while benefiting much lower model complexity. Note that
our ECA also has great potential to further improve the per-
formance of the compared CNN models.
4.3. Object Detection on MS COCO
In this subsection, we evaluate our ECA-Net on object
detection task using Faster R-CNN [26], Mask R-CNN [10]
and RetinaNet [22]. We mainly compare ECA-Net with
ResNet and SENet. All CNN models are pre-trained on Im-
ageNet, then are transferred to MS COCO by fine-tuning.
4.3.1 Comparisons Using Faster R-CNN
Using Faster R-CNN as the basic detector, we employ
ResNets of 50 and 101 layers along with FPN [21] as back-
bone models. As shown in Table 5, integration of either
SE block or our ECA module can improve performance of
object detection by a clear margin. Meanwhile, our ECA
outperforms SE block by 0.3% and 0.7% in terms of AP
using ResNet-50 and ResNet-101, respectively.
4.3.2 Comparisons Using Mask R-CNN
We further exploit Mask R-CNN to verify the effectiveness
of our ECA-Net on object detection task. As shown in Ta-
ble 5, our ECA module is superior to the original ResNet
by 1.8% and 1.9% in terms of AP under the settings of
Methods Detectors #.Param. GFLOPs AP AP50 AP75 APS APM APL
ResNet-50
Faster R-CNN
41.53 M 207.07 36.4 58.2 39.2 21.8 40.0 46.2
+ SE block 44.02 M 207.18 37.7 60.1 40.9 22.9 41.9 48.2
+ ECA (Ours) 41.53 M 207.18 38.0 60.6 40.9 23.4 42.1 48.0
ResNet-101 60.52 M 283.14 38.7 60.6 41.9 22.7 43.2 50.4
+ SE block 65.24 M 283.33 39.6 62.0 43.1 23.7 44.0 51.4
+ ECA (Ours) 60.52 M 283.32 40.3 62.9 44.0 24.5 44.7 51.3
ResNet-50
Mask R-CNN
44.18 M 275.58 37.2 58.9 40.3 22.2 40.7 48.0
+ SE block 46.67 M 275.69 38.7 60.9 42.1 23.4 42.7 50.0
+ 1 NL 46.50 M 288.70 38.0 59.8 41.0 N/A N/A N/A
+ GC block 46.90 M 279.60 39.4 61.6 42.4 N/A N/A N/A
+ ECA (Ours) 44.18 M 275.69 39.0 61.3 42.1 24.2 42.8 49.9
ResNet-101 63.17 M 351.65 39.4 60.9 43.3 23.0 43.7 51.4
+ SE block 67.89 M 351.84 40.7 62.5 44.3 23.9 45.2 52.8
+ ECA (Ours) 63.17 M 351.83 41.3 63.1 44.8 25.1 45.8 52.9
ResNet-50
RetinaNet
37.74 M 239.32 35.6 55.5 38.2 20.0 39.6 46.8
+ SE block 40.23 M 239.43 37.1 57.2 39.9 21.2 40.7 49.3
+ ECA (Ours) 37.74 M 239.43 37.3 57.7 39.6 21.9 41.3 48.9
ResNet-101 56.74 M 315.39 37.7 57.5 40.4 21.1 42.2 49.5
+ SE block 61.45 M 315.58 38.7 59.1 41.6 22.1 43.1 50.9
+ ECA (Ours) 56.74 M 315.57 39.1 59.9 41.8 22.8 43.4 50.6
Table 5. Object detection results of different methods on COCO val2017.
50 and 101 layers, respectively. Meanwhile, ECA module
achieves 0.3% and 0.6% gains over SE block using ResNet-
50 and ResNet-101 as backbone models, respectively. Us-
ing ResNet-50, ECA is superior to one NL [32], and is com-
parable to GC block [2] using lower model complexity.
4.3.3 Comparisons Using RetinaNet
Additionally, we verify the effectiveness of our ECA-Net
on object detection using one-stage detector, i.e., RetinaNet.
As compared in Table 5, our ECA-Net outperforms the orig-
inal ResNet by 1.8% and 1.4% in terms of AP for the net-
works of 50 and 101 layers, respectively. Meanwhile, ECA-
Net improves SE-Net over 0.2% and 0.4% for ResNet-50
and ResNet-101, respectively. In summary, the results in
Table 5 demonstrate that our ECA-Net can well generalize
to object detection task. Specifically, ECA module brings
clear improvement over the original ResNet, while outper-
forming SE block using lower model complexity. In par-
ticular, our ECA module achieves more gains for small ob-
jects, which are usually more difficult to be detected.
4.4. Instance Segmentation on MS COCO
Then, we give instance segmentation results of our ECA
module using Mask R-CNN on MS COCO. As compared in
Table 6, ECA module achieves notable gain over the origi-
nal ResNet while performing better than SE block with less
model complexity. For ResNet-50 as backbone, ECA with
lower model complexity is superior one NL [32], and is
comparable to GC block [2]. These results verify our ECA
module has good generalization ability for various tasks.
Methods AP AP50 AP75 APS APM APL
ResNet-50 34.1 55.5 36.2 16.1 36.7 50.0
+ SE block 35.4 57.4 37.8 17.1 38.6 51.8
+ 1 NL 34.7 56.7 36.6 N/A N/A N/A
+ GC block 35.7 58.4 37.6 N/A N/A N/A
+ ECA (Ours) 35.6 58.1 37.7 17.6 39.0 51.8
ResNet-101 35.9 57.7 38.4 16.8 39.1 53.6
+ SE block 36.8 59.3 39.2 17.2 40.3 53.6
+ ECA (Ours) 37.4 59.9 39.8 18.1 41.1 54.1
Table 6. Instance segmentation results of different methods using
Mask R-CNN on COCO val2017.
5. Conclusion
In this paper, we focus on learning effective channel at-
tention for deep CNNs with low model complexity. To
this end, we propose an efficient channel attention (ECA)
module, which generates channel attention through a fast
1D convolution, whose kernel size can be adaptively de-
termined by a non-linear mapping of channel dimension.
Experimental results demonstrate our ECA is an extremely
lightweight plug-and-play block to improve the perfor-
mance of various deep CNN architectures, including the
widely used ResNets and lightweight MobileNetV2. More-
over, our ECA-Net exhibits good generalization ability in
object detection and instance segmentation tasks. In future,
we will apply our ECA module to more CNN architectures
(e.g., ResNeXt and Inception [31]) and further investigate
incorporation of ECA with spatial attention module.
References
[1] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,
and Quoc V. Le. Attention augmented convolutional net-
works. arXiv:1904.09925, 2019.
[2] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han
Hu. Gcnet: Non-local networks meet squeeze-excitation net-
works and beyond. In ICCV Workshops, 2019.
[3] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei
Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu,
Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu,
Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli
Ouyang, Chen Change Loy, and Dahua Lin. MMDe-
tection: Open mmlab detection toolbox and benchmark.
arXiv:1906.07155, 2019.
[4] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng
Yan, and Jiashi Feng. A2
-Nets: Double attention networks.
In NIPS, 2018.
[5] François Chollet. Xception: Deep learning with depthwise
separable convolutions. In CVPR, 2017.
[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. ImageNet: A large-scale hierarchical image database.
In CVPR, 2009.
[7] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei
Fang, and Hanqing Lu. Dual attention network for scene
segmentation. In CVPR, 2019.
[8] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Chan-
nelnets: Compact and efficient convolutional neural net-
works via channel-wise convolutions. In NeurIPS, 2018.
[9] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global
second-order pooling convolutional networks. In CVPR,
2019.
[10] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B.
Girshick. Mask R-CNN. In ICCV, pages 2980–2988, 2017.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR,
2016.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In ECCV,
2016.
[13] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea
Vedaldi. Gather-excite: Exploiting feature context in convo-
lutional neural networks. In NeurIPS, 2018.
[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In CVPR, 2018.
[15] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely connected convolutional net-
works. In CVPR, 2017.
[16] Yani Ioannou, Duncan Robertson, Roberto Cipolla, and An-
tonio Criminisi. Deep roots: Improving cnn efficiency with
hierarchical filter groups. In CVPR, 2017.
[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
ImageNet classification with deep convolutional neural net-
works. In NIPS, 2012.
[18] Huayu Li. Channel locality block: A variant of squeeze-and-
excitation. arXiv, 1901.01493, 2019.
[19] Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng Zuo.
Is second-order information helpful for large-scale visual
recognition? In ICCV, 2017.
[20] Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou.
Factorized bilinear models for image recognition. In ICCV,
2017.
[21] Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He,
Bharath Hariharan, and Serge J. Belongie. Feature pyramid
networks for object detection. In CVPR, pages 936–944,
2017.
[22] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Dollár. Focal loss for dense object detection. In
ICCV, 2017.
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
ECCV, 2014.
[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufflenet V2: Practical guidelines for efficient CNN archi-
tecture design. In ECCV, 2018.
[25] Vinod Nair and Geoffrey E. Hinton. Rectified linear units
improve restricted boltzmann machines. In ICML, 2010.
[26] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. IEEE Trans. Pattern Anal. Mach.
Intell., 39(6):1137–1149, 2017.
[27] Abhijit Guha Roy, Nassir Navab, and Christian Wachinger.
Recalibrating fully convolutional networks with spatial and
channel ”squeeze and excitation” blocks. IEEE Trans. Med.
Imaging, 38(2):540–549, 2019.
[28] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey
Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR, 2018.
[29] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[30] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, 2015.
[31] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision. In CVPR, 2016.
[32] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In CVPR, 2018.
[33] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So
Kweon. CBAM: Convolutional block attention module. In
ECCV, 2018.
[34] Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu,
and Kaiming He. Aggregated residual transformations for
deep neural networks. In CVPR, 2017.
[35] Matthew D. Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In ECCV, pages 818–833,
2014.
[36] Dong-Qing Zhang. Clcnet: Improving the efficiency of con-
volutional neural network using channel local convolutions.
In CVPR, 2018.
Method CNNs #.Param. GFLOPs Top-1 Top-5
ResNet [11]
R-18
11.148M 1.699 70.40 89.45
SENet [14] 11.231M 1.700 70.59 89.78
CBAM [33] 11.234M 1.700 70.73 89.91
ECA-Net (Ours) 11.148M 1.700 70.78 89.92
ResNet [11]
R-34
20.788M 3.427 73.31 91.40
SENet [14] 20.938M 3.428 73.87 91.65
CBAM [33] 20.943M 3.428 74.01 91.76
ECA-Net (Ours) 20.788M 3.428 74.21 91.83
Table 7. Comparison of different methods using ResNet-18 (R-18)
and ResNet-34 (R-34) on ImageNet in terms of network parame-
ters (#.Param.), floating point operations per second (FLOPs), and
Top-1/Top-5 accuracy (in %).
[37] Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. In-
terleaved group convolutions. In ICCV, 2017.
[38] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufflenet: An extremely efficient convolutional neural net-
work for mobile devices. In CVPR, 2018.
Appendix I: Comparison of Different Methods
using ResNet-18 and ResNet-34 on ImageNet
Here, we compare different attention methods using
ResNet-18 and ResNet-34 on ImageNet. The results are
listed in Table 7, where the results of ResNet, SENet and
CBAM are duplicated from [33], and we train ECA-Net us-
ing the settings of hyper-parameters with [33]. From Ta-
ble 7, we can see that our ECA-Net improves the original
ResNet-18 and ResNet-34 over 0.38% abd 0.9% in Top-1
accuracy, respectively. Comparing with SENet and CBAM,
our ECA-Net achieves better performance using less model
complexity, showing the effectiveness of the proposed ECA
module.
Appendix II: Stacking More 1D Convolutions
in ECA Module
Intuitively, more 1D convolutions stacked in ECA mod-
ule may bring further improvement, due to increase of mod-
eling capability. Actually, we found that one extra 1D con-
volution brings trivial gains (∼0.1%) at the cost of slightly
increasing complexity, but more 1D convolutions degrade
performance, which may be caused by that more 1D con-
volutions make gradient backpropagation more difficult.
Therefore, our final ECA module contains only one 1D con-
volution.
Appendix III: Visualization of Weights
Learned by ECA Modules and SE Blocks
To further analyze the effect of our ECA module on
learning channel attention, we visualize the weights learned
hammer shark ambulance
medicine chest butternut squash
Figure 5. Example images of four random sampled classes on Im-
ageNet, including hammerhead shark, ambulance, medicine chest
and butternut squash.
by ECA modules and compare with SE blocks. Here,
we employ ResNet-50 as backbone model, and illus-
trate weights of different convolution blocks. Specifically,
we randomly sample four classes from ImageNet dataset,
which are hammerhead shark, ambulance, medicine chest
and butternut squash, respectively. Some example images
are illustrated in Figure 5. After training the networks, for
all images of each class collected from validation set of
ImageNet, we compute the channel weights of convolution
blocks on average. Figure 6 visualizes the channel weights
of conv i j, which indicates j-th convolution block in i-
th stage. Besides the visualization results of four random
sampled classes, we also give the distribution of the aver-
age weights across 1K classes as reference. The channel
weights learned by ECA modules and SE blocks are illus-
trated in bottom and top of each row, respectively.
From Figure 6 we have the following observations.
Firstly, for both ECA modules and SE blocks, the distri-
butions of channel weights for different classes are very
similar at the earlier layers (i.e., ones from conv 2 1 to
conv 3 4), which may be by reason of that the earlier layers
aim at capturing the basic elements (e.g., boundaries and
corners) [35]. These features are almost similar for differ-
ent classes. Such phenomenon also was described in the ex-
tended version of [14]2
. Secondly, for the channel weights
of different classes learned by SE blocks, most of them tend
to be the same (i.e., 0.5) in conv 4 2 ∼ conv 4 5 while the
differences among various classes are not obvious. On the
contrary, the weights learned by ECA modules are clearly
different across various channels and classes. Since convo-
lution blocks in 4-th stage prefer to learn semantic infor-
mation, so the weights learned by ECA modules can better
distinguish different classes. Finally, convolution blocks in
the final stage (i.e., conv 5 1, conv 5 2 and conv 5 3) cap-
ture high-level semantic features and they are more class-
2https://arxiv.org/abs/1709.01507
0 50 100 150 200 250
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 50 100 150 200 250
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(a) conv 2 1
0 50 100 150 200 250
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 50 100 150 200 250
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(b) conv 2 2
0 50 100 150 200 250
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 50 100 150 200 250
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(c) conv 2 3
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(d) conv 3 1
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(e) conv 3 2
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(f) conv 3 3
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 100 200 300 400 500
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(g) conv 3 4
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(h) conv 4 1
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(i) conv 4 2
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(j) conv 4 3
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(k) conv 4 4
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(l) conv 4 5
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 200 400 600 800 1000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(m) conv 4 6
0 500 1000 1500 2000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 500 1000 1500 2000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(n) conv 5 1
0 500 1000 1500 2000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 500 1000 1500 2000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(o) conv 5 2
0 500 1000 1500 2000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
0 500 1000 1500 2000
Channel Index
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Weight
hammerhead shark
ambulance
medicine chest
butternut squash
all
(p) conv 5 3
Figure 6. Visualization the channel weights of conv i j, where i indicate i-th stage and j is j-th convolution block in i-th stage. The
channel weights learned by ECA modules and SE blocks are illustrated in bottom and top of each row, respectively. Better view with
zooming in.
specific. Obviously, the weights learned by ECA modules
are more class-specific than ones learned by SE blocks.
Above results clearly demonstrate that the weights learned
by our ECA modules have better discriminative ability.
