                     Comparing Clusterings – An Axiomatic View
Marina Meilă                                                                          mmp@stat.washington.edu
University of Washington, Box 354322, Seattle, WA 98195-4322
                        Abstract                            will underscore, it is probably meaningless to search
     This paper views clusterings as elements of            for a best criterion for comparing clusterings, just as
     a lattice. Distances between clusterings are           it is meaningless to search for the best clustering al-
     analyzed in their relationship to the lattice.         gorithm. Algorithms are “good” in as much as they
     From this vantage point, we first give an ax-          match the task at hand. With respect to distances
     iomatic characterization of some criteria for          between clusterings, our goal is to better understand
     comparing clusterings, including the varia-            their properties, their limitations, and the implied as-
     tion of information and the unadjusted Rand            sumptions underlying them. This paper does so via
     index. Then we study other distances be-               an axiomatic study of several distances.
     tween partitions w.r.t these axioms and prove          We first discuss some desirable properties of distances
     an impossibility result: there is no “sensi-           between clusterings that amount to decomposing them
     ble” criterion for comparing clusterings that          additively over elementary operations on clusterings
     is simultaneously (1) aligned with the lattice         like splitting a cluster, merging two data sets, etc.
     of partitions, (2) convexely additive, and (3)         Then we give an axiomatic characterization for one
     bounded.                                               criterion, the variation of information. The choice is
                                                            not incidental: this criterion is closely matched to the
                                                            lattice of partitions, as it will be shown. It will turn
1. Introduction                                             out that the resulting axioms have each an intuitive
                                                            interpretation.
This paper views clusterings as elements of a lattice,
the natural algebraic structure for the partitions of       The axiomatic framework introduced is extended to
a set. A criterion for comparing clusterings is thus        characterize other distances between clusterings (the
seen as a function of pairs of elements in the lattice.     Mirkin metric, the Rand index and the van Dongen
The goal in doing this is to contribute to the better       metric). We also discuss the “classification error” clus-
understanding of the space of all clusterings of a set      tering distance. Finally, we derive an impossibility
and of the problem of comparing clusterings.                result for comparing clusterings: we show that no dis-
                                                            tance on the space of partitions can simultaneously
This task is is an important component in the evalua-
                                                            satisfy three desirable properties, each of which makes
tion of clustering algorithms. A clustering, or a clus-
                                                            the distance intuitive in some sense.
tering algorithm, can be evaluated by internal crite-
ria, e.g distortion, likelihood, that are generally prob-
lem and algorithm dependent. There is another kind          2. Additive properties for distances and
of evaluation, called external evaluation, that simply          the lattice of partitions
measures how close is the obtained clustering to a gold
standard clustering. The comparison criteria studied        This section introduces the basic notation, then in-
here are all external. As such, they are independent        troduces the lattice of partitions and and lists some
of the algorithm or of the way the clusterings were         properties related to the lattice representation which
obtained.                                                   will be relevant in the rest of the paper.
There are many competing criteria for comparing clus-       A clustering C is a partition of a data set D into sets
terings, with no clear best choice. In fact, as this paper  C1 , C2 , . . . CK called clusters such that
Appearing in Proceedings of the 22 nd International Confer-                                     K
ence on Machine Learning, Bonn, Germany, 2005. Copy-                                            [
right 2005 by the author(s)/owner(s).                                      Ck ∩ Cl = ∅ and         Ck = D.
                                                                                               k=1

                                      Comparing Clusterings – An Axiomatic View
Let the number of data points in D and in cluster Ck           tances between clusterings that are “aligned” with the
be n P  and nk respectively. We have, of course, that          lattice, in the sense that d(C, C 0 ) can be expressed as
           K
n =        k=1 nk . We also assume that nk ≥ 1; in other       a sum of distances along edges of the lattice?
words, that K represents the number of non-empty
                                                               In the following we give a formal definition of three
clusters.
                                                               additivity properties, all related to the lattice of parti-
Let a second clustering of the same data set D be              tions, and we explain why they are desirable.
                          0
C 0 = {C10 , C20 , . . . CK                          0
                            0 }, with cluster sizes nk 0 . The
                                                               Additivity w.r.t refinement (AR) If C 0 is obtained
two clusterings may have different numbers of clusters.
                                                               from C by splitting one or more clusters, then we say
To compare C and C 0 is to define a symmetric, non-
                                                               that C 0 is a refinement of C. A distance d is additive
negative function d(C, C 0 ) that measures how different
                                                               w.r.t refinement iff for any clusterings C, C 0 , C 00 such
are the two clusterings. If C = C 0 then d(C, C 0 ) = 0.
                                                               that C 0 is a refinement of C and C 00 a refinement of C 0
We shall call the function d a distance although in
                                                               we have
general it may not satisfy the triangle inequality. If
the triangle inequality is satisfied, then d is called a                      d(C, C 00 ) = d(C, C 0 ) + d(C 0 , C 00 )   (1)
metric.
It should also be noted that this definition is not            For instance, C 00 = {{a}, {b}, {c}, {d}} is a refinement
aligned to the majority of clustering comparison cri-          of C 0 = {{a}, {b}, {c, d}}, which in turn is a refinement
teria, like the Rand (Rand, 1971), Fowlkes-Mallows             of C = {{a, b}, {c, d}}. The AR property says that the
(Fowlkes & Mallows, 1983) and other indices. These             distance d(C, C 00 ) is a sum of the distances correspond-
are taking values in [0, 1], with larger values as the         ing to the two succesive refinements that transform C
clusterings become more similar (and being 1 when              into C 00 . Cluster splitting corresponds to taking down-
C = C 0 ). This will not preclude us to consider these         ward steps in the Hasse diagram.
indices, as one can turn any index i(C, C 0 ) into a dis-      Additivity w.r.t the join (AJ). The join of clus-
tance by simply setting d(C, C 0 ) = 1 − i(C, C 0 ).           terings C and C 0 is defined as
The number of points in the intersection of clusters Ck
of C and Ck0 0 of C 0 is denoted nkk0 .                        C × C 0 = {Ck ∩ Ck0 0 | Ck ∈ C, Ck0 0 ∈ C 0 , Ck ∩ Ck0 0 6= ∅}
                                                                                                                          (2)
                        nkk0 = |Ck ∩ Ck0 0 |                   Hence, the join of two clusterings is the clustering
                                                               formed from all the nonempty intersections of clusters
A distance d between that depends only on the relative
                                                               from C with clusters from C 0 . A distance d is additive
values nkk0 /n and does not directly depend on n is said
                                                               w.r.t to the join iff for any clusterings C, C 0
to be n-invariant.
The lattice of partitions One can represent all clus-                     d(C, C 0 ) = d(C, C × C 0 ) + d(C 0 , C × C 0 ) (3)
terings of a finite set D as the nodes of a graph, as
illustrated by figure 1; in this graph an edge between         This property is relevant for clusterings C, C 0 which are
C, C 0 will be present if C 0 is obtained by splitting a clus- not a refinement of each other. One can think of ob-
ter of C into two parts. The set of all clusterings of a       taining such a C 0 from C by a series of cluster splits
dataset D forms a lattice called the lattice of partitions     (downward steps along the lattice edges) followed by a
(Stanley, 1997). The graph just described is known as          set of cluster mergers (upward steps in the lattice). A
the Hasse diagram1 of this lattice. At the top of the          distance d is AJ if it can be expressed as a sum of dis-
diagram is the clustering with just one cluster, denoted       tances along such a path. Note that usually there are
by 1̂ = {D}. At the bottom is 0̂ = {{1}, {2}, . . . {n}}       several possible paths between two clusterings (see for
the clustering with n clusters each containing a sin-          instance {{a}, {b, c, d}} and {{a, c}, {b}, {d}} in figure
gle point. For all but the smallest n, the space of all        1); the sum is the same no matter what path is taken.
clusterings, although finite, is huge (superexponential        From a practical point of view, AJ also means that for
in n (Stanley, 1997)); a graphical representation like         any two clusterings C, C 0 there is a clustering C × C
the Hasse diagram can aid the understanding of the             (and usually others) which has more clusters than
complex relationships between and thus guide us in             both C and C 0 but is closer to both of them than
choosing or designing the most relevant distances on           d(C, C 0 ). In a geometric sense, the join C, C 0 is “on
this space. In particular, we may ask if there are dis-        the line segment” between C, C 0 . If in an applica-
    1
      The emphasized terms in this section represent stan-     tion two clusterings should be close only if the num-
dard lattice terminology. Their precise definitions can be     ber of clusters K, K 0 are nearly equal, then the dis-
found in e.g (Stanley, 1997).                                  tance d should not be AJ. However, there are other

                                         Comparing Clusterings – An Axiomatic View
                                                               {a,b,c,d}
            {a}{b,c,d}         {b}{a,c,d}   {c}{a,b,d}        {d}{a,b,c}      {a,b}{c,d}      {a,c}{b,d}  {a,d}{b,c}
          {a},{b}{c,d} {a},{c}{b,d} {a},{d}{b,c}             {a,b}{c},{d}    {a,c}{b},{d} {a,d}{b},{c}
                                                            {a},{b}{c},{d}
Figure 1. The lattice of partitions of D ={a,b,c,d}. Note that 1̂ ={{a,b,c,d}}, 0̂ ={{a},{b},{c},{d}}; the clusterings
1̂, {{a},{b, c, d}}, {{a},{b},{c, d}}, 0̂ are collinear according to A3; the clusterings {{a},{b, c, d}}, {{a},{b},{c, d}},
{{a,b},{c, d}} are collinear according to A4; the clusterings {{a},{b, c, d}}, {{a},{b},{c, d}}, {{b},{a, c, d}} and 1̂ are
on a closed straight line; and there are 3 straight lines from {{d},{a, b, c}} to 0̂.
applications, image segmentation being a prime ex-                      we will ask what properties uniquely define a certain
ample, where breaking off a set b of pixels from                        distance. The focus will be on the additive properties
a segment {a, b} is less of a mistake than break-                       discussed above. Hence the complementary question:
ing them off and attaching them to another seg-                         how can one characterize all the distances that satisfy
ment c (i.e we want d({{a, b}, {c}}, {{a}, {b}, {c}}) <                 them?
d({{a, b}, {c}}, {{a}, {b, c}})). In such applications,
                                                                        The reason we pay special attention to the properties
the number of segments is less meaningful than the
                                                                        AV, CA and AJ in this paper, is because splitting a
grouping of the pixels; therefore AJ is reasonable as
                                                                        cluster, forming a clustering by taking the union of two
it represents the total error as as sum of two kinds of
                                                                        clusterings on two subsets of the data, and merging two
error.
                                                                        clusters are (sequences of) elementary and intuitive
Convex additivity (CA). Let C = {C1 , . . . CK } be a                   operations that one can do on clusterings. It is easy
clustering and C 0 , C 00 be two refinements of C. Denote               and natural to think of changing a clustering C into
by Ck0 (Ck00 ) the partitioning induced by C 0 (respectively            C 0 by appying these operations one after the other. If
C 00 ) on Ck . Let P (k) represent the proportion of data               the distance d satisfies AV, CA and AJ, then d will
points that belong to cluster Ck . Then                                 measure the change between C and C 0 as a sum of
                                  K                                     elementary changes, each corresponding to one step.
                                 X
                d(C 0 , C 00 ) =     P (k)d(Ck0 , Ck00 )        (4)     Finally, if d is also a metric, some additional geometric
                                 k=1                                    insights are possible. One can extend the notion of a
This property expresses additivity of d over the sub-                   straight line from Euclidean space to a metric space: 3
lattices corresponding to the individual clusters Ck . In               points in a metric space for which triangle inequality is
particular, CA implies that if only some cluster(s) of                  satisfied with equality are said to lie on a straight line,
C are changed to obtain C 0 , then then d(C, C 0 ) depends              in other words to be collinear. Additivity w.r.t refine-
only on the affected clusters, and is independent on                    ment implies that the clusterings along a vertical chain
how the unaffected part of D is partitioned. For an                     of lattice edges are collinear. As all “vertical” straight
example, consider the pairs C = {{a, b}, {c, d}}, C 0 =                 lines meet both at 1̂ and 0̂, this space is clearly non-
C = {{a}, {b}, {c, d}} and C˜ = {{a, b}, {c}, {d}}, C˜ =                Euclidean. In general, if C 0 is a refinement of C, then
{{a}, {b}, {c}, {d}}. In both cases the first cluster is                each of the possible ways of subdividing C to obtain
split while the remaining cluster(s) are left unchanged.                C 0 generates a straight line in the lattice. Unless C, C 0
Therefore, if d is CA then d(C, C 0 ) = d(C,       ˜ C˜0 ).             are connected by a single edge, there will be multiple
                                                                        “straight lines” between the two clusterings. Figure 1
Properties AJ and CA were described in (Meilă, 2003),
                                                                        illustrates these properties on a simple example. An
where several distances were discussed w.r.t satisfying
                                                                        even more interesting picture is implied by the “hor-
them. In this section we extend and deepen the geo-
                                                                        izontal” straight lines that exist because of the addi-
metric view suggested there. The remainder of the pa-
                                                                        tivity w.r.t the joint. These lines are composed of the
per addresses the reverse question: instead of checking
                                                                        vertical “descending” segment C, C × C 0 , continued by
whether a given distance satisfies a certain property,

                                           Comparing Clusterings – An Axiomatic View
the “ascending” segment C × C 0 , C 0 . Figure 1 shows an           A5 Scale Denote by CK         U
                                                                                                     the “uniform” clustering, i.e
                                                                                                                           U
example of such a straight line. Moreover, using both                    the clustering with K equal clusters. If CK          exists,
properties, one can derive that the union of any two                     then
chains that have the same endpoints forms a “closed”                                                 U
                                                                                              d(1̂, CK ) = log K
straight line according to such a metric.
                                                                   In the above, 1̂nk is the 1̂ clustering of the dataset
3. The axioms of the variation of                                  Ck containing nk points. The proof of the theorem is
    information                                                    constructive and is given in the appendix. Note that
                                                                   the axioms do not require d to be a metric; this follows
The variation of information (dVI ), introduced in                 implicitly.
(Meilă, 2003), measures the distance between two clus-
terings in terms of the information difference between             Intuitively, axioms A2 and A3 describe the geomet-
them.                                                              ric properties of dVI , i.e that it is aligned with the
                                                                   lattice of partitions. Axiom A2 is a weak version of
                                                                   the AR property defined in the previous section. Ax-
          dVI (C, C 0 ) = H(C) + H(C 0 ) − 2I(C, C 0 )         (5) ioms A4 and A5 set the scale of d and in particular its
                                                              2    logarithmic growth rate. They are reminiscent of the
where H and I represent respectively the entropies of
and the mutual information3 between the two cluster-               postulates III and IV of entropy as given in (Rènyi,
ings.                                                              1970).
In (Meilă, 2003) the variation of information was                 It is interesting to see what happens if the last two
shown to satisfy AJ and AC and that it is a metric,                axioms are changed. In other words, if one maintains
among several other properties. It is also easy to show            that the distance has to be aligned with the lattice of
that it satisfies additivity w.r.t refinement (see the ap-         partitions, but allows the scale to differ. This is what
pendix).                                                           we are going to do in the next section.
Now we show that a subset of these properties uniquely
defines dVI .                                                      4. Other metrics for comparing
                                                                       clusterings
Theorem 1 The variation of information is the
unique cluster comparison criterion d that satisfies the           The clustering literature contains quite a number of
axioms:                                                            criteria for comparing clusterings: the Rand index
                                                                   (Rand, 1971), the Jaccard index (Ben-Hur et al.,
 A1 Symmetry For any two clusterings C, C 0                        2002), the Folwkes-Mallows index (Fowlkes & Mallows,
                                                                   1983), the Huber and Arabie indices (Hubert & Ara-
                           d(C, C 0 ) = d(C 0 , C)                 bie, 1985), the Mirkin metric (Mirkin, 1996), the Van
                                                                   Dongen metric (van Dongen, 2000), as well as statisti-
 A2 Additivity w.r.t refinement Denote by 0̂ and                   cally “adjusted” versions of some of the above (Hubert
      1̂ the unique clusterings having K = n respectively          & Arabie, 1985).
      K = 1 clusters. For any clustering C
                                                                   We shall start with two criteria, the Mirkin and Van
                      d(0̂, C) + d(C, 1̂) = d(0̂, 1̂)              Dongen metrics, for which we give an axiomatic char-
                                                                   acterization.
 A3 Additivity w.r.t the join For any two cluster-
      ings C, C 0                                                  The Mirkin metric is defined by (Mirkin, 1996)
              d(C, C 0 ) = d(C, C × C 0 ) + d(C 0 , C × C 0 )                           X            X             XX
                                                                      d0M (C, C 0 ) =       n2k +       n0k0 2 − 2        n2kk0  (6)
                                                                                         k           k0            k   k0
 A4 Convex additivity Let C = {C1 , . . . CK } be a
      clustering and C 0 be a refinement of C. Denote by           This metric can also be rewritten (Ben-Hur et al.,
      Ck0 the partitioning induced by C 0 on Ck . Then             2002) as
                                      K
                                          nk
                                                                                   d0M (C, C 0 ) = 2Ndisagree (C, C 0 )
                                     X
                   d(C, C 0 ) =              d(1̂nk , Ck0 )                                                                      (7)
                                          n
                                     k=1
                   PK                                              where Ndisagree is defined as the number of point pairs
   2
     H(C) = − k=1 nnk log nnk                                      which are in the same cluster under C but in different
                   PK PK 0 nk,k0                nk,k0 n n0 0
   3        0
     I(C, C ) =         k=1    k0 =1    n
                                            log   n
                                                        k
                                                       n n
                                                            k
                                                                   clusters under C 0 or viceversa. The Rand index is de-

                                         Comparing Clusterings – An Axiomatic View
fined as                                                         Theorem 4 The unique cluster comparison criterion
                                                          0      d that satisfies axioms A1–A3, A5.D and
                          n(n − 1) − 2Ndisagree (C, C )
        iR (C, C 0 ) =                                       (8)
                                     n(n − 1)
                                                                 A4.M Let C 0 be a refinement of C and denote by Ck0 the
Therefore the characterization of dM below will reflect          clustering induced by C 0 on Ck ∈ C. Then
immediately on the (unadjusted) Rand index. In what
follows we shall use an rescaled form of the Mirkin                                           K
                                                                                             X    n2k
metric which is n-invariant and bounded.                                              0
                                                                               d(C, C ) =             d(1̂nk , Ck0 )
                                                                                                  n2
                                                                                             k=1
                                     d0M (C, C 0 )
                    dM (C, C 0 ) =                           (9)
                                           n2                    is the invariant Mirkin metric dM .
The Van Dongen criterion was also proved to be a                 Thus, the invariant Mirkin metric is also aligned with
metric (van Dongen, 2000)                                        the lattice of partitions. The additivity axiom A4.M
                          X                   X                  has several consequences. First, it shows that dM is lo-
  d0D (C, C 0 ) = 2n −         max   n kk 0 −      max nkk0 (10)
                                 k 0                 k           cal, i.e changes inside one cluster depend only on the
                           k                   k0
                                                                 relative size of that cluster and of the nature of the
As with the Mirkin metric, we shall use its bounded              change, and are not affected by how the rest of the
n-invariant version                                              data set is partitioned. But the union of two or more
                                                                 datasets shrinks distances (because of the weighting
                                     d0D (C, C 0 )
                    dD (C, C 0 ) =                          (11) with the squares of the proportions), a rather counter-
                                          2n                     intuitive behavior. It is worth recalling that all these
                                                                 properties of the Mirkin metric are readily translated
To proceed with our axiomatic study, we first compute
                                         U                       into similar properties of the unadjusted Rand index.
the distances between 1̂ and CK             under the invariant
                                                                 The “unnatural” behavior of the Rand index with in-
Van Dongen and Mirkin metrics.
                                                                 creasing K has been noted early on and is the main
                                                                 reason why this index is used mostly in its adjusted
                                                  
                          U          1           1
                 dD (1̂, CK ) =            1−                    form (Hubert & Arabie, 1985).
                                     2          K
                          U                 1                    The van Dongen metric is horizontally aligned with the
                 dM (1̂, CK ) = 1−
                                            K                    lattice of partitions but not vertically, that is it satisfies
                                                                 axioms A1, A3, A4, A5.D. To uniquely characterize it,
With respect to convex additivity, it is easy to prove
                                                                 we introduced axiom A2.D, which implies A5.D but is
(see also (Meilă, 2003)) that dD is convexely additive,
                                                                 significantly stronger4 .
while the Mirkin metric satisfies
                                 K                               How about other, more popular indices for comparing
                                X    n2k                         partitions? The space does not permit us to describe
                        0
               dM (C, C ) =              dM (1̂nk , Ck0 )   (12)
                                     n2                          and compare all of them here (such comparisons can be
                                k=1
                                                                 found in (Ben-Hur et al., 2002; Hubert & Arabie, 1985;
whenever C 0 is a refinement of C and Ck0 represents the         Meilă, 2003; Wallace, 1983) and others). One fact
partitioning induced by C 0 on cluster Ck of C.                  shown in (Meilă, 2003) is that the Jaccard, Fowlkes-
We now can replace the scale axioms of dVI with ax-              Mallows, some of the Huber and Arabie and all the
ioms corresponding to dD and dM respectively. We                 adjusted indices, including the widely used adjusted
obtain a negative                                                Rand index, are non-local (that is, a change inside a
                                                                 single cluster counts differently depending on how the
Theorem 2 There is no cluster comparison criterion               rest of the data is clustered). Therefore they will rate
that satisfies axioms A1–A4 and                                  worse on the scale of understandability and in par-
A5.D d(1̂, CK   U
                  ) = 1 − 1/K                                    ticular cannot satisfy the convex additivity property.
                                                                 Also, it is easy to show that most of the above in-
Theorem 3 The unique cluster comparison criterion                dices are only asymptotically n-invariant, so their ax-
d that satisfies axioms A1, A3, A4, and                          iomatic description would be much more complicated
                                                                 (and consequently a much less illuminating exercise).
                                     1h       maxk nk i
           A2.D        d(1̂, C) =        1−                          4
                                     2             n                   We have not proved that A2.D is the weakest possible
                                                                 axiom that uniquely deterimines dD . But we believe this
is the invariant van Dongen metric dD .                          as likely and will consider it in further research.

                                   Comparing Clusterings – An Axiomatic View
Another very interesting criterion, the classification er-     Then, the result below shows that A5 is essentially
ror, is discussed in the next section.                         superfluous.
                                                               Theorem 6 Any clustering comparison criterion sat-
5. The classification error metric                             isfying A1–A4 and A5.H is identical to dVI up to a
The classification error (CE) distance dCE is defined          multiplicative constant.
as
                                       K                       In other words, the variation of information is the
                               1      X
           dCE (C, C 0 ) = 1 − max        nk,σ(k)         (13) only “sensible” (that is symmetric, n-invariant, with
                               n σ                                    U
                                      k=1                      d(1̂, CK ) non-decreasing) criterion that is convexely ad-
                                                               ditive and aligned to the lattice of partitions.
In the above, it is assumed w.l.o.g that K ≤ K 0 , σ is
an injective mapping of {1, . . . K} into {1, . . . K 0 }, and From theorem 6 the following impossibility result fol-
the maximum is taken over all such mappings. In other          lows immediately.
words, for each σ we have a (partial) correspondence
between the cluster labels in C and C 0 ; now looking at       Corrollary 7 There is no d symmetric, n-invariant,
                                                                            U
clustering as a classification task with the fixed label       with d(1̂, CK  ) non-decreasing, that satisfies simultane-
correspondence, we compute the “classification error”          ously the following three properties:
of C 0 w.r.t C. The minimum possible “classification           • d is aligned to the lattice of partitions (axioms A2,
error” under all correspondences is dCE .                      A3)
                                                               • d is convexly additive (axiom A4)
From the computational point of view, it is not neces-         • d is bounded
sary to explicitly enumerate all correspondences (order
K!). The maximum can be computed in polinomial                 Hence, there is no criterion for comparing clusterings
time as the solution of a linear program identical to          that can satisfy all three of the above desirable prop-
the maximum bipartite matching algorithm in graph              erties. The users of distances between clusterings will
theory (Golumbic, 1980). The CE distance is simple             have to make choices. What are the tradeoffs?
and intuitive, especially if the two clusterings are close
together. The following lemma describes its properties         The first property gives geometric intuition in addition
from the point of view of the axioms considered in this        to the intuition one would get from having a metric.
paper.                                                         Preserving this property would be useful in designing
                                                               search algorithms in the space of clusterings and prov-
Lemma 5 The classification error distance dCE sat-             ing their properties.
isfies axioms A1 (symmetry), A4 (convex additivity),           The second one is, in our opinion, the most important
A5.D (scales like 1 − 1/K). It violates axioms A2 and          for the “understandability” of a distance, because it is
A3, hence it is not aligned with the lattice of partitions.    a law of composition. It says that under unions of sets,
                                                               the distances on the parts are weighted in proportion
The proof is sufficiently simple and has been omitted.         to the sizes of the parts and then summed.
The above result shows similarities betweed dCE and
especially the dD metric, and underscores its convex           The argument for the third property is partly “his-
additivity, locality, and n-invariance, all of which con-      torical”. The vast majority of criteria for comparing
tribute to making dCE a most intutitive criterion. The         clusterings are bounded between 0 and 1. Moreover,
dissimilarity is of course the non-alignment with the          in statistics, the tradition is to indicate identity be-
lattice of partitions. We have not yet found a complete        tween two clusterings by a 1 (like in the Rand, Fowlkes-
characterization of dCE , this is a matter of further re-      Mallows, Jaccard indices and their adjusted versions).
search.                                                        Another very appealing reason to use a distance that
                                                               is bounded (for instance between 0 and 1) is to inter-
                                                               pret it as a probability. When can we do this? The
6. An impossibility result for                                 aforementioned indices can all be interpreted as prob-
    comparing partitions                                       abilities (see the original papers (Rand, 1971; Fowlkes
                                                               & Mallows, 1983; Hubert & Arabie, 1985; Ben-Hur
Theorem 2 prompts the question: what kind of scal-
                                                               et al., 2002) for details), but their adjusted versions
ings in A5 are compatible with A1–A4? To answer
                                                               can not (Hubert & Arabie, 1985). In this respect, per-
this question, we change A5 to the weaker A5.H:
                                                               haps the most interpretable is the classification error
             U
A5.H d(1̂, CK  ) = h(K) where h is a non-decreasing            dCE , which is indeed the optimal error probability if
function of K.                                                 clustering was regarded as classification.

                                  Comparing Clusterings – An Axiomatic View
Note however that a criterion that is bounded between      Acknowledgements
0 and 1 carries the implicit assumption that cluster-
ings can only get negligibly more diverse as the number    Thanks to Nilesh Dalvi and Pavel Krivitsky who each
of clusters increases. Thus, while using dCE may be        gave a proof for lemma 8 and to the anonymous re-
the most natural and intuitive when K is small, this       viewer who pointed me to the work of C. Rajski.
distance will loose its resolution power for large K.      This research was partly supported by NSF grant IIS-
Whether a bounded or unbounded criterion for com-          0313339.
paring clusterings is better depends ultimately on the
clustering application at hand. This paper’s aim in        Proofs
this respect is to underscore the possible choices and
their consequences.                                        Proof of AR for the dVI distance. dVI can be ex-
                                                           pressed as a sum of conditional entropies (Meilă, 2003)
7. Conclusion                                                          dVI (C, C 0 ) = H(C|C 0 ) + H(C 0 |C 0 )   (14)
This is the first axiomatic approach to comparing clus-    The proof then follows from elementary properties of
terings. What are the benfits of such an exercise?         the conditional entropy (see (Cover & Thomas, 1991)
Characterizing distances between clusterings in terms      for details): first, if C 0 is a refinement of C, then
of axioms highlights the essential properties of these     H(C|C 0 ) = 0; then, one applies the chain rule for con-
distances, from which all others follow. For example,      ditional entropy.
we have seen that being aligned to the lattice of par-
                                                           Proof of Theorem 1 From A4 and A5 we have              that
titions is a very strong requirement: together with an
                                                                                      X nk
additivity constraint (like A4 or A4.M), it completely                d(0̂, C) =              d(1̂, CnUk )        (15)
determines the values of a distance on the lattice.                                         n
                                                                                       k
                                                                                      X nk
An impossibility result is also important, because it re-                         =            log nk             (16)
veals an essential characteristic of the problem itself.                                    n
                                                                                       k
In this case, very loosely speaking, we have shown that                               X nk          nk
                                                                                  =           (log       + log n) (17)
the lattice of partitions, which is the space where all                                     n        n
                                                                                       k
possible clusterings lie, has too rich a structure to be
                                                                                  =   log n − H(C)                (18)
packed inside a bounded ball without breaking some of
the structure. For an analogy, one can say that the Eu-    From A2 we get d(1̂, C) = log n − d(0̂, C) = H(C).
clidean n-dimensional space, another space with rich       For any two clusterings C, C 0 define by Ck the clustering
structure, cannot be folded into a sphere without los-     induced by C 0 on Ck ∈ C.
ing some of its properties. Thus, presenting distances
                                                                                               K
between clusterings in an axiomatic framework helps                                           X    nk
better understand their properties and their limita-                    d(C, C × C 0 )   =             d(1̂, Ck ) (19)
                                                                                                    n
                                                                                              k=1
tions. It is interesting to note that an impossibility
                                                                                               K
result for clustering exists in (Kleinberg, 2002). The                                        X    nk
                                                                                         =             H(Ck )     (20)
Kleinberg paper refers to clustering criteria (e.g single                                           n
                                                                                              k=1
linkage, mean-squared error, etc) and shows that there
is none that satisfies three desirable properties.                                       =    H(C|C 0 )           (21)
We have given a prominent role to the lattice of par-      Therefore, by A2, d(C, C 0 ) = H(C|C 0 )+H(C 0 |C), Q.E.D
titions, interpreting distances as functions on pairs of   Proof of Theorem 2
points in the lattice. We believe this view will also                                    X nk
become useful in the future, whether one will use dis-                   d(0̂, C)   =            d(1̂, CnUk )     (22)
tances aligned to the lattice like the dVI , or completely                                    n
                                                                                          k
unaligned like dCE . Clustering is a hard problem and                                    X nk                
                                                                                                           1
seeing clusterings as nodes in a graph opens the possi-                             =               1−            (23)
                                                                                              n           nk
                                                                                          k
bility of applying new techniques, based on graph and
lattice theory, to this task.                                                                 K
                                                                                    = 1−                          (24)
                                                                                              n
                                                           Therefore d(1̂, C) = (1−1/n)−(1−K/n) = (K −1)/n
                                                           if |C| = K. This contradicts A5 according to which
                                                                  U
                                                           d(1̂, CK ) = (K − 1)/K.

                                      Comparing Clusterings – An Axiomatic View
Proof of Theorems 3 and 4 These proofs follow the            Fowlkes, E. B., & Mallows, C. L. (1983). A method for
same steps as the proof of Theorem 1 and are therefore         comparing two hierarchical clusterings. Journal of
omitted.                                                       the American Statistical Association, 78, 553–569.
Proof of Theorem 6 We have consecutively:                    Golumbic, M. (1980). Algorithmic graph theory and
                                                               perfect graphs. Academic Press, New York.
            d(1̂, 0̂) = h(n) by A5.H                   (25)
            d(1̂, C) = h(n) − d(0̂, C) by A2           (26)  Hubert, L., & Arabie, P. (1985). Comparing partitions.
                           X nk                                Journal of Classification, 2, 193–218.
            d(0̂, C) =             h(nk ) by A4        (27)
                                n
                            k                                Kleinberg, J. (2002). An impossibility theorem for
                 U
          d(1̂, CK  )  =   h(n) − d(0̂, CKU
                                            )          (28)    clustering. Advances in Neural Information Process-
                                        1   n                  ing Systems. Cambridge, MA: MIT Press.
                       = h(n) − K h( )                 (29)
                                       K K                   Meilă, M. (2003). Comparing clusterings by the varia-
Since n/K = M is an integer, and recalling A5.H we             tion of information. Proceedings of the Sixteenth An-
can rewrite the last equality as                               nual Conference ofn Computational Learning The-
                                                               ory (COLT). Springer.
                 h(K) = h(KM ) − h(M )
                                                             Mirkin, B. (1996). Mathematical classification and
or equivalently                                                clustering. Kluwer Academic Press.
                 h(KM ) = h(K) + h(M )                 (30)  Rand, W. M. (1971). Objective criteria for the evalua-
                                                               tion of clustering methods. Journal of the American
for any positive integers K, M . By lemma 8 below,             Statistical Association, 66, 846–850.
this implies that h(n) = C log n for all n = 1, 2, 3, . . ..
                                                             Rènyi, A. (1970). Probability theory. North-Holland.
It follows that A1-A4 together with A5.H imply essen-
tially the original A5 (up to the multiplicative constant    Stanley, R. P. (1997). Enumerative combinatorics.
C) and therefore d cannot be but proportional to the           Cambridge Unversity Press.
VI.
                                                             van Dongen, S. (2000). Performance criteria for graph
Lemma 8 Let h : {1, 2, . . .} → [0, ∞) be a non-               clustering and Markov cluster experiments (Techni-
decreasing function satisfying (30) for any positive in-       cal Report INS-R0012). Centrum voor Wiskunde en
tegers K, M . Then h(n) = C log n for any n.                   Informatica.
                                                             Wallace, D. L. (1983). Comment. Journal of the Amer-
Proof Let h(2) = C. We prove that h(n) = C log n.
                                                               ican Statistical Association, 78, 569–576.
Let a = log(nq ) = q log n with q a large positive inte-
ger. Then
        h(2bac )     ≤ h(2a ) = h(nk ) ≤      h(2dae ) (31)
           bacC          ≤ qh(n); ≤           daeC     (32)
            bac               h(n)            dae
              a
                         ≤  C log n ; ≤        a
                                                       (33)
The middle term does not depend on q, while the left
and right tend to 1 for q increasing to infinity, which
implies h(n) = C log n.
References
Ben-Hur, A., Elisseeff, A., & Guyon, I. (2002). A
   stability based method for discovering structure in
   clustered data. Pacific Symposium on Biocomputing
   (pp. 6–17).
Cover, T. M., & Thomas, J. A. (1991). Elements of
   information theory. Wiley.

