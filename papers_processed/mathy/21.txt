       Iterative Learning for Reliable Crowdsourcing Systems
           The MIT Faculty has made this article openly available. Please share
           how this access benefits you. Your story matters.
Citation                David R. Karger, Sewoong Oh, Devavrat Shah. "Iterative Learning for
                        Reliable Crowdsourcing Systems" Neural Information Processing
                        Systems, 2011: 1953-1961.
As Published            http://nips.cc/Conferences/2011/Program/accepted-papers.php
Publisher               Neural Information Processing Systems
Version                 Author's final manuscript
Citable link            http://hdl.handle.net/1721.1/73460
Terms of Use            Creative Commons Attribution-Noncommercial-Share Alike 3.0
Detailed Terms          http://creativecommons.org/licenses/by-nc-sa/3.0/

         Iterative Learning for Reliable Crowdsourcing
                                                 Systems
                     David R. Karger               Sewoong Oh               Devavrat Shah
                       Department of Electrical Engineering and Computer Science
                                   Massachusetts Institute of Technology
                                           Cambridge, MA 02139
                                {karger,swoh,devavrat}@mit.edu
                                                   Abstract
            Crowdsourcing systems, in which tasks are electronically distributed to numerous
            “information piece-workers”, have emerged as an effective paradigm for human-
            powered solving of large scale problems in domains such as image classification,
            data entry, optical character recognition, recommendation, and proofreading. Be-
            cause these low-paid workers can be unreliable, nearly all crowdsourcers must
            devise schemes to increase confidence in their answers, typically by assigning
            each task multiple times and combining the answers in some way such as ma-
            jority voting. In this paper, we consider a general model of such crowdsourcing
            tasks, and pose the problem of minimizing the total price (i.e., number of task as-
            signments) that must be paid to achieve a target overall reliability. We give a new
            algorithm for deciding which tasks to assign to which workers and for inferring
            correct answers from the workers’ answers. We show that our algorithm signifi-
            cantly outperforms majority voting and, in fact, is asymptotically optimal through
            comparison to an oracle that knows the reliability of every worker.
1     Introduction
Background. Crowdsourcing systems have emerged as an effective paradigm for human-powered
problem solving and are now in widespread use for large-scale data-processing tasks such as image
classification, video annotation, form data entry, optical character recognition, translation, recom-
mendation, and proofreading. Crowdsourcing systems such as Amazon Mechanical Turk1 provide
a market where a “taskmaster” can submit batches of small tasks to be completed for a small fee by
any worker choosing to pick them up. For example, a worker may be able to earn a few cents by indi-
cating which images from a set of 30 are suitable for children (one of the benefits of crowdsourcing
is its applicability to such highly subjective questions).
Since typical crowdsourced tasks are tedious and the reward is small, errors are common even among
workers who make an effort. At the extreme, some workers are “spammers”, submitting arbitrary
answers independent of the question in order to collect their fee. Thus, all crowdsourcers need
strategies to ensure the reliability of answers. Because the worker crowd is large, anonymous, and
transient, it is generally difficult to build up a trust relationship with particular workers.2 It is also
difficult to condition payment on correct answers, as the correct answer may never truly be known
and delaying payment can annoy workers and make it harder to recruit them for your future tasks.
Instead, most crowdsourcers resort to redundancy, giving each task to multiple workers, paying
them all irrespective of their answers, and aggregating the results by some method such as majority
    1
      http://www.mturk.com
    2
      For certain high-value tasks, crowdsourcers can use entrance exams to “prequalify” workers and block
spammers, but this increases the cost and still provides no guarantee that the prequalified workers will try hard.
                                                        1

voting. For such systems there is a natural core optimization problem to be solved. Assuming the
taskmaster wishes to achieve a certain reliability in their answers, how can they do so at minimum
cost (which is equivalent to asking how they can do so while asking the fewest possible questions)?
Several characteristics of crowdsourcing systems make this problem interesting. Workers are neither
persistent nor identifiable; each batch of tasks will be solved by a worker who may be completely
new and who you may never see again. Thus one cannot identify and reuse particularly reliable
workers. Nonetheless, by comparing one worker’s answer to others’ on the same question, it is
possible to draw conclusions about a worker’s reliability, which can be used to weight their answers
to other questions in their batch. However, batches must be of manageable size, obeying limits on
the number of tasks that can be given to a single worker. Another interesting aspect of this problem
is the choice of task assignments. Unlike many inference problems which makes inferences based
on a fixed set of signals, our algorithm can choose which signals to measure by deciding which
questions to ask which workers.
In the following, we will first define a formal model that captures these aspects of the problem. We
will then describe a scheme for deciding which tasks to assign to which workers and introduce a
novel iterative algorithm to infer the correct answers from the workers’ responses. We show our
algorithm is optimal for a broad range of parameters, by comparison to an oracle that knows the
error probability of workers and can thus make optimal decisions.
Setup. We model a set of m tasks {ti }i∈[m] as each being associated with an unobserved ‘correct’
answer si ∈ {±1}. Here and after, we use [N ] to denote the set of first N integers. In the earlier im-
age categorization example, each task corresponds to labeling an image as suitable for children (+1)
or not (−1). We assign these tasks to n workers from the crowd, which we denote by {wj }j∈[n] .
When a task is assigned to a worker, we get a possibly inaccurate answer from the worker. We
use Aij ∈ {±1} to denote the answer if task ti is assigned to worker wj . Some workers are more
diligent or have more expertise than others, while some other workers might be spammers. We
choose a simple model to capture this diversity in workers’ reliability: we assume that each worker
wj is characterized by a reliability pj ∈ [0, 1], and that they make errors randomly on each question
they answer. Precisely, if task ti is assigned to worker wj then
                                        
                                             si with probability pj ,
                            Aij =
                                           −si with probability 1 − pj ,
and Aij = 0 if ti is not assigned to wj . The random variable Aij is independent of any other
event given pj . (Throughout this paper, we use boldface characters to denote random variables and
random matrices unless it is clear from the context.) The underlying assumption here is that the
error probability of a worker does not depend on the particular task and all the tasks share an equal
level of difficulty. Hence, each worker’s performance is consistent across different tasks.
We further assume that the reliability of workers {pj }j∈[n] are independent and identically dis-
tributed random variables with a given distribution on [0, 1]. One example is spammer-hammer
model where, each worker is either a ‘hammer’ with probability q or is a ‘spammer’ with probability
1 − q. A hammer answers all questions correctly, in which case pj = 1, and a spammer gives
random answers, in which case pj = 1/2. Given this random variable pj , we define an important
parameter q ∈ [0, 1], which captures the ‘average quality’ of the crowd:
                                         q  ≡    E[(2pj − 1)2 ] .
A value of q close to one indicates that a large proportion of the workers are diligent, whereas q
close to zero indicates that there are many spammers in the crowd. The definition of q is consistent
with use of q in the spammer-hammer model. We will see later that our bound on the error rate
of our inference algorithm holds for any distribution of pj but depends on the distribution only
through this parameter q. It is quite realistic to assume the existence of a prior distribution for pj .
The model is therefore quite general: in particular, it is met if we simply randomize the order in
which we upload our task batches, since this will have the effect of randomizing which workers
perform which batches, yielding a distribution that meets our requirements. On the other hand, it
is not realistic to assume that we know what the prior is. To execute our inference algorithm for a
given number of iterations, we do not require any knowledge of the distribution of the reliability.
However, q is necessary in order to determine how many times a task should be replicated and how
many iterations we need to run to achieve certain reliability.
                                                    2

Under this crowdsourcing model, a taskmaster first decides which tasks should be assigned to which
workers, and then estimates the correct solutions {si }i∈[m] once all the answers {Aij } are submit-
ted. We assume a one-shot scenario in which all questions are asked simultaneously and then an
estimation is performed after all the answers are obtained. In particular, we do not allow allocating
tasks adaptively based on the answers received thus far. Then, assigning tasks to nodes amounts to
designing a bipartite graph G({ti }i∈[m] ∪ {wj }j∈[n] , E) with m task and n worker nodes. Each
edge (i, j) ∈ E indicates that task ti was assigned to worker wj .
Prior Work. A naive approach to identify the correct answer from multiple workers’ responses is to
use majority voting. Majority voting simply chooses what the majority of workers agree on. When
there are many spammers, majority voting is error-prone since it weights all the workers equally.
We will show that majority voting is provably sub-optimal and can be significantly improved upon.
To infer the answers of the tasks and also the reliability of workers, Dawid and Skene [1] proposed
an algorithm based on expectation maximization (EM) [2]. This approach has also been applied in
classification problems where the training data is annotated by low-cost noisy ‘labelers’ [3, 4]. In
[5] and [6], this EM approach has been applied to more complicated probabilistic models for image
labeling tasks. However, the performance of these approaches are only empirically evaluated, and
there is no analysis that proves performance guarantees. In particular, EM algorithms require an
initial starting point which is typically randomly guessed. The algorithm is highly sensitive to this
initialization, making it difficult to predict the quality of the resulting estimate. The advantage of
using low-cost noisy ‘labelers’ has been studied in the context of supervised learning, where a set
of labels on a training set is used to find a good classifier. Given a fixed budget, there is a trade-
off between acquiring a larger training dataset or acquiring a smaller dataset but with more labels
per data point. Through extensive experiments, Sheng, Provost and Ipeirotis [7] show that getting
repeated labeling can give considerable advantage.
Contributions. In this work, we provide a rigorous treatment of designing a crowdsourcing sys-
tem with the aim of minimizing the budget to achieve completion of a set of tasks with a certain
reliability. We provide both an asymptotically optimal graph construction (random regular bipartite
graph) and an asymptotically optimal algorithm for inference (iterative algorithm) on that graph.
As the main result, we show that our algorithm performs as good as the best possible algorithm.
The surprise lies in the fact that the optimality of our algorithm is established by comparing it with
the best algorithm, one that is free to choose any graph, regular or irregular, and performs optimal
estimation based on the information provided by an oracle about reliability of the workers. Previous
approaches focus on developing inference algorithms assuming that a graph is already given. None
of the prior work on crowdsourcing provides any systematic treatment of the graph construction.
To the best of our knowledge, we are the first to study both aspects of crowdsourcing together and,
more importantly, establish optimality.
Another novel contribution of our work is the analysis technique. The iterative algorithm we in-
troduce operates on real-valued messages whose distribution is a priori difficult to analyze. To
overcome this challenge, we develop a novel technique of establishing that these messages are sub-
Gaussian (see Section 3 for a definition) using recursion, and compute the parameters in a closed
form. This allows us to prove the sharp result on the error rate, and this technique could be of
independent interest for analyzing a more general class of algorithms.
2    Main result
Under the crowdsourcing model introduced, we want to design algorithms to assign tasks and es-
timate the answers. In what follows, we explain how to assign tasks using a random regular graph
and introduce a novel iterative algorithm to infer the correct answers. We state the performance
guarantees for our algorithm and provide comparisons to majority voting and an oracle estimator.
Task allocation.
                      Assigning tasks amounts to designing a bipartite graph G {ti }i∈[m] ∪
{wj }j∈[n] , E , where each edge corresponds to a task-worker assignment. The taskmaster makes a
choice of how many workers to assign to each task (the left degree l) and how many tasks to assign
to each worker (the right degree r). Since the total number of edges has to be consistent, the number
of workers n directly follows from ml = nr. To generate an (l, r)-regular bipartite graph we use
                                                    3

a random graph generation scheme known as the configuration model in random graph literature
[8, 9]. In principle, one could use arbitrary bipartite graph G for task allocation. However, as we
show later in this paper, random regular graphs are sufficient to achieve order-optimal performance.
Inference algorithm. We introduce a novel iterative algorithm which operates on real-valued task
messages {xi→j }(i,j)∈E and worker messages {yj→i }(i,j)∈E . The worker messages are initialized
as independent Gaussian random variables. At each iteration, the messages are updated according
to the described update rule, where ∂i is the neighborhood of ti . Intuitively, a worker message yj→i
represents our belief on how ‘reliable’ the worker j is, such that our             Pfinal estimate is a weighted sum
of the answers weighted by each worker’s reliability: ŝi = sign( j∈∂i Aij yj→i ).
                      Iterative Algorithm
                      Input: E, {Aij }(i,j)∈E , kmax
                      Output: Estimation ŝ {Aij }
                      1: For all (i, j) ∈ E do
                                               (0)
                                 Initialize yj→i with random Zij ∼ N (1, 1) ;
                      2: For k = 1, . . . , kmax do
                                                               (k)        P                  (k−1)
                                 For all (i, j) ∈ E do xi→j ← j 0 ∈∂i\j Aij 0 yj 0 →i ;
                                                              (k)        P                   (k)
                                 For all (i, j) ∈ E do yj→i ← i0 ∈∂j\i Ai0 j xi0 →j ;
                                                           P                  (kmax −1)
                      3: For all i ∈ [m] do xi ← j∈∂i             Aij yj→i              ;
                      4: Output estimate vector ŝ {Aij } = [sign(xi )] .
While our algorithm is inspired by the standard Belief Propagation (BP) algorithm for approximat-
ing max-marginals [10, 11], our algorithm is original and overcomes a few critical limitations of the
standard BP. First, the iterative algorithm does not require any knowledge of the prior distribution of
pj , whereas the standard BP requires the knowledge of the distribution. Second, there is no efficient
way to implement standard BP, since we need to pass sufficeint statistics (or messages) which under
our general model are distributions over the reals. On the otherhand, the iterative algorithm only
passes messages that are real numbers regardless of the prior distribution of pj , which is easy to im-
plement. Third, the iterative algorithm is provably asymptotically order-optimal. Density evolution,
explained in detail in Section 3, is a standard technique to analyze the performance of BP. Although
we can write down the density evolution for the standard BP, we cannot analyze the densities, ana-
lytically or numerically. It is also very simple to write down the density evolution equations for the
iterative algorithm, but it is not trivial to analyze the densities in this case either. We develop a novel
technique to analyze the densities and prove optimality of our algorithm.
2.1    Performance guarantee
We state the main analytical result of this paper: for random (l, r)-regular bipartite graph based task
assignments with our iterative inference algorithm, the probability of error decays exponentially in
lq, up to a universal constant and for a broad range of the parameters l, r and q. With a reasonable
choice of l = r and both scaling like (1/q) log(1/), the proposed algorithm is guarantted to achieve
error less than  for any  ∈ (0, 1/2). Further, an algorithm independent lower bound that we
establish suggests that such an error dependence on lq is unavoidable. Hence, in terms of the task
allocation budget, our algorithm is order-optimal. The precise statements follow next. Let µ =
E[2pj − 1] and recall q = E[(2pj − 1)2 ]. To lighten the notation, let ˆl ≡ l − 1 and r̂ ≡ r − 1. Define
                                            2q                    1  1 − (1/q 2 ˆlr̂)k−1
                           ρ2k  ≡                       + 3+                                   .
                                      µ2 (q 2 ˆlr̂)k−1            qr̂      1 − (1/q 2 ˆlr̂)
For q 2 ˆlr̂ > 1, let ρ2∞ ≡ limk→∞ ρ2k such that
                                   ρ2∞             3 + (1/qr̂) q 2 ˆlr̂/(q 2 ˆlr̂ − 1) .
                                                              
                                          =
Then we can show the following bound on the probability of making an error.
Theorem 2.1. For fixed l > 1 and r > 1, assume that m tasks are assigned to n = ml/r workers
according to a random (l, r)-regular graph drawn from the configuration model. If the distribution
                                                           4

of the worker reliaiblity satisfy µ ≡ E[2pj − 1] > 0 and q 2 > 1/(ˆlr̂), then for any s ∈ {±1}m , the
estimates from k iterations of the iterative algorithm achieve
                                 m
                             1 X                                              2
                       lim          P si 6= ŝi {Aij }(i,j)∈E       ≤   e−lq/(2ρk ) .                 (1)
                     m→∞ m
                                i=1
As we increase k, the above bound converges to a non-trivial limit.
Corollary 2.2. Under the hypotheses of Theorem 2.1,
                                    m
                                1 X                                               2
                   lim lim             P si 6= ŝi {Aij }(i,j)∈E      ≤   e−lq/(2ρ∞ ) .               (2)
                  k→∞ m→∞ m
                                   i=1
Even if we fix the value√of q = E[(2pj − 1)2 ], different distributions of pj can have different values
of µ in the range of [q, q]. Surprisingly, the asymptotic bound on the error rate does not depend on
µ. Instead, as long as q is fixed, µ only affects how fast the algorithm converges (cf. Lemma 2.3).
Notice that the bound in (2) is only meaningful when it is less than a half, whence ˆlr̂q 2 > 1 and
lq > 6 log(2) > 4. While as a task master the case of ˆlr̂q 2 < 1 may not be of interest, for the purpose
of completeness we comment on the performance of our algorithm in this regime. Specifically, we
empirically observe that the error rate increases as the number of iterations k increases. Therefore,
it makes sense to use k = 1. In which case, the algorithm essentially boils down to the majority
rule. We can prove the following error bound. The proof is omitted due to a space constraint.
                                  m
                               1 X                                         2
                         lim          P si 6= ŝi {Aij }(i,j)∈E      ≤ e−lµ     /4
                                                                                   .                  (3)
                       m→∞ m
                                 i=1
2.2    Discussion
Here we make a few comments relating to the execution of the algorithm and the interpretation of
the main results. First, the iterative algorithm is efficient with runtime comparable to the simple
majority voting which requires O(ml) operations.
Lemma 2.3. Under the hypotheses of Theorem 2.1, the total computational cost sufficient to achieve
the bound in Corollary 2.2 up to any constant factor in the exponent is O(ml log(q/µ2 )/ log(q 2 ˆlr̂)).
                                     √
By definition, we have q ≤ µ ≤ q. The runtime is the worst     √ when µ = q, which happens√under
the spammer-hammer model, and it is the best when µ = q which happens if pj = (1 + q)/2
deterministically. There exists a (non-iterative) polynomial time algorithm with runtime independent
of q for computing the estimate which achieves (2), but in practice we expect that the number
of iterations needed is small enough that the iterative algorithm will outperform this non-iterative
algorithm. Detailed proof of Lemma 2.3 will be skipped here due to a space constraint.
Second, the assumption that µ > 0 is necessary. If there is no assumption on µ, then we cannot
distinguish if the responses came from tasks with {si }i∈[m] and workers with {pj }j∈[n] or tasks
with {−si }i∈[m] and workers with {1 − pj }j∈[n] . Statistically, both of them give the same output.
In the case when we know that µ < 0, we can use the same algorithm changing the sign of the final
output and get the same performance guarantee.
Third, our algorithm does not require any information on the distribution of pj . Further, unlike
other EM based algorithms, the iterative algorithm is not sensitive to initialization and with random
initialization converges to a unique estimate with high probability. This follows from the fact that
the algorithm is essentially computing a leading eigenvector of a particular linear operator.
2.3    Relation to singular value decomposition
The leading singular vectors are often used to capture the important aspects of datasets in matrix
form. In our case, the leading left singular vector of A can be used to estimate the correct answers,
where A ∈ {0, ±1}m×n is the m × n adjacency matrix of the graph G weighted by the submitted
                                                    5

answers. We can compute it using power iteration: for u ∈ Rm and v ∈ Rn , starting with a
randomly initialized v, power iteration iteratively updates u and v according to
                                       X                                  X
                     for all i, ui =        Aij vj , and for all j, vj =       Aij ui .
                                       j∈∂i                               i∈∂j
It is known that normalized u converges exponentially to the leading left singular vector. This update
rule is very similar to that of our iterative algorithm. But there is one difference that is crucial in the
analysis: in our algorithm we follow the framework of the celebrated belief propagation algorithm
[10, 11] and exclude the incoming message from node j when computing an outgoing message
to j. This extrinsic nature of our algorithm and the locally tree-like structure of sparse random
graphs [8, 12] allow us to perform asymptotic analysis on the average error rate. In particular, if we
use the leading singular vector of A to estimate s, such that si = sign(ui ), then existing analysis
techniques from random matrix theory does not give the strong performance guarantee we have.
These techniques typically focus on understanding how the subspace spanned by the top singular
vector behaves. To get a sharp bound, we need to analyze how each entry of the leading singular
vector is distributed. We introduce the iterative algorithm in order to precisely characterize how
each of the decision variable xi is distributed. Since the iterative algorithm introduced in this paper
is quite similar to power iteration used to compute the leading singular vectors, this suggests that
our analysis may shed light on how to analyze the top singular vectors of a sparse random matrix.
2.4    Optimality of our algorithm
As a taskmaster, the natural core optimization problem of our concern is how to achieve a certain
reliability in our answers with minimum cost. Since we pay equal amount for all the task assign-
ments, the cost is proportional to the total number of edges of the graph G. Here we compute the
total budget sufficient to achieve a target error rate using our algorithm and show that this is within a
constant factor from the necessary budget to achieve the given target error rate using any graph and
the best possible inference algorithm. The order-optimality is established with respect to all algo-
rithms that operate in one-shot, i.e. all task assignments are done simultaneously, then an estimation
is performed after all the answers are obtained. The proofs of the claims in this section are skipped
here due to space limitations.
Formally, consider a scenario where there are m tasks to complete and a target accuracy  ∈ (0, 1/2).
To measure
        P accuracy, we use the average probability of error per           task denoted by dm (s, ŝ) ≡
(1/m) i∈[m] P(si 6= ŝi ). We will show that Ω (1/q) log(1/) assignments per task is neces-
sary and sufficient to achieve the target error rate: dm (s, ŝ) ≤ . To establish this fundamental limit,
we use the following minimax bound on error rate. Consider the case where nature chooses a set of
correct answers s ∈ {±1}m and a distribution of the worker reliability pj ∼ f . The distribution f
is chosen from a set of all distributions on [0, 1] which satisfy Ef [(2pj − 1)2 ] = q. We use F(q) to
denote this set of distributions. Let G(m, l) denote the set of all bipartite graphs, including irregular
graphs, that have m task nodes and ml total number of edges. Then the minimax error rate achieved
by the best possible graph G ∈ G(m, l) using the best possible inference algorithm is at least
                                                                                    2
                      inf          sup     dm (s, ŝG,ALGO ) ≥ (1/2)e−(lq+O(lq        ))
                                                                                         ,              (4)
                ALGO,G∈G(m,l) s,f ∈F (q)
where ŝG,ALGO denotes the estimate we get using graph G for task allocation and algorithm ALGO
for inference. This minimax bound is established by computing the error rate of an oracle esitimator,
which makes an optimal decision based on the information provided by an oracle who knows how
reliable each worker is. Next, we show that the error rate of majority voting decays significantly
slower: the leading term in the error exponent scales like −lq 2 . Let ŝMV be the estimate produced
by majority voting. Then, for q ∈ (0, 1), there exists a numerical constant C1 such that
                                                                       2
                                                                         +O(lq 4 +1))
                          inf       sup    dm (s, ŝMV )    = e−(C1 lq                .                 (5)
                       G∈G(m,l) s,f ∈F (q)
The lower bound in (4) does not depend on how many tasks are assigned to each worker. However,
our main result depends on the value of r. We show that for a broad range of parameters l, r, and q
our algorithm achieves optimality. Let ŝIter be the estimate given by random regular graphs and the
iterative algorithm. For ˆlq ≥ C2 , r̂q ≥ C3 and C2 C3 > 1, Corollary 2.2 gives
                                 lim     sup    dm (s, ŝIter ) ≤ e−C4 lq .                             (6)
                                m→∞ s,f ∈F (q)
                                                       6

                     1                                                            1
                                                                                0.1
                   0.1
           PError
                 0.01                                                  PError0.01
                                                                             0.001
                0.001
                                                                            0.0001
               0.0001            Majority Voting                                                 Majority Voting
                                   EM Algorithm                              1e-05                EM Algorithm
                              Iterative Algorithm                                            Iterative Algorithm
                                    Lower Bound                                                     Lower Bound
                1e-05                                                        1e-06
                       0    5         10       15     20     25     30              0   0.05    0.1   0.15    0.2 0.25 0.3 0.35 0.4
                                              l                                                              q
        Figure 1: The iterative algorithm improves over majority voting and EM algorithm [7].
This is also illustrated in Figure 1. We ran numerical experiments with 1000 tasks and 1000 work-
ers from the spammer-hammer model assigned according to random graphs with l = r from the
configuration model. For the left figure, we fixed q = 0.3 and for the right figure we fixed l = 25.
Now, let ∆LB be the minimum cost per task necessary to achieve a target accuracy  ∈ (0, 1/2)
using any graph and any possible algorithm. Then (4) implies ∆LB ∼ (1/q) log(1/), where x ∼ y
indicates that x scales as y. Let ∆Iter be the minimum cost per task sufficient to achieve a target
accuracy  using our proposed algorithm. Then from (6) we get ∆Iter ∼ (1/q) log(1/). This
establishes the order-optimality of our algorithm. It is indeed surprising that regular graphs are
sufficient to achieve this optimality. Further, let ∆Majority be the minimum cost per task necessary
to achieve a target accuracy  using the Majority voting. Then ∆Majority ∼ (1/q 2 ) log(1/), which
significantly more costly than the optimal scaling of (1/q) log(1/) of our algorithm.
3     Proof of Theorem 2.1
By symmetry, we can assume all si ’s are +1. If I is a random integer drawn uniformly in [m], then
          P                                       (k)                      (k)
(1/m) i∈[m] P(si 6= ŝi ) ≤ P(xI ≤ 0), where xi denotes the decision variable for task i after
k iterations of the iterative algorithm. Asymptotically, for a fixed k, l and r, the local neighborhood
                                                                                       (k)
of xI converges to a regular tree. To analyze limm→∞ P(xI ≤ 0), we use a standard probabilistic
analysis technique known as ‘density evolution’ in coding theory or ‘recursive distributional equa-
tions’ in probabilistic combinatorics [8, 12]. Precisely, we use the following equality that in the
large system limit,
                                                          (k)
                                            lim P(xI           ≤ 0) = P(x̂(k) ≤ 0) ,                                                (7)
                                          m→∞
where x̂(k) is defined through density evolution equations (8) and (9) in the following.
Density evolution. In the large system limit as m → ∞, the (l, r)-regular random graph locally
converges in distribution to a (l, r)-regular tree. Therefore, for a randomly chosen edge (i, j), the
messages xi→j and yj→i converge in distribution to x and ypj defined in the following density
evolution equations (8). Here and after, we drop the superscript k denoting the iteration number
whenever it is clear from the context. We initialize yp with a Gaussian distribution independent of
      (0)                      d
p: yp ∼ N (1, 1). Let = denote equality in distribution. Then, for k ∈ {1, 2, . . .},
                                 d                         (k−1)                  d                          (k)
                                        X                                               X
                         x(k) =                   zpi ,i ypi ,i , yp(k) =                          zp,j xj ,                        (8)
                                      i∈[l−1]                                         j∈[r−1]
where xj ’s, pi ’s, and yp,i ’s are independent copies of x, p, and yp , respectively. Also, zp,i ’s
and zp,j ’s are independent copies of zp . p ∈ [0, 1] is a random variable distributed according to
the distribution of the worker’s quality. zp,j ’s and xj ’s are independent. zpi ,i ’s and ypi ,i ’s are
conditionally independent conditioned on pi . Finally, zp is a random variable which is +1 with
probability p and −1 with probability 1 − p. Then, for a randomly chosen I, the decision variable
  (k)
xI converges in distribution to
                                                           d                (k−1)
                                                              X
                                                   x̂(k) =          zpi ,i ypi ,i .                                                 (9)
                                                              i∈[l]
                                                                       7

Analyzing the density. Our strategy to provide an upper bound on P(x̂(k) ≤ 0) is to show
that x̂(k) is sub-Gaussian with appropriate parameters and use the Chernoff bound. A random
variable z with mean m is said to be sub-Gaussian with parameter σ if for all λ ∈ R the fol-
                                                                     2 2
lowing inequality holds: E[eλz ] ≤ emλ+(1/2)σ λ . Define σk2 ≡ 2ˆl(ˆlr̂)k−1 + µ2 ˆl3 r̂(3qr̂ +
1)(qˆlr̂)2k−4 (1 − (1/q 2 ˆlr̂)k−1 )/(1 − (1/q 2 ˆlr̂)) and mk ≡ µˆl(qˆlr̂)k−1 for k ∈ Z. We will first
show that, x(k) is sub-Gaussian with mean mk and parameter σk2 for a regime of λ we are interested
in. Precisely, we will show that for |λ| ≤ 1/(2mk−1 r̂),
                                                   (k)                        2  2
                                       E[eλx           ] ≤ emk λ+(1/2)σk λ .                                                  (10)
                                                                                       (k)              (k)
By definition, due to distributional independence, we have E[eλx̂ ] = E[eλx ](l/l̂) . Therefore, it
                                                         (k)                           2 2
follows from (10) that x̂(k) satisfies E[eλx̂ ] ≤ e(l/l̂)mk λ+(l/2l̂)σk λ . Applying the Chernoff bound
                         2
with λ = −mk /(σk ), we get
                                                           (k)                  2        2
                               P x̂(k) ≤ 0 ≤ E eλx̂                    ≤ e−l mk /(2 l̂ σk ) ,
                                               
                                                                                                                              (11)
Since mk mk−1 /(σk2 ) ≤ µ2 ˆl2 (qˆlr̂)2k−3 /(3µ2 qˆl3 r̂2 (qˆlr̂)2k−4 ) = 1/(3r̂), it is easy to check that
|λ| ≤ 1/(2mk−1 r̂). Substituting (11) in (7), this finishes the proof of Theorem 2.1.
Now we are left to prove that x(k) is sub-Gaussian with appropriate parameters. We can write down
a recursive formula for the evolution of the moment generating functions of x and yp as
                           (k)                h                (k−1)                     (k−1)
                                                                                                      il̂
                         E eλx        =          Ep pE[eλyp             |p] + p̄E[e−λyp           |p]      ,                  (12)
                                                                                (k)  r̂
                           (k)                     (k)                          
                         E eλyp                  pE eλx           + p̄E e−λx
                                                                         
                                      =                                                  ,                                    (13)
where p̄ = 1 − p and p̄ = 1 − p. We can prove that these are sub-Gaussian using induction.
First, for k = 1, we show that x(1) is sub-Gaussian with mean m1 = µˆl and parameter σ12 = 2ˆl,
where µ ≡ E[2p − 1]. Since yp is initialized as Gaussian with unit mean and variance, we have
       (0)                   2                                                                                             (1)
E[eλyp ] = eλ+(1/2)λ regardless of p. Substituting this into (12), we get for any λ, E[eλx ] =
                                                2                  2
(E[p]eλ + (1 − E[p])e−λ )l̂ e(1/2)l̂λ ≤ el̂µλ+l̂λ , where the inequality follows from the fact that
                                                  2
aez + (1 − a)e−z ≤ e(2a−1)z+(1/2)z for any z ∈ R and a ∈ [0, 1] (cf. [13, Lemma A.1.5]).
                             (k)                         2   2                                                          (k+1)
Next, assuming E[eλx ] ≤ emk λ+(1/2)σk λ for |λ| ≤ 1/(2mk−1 r̂), we show that E[eλx                                            ]≤
                 2
  mk+1 λ+(1/2)σk+1      λ2                                                                                   2
e                          for |λ| ≤ 1/(2mk r̂), and compute appropriate mk+1 and σk+1 . Substituting
                    (k)                        2   2                             (k)                                          2 2
the bound E[eλx ] ≤ emk λ+(1/2)σk λ in (13), we get E[eλyp ] ≤ (pemk λ + p̄e−mk λ )r̂ e(1/2)r̂σk λ .
Further applying this bound in (12), we get
       h (k+1) i               h                                                                       il̂            2 2
    E eλx                ≤      Ep p(pemk λ + p̄e−mk λ )r̂ + p̄(pe−mk λ + p̄emk λ )r̂                        e(1/2)l̂r̂σk λ .(14)
To bound the first term in the right-hand side, we use the next key lemma.
Lemma 3.1. For any |z| ≤ 1/(2r̂) and p ∈ [0, 1] such that q = E[(2p − 1)2 ], we have
                      h                                                    i                           2       2
                Ep p(pez + p̄e−z )r̂ + p̄(p̄ez + pe−z )r̂                       ≤ eqr̂z+(1/2)(3qr̂ +r̂)z .
For the proof, we refer to the journal version of                 this paper. Applying this inequality to (14) gives
    λx(k+1)
                                                               2
                   q l̂r̂mk λ+(1/2) (3q l̂r̂ 2 +l̂r̂)m2k +l̂r̂σk  λ2
E[e          ]≤e                                                     , for |λ| ≤ 1/(2mk r̂). In the regime where qˆlr̂ ≥
1 as per our assumption, mk is non-decreasing in k. At iteration k, the above recursion holds for
|λ| ≤ min{1/(2m1 r̂), . . . , 1/(2mk−1 r̂)} = 1/(2mk−1 r̂). Hence, we get a recursion for mk and
σk such that (10) holds for |λ| ≤ 1/(2mk−1 r̂):
                        mk+1 = qˆlr̂mk ,                   2
                                                         σk+1     = (3qˆlr̂2 + ˆlr̂)m2k + ˆlr̂σk2 .
With the initialization m1 = µˆl and σ12 = 2ˆl, we have mk = µˆl(qˆlr̂)k−1 for k ∈ {1, 2, . . .} and
σk2 = aσk−12
               + bck−2 for k ∈ {2, 3, . . .}, with a = ˆlr̂, b = µ2 ˆl2 (3qˆlr̂2 + ˆlr̂), and c = (qˆlr̂)2 . After
                                                                        Pk−2
some algebra, it follows that σk2 = σ12 ak−1 + bck−2 `=0 (a/c)` . For ˆlr̂q 2 6= 1, we have a/c 6= 1,
whence σk2 = σ12 ak−1 + bck−2 (1 − (a/c)k−1 )/(1 − a/c). This finishes the proof of (10).
                                                                   8

References
 [1] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the
     em algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–
     28, 1979.
 [2] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
     the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):pp.
     1–38, 1977.
 [3] R. Jin and Z. Ghahramani. Learning with multiple labels. Advances in neural information
     processing systems, pages 921–928, 2003.
 [4] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning
     from crowds. J. Mach. Learn. Res., 99:1297–1322, August 2010.
 [5] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose vote should count more:
     Optimal integration of labels from labelers of unknown expertise. Advances in Neural Infor-
     mation Processing Systems, 22:2035–2043, 2009.
 [6] P. Welinder, S. Branson, S. Belongie, and P. Perona. The Multidimensional Wisdom of Crowds.
     pages 2424–2432, 2010.
 [7] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data
     mining using multiple, noisy labelers. In Proceeding of the 14th ACM SIGKDD international
     conference on Knowledge discovery and data mining, KDD ’08, pages 614–622. ACM, 2008.
 [8] T. Richardson and R. Urbanke. Modern Coding Theory. Cambridge University Press, march
     2008.
 [9] B. Bollobás. Random Graphs. Cambridge University Press, January 2001.
[10] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann Publ., San Mateo,
     Califonia, 1988.
[11] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its gen-
     eralizations, pages 239–269. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,
     2003.
[12] M. Mezard and A. Montanari. Information, Physics, and Computation. Oxford University
     Press, Inc., New York, NY, USA, 2009.
[13] N. Alon and J. H. Spencer. The Probabilistic Method. John Wiley, 2008.
                                                   9

