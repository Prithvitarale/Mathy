                                              Measuring the Robustness of Graph Properties
                                                           Yali Wan                                    Marina Meila
                                                          Amazon.com                              University of Washington
                                                      Palo Alto, CA 94301                            Seattle, WA 98195
                                                     yalwan@amazon.com                         mmp@stat.washington.edu
arXiv:1901.09661v1 [cs.SI] 3 Dec 2018
                                                                                      Abstract
                                                 In this paper, we propose a perturbation framework to measure the robustness of
                                                 graph properties. Although there are already perturbation methods proposed to
                                                 tackle this problem, they are limited by the fact that the strength of the perturbation
                                                 cannot be well controlled. We firstly provide a perturbation framework on graphs
                                                 by introducing weights on the nodes, of which the magnitude of perturbation can
                                                 be easily controlled through the variance of the weights. Meanwhile, the topology
                                                 of the graphs are also preserved to avoid uncontrollable strength in the perturbation.
                                                 We then extend the measure of robustness in the robust statistics literature to the
                                                 graph properties.
                                        1   Introduction
                                        In data science it is natural to consider an observed graph, or network, G as a realization of a
                                        random process. Consequently, the properties of G (such as diameter, conductance, clustering)
                                        and the inferences drawn from them are incomplete without some measure of variability or confidence.
                                        We propose a new methodology for evaluating the robustness of graph properties by measuring the
                                        effect of small perturbations of the graph on the respective property. This methodology is based on
                                        concepts from robust statistics [4] and exploits the technique introduced in [8] to augment a graph
                                        Laplacian with user-defined weights. Let G = (V, A) be a graph with n = |V| nodes and adjacency
                                        matrix A; A can be symmetric or asymmetric (corresponding to a directed network), and we assume
                                        Aij ∈ {0, 1}, Aii = 0 (simple graph) or Aij ≥ 0 (weighted graph). Our methodology applies to
                                        both scenarios, and makes no assumptions on how the graph was generated.
                                        Our methodology consists of four key components:
                                              1. Perturb the nodes of the graph by assigning them multiplicative weights, with wi the weight
                                                 of node i, i = 1, 2, . . . n. If wi = 1 for all i, we have the original graph G.
                                              2. Express the desired graph property f (G) as a smooth function of the weights.
                                              3. Construct measures of robustness inspired by the robust statistics literature, such as influence
                                                 function (IF ), and breakdown point (BP ).
                                              4. Evaluate these measures on the current graph G.
                                        We exemplify our approach by examining the robustness of weighted cut (WCut), number of weakly
                                        Connected Components (wCC’s), eigengap and clustering. In this paper, we make use of the
                                        following notations. We define wi , i = 1, 2, · · · , n, the weight associated
                                                                                                             Pn        with node i. Since we are
                                        interested in both directed and undirected graphs, we define di = j=1 Aij as the out-degree of node
                                        i. We use the definition in [8] and define L = I − 21 D−1/2 (A + AT )D−1/2 as the Laplacian matrix
                                        associated with A. This definition is consistent with the usual definition of L = I − D−1/2 AD−1/2

when the graph is symmetric. We define A, d, D, L as the properties of the observed graph. The
above quantities will be marked with a symbol˜when they are perturbed. This is consistent with the
notation in matrix perturbation literature. To demonstrate, Ã represents the adjacency matrix of the
perturbed graph.
In the rest of this paper, we proceed as follows. In Section 2, we describe our method of
bootstrapping and perturbing the networks. In Section 3, we talk about the graph properties of
interest. In Section 4, we discuss measures for evaluating robustness of graph properties. In
Section 5, we discuss breakdown points. In Section 6, we discuss related work. In Section 7,
we use both synthetic and real datasets to analyze our methods. We conclude our findings in Section 8.
2    Perturbing the network
Existing methods of perturbing networks can be found in recent works [5, 3, 2, 1]. They mostly
involve removing or duplicating edges or nodes randomly and independently in the graph. [3]
randomly removes edges from the graph. [5] maintains the the number of vertices and edges of the
original graph, and perturbs the graph by moving the edges to the other locations. On the other
hand, [2] and [1] bootstrap the network by subsampling the nodes. Although their approach is
straightforward, the perturbation cannot be well controlled. Firstly, the topology of the graphs can
change dramatically for sparse graphs. Randomly removing or adding the nodes and edges makes the
strength of perturbation hard to control. For example, assume we have two densely clusters C1 and C2
connected by a single edge e. The effect of removing e versus removing an edge within C1 or C2 is
very different, since the former will change the graph structure drastically by making it disconnected.
Moreover, removing or adding edges (nodes) are discrete moves, therefore the perturbation cannot be
arbitrarily small.
In order to have a fine control on the amount of perturbation and make it as smooth as possible, in
our approach, we preserve the graph topology by keeping all the nodes and edges, and we perturb
the graph by assigning random weights to the nodes and then distribute these weights to edges.
Specifically, we assign weight wi to node i, where wi is generated i.i.d from a distribution with
E(wi ) = 1, standard deviation σw , and support on (0, ∞). The perturbation can be controlled
smoothly with σw . Since wi > 0, no nodes or edges is removed or added, and the topology of the
graph is preserved.
We propose two ways of constructing Ã from weighted nodes.
       • Asymmetric perturbation: perturb outgoing edges of node i, so that Ãij = wi Aij (whereas
          Ãji = wj Aji ). The out-degree becomes d˜i = di wi . The perturbed Laplacian is,
                                             Ãij + Ãji          wi Aij + wj Aji
                                 L̃ij = 1 −     q          =1− p                                   (1)
                                               2 d˜i d˜j           2 wi wj di dj
       • Symmetric perturbation: place wi on both outgoing edges and incoming edges of node i.
          Ãij = (wi + wj − 1)Aij . Since this method leads to complicated L̃ in form, we discuss
          perturbing one node at a time. If node t is perturbed, the Laplacian becomes,
                                           Atj wt +Ajt wt
                             
                              1 − 2 √d w ( P                                    i = t, j 6= t
                                        t t    m6=t Ajm +Ajt wt )
                             
                             
                                                      Aij +Aji
                      L̃ij = 1 − √ P                        P                    i, j 6= t         (2)
                                   2 ( m6=t Ajm +Ajt wt )( m6=t Aim +Ait wt )
                             
                               0                                                 i, j = t
                             
Both of the above methods ensure E(Ãij ) = Aij . Notice that in the current methods, it can be
easily shown that bias is introduced in Aij , in which case one cannot separate the perturbation from
structural change and the change of values in adjacency matrix.
                                                     2

Although both perturbation methods seem to be reasonable, they have graph-specific advantages.
The asymmetric method provides a very simple way for calculating a graph property like d˜ after
perturbation. On the other hand, it is not interesting for the perturbation of weakly Connected
Components (wCC’s), as we shall see in Section 4. On the other hand, the symmetric method
maintains a symmetric perturbation of adjacency matrix. If Aij = Aji , then Ãij = Ãji after
perturbation. However, L̃ is complicated in form. The symmetric perturbation is not very interpretable
for some graph properties (WCut), but it is useful for evaluating the robustness in eigengap and wCC’s.
In the above methods, we put weights on nodes and then distribute the weights to edges. The
reason for doing this instead of directly perturbing the edges is that it is more natural to maintain
the association between nodes and edges. The edges connected to the same node should be
dependent rather than independent. Perturbing the weights on edges [5] independently neglects these
relationships, which has been found both unrealistic and disrupts the topology of sparse network.
Moreover, in many applications, nodes are more meaningful than edges. Firstly, a node often carries
its own attributes. Secondly, a node is described by the multiple nodes it is connected to. On the
other hand, it is more difficult to glean information from an edge. For example, in a Facebook
network, a node is a person with complex information including age, birth place, school, the friends
he connected to, etc. While an edge is formed when two people befriend each other, and we
cannot even learn how well these two people know each other and it tells us little on other information.
2.1    The bias in L̃
The graph properties we shall study are closely related to L. It would be nice if the changes in graph
properties are only from the noise introduced in graph structure and keeping the entires in L̃ to be the
same with that in L in expectation. Since having bias in L introduces another factor for the change in
graph property, of which the strength of perturbation is hard to control. Therefore in principle, we
would want E(L̃ij ) = Lij . In the following paragraphs we prove that this is impossible under all the
current perturbation methods. For the current methods, where people move the edges and subsample
the nodes, the bias in L̃ is apparent. We show in Proposition 1 that both of our perturbation methods
introduce bias too.
Proposition 1 Assume wi , i = 1 : n are generated i.i.d from a distribution with E(wi ) = 1, σw 6= 0,
wi > 0. For asymmetric perturbation, E(L̃ij ) < Lij .
If we further assume there exists a triangle < t, p, q > in the graph. For symmetric perturbation,
with node t perturbed, E(L̃ij ) < Lij .
Proof:
In the asymmetric perturbation, L̃ is shown in Equation 1. Assume Lij 6= 0. In order to have
E(L̃ij ) = Lij , we need
                                       wi Aij + wj Aji          Aij + Aji
                               E(1 −      p            )=1− p                                        (3)
                                         2 wi wj di dj           2 di dj
                                                                 r
                                             wi                     wj
                                           r
                              Aij (1 − E(       )) + Aji (1 − E(       )) = 0                        (4)
                                             wj                     wi
Since wi and wj are i.i.d, we then have,
                                                √                     √
                                   r
                       wi             wj                   1                    1
                    r
                  E(       ) = E(        ) = E( wi )E( √ ) = E( wi )E( √ ) = 1                       (5)
                       wj             wi                   wj                    wi
                 √
Assume x = wi . Equivalently we have,
                                                  1       1
                                              E( ) =                                                 (6)
                                                 x     E(x)
Assume f (x) = x1 . Since f (x) is strictly convex and Jensen’s inequality yields, E(f (x)) ≥ f (E(x)),
                                                                                    q
                                                                                       wi
                     1
that is, E( x1 ) ≥ E(x) . where equality holds only when σ(x) = 0. Therefore E( w       j
                                                                                          ) > 1. We have
                                                     3

reached a contradiction. E(L̃) < L.
In the symmetric perturbation, L̃ after perturbing node t is shown in Equation 2. In order have
E(L̃ij ) = Lij , we need,
                               Atj wt + Ajt wt                        Atj + Ajt
             E(1 − q                                        )=1− p                when i = t, j 6= t                 (7)
                                 P
                      2 dt wt ( m6=t Ajm + Ajt wt )                     2 dt dj
                                   Aij + Aji                                     Aij + Aji
   E(1 − q P                                                           )=1− p                     when i, j 6= t     (8)
                                            P
            2 ( m6=t Ajm + Ajt wt )( m6=t Aim + Ait wt )                          2 di dj
That is,
                            √                   1
                        E( wt )E( p                            ) = 1, when i = t, j 6= t,                            (9)
                                         aj + (1 − aj )wj
                                1                            1
                   E( p                     )E( p                         ) = 1, when i, j 6= t,                   (10)
                         ai + (1 − ai )wi           aj + (1 − aj )wj
               P
                      Ajm
where ai = m6=dtj          and 0 ≤ ai ≤ 1.
Since
   √   there  is a triangle < t, p, q > in the graph. We can easily derive from Equation 9 and 10 that
                                 1
E( wt ) = 1, and E( √                     ) = 1, ∀i.
                            ai +(1−ai )wi
           1
E( √                 ) is  a continous and differentiable function of ai .                        When ai        =    1,
      ai +(1−ai )wi
           1                                                  1                                    1
E( √                 ) =  1. When ai = 0, E( √                         )  = E( √1wi ) >            √
                                                                                               E( wi )    = 1. Since
      ai +(1−ai )wi                                     ai +(1−ai )wi
 ∂      √       1                            1
∂ai E( ai +(1−ai )wi )    < 0,    E( √                )  > 1 for 0 < ai < 1. We have a contradiction.
                                        ai +(1−ai )wi
Therefore in conclusion, E(L̃) < L.
We have proved that in theory there will be bias introduced in L̃, it would be a good idea to examine
how much bias will actually be introduced in practice. Under asymmetric perturbation, one can
                                                                  √                  L̃
easily derive from the proof in Proposition 1 that E( w)E( √1w ) = Lij                     . Therefore, the more
   √          1
                                                                                        ij
E( w)E( w ) deviates from 1, the more bias introduced in L̃ij . Since there are very limited
             √
number of conventional distributions that allow support on (0, ∞) and mean equals 1, we propose a
                                                      2     2
class of mixture distributions M ixture(a, b, σ−        , σ+  , T− , T+ , p), where ap + b(1 − p) = 1, T− is a
                                            2
distribution with E(x) = a, σ(x) = σ− , with support on (0, 1), T+ is a distribution with E(x) = b,
             2                                                                            2      2
σ(x) = σ+      , with support on (1, ∞). Assume w ∼ M ixture(a, b, σ−                        , σ+  , T− , T+ , p), then
                                                                                           2               2          2
p(w ∼ T− ) = p, p(w ∼ T+ ) = 1 − p. We can easily show that E(w) = 1, σ (w) = p− σ−                            + p + σ+ .
The mixture distribution subsumes all the distributions concentrated on 1, with support on (0, ∞).
In the following experiment, we generate wi from the distributions below accordingly.
       1. Node resampling: wi is obtained from resampling the nodes with replacement to q                       form a
          sample of size N . Assume node i appears m times in the sample, wi = mn                   N . σw =
                                                                                                                   n−1
                                                                                                                     N .
          We can easily see as N goes up, σw decreases. We can then control σw by varying the size
          of N .
       2. Binary distribution: wi follows binary distribution on {a, b}, with P (wi = a) = p, P (wi =
          b) = 1 − p. b = 1−ap                        2                2
                               1−p . E(wi ) = 1, σw = p(a − 1) + (1 − p)(b − 1) .
                                                                                                2
       3. Gamma distribution: wi ∼ Gamma(a, a1 ). E(wi ) = 1, σw               2
                                                                                 = a1 .
       4. Mixture-Gamma-Uniform distrbution: a = 0.5, T− = unif orm(0, 1), T+ =
          Gamma( b−1   σ+ , σ+ ) + 1.
       5. Mixture-Lognormal-Uniform distribution: a = 0.5, T− = unif orm(0, 1), T+ =
          lognormal(b − 1, σ+ ).
                                                          4

The results are shown in Figure 1. We observe that the bias is sensitive to the choice of weight
distribution. In specific, the bias introduced by Gamma distribution is growing exponentially
with σw , thus not a good option. Node resampling and Binary distribution generate smallest
bias. However node resampling can only allow the perturbation strength σw to be as much
as 1, otherwise the topology of the graph is changed. Binary distribution only allows two
choices of weight, which is very limited. The mixture distributions behave similarly in terms of
introducing bias. Although the bias is nontrivial, the fact that it is upper bounded and that wi can be
generated in a continuous manner make them good candidates for performing perturbations on graphs.
              √
 Figure 1: E( w)E( √1w ) under asymmetric perturbation. Each boxplot represents 100 repetitions.
We also evaluate the bias introduced in L̃ in the current methods including [5] and [2]. Since they
move the edges or delete nodes, the bias depends on the graph. We generate a graph from DC-SBM
                                                                                 E(L̃ )
model [10] with n = 800, wi ∼ 0.5 + unif orm(0.5, 1), and evaluate E(Lij             ij )
                                                                                          . As a reminder,
E(L̃ij )      √
                      √1
E(Lij ) = E( w)E( w ). In Figure 2, we show that both methods introduce large bias compare to
                 perturbation strength                                proportion of nodes sampled
Figure 2: Left: the bias from the method in [5]. α indicates the strength of perturbation. Right:
the bias from the method in [2]. β indicates the proprotion of nodes that are subsampled. Larger β
indicates larger perturbation strength.
                                                   5

our perturbation methods.
2.2    Partial perturbation and full perturbation
In this section we discussed perturbing the graph by adding i.i.d weights to the nodes. Although we
make i.i.d assumption on wi , it can be relaxed. In this paper, we consider two approaches to utilize
the weight perturbation. First, in order to evaluate the sensitivity of graph properties, we perturb all
the nodes i.i.d with different perturbation strength σw , and then investigate how the graph property
changes with respect to it. In specific, we fix E(w) = 1 and vary σw for all the nodes. Alternatively, in
order to probe the source of sensitivity, we can perturb a subset of nodes. For these nodes, we perturb
their weights i.i.d by fixing σw and vary E(w), from which we can discover the source of the ro-
bustness in graph properties. The use of these two perturbation approaches will be shown in Section 7.
3     Expressing graph properties with weights
The success of our approachs depend on our ability to express properties of interest as functions
f (G, w), which are continuous and differentiable w.r.t. the node weights w1:n . In this project we
focus on graph properties that depend on graph Laplacians. Many important graph properties depend
on Laplacians. For instance, the mixing time of the graph depends on the second smallest eigenvalue
λ2 (L). Diffusion distances [9] between nodes can also be approximated by the principal eigenvectors
and values of L. The number of connected components C of G is equal to the multiplicity of 0 in the
spectrum of L, etc. There are four graph properties that we are particularly interested in, which will
be used as examples in the following sections: the weighted cut of the graph (WCut), the number of
weakly connected components, the eigengap and clustering.
3.1    Weighted Cut(WCut)
Weighted cut is defined as a graph property associated with clustering in the general graph setting
[8], which can be used for both directed and undirected graphs. It is similar in motivation to the
normalized cut for undirected graph. Both aim in finding a cut of low weight in the graph while
balancing the sizes of the clusters. The multiway version of the normalized cut MNCut of [8] is a
special case of WCut.
Formally, WCut with respect to clustering C with K clusters is defined as
                                                  K
                                                 X     1 X ˜         X
                             W Cut(G, w, C) =                 (di −      Ãij )                     (11)
                                                 k=1
                                                     D̃k i∈Ck       j∈Ck
                      d˜i . Further define D̃kk = i∈Ck j∈Ck Ãij , we can then equivalently write
               P                                    P      P
where D̃k =      i∈Ck
Wcut as,
                                                         K
                                                        X        D̃kk
                                   W Cut(G, w, C) =        (1 −       )                             (12)
                                                        k=1
                                                                  D̃k
Small WCut suggests sparse connections between clusters, thus better quality of clustering.
3.2    Number of weakly Connected Components (wCC’s) and eigengap
It is already know that the number of connected components (CC’s) is not robust, since randomly
adding a node or removing some edges can easily change the number of CC’s. Instead, we study the
number of weakly Connected Components (wCC’s), where “weakly” means sparse connections
between CC’s.
                                                     6

                                                                                           PK
We propose a pair of functions fu (G, w, K)                     =      λK+1 (L(G̃)) −         k=1 λi (L(G̃)),
                   PK
fl (G, w, K) =            λ
                      k=1 k (L(  G̃)) to  describe the   number    of  wCC’s.   The    reason  we use fl is
that, if there are K number of CC’s, then fl = 0. We would then expect fl to be close to 0 for K
number of wCC’s. We choose fu because we expect a significant gap between fl and λK+1 (L(G̃))
for a stable number of wCC’s. When fu is away from 0 (respectively fl near 0) there are exactly K
“weakly connected” components in G̃. If this holds for large perturbations, then K can be considered
robust.
The Kth eigengap is defined as fe (G̃, w, K) = λK+1 (L(G̃)) − λK (L(G̃)). It indicates K principal
subspace when fe is large compared to the other eigengaps.
Notice that function f defined for number of wCC’s and eigengap are only meaningful for undirected
graphs. Since the eigenvalues is not very interpretable for the directed graphs.
4     Influence functions
In order to evaluate the graph properties, we construct the target properties as differentiable functions
f (G, w) and see how much f (G, w) changes with respect to the size of perturbation of w. In this
section, we present tools for quantifying robustnes including Influence Function (IF) and Breakdown
Points. We firstly talk about Influence function (IF), which was invented by Hampel in [4]. The
importance of IF lies in its interpretation: it gives a picture of the infinitesimal behavior of the
asymptotic value. While numerous studies have been done on methods for analysis of data sampled
from a known distribution i.i.d, there has not been much work on using these tools to evaluate the
robustness of graph properties. Here we define for a perturbed graph G̃,
                                                ∂f (G, w)
                                        IFt =                        ,                                  (13)
                                                   ∂wt       w1:n =1
which measures the local influence of wt on f .
4.1    Influence function of WCut
Proposition 2 Assume
P                     P a graph G with C,Pdi defined as usual. Assume node t ∈ Ck0 . dik =
    j∈Ck Aij , dki =    j∈Ck Aji . Dk0 k0 =               / k0 Aij . Then using asymmetric perturbation,
                                          ¬
                                                 i∈Ck0 ,j ∈C
the influence function for node t is
                                                       dt Dkk − dtk Dk
                                   IFtW Cut (G, C) =                      .                             (14)
                                                               Dk2
Using symmetric perturbation.
                                n
                                                                                    P
                               X    Dkk dkt     Dk0 dk0 t + Dk0 k0¬ Dtk0 − Dk0 k0 j ∈C   / k0 Atj
               W Cut
            IFt      (G, C) =            2   −                           2                              (15)
                                      Dk                               Dk0
                               k=1
Proof:
We firstly perturb the graph using the asymmetric method. Assume t ∈ Ck , we have
                        ∂W Cut(G, w)          ∂          1        X
                                          =       P                    wj (dj − djk )                   (16)
                              ∂wt            ∂wt j∈Ck dj wj
                                                                  j∈Ck
                                               P                       P
                                             dt j∈Ck wj djk − dtk j∈Ck dj wj
                                          =               P                          .                  (17)
                                                        ( j∈Ck dj wj )2
The influence function is then derived asP                      P
             ∂W Cut(G, w)                 dt j∈Ck djk − dtk j∈Ck dj            dt Dkk − dtk Dk
                             |w1:n =1 =                                     =                           (18)
                                                                                       Dk2
                                                   P            2
                   ∂wt                            ( j∈Ck dj )
                                                                                                        (19)
                                                     7

In the symmetric perturbation, since the perturbation can be explained as perturbing one node at a
time, one can easily show that the influence function is same for perturbing one node or multiple
nodes. For simplicity, we assume perturbing node t with weight wt . WCut can be written as
                                                                      D
                                                                P kk
                                          X
                      W Cut(G, w) =            (1 − P                                         )+
                                                         i∈C  [(   j6 = t Aij ) + Ait wt ]
                                        k,t∈C
                                           / k              k
                            P                   P                      P                                       (20)
                              i∈Ck0  A it w t+     j∈Ck0 Atj wt +          i,j6=t,i,j∈Ck0 Aij
                      (1 −                  P              P                                    )
                                  dt wt + i∈Ck ,i6=t [( j6=t Aij ) + Ait wt ]
                                                    0
                                                            P                     P                                  P
                                                           ( i∈Ck Ait + j∈Ck Atj )Dk0 − Dk0 k0 (dt + i∈Ck Ait )
                                           P
∂W Cut(G, w)                  X Dkk i∈C Ait
                  |w1:n =1 =                     k
                                                         −            0                    0                                0
       ∂wt                                  Dk2                                                   Dk20
                             k,t∈C
                                / k
                                                                                                               (21)
                              n
                                                              P                              P                      P
                                                         Dk0              Ait + D                      Atj − Dk0 k0         Atj
                                       P
                             X    Dkk      i∈Ck Ait              i∈Ck0                k0 k0¬    j∈Ck0                j ∈C
                                                                                                                       / k0
                           =                          −
                                          Dk2                                                  Dk20
                             k=1
                                                                                                               (22)
                              n
                                                                                         P
                             X    Dkk dkt      Dk0 dk0 t + Dk0 k0¬ dtk0 − Dk0 k0             j ∈C
                                                                                               / k0 Atj
                           =           2    −                                                                  (23)
                                    Dk                                   Dk20
                             k=1

In the asymmetric perturbation, IF has an intuitive interpretation when a point has no influence, i.e,
IFt = 0,
                                       dtk      Dkk       meani∈Ck dik
                                            =         =                                                        (24)
                                        di      Dk        meani∈Ck di
It means that, node t has 0 influence in WCut when the proportion of edges that goes to CK equals
the cluster level ratio of averages. If IFt > 0, node t tends to make WCut larger when more weight
is put upon t, the quality of clustering decreases since the clustering becomes less well separated.
Node t is therefore considered unstable to the clustering, or not well clustered. If IFt < 0, node t is
well clustered since WCut will decrease if t is weighted more. Hence IF measures the robustness of
clustering in node level. It is worth noticing that the influence of a node depends only on the cluster
that the node belongs to, and is independent of the rest of the clusters. Moreover, the influences of
the nodes within a cluster always cancel each other. In a well separated clustering, we would expect
the influences of the nodes the be concentrated around 1. When the clusters are completed separated,
that is, there is no edge between clusters, it can be easily shown that all the node influence equals 0.
                                             P           P                  P              P
           X ∂W Cut(G, w)                       i∈Ck  di   j∈Ck  djk − i∈Ck dik j∈Ck dj
                                |w1:n =1 =                       P                                     = 0,    (25)
                      ∂wi                                       ( j∈Ck dj )2
           i∈Ck
For the symmetric perturbation, the meaning for the above results is not very interpretable, since its
form is very complicated and IFt = 0 does not provide us with any clear interpretation in its balance
state.
4.2    Influence function of number of wCC’s and eigengap
Here we consider the property of wCC’s and eigengap described by fu , fl and fe , which are proposed
in Section 3.2. Since they both depend on λ, we firstly study ∂λ          ∂wt for a single λk in Proposition 5..
                                                                             k
The influence functions of interest can be easily derived from there. This is because the influence
functions can be written as,
                                                                K
                                           fu     ∂λK+1 X ∂λi
                                        IFt =              −                                                   (26)
                                                    ∂wt        i=1
                                                                   ∂wt
                                                        8

                                                                  K
                                                                 X     ∂λi
                                                      IFtfl =                                                          (27)
                                                                 i=1
                                                                      ∂wt
                                                            ∂λK+1          ∂λK
                                                 IFtfe =               −                                               (28)
                                                              ∂wt          ∂wt
Unfortunately, asymmetric perturbation is not very interesting for undirected graphs, since the
properties remain untouched despite the perturbation, as will be shown in Proposition 3. This is
because the eigengap of L is equal to the eigengap of the transition matrix P = D−1 A, and P stays
unchanged after the asymmetric perturbation. This motivates the use of symmetric perturbation, the
results of which are shown in Proposition 5.
Proposition 3 Using asymmetric perturbation, assume A symmetric and λk of multiplicity 1.
∂λk
∂wt |w1:n =1 = 0. vi is the i-th element of the k-th eigenvector of L.
Proof:
                                       ∂ λ̃k     X ∂ λ̃k ∂ L̃ij           X            ∂ L̃ij
                                              =                       =          vi vj                                 (29)
                                       ∂wt        ij
                                                       ∂ L̃ij  ∂w  t       ij
                                                                                       ∂wt
                    Aij wi +Aji wj
Since L̃ij = 1 − √                       We then obtain,
                     2   wi wj di dj
                                         p                        p                           q
                 ∂ L̃ij       −2Aij          wi wj di dj + Aij       wi wj di dj + Aji          wj3 di dj /wi
                          =                                                                                            (30)
                  ∂wi                                           4wi wj di dj
                                ∂ L̃ij
For an undirected graph,        ∂wi |w1:n =1      = 0 always. 
                       ∂λk         P
Proposition 4 [7]      ∂Lij   =        i,j6=t vi vj , where vi is the i-th element of the k-th eigenvector of L.
Proposition 5 Define A, d, λk , L, L̃, w as usual. Assume λk is of multiplicity 1. Define transition
matrix P = D−1 A, vi is the i-th element of the k-th eigenvector of L.
In symmetric perturbation,
                                      ∂ λ̃k                            X
                                            |w1:n =1 = (1 − λk )(             vi2 Pit − vt2 )                          (31)
                                      ∂wt                                i
Proof: After performing the symmetric perturbation, we obtain L̃ from Equation 2. We then calculate
                           ∂ L̃
the influence function ∂wijt |w1:n =1 ,
                                                  
                                                        Atj +Ajt          Ajt
                                                          √
                                                  − 4 dt dj (1 − dj )                  i = t, j 6= t
                                                  
                                                  
                          ∂ L̃ij                     A    +A        A   d  +A     d
                                   |w =1 =             ij    ji
                                                                × jt     i      it j
                                                                                        i, j 6= t                      (32)
                          ∂wt 1:n                  4
                                                                     (di dj )3/2
                                                  0                                    i, j = t
                        ∂λk
Then we can derive      ∂wt |w1:n =1      using Proposition 4,
∂ λ̃k            X ∂ λ̃k ∂ L̃ij
      |w1:n =1 =                                                                                                       (33)
∂wt               ij
                       ∂ L̃ij ∂wt
                                                                                  n
                  X            Aij + Aji           Ait dj + Ajt di             X             Atj + Ajt        1        Ajt
               =        vi vj                  ×                       −   v t       vj ×                 (p       − p         )
                 i,j6=t
                                       2              2(di dj )3/2              j=1
                                                                                                  2           d d
                                                                                                               j t  dj   dj dt
                                                                                                                       (34)
                                X
               = (1 − λk )(           vi2 Pit − vt2 ),                                                                 (35)
                                  i

                                                                9

4.3   Clustering
Clustering, defined as a partition of nodes, cannot be written as a smooth function of weights, since
the clustering can only be changed discretely. Therefore we cannot use IF to measure the robustness
of clustering. However, a clustering can be evaluated by breakdown points (BP ), which will be
discussed in the next section.
5    Breakdown points (BP )
While IF measures local infinitesimal influences, the breakdown point (BP ) measures the global
reliability of the graph properties. It is developed by [4] and widely used in robust statistics literature.
It informs the range of perturbations that can be tolerated before the “structural information in the
data” is lost. Similar with Influence function, the use of BP is limited by the assumption that the
data is generated i.i.d from known distributions, thus not applicable to the graph properties.
We are the first to extend the definition of BP to the graph properties. For a graph property f (G), a
BP is defined as,
                       ∗
                      σw := max{σw ; |f (G̃) − f (G)| ≤  with probability 1 − α}                      (36)
where  and α are defined by users. Through finding BP , we define meaningful and com-
putable thresholds where information about a specific graph property is lost. For instance, for
f (G, w) = λ2 (L(Gw )), a BP can be defined when the eigengap between λ2 and the next largest
eigenvalue vanishes.
Another advantage of BP is that it allows robustness measure for non-differentiable graph property
functions, such as clustering. It does not require the graph properties to be written as smooth functions
of weights. It is a descriptive measure that allows global measure of robustness.
6    Related Work
In the past literature, there has been some work in perturbing the social networks. In [5], they
restrict their perturbed networks to maintain the same number of vertices and edges as the original
unperturbed network, and the perturbation is meant for the position of the edges only. The amount
of perturbation is controlled by the number of edges being moved. In [2] and [1], they focus on
subsampling the nodes of the networks. [2] propose uniform subsampling bootstrap scheme, in
which they iteratively select a subset of vertices without replacement and consider the graph induced
by the subset of vertices. They also consider a subgraph subsampling bootstrap, where they use a
enumeration scheme to find all possible subgraphs with a fixed vertice size, and they selecte the
subgraph with a fixed probability p.
We have also seen work in detecting dense communities and largest connected component in [11].
They formalize tests for the existence of a dense random subgraph based on a variant of scan statistics.
Although they offer sharpe detection bounds, their theorems only make a judgement on the existence
of the subgraph instead of finding out where the subgraph is. Moreover, one has to go through all
the subgraphs in order to calculate the test statistics, which in application, could be computationally
intensive.
7    Experiments
In this section, we perform experiments to test the robustness of clustering, WCut, number of wCC’s
and eigengap. We employ both synthetic datasets and a Facebook dataset. Because symmetric
perturbation is less interpretable, we only apply asymmetric perturbation when studying connected
components and eigengap, and apply asymmetric perturbation for the remainder of the study.
                                                    10

7.1   Datasets
Synthetic datasets The synthetic datasets are generated from the DC-SBM model with the
number of clusters K = 5, and the distribution of the cluster sizes nnk = (0.1, 0.2, 0.3, 0.2, 0.2).
DC-SBM is defined as a class of network model that Aij = wi wj Bkl whenever i ∈ k, j ∈ l, with
B = [Bkl ] ∈ RK×K symmetric and non-negative and w1 , . . . , wn non-negative weights associated
with the graph nodes. . The clustering is guaranteed to be recovered with small errors by spectral
clustering algorithm if the graph is generated from DC-SBM [10, 12]. The weights of DC-SBM,
  i
wDC−SBM       ∼ 0.5 + 0.5 × U nif orm(0, 1) if not otherwise specified. The graphs are generated with
different n and spectrum in the following experiments. The visualization of the graphs and the model
parametrization are shown in Figure 3.
                                                                                         i
Figure 3: The synthetic datasets. Top left: n = 800, λ1:K = (0, 0.2, 0.4, 0.6, 0.8), wDC−SBM         ∼
                                                                             i
0.5 + 0.5 × U nif orm(0, 1). Top right: n and λ1:K the same with top left, wDC−SBM ∼ 0.4 + 0.6 ×
U nif orm(0, 1). Down left: n = 2000, λ1:K = (0, 0.1, 0.11, 0.12, 0.13). Down right: n = 800,
λ1:K = (0, 0.1, 0.2, 0.3, 0.4)
Facebook dataset The Facebook dataset [6] is an undirected connected graph which consists of
10 anonymized ego networks. It has 4039 nodes and 88234 edges. The data was collected from
survey participants using a Facebook application called Social Circles. Each cluster is consists of the
members within an ego network. The visualization of the Facebook dataset is shown in Figure 4.
  In our experiments, we only examines undirected graphs. Since the definition of L is uni-
versal for all graphs, the robustness of WCut and clustering can be measured in the same
way. The graph properties we defined for the number of wCC’s and eigengap are only meaning-
ful for undirected graphs, and we do not know yet how to measure their robustness for directed graphs.
We say a dataset or a graph is hard if there are many edges between clusters, and graph properties
calculated from them are expected to be sensitive. A harder graph usually has smaller n, larger
eigengap, denser connections between clusters and sparser connections within clusters. From the
                                                 11

                                     Figure 4: Facebook dataset
theory of the recovery of clustering [13, 12, 14], it can be indicated that the graph properties in the
harder graphs will be less robustness.
Notice that in the real datasets, e.g. Facebook, one may argue that the perturbation may not be
meaningful since the edges are given continuous weights after the perturbation, while the edges
can only take the values 0 and 1, e.g, two people are either friends or not. It is true that in reality,
the change of this kind of social networks is restricted to adding or deleting nodes or edges. Our
perturbation methods do allow for deletion of nodes and edges when the weight are generated node
resampling mechanism. They can be viewed as deletions from the graph. We can also utilize node
resampling to generate discrete weights for the nodes and edges to make it more meaningful. One
potentail concern is that we do have the constraint that no new edges or nodes can be added from our
perturbation framework.
7.2   Robustness of clustering
We design this experiment to answer two questions. Firstly, For fixed σw , does BP depend on
weight distribution? Secondly, Is BP informative? We perform the experiment using the following
steps. Firstly, we generate w from the four weight distributions: node resampling, binary, gamma
and mixture distributions. with E(w) = 1 and σw , as described in Section 2. We assign various
magnitude of σw to capture the amount of perturbation the clusterings can tolerate.
Secondly, for each perturbed graph G̃, we perform spectral clustering algorithm and obtain C.     ˜ We
then compute misclassification errors dist(Ctrue , C), ˜ and dist(Cspc , C), ˜ where Ctrue is the true
clustering of the underlying model, and Cspc is the clustering obtained from the unperturbed graph
through spectral clustering. The misclassification error is define in Section ??. If there is a value of
σw where C˜ becomes very unstable, that the misclassification error starts to have high variance, we
will call it the breakdown point.
We employ two undirected synthetic datasets from DC-SBM. They both have n = 800, and
                                                                                            i
λ1:K = (0, 0.2, 0.4, 0.6, 0.8). The difference is that the first dataset is generated with wDC−SBM    ∼
                                                                              i
0.5 + 0.5 × U nif orm(0, 1), and the second one is generated with wDC−SBM ∼ 0.4 + 0.6 ×
U nif orm(0, 1). The results are shown in Figure 5.
For all the cases, we observe a significant break in the variance of misclassification errors, which we
define as BP . For example, the BP for binary distribution in the easy dataset is around 0.8, which
becomes 0.6 in the hard dataset. Notice that with all weight distributions, BP ’s in the harder dataset
                                                   12

                                         i
Figure 5: Left: easy dataset with wDC−SBM           ∼ 0.5 + 0.5 × U nif orm(0, 1). Right: hard dataset
        i
with wDC−SBM        ∼ 0.4 + 0.6 × U nif orm(0, 1). 1st row: Node resampling. 2And row: Binary . 3rd
row: gamma distribution. 4th row: mixture-uniform gamma distirbution. C˜ is the weighted version of
clustering obtained from spectral clustering algorithm. Each boxplot is consist of 100 repetitions.
are always smaller comparing to that in the eaiser dataset. This confirms that BP is informative,
since it predicts the sensitivity of graph properties in harder (less robust) dataset. For the same dataset,
across different weight distributions, the misclassification errors break at different σw , which suggests
that BP varies with different weight distributions.
7.3   Robustness of WCut
In the experiments here we want to verify that IF of WCut is informative, and then probe the source
of robustness of WCut at the node level.
For the synthetic dataset with n = 800, w ∼ 0.5 + 0.5 × U nif orm(0, 1), We firstly obtain its
clustering C from spectral clustering algorithm. We then add noise to C by randomly picking 200
nodes and reassigning them to other clusters randomly, in which way we obtain C.           ˜ The WCut of
                                                     13

           Figure 6: The histograms of IF W Cut for synthetic datasets. Left: C. Right: C.˜
C˜ is expected to be less robust comparing to that of C, since there are already more noises in C.   ˜
After computing IF W Cut for both C and C,    ˜ we plot the histograms in Figure 6. We observe that
in the histogram for C, IF W Cut is more concentrated on 0 with fewer large influence nodes, thus
indicating a robust clustering. On the other hand, the histogram for C˜ indicates the clustering being
sensitive. These findings correspond with our expectations and show that IF W Cut is informative.
The other experiment we do is to probe where the sensitivity comes from through a partial
perturbation. We generate synthetic dataset with n = 800, and λ1:K = (0, 0.2, 0.4, 0.6, 0.8). We
also perform the experiment on the Facebook dataset. We select the nodes with IFiW Cut (Ctrue ) > 0.
These nodes are considered not well clustered because increasing the weights on them is expected
to increase WCut, thus worse quality of clustering. For these nodes, we generate w from
Gamma(0.1/µ, 10µ2 ), where E(w) = µ, V ar(w) = 0.1. The rest of the nodes have w = 1. We
then assign the weights to the edges through asymmetric perturbation. We also examine the perturbed
clustering from spectral clustering algorithm.
The results are shown in Figure 7, we observe that with both datasets, WCut increases as the weights
on the nodes with bad influence increase, which indicates that the nodes with IF W Cut causes the
quality of clustering to drop, and could potentially be not well clustered. In the synthetic dataset,
dist(C,˜ Ctrue ) increases as E(w) increases, and decreases slightly but steadily as E(w) decreases.
This is because spectral clustering is equivalent to optimizing WCut [8], when less weight is imposed
on nodes with bad influence for IFiW Cut (Ctrue ), WCut gets smaller and C˜ becomes closer to Ctrue .
In the Facebook dataset, dist(C,˜ Ctrue ) decreases as E(w) increases. This is because the underlying
clustering Ctrue in Facebook does not correspond to the clustering that minimizing WCut. Therefore
minimizing WCut does not lead to C˜ being close to Ctrue .
7.4    Robustness of wCC’s and eigengap
We firstly probe the sensitivity of wCC’s and eigengap. We calculate IFifu , IFifl and IF fe for
the nodes and select the nodes with bad influence. We call the nodes with IFifu < 0 or IFifl > 0
or IF fe < 0 nodes with bad influence, since these nodes make the distinction between wCC’s or
eigengap less significant. We do a partial perturbation on the graphs by perturbing the weights of
these nodes with wi ∼ U nif orm(a, a + 0.25), a ∈ [0.5, 0.6, · · · , 1.5].
We also examine their robustness by making full perturbation on the entire graph. We generate
wi ∼ M ixture(0.5, b, 0, 0.1, T− , Gamma, p) for all the nodes, where T− is a distribution centered
around 0.51 with probability 1, p ranges from 0.1 to 0.9. We choose T− to maintain the weights on
the edges to be positive. Since the strength of perturbation is mostly coming from T+ and the choice
of the base binary distribution, we do not lose generosity. In this way E(w) = 1 and σ(w) = σw
varies.
The synthetic dataset for testing the robustness of wCC’s is generated with n = 2000, and
λ1:K = (0, 0.1, 0.11, 0.12, 0.13). In the synthetic dataset, because of the nature of the model, the
                                                    14

Figure 7: Left column: the change of WCut with respect to E(w). Right column: the change of
classification error with respect of E(w). First row: synthetic datset. Second row: Facebook datset.
Each boxplot is consist of 100 repetitions.
largest fl in the graph appears when K = 5. In the Facebook dataset, through calculation ,we find
that the largest fl appears when K = 7. Therefore, we assume initially there are 5 wCC’s in the
synthetic dataset and 7 connected components in Facebook dataset. The results are shown in Figure
8.
The synthetic dataset for testing the robustness of eigengap is generated with n = 800 and λ1:K =
(0, 0.1, 0.2, 0.3, 0.4). We call fe (i) = λi+1 − λi the ith eigengap. Through calculation, we find that
the largest eigengap is the 5th in synthetic dataset, and 7th in the Facebook dataset.
We observe that, fl increases while fu and fe decrease as E(w) increases in both datasets, which
indicates that, as the bad influence increases, there is greater connectivity between the connected
components and the eigengap becomes less significant. BP is defined as the σw where fu , fl and fe
is dominated with another K. We observe the BP ’s for both the number of wCC’s and eigengap.
In the Facebook dataset, they seem to be robust to perturbation. Note that the Facebook dataset is
consist of 10 ego-networks while our experiments indicate that there are only 7 robustness wCC’s
inside, meaning there are 3 more connected users who are grouped to one wCC.
8    Discussion
This paper makes several contributions. Firstly, it provides an innovative way to perturb the network
by assigning the weights on the nodes and edges. The strength of perturbation can be well controlled
and can be arbitrarily small. The topology of the graph is also preserved after the perturbation.
Secondly, it extends the definitions of influence function and breakdown point to the graph properties.
                                                   15

Figure 8: Topleft: IF (λ2 , w). Left column: synthetic datset. Right column: Facebook dataset. First
row: IF (fl ). Second row: IF (fu ). Third row: breakdown point of fu . Each boxplot is consist of
100 repetitions.
Although these measures are widely used in the robust statistics literature, using them on graph
properties is the first time. Last but not the least, we are also able to probe the source of robustness by
quantifying the influence of nodes on the robustness of graph properties, which provides a deeper
insight into the problem.
Our perturbation framework also have its limitations. For example, no new edges or nodes can be
added to the graphs through our perturbation methods, and this may not be natural evaluating the
graph properties in some social networks. Moreover, the perturbation methods is not suitable for
evaluating some graph properties, e.g. properties related to graph distances. These could be the area
for future explorations.
                                                      16

Figure 9: Left column: synthetic dataset. Right column: Facebook dataset. First row: IF (fe ).
Second row: breakdown point of fe . Each boxplot is consist of 100 repetitions.
References
 [1] Waqar Ali, Anatol E Wegner, Robert E Gaunt, Charlotte M Deane, and Gesine Reinert. Com-
     parison of large networks with sub-sampling strategies. Scientific reports, 6:28955, 2016.
 [2] Sharmodeep Bhattacharyya, Peter J Bickel, et al. Subsampling bootstrap of count features of
     networks. The Annals of Statistics, 43(6):2384–2411, 2015.
 [3] David Gfeller, Jean-Cédric Chappelier, and Paolo De Los Rios. Finding instabilities in the
     community structure of complex networks. Physical Review E, 72(5):056135, 2005.
 [4] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust
     statistics: the approach based on influence functions, volume 114. John Wiley & Sons, 2011.
 [5] B. Karrer, E. Levina, and M.E.J. Newman. Robustness of community structure in networks.
     Physical Revieww E, 77(4)::046119, 2008.
 [6] Jure Leskovec and Julian J Mcauley. Learning to discover social circles in ego networks. In
     Advances in neural information processing systems, pages 539–547, 2012.
 [7] Jan R Magnus and Heinz Neudecker. Matrix differential calculus with applications in statistics
     and econometrics. Wiley series in probability and mathematical statistics, 1988.
 [8] Marina Meilă and William Pentney. Clustering by weighted cuts in directed graphs. In Chid
     Apte, David Skillicorn, and Vipin Kumar, editors, Proceedings of the SIAM Data Mining
     Conference, SDM. SIAM, 2007.
 [9] Boaz Nadler, Stephane Lafon, Ronald Coifman, and Ioannis Kevrekidis. Diffusion maps,
     spectral clustering and eigenfunctions of fokker-planck operators. In Y. Weiss, B. Schölkopf,
                                                 17

     and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 955–962,
     Cambridge, MA, 2006. MIT Press.
[10] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic
     blockmodel. In Advances in Neural Information Processing Systems, pages 3120–3128, 2013.
[11] Nicolas Verzelen, Ery Arias-Castro, et al. Community detection in sparse random networks.
     The Annals of Applied Probability, 25(6):3465–3510, 2015.
[12] Yali Wan and Marina Meila. Benchmarking recovery theorems for the DC-SBM. In Interna-
     tional Symposium on Artificial Intelligence and Mathematics (ISAIM), 2015.
[13] Yali Wan and Marina Meila. A class of network models recoverable by spectral clustering.
     In Daniel Lee and Masashi Sugiyama, editors, Advances in Neural Information Processing
     Systems (NIPS), 2015.
[14] Yali Wan and Marina Meilă. Graph clustering: block-models and model-free results. In
     Advances in Neural Information Processing Systems (NIPS), 2016.
                                                 18

