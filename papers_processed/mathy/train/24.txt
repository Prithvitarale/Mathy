UniversityofPennsylvaniaScholarlyCommonsStatisticsPapersWhartonFacultyResearch2001ANaturalPolicyGradientShamM.KakadeUniversityofPennsylvaniaFollowthisandadditionalworksat:http://repository.upenn.edu/statistics_papersPartoftheBiostatisticsCommons,andtheStatisticalMethodologyCommonsThispaperispostedatScholarlyCommons.http://repository.upenn.edu/statistics_papers/471Formoreinformation,pleasecontactrepository@pobox.upenn.edu.RecommendedCitationKakade,ANaturalPolicyGradientShamKakadeGatsbyComputationalNeuroscienceUnit17QueenSquare,London,UKWC1N3ARhttp://www.gatsby.ucl.ac.uksham@gatsby.ucl.ac.ukAbstractWeprovideanaturalgradientmethodthatrepresentsthesteepestdescentdirectionbasedontheunderlyingstructureoftheparam-eterspace.Althoughgradientmethodscannotmakelargechangesinthevaluesoftheparameters,weshowthatthenaturalgradi-entismovingtowardchoosingagreedyoptimalactionratherthanjustabetteraction.Thesegreedyoptimalactionsarethosethatwouldbechosenunderoneimprovementstepofpolicyiterationwithapproximate,compatiblevaluefunctions,asdefinedbySut-tonetal.[9].WethenshowdrasticperformanceimprovementsinsimpleMDPsandinthemorechallengingMDPofTetris.1thedistribution).Wemaketheassumptionthateverypolicy7risergodic,iehasawell-definedstationarydistributionp7f.Underthisassumption,theaveragereward(orundiscountedreward)is1](7r)==2::s,ap7f(s)7r(a;S)R(s,a),thestate-actionvalueis3TheNaturalGradientandPolicyIterationWenowcomparepolicyimprovementunderthenaturalgradienttopolicyiteration.Foranappropriatecomparison,considerthecaseinwhichQ7r(s,a)isapproximatedbysomecompatiblefunctionapproximatorr(s,a;w)parameterizedbyw[9,6].3.1CompatibleFunctionApproximationForvectors(),wE~m,wedefine:'IjJ(s,a)7r=\7logn(a;s,()),r(s,a;w)=wT'ljJ7r(s,a)(5)where[\7logn(a;s,())]i=8logn(a;s,())!8()i.Letwminimizethesquarederrorf(W,n)==L,s,ap7r(s)n(a;s,())(r(s,a;w)_Q7r(s,a))2.Thisfunctionapproximatoriscompatiblewiththepolicyinthesensethatifweusetheapproximationsf7r(s,a;w)inlieuoftheirtruevaluestocomputethegradient(equation1),thentheresultwouldstillbeexact[9,6](andisthusasensiblechoicetouseinactor-criticschemes).Theorem1.Letwminimizethesquarederrorf(W,no).Thenw=~1}(()).Proof.Sincewminimizesthesquarederror,itsatisfiesthecondition8f!8wi=0,whichimplies:LP7r(s)n(a;s,())'ljJ7r(s,a)('ljJ7r(s,a?w-Q7r(s,a))=O.s,aorequivalently:s,as,aBydefinitionof'ljJ7r,\7n(a;s,())=n(a;s,())'ljJ7r(s,a)andsotherighthandsideisequaltospeaking,theprobabilitymanifoldof7r(a;s,0)couldbecurved,soatranslationofapointbyatangentvectorwouldnotnecessarilykeepthepointonthemanifold(suchasonasphere).Weconsiderthegeneral(non-exponential)caselater.Wenowshow,4MetricsandCurvaturesObviously,ourchoiceofFisnotuniqueandthequestionarisesastowhetherornotthereisabettermetrictousethanF.Inthedifferentsettingofparameterestimation,theFisherinformationconvergestotheHessian,soB~aC21rlD-unsealed-8'I'--''''''~''$=10s=1......i1r______-11',05..•~......12"EI-'::',$,=1$2=10_.--(',\2~":.~R=O)~0'::-0--0~5C------:-'-----:-':'5C------::'2"\::.:-.h~21timex107/:--------1.Q-"\""::>:"W~,L--------------.\';20--"-,-,~L---::-,::::;·7J~========-~L_-=--2--':-'':::::;0:::=:::'~2:=':3=::l4'0a0.511.522.5358.,1015I0910(time)timeFigure1:A)ThecostVs.10glo(time)foranLQG(with20timesteptrajectories).Thepolicyusedwas7f(u;x,())ex:exp(()lslX2+()2S2X)wheretherescalingconstants,SlandS2,areshowninthelegend.Underequivalentstartingdistributions(()lSl=()2S2=-.8),theright-mostthreecurvesaregeneratedusingthestandardgradientmethodandtherestusethenaturalgradient.B)Seetext.Ctop)Theaveragerewardvs.time(ona107scale)ofapolicyunderstandardgradientdescentusingthesigmoidalpolicyparameterization(7f(I;s,()i)ex:exp(()i)/(1+exp(()i)),withtheinitialconditions7f(i,1)=.8and7f(j,1)=.1.Cbottom)Theaveragerewardvs.time(unscaled)understandardgradientdescent(solidline)andnaturalgradientdescent(dashedline)foranearlywindowoftheaboveplot.D)Phasespaceplotforthestandardgradientcase(thesolidline)andthenaturalgradientcase(dashedline).faster.Alsonoticethatthecurvesunderdifferentrescalingarenot