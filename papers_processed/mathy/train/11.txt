Graph Clustering: Block-models and model free
results
Anonymous Author(s)
Affiliation
Address
email
Abstract
Clustering graphs under the Stochastic Block Model (SBM) and extensions are
1
well studied. Guarantees of correctness exist under the assumption that the data
2
is sampled from a model. In this paper, we propose a framework, in which we
3
obtain â€œcorrectnessâ€ guarantees without assuming the data comes from a model.
4
The guarantees we obtain depend instead on the statistics of the data that can be
5
checked. We also show that this framework ties in with the existing model-based
6
framework, and that we can exploit results in model-based recovery, as well as
7
strengthen the results existing in that area of research.
8
1 Introduction: a framework for clustering with guarantees without model
9
assumptions
10
In the last few years, model-based clustering in networks has witnessed spectacular progress. At
11
the central of intact are the so-called block-models, the Stochastic Block Model (SBM), Degree-
12
Corrected SBM (DC-SBM) and Preference Frame Model (PFM). The understanding of these models
13
has been advanced, especially in understanding the conditions when recovery of the true clustering is
14
possible with small or no error. The algorithms for recovery with guarantees has also been improved.
15
However, the impact of the above results is limited by the assumption that the observed data come
16
from the model.
17
This paper proposes a framework to provide theoretical guarantees for the results of model based
18
clustering algorithms, without making any assumption about the data generating process. To de-
19
scribe the idea, we need some notation. Assume that a graph G on n nodes is observed. A model-
20
based algorithm clusters G, and outputs clustering C and parameters M(G, C).
21
The framework is as follows: if M(G, C) fits the data G well, then we shall prove that any other
22
clustering C0
of G that also fits G well will be a small perturbation of C. If this holds, then C with
23
model parameters M(G, C) can be said to capture the data structure in a meaningful way.
24
We exemplify our approach by obtaining model-free guarantees for the SBM and PFM models.
25
Moreover, we show that model-free and model-based results are intimately connected.
26
2 Background: graphs, clusterings and block models
27
Graphs, degrees, Laplacian, and clustering Let G be a graph on n nodes, described by its ad-
28
jacency matrix AÌ‚. Define Ë†
di =
Pn
j=1 AÌ‚ij the degree of node i, and DÌ‚ = diag{ Ë†
di} the diagonal
29
matrix of the node degrees. The (normalized) Laplacian of G is defined as1
LÌ‚ = DÌ‚âˆ’1/2
AÌ‚DÌ‚âˆ’1/2
. In
30
1
Rigorously speaking, the normalized graph Laplacian is I âˆ’ LÌ‚ [10].
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.
extension, we define the degree matrix D and the Laplacian L associated to any matrix A âˆˆ RnÃ—n
,
31
with Aij = Aji â‰¥ 0, in a similar way.
32
Let C be a partitioning (clustering) of the nodes of G into K clusters. We use the shorthand notation
33
i âˆˆ k for â€œnode i belongs to cluster kâ€. We will represent C by its nÃ—K indicator matrix Z, defined
34
by
35
Zik = 1 if i âˆˆ k, 0 otherwise, for i = 1, . . . n, k = 1, . . . K. (1)
Note that ZT
Z = diag{nk} with nk counting the number of nodes in cluster k, and ZT
AÌ‚Z =
36
[nkl]K
k,l=1 with nkl counting the edges in G between clusters k and l. Moreover, for two indicator
37
matrices Z, Z0
for clusterings C, C0
, (ZT
Z0
)kk0 counts the number of points in the intersection of
38
cluster k of C with cluster k0
of C0
, and (ZT
DÌ‚Z0
)kk0 computes
P
iâˆˆkâˆ©k0
Ë†
di the volume of the same
39
intersection.
40
â€œBlock modelsâ€ for random graphs (SBM, DC-SBM, PFM) This family of models contains
41
Stochastic Block Models (SBM) [19, 1], Degree-Corrected SBM (DC-SBM) [18] and Prefer-
42
ence Frame Models (PFM) [21]. Under each of these model families, a graph G with adja-
43
cency matrix AÌ‚ over n nodes is generated by sampling its edges independently following the law
44
AÌ‚ij âˆ¼ Bernoulli(Aij), for all i > j. The symmetric matrix A = [Aij] describing the graph is the
45
edge probability matrix. The three model families differ in the constraints they put on an acceptable
46
A. Let Câˆ—
be a clustering. The entries of A are defined w.r.t Câˆ—
as follows (and we say that A is
47
compatible with Câˆ—
).
48
SBM: Aij = Bkl whenever i âˆˆ k, j âˆˆ l, with B = [Bkl] âˆˆ RKÃ—K
symmetric and non-negative.
49
DC-SBM: Aij = wiwjBkl whenever i âˆˆ k, j âˆˆ l, with B as above and w1, . . . wn non-negative
50
weights associated with the graph nodes.
51
PFM: A satisfies D = diag(A1), Dâˆ’1
AZ = ZR where 1 denotes the vector of all ones, Z is
52
the indicator matrix of Câˆ—
, and R is a stochastic matrix (R1 = 1, Rkl â‰¥ 0), the details are
53
in [21]
54
While perhaps not immediately obvious, the SBM is a subclass of the DC-SBM, and the latter a
55
subclass of the PFM. Another common feature of block-models, that will be significant throughout
56
this work is that for all three, Spectral Clustering algorithms [16] have been proved to work well
57
estimating Câˆ—
.
58
3 Main theorem: blueprint and results for PFM, SBM
59
Let M be a model class, such as SBM, DC-SBM, PFM, and denote M(G, C) âˆˆ M to be a model
60
that is compatible with C and is fitted in some way to graph G (we do not assume in general that this
61
fit is optimal).
62
Theorem 1 (Generic Theorem) We say that clustering C fits G well w.r.t M iff M(G, C) is â€œclose
63
toâ€ G. If C fits G well w.r.t M, then (subject to other technical conditions) any other clustering C0
64
which also fits G well is close to C, i.e. dist(C, C0
) is small.
65
In what follows, we will instantiate this Generic Theorem, and the concepts therein; in
66
particular the following will be formally defined. (1) Model construction, i.e an algorithm
67
to fit a model in M to (G, C). This is necessary since we want our results to be
68
computable in practice. (2) A goodness of fit measure between M(C, G) and the data G.
69
(3) A distance between clusterings. We adopt the widely used Misclassification Error (or Hamming)
70
distance defined below.
71
The Misclassification Error (ME) distance between two clusterings C, C0
over the same set of n
72
points is
73
dist(C, C0
) = 1 âˆ’
1
n
max
Ï€âˆˆSK
X
iâˆˆkâˆ©Ï€(k)
1, (2)
where Ï€ ranges over all permutations of K elements SK, and Ï€(k) indexes a cluster in C0
. If the
74
points are weighted by their degrees, a natural measure on the node set, the Weighted ME (wME)
75
2
distance is
76
distË†
d(C, C0
) = 1 âˆ’
1
Pn
i=1
Ë†
di
max
Ï€âˆˆSK
X
iâˆˆkâˆ©Ï€(k)
Ë†
di . (3)
In the above,
P
iâˆˆkâˆ©k0
Ë†
di represents the total weight of the set of points assigned to cluster k by C
77
and to cluster k0
by C0
. Note that in the indicator matrix representation of clusterings, this is the
78
k, k0
element of the matrix ZT
DÌ‚Z0
âˆˆ RKÃ—K
. While dist is more popular, we believe distË†
d is more
79
natural, especially when node degrees are dissimilar, as Ë†
d can be seen as a natural measure on the
80
set of nodes, and distË†
d is equivalent to the earth-moverâ€™s distance.
81
3.1 Main result for PFM
82
Constructing a model Given a graph G and clustering C of its nodes, we wish to construct a PFM
83
compatible with C, so that its Laplacian L satisfies that ||LÌ‚ âˆ’ L|| is small.
84
Let the spectral decomposition of LÌ‚ be
85
LÌ‚ = [YÌ‚ YÌ‚low]

Î›Ì‚ 0
0 Î›Ì‚low
 
YÌ‚ T
YÌ‚ T
low

= YÌ‚ Î›Ì‚YÌ‚ T
+ YÌ‚lowÎ›Ì‚lowYÌ‚ T
low (4)
where YÌ‚ âˆˆ RnÃ—K
, YÌ‚low âˆˆ RnÃ—(nâˆ’K)
and Î›Ì‚, Î›Ì‚low diagonal matrices of dimension K, respectively
86
n âˆ’ K. To ensure that the matrices YÌ‚ , YÌ‚low are uniquely defined we assume throughout the paper
87
that LÌ‚â€™s K-th eigengap, i.e, |Î»K| âˆ’ |Î»K+1|, is non-zero.
88
Assumption 1 The eigenvalues of LÌ‚ satisfy Î»Ì‚1 = 1 â‰¥ |Î»Ì‚2| â‰¥ . . . â‰¥ |Î»Ì‚K| > |Î»Ì‚K+1| â‰¥ . . . |Î»Ì‚n|.
89
Denote the subspace spanned by the columns of M, for any M matrix, by R(M), and || || the
90
Euclidean or spectral norm.
91
PFM Estimation Algorithm
Input Graph G with AÌ‚, DÌ‚, LÌ‚, YÌ‚ , Î›Ì‚, clustering C with indicator matrix Z.
Output (A, L) = PFM(G, C)
1. Construct an orthogonal matrix derived from Z.
YZ = DÌ‚1/2
ZCâˆ’1/2
, with C = ZT
DÌ‚Z the column normalization of Z. (5)
Note Ckk =
P
iâˆˆk
Ë†
di the volume of cluster k.
2. Project YZ on YÌ‚ and perform Singular Value Decomposition.
F = Y T
Z YÌ‚ = UÎ£V T
(6)
3. Change basis in R(YZ) to align with YÌ‚ .
Y = YZUV T
. Complete Y to an orthonormal basis [Y B] of Rn
. (7)
4. Construct Laplacian L and edge probability matrix A.
L = Y Î›Ì‚Y T
+ (BBT
)LÌ‚(BBT
), A = DÌ‚1/2
LDÌ‚1/2
. (8)
92
Proposition 2 Let G, AÌ‚, DÌ‚, LÌ‚, YÌ‚ , Î›Ì‚ and Z be defined as above, and (A, L) = PFM(G, C). Then,
93
1. DÌ‚ and L, or A define a PFM with degrees Ë†
d1:n.
94
2. The columns of Y are eigenvectors of L with eigenvalues Î»Ì‚1:K.
95
3. DÌ‚1/2
1 is an eigenvector of both L and LÌ‚ with eigenvalue Î»Ì‚1 = 1.
96
3
The proof is relegated to the Supplement, as are all the omitted proofs.
97
PFM(G, C) is an estimator for the PFM parameters given the clustering. It is evidently not the
98
Maximum Likelihood estimator, but we can show that it is consistent in the following sense.
99
Proposition 3 (Informal) Assume that G is sampled from a PFM with parameters Dâˆ—
, Lâˆ—
and com-
100
patible with Câˆ—
, and let L = PFM(G, Câˆ—
). Then, under standard recovery conditions for PFM (e.g
101
[21]) ||Lâˆ—
âˆ’ L|| = o(1) w.r.t. n.
102
Assumption 2 (Goodness of fit for PFM) ||LÌ‚ âˆ’ L|| â‰¤ Îµ.
103
PFM(G, C) instantiates M(G, C), and Assumption 2 instantiates the goodness of fit measure. It
104
remains to prove an instance of Generic Theorem 1 for these choices.
105
Theorem 4 (Main Result (PFM)) Let G be a graph with Ë†
d1:n, DÌ‚, LÌ‚, Î»Ì‚1:n as defined, and LÌ‚ sat-
106
isfy Assumption 1. Let C, C0
be two clusterings with K clusters, and L, L0
their correspond-
107
ing Laplacians, defined as in (8), and satisfy Assumption 2. Set Î´ = 4(Kâˆ’1)Îµ2
(|Î»Ì‚K |âˆ’|Î»Ì‚K+1|)2
and Î´0 =
108
mink Ckk/ maxk Ckk with C defined as in (5), where k indexes the clusters of C. Then, whenever
109
Î´ â‰¤ Î´0,
110
distË†
d(C, C0
) â‰¤
maxk Ckk
P
k Ckk
Î´, (9)
with distË†
d being the weighted ME distance (3).
111
In the remainder of this section we outline the proof steps, while the partial results of Proposition 5,
112
6, 7 are proved in the Supplement. First, we apply the perturbation bound called the Sinus Theorem
113
of Davis and Kahan, in the form presented in Chapter V of [20].
114
Proposition 5 Let YÌ‚ , Î»Ì‚1:n, Y be defined as usual. If Assumptions 1 and 2 hold, then
115
|| diag(sin Î¸1:K(YÌ‚ , Y ))|| â‰¤
Îµ
|Î»Ì‚K| âˆ’ |Î»Ì‚K+1|
= Îµ0
(10)
where Î¸1:K are the canonical (or principal) angles between R(YÌ‚ ) and R(Y ) (see e.g [8]).
116
The next step concerns the closeness of Y, YÌ‚ in Frobenius norm. Since Proposition 5 bounds the
117
sinuses of the canonical angles, we exploit the fact that the cosines of the same angles are the singular
118
values of F = Y T
YÌ‚ of (6).
119
Proposition 6 Let M = Y Y T
, MÌ‚ = YÌ‚ YÌ‚ T
and F, Îµ0
as above. Assumptions 1 and 2 imply that
120
1. ||F||2
F = trace MMÌ‚T
â‰¥ K âˆ’ (K âˆ’ 1)Îµ02
.
121
2. ||M âˆ’ MÌ‚||2
F â‰¤ 2(K âˆ’ 1)Îµ02
.
122
Now we show that all clusterings which satisfy Proposition 6 must be close to each other in the
123
weighted ME distance. For this, we first need an intermediate result. Assume we have two clus-
124
terings C, C0
, with K clusters, for which we construct YZ, Y, L, M, respectively Y 0
Z, Y 0
, L0
, M0
as
125
above. Then, the subspaces spanned by Y and Y 0
will be close.
126
Proposition 7 Let LÌ‚ satisfy Assumption 1 and let C, C0
represent two clusterings for which L, L0
127
satisfy Assumption 2. Then, ||Y T
Z Y 0
Z||2
F â‰¥ K âˆ’ 4(K âˆ’ 1)Îµ02
= K âˆ’ Î´
128
The main result now follows from Proposition 7 and Theorem 9 of [14], as shown in the Supplement.
129
This proof approach is different from the existing perturbation bounds for clustering, which all use
130
counting arguments. The result of [14] is a local equivalence, which bounds the error we need in
131
terms of Î´ defined above (â€œlocalâ€ meaning the result only holds for small Î´).
132
4
3.2 Main Theorem for SBM
133
In this section, we offer an instantiation of Generic Theorem 1 for the case of the SBM. As before,
134
we start with a model estimator, which in this case is the Maximum Likelihood estimator.
135
SBM Estimation Algorithm
Input Graph with AÌ‚, clustering C with indicator matrix Z.
Output A = SBM(G, C)
1. Construct an orthogonal matrix derived from Z: YZ = ZCâˆ’1/2
with C = ZT
Z.
2. Estimate the edge probabilities: B = Câˆ’1
ZT
AÌ‚ZCâˆ’1
.
3. Construct A from B by A = ZBZT
.
136
Proposition 8 Let BÌƒ = C1/2
BC1/2
and denote the eigenvalues of BÌƒ, ordered by decreasing mag-
137
nitude, by Î»1:K. Let the spectral decomposition of BÌƒ be BÌƒ = UÎ›UT
, with U an orthogonal matrix
138
and Î› = diag(Î»1:K). Then
139
1. A is a SBM.
140
2. Î»1:K are the K principal eigenvalues of A. The remaining eigenvalues of A are zero.
141
3. A = Y Î›Y T
where Y = YZU.
142
Assumption 3 (Eigengap) B is non-singular (or, equivalently, |Î»K| > 0.
143
Assumption 4 (Goodness of fit for SBM) ||AÌ‚ âˆ’ A|| â‰¤ Îµ.
144
With the model (SBM), estimator, and goodness of fit defined, we are ready for the main result.
145
Theorem 9 (Main Result (SBM)) Let G be a graph with incidence matrix AÌ‚, and Î»Ì‚A
K the K-th
146
singular value of AÌ‚. Let C, C0
be two clusterings with K clusters, satisfying Assumptions 3 and 4.
147
Set Î´ = 4KÎµ2
|Î»Ì‚A
K |2
and Î´0 = mink nk/ maxk nk, where k indexes the clusters of C. Then, whenever
148
Î´ â‰¤ Î´0, dist(C, C0
) â‰¤ Î´ maxk nk/n, where dist represents the ME distance (2).
149
Note that the eigengap of AÌ‚, Î›Ì‚A
K is not bounded above, and neither is Îµ. Since the SBM is less
150
flexible than the PFM, we expect that for the same data G, Theorem 9 will be more restrictive than
151
Theorem 4.
152
4 The results in perspective
153
4.1 Cluster validation
154
Theorems like 4, 9 can provide model free guarantees for clustering. We exemplify this procedure in
155
the experimental Section 6, using standard spectral clustering as described in e.g [19, 18, 16]. What
156
is essential is that all the quantities such as Îµ and Î´ are computable from the data.
157
Moreover, if Y is available, then the bound in Theorem 4 can be improved.
158
Proposition 10 Theorem 4 holds when Î´ is replaced by Î´Y = Kâˆ’ < MÌ‚, M >F +(K âˆ’ 1)(Îµ0
)2
+
159
2
p
2(K âˆ’ 1)Îµ0
||MÌ‚ âˆ’ M||F , with Îµ0
= Îµ/(|Î»Ì‚K| âˆ’ |Î»Ì‚K+1|) and M, MÌ‚ defined in Proposition 6.
160
4.2 Using existing model-based recovery theorems to prove model-free guarantees
161
We exemplify this by using (the proof of) Theorem 3 of [21] to prove the following.
162
Theorem 11 (Alternative result based on [21] for PFM) Under the same conditions as in Theo-
163
rem 4, distË†
d(C, C0
) â‰¤ Î´WM , with Î´WM = 128 KÎµ2
(|Î»Ì‚K |âˆ’|Î»Ì‚K+1|)2
.
164
5
It follows, too, that with the techniques in this paper, the error bound in [21] can be improved by a
165
factor of 128.
166
Similarly, if we use the results of [19] we obtain alternative model-free guarantee for the SBM.
167
Assumption 5 (Alternative goodness of fit for SBM) ||LÌ‚2
âˆ’L2
||F â‰¤ Îµ, where LÌ‚, L are the Lapla-
168
cians of AÌ‚ and A = SBM(G, C) respectively.
169
Theorem 12 (Alternative result based on [19] for SBM) Under the same conditions as in Theo-
170
rem 9, except for replacing Assumption 4 with 5, dist(C, C0
) â‰¤ Î´RCY with Î´RCY = Îµ2
|Î»Ì‚K |4
16 maxk nk
n .
171
A problem with this result is that Assumption 5 is much stronger than 4 (being in Frobenius norm).
172
The more recent results of [18] (with unspecified constants) in conjunction with our original As-
173
sumptions 3, 4, and the assumption that all clusters have equal sizes, give a bound of O(KÎµ2
/Î»Ì‚2
K)
174
for the SBM; hence our model-free Theorem 9 matches this more restrictive model-based theorem.
175
4.3 Sanity checks and Extensions
176
It can be easily verified that if indeed G is sampled from a SBM, or PFM, then for large enough n,
177
and large enough model eigengap, Assumptions 1 and 2 (or 3 and 4) will hold.
178
Some immediate extensions and variations of Theorems 4, 9 are possible. For example, one could
179
replace the spectral norm by the Frobenius norm in Assumptions 2 and 4, which would simplify
180
some of the proofs. However, using the Frobenius norm would be a much stronger assumption [19]
181
Theorem 4 holds not just for simple graphs, but in the more general case when AÌ‚ is a weighted graph
182
(i.e. a similarity matrix). The theorems can be extended to cover the case when C0
is a clustering
183
that is Î±-worse than C, i.e when ||L0
âˆ’ LÌ‚|| â‰¥ ||L âˆ’ LÌ‚||(1 âˆ’ Î±).
184
4.4 Clusterability and resilience
185
Our Theorems also imply the stability of a clustering to perturbations of the graph G. Indeed, let LÌ‚0
186
be the Laplacian of G0
, a perturbation of G. If ||LÌ‚0
âˆ’ LÌ‚|| â‰¤ Îµ, then ||LÌ‚0
âˆ’ L|| â‰¤ 2Îµ, and (1) G0
is
187
well fitted by a PFM whenever G is, and (2) C is Î´ stable w.r.t G0
, hence C is what some authors [9]
188
call resilient.
189
A graph G is clusterable when G can be fitted well by some clustering Câˆ—
. Much work [4, 7] has
190
been devoted to showing that clusterability implies that finding a C close to Câˆ—
is computationally
191
efficient. Such results can be obtained in our framework, by exploiting existing recovery theorems
192
such as [19, 18, 21], which give recovery guarantees for Spectral Clustering, under the assumption of
193
sampling from the model. For this, we can simply replace the model assumption with the assumption
194
that there is a Câˆ—
for which L (or A) satisfies Assumptions 1 and 2 (or 3 and 4).
195
5 Related work
196
To our knowledge, there is no work of the type of Theorem 1 in the literature on SBM, DC-SBM,
197
PFM. The closest work is by [6] which guarantees approximate recovery assuming G is close to a
198
DC-SBM.
199
Spectral clustering is also used for loss-based clustering in (weighted) graphs and some stability
200
results exist in this context. Even though they measure clustering quality by different criteria, so that
201
the Îµ values are not comparable, we review them here. The recent paper of [17], Theorem 1.2 states
202
that if the K-way Cheeger constant of G is Ï(k) â‰¤ (1 âˆ’ Î»Ì‚K+1)/(cK3
) then the clustering error2
203
distË†
d(C, Copt
) â‰¤ C/c = Î´P SZ. In the current proof, the constant C = 2 Ã— 105
; moreover, Ï(K)
204
cannot be computed tractably. In [15], the bound Î´MSX depends on ÎµMSX, the Normalized Cut
205
scaled by the eigengap. Since both bounds refer to the result of spectral clustering, we can compare
206
the relationship between Î´MSX and ÎµMSX; for [15], this is Î´MSX = 2ÎµMSX[1 âˆ’ ÎµMSX/(K âˆ’ 1)],
207
2
The results is stronger, bounding the perturbation of each cluster individually by Î´P SZ , but it also includes
a factor larger than 1, bounding the error of K-means algorithm.
6
which is about K âˆ’ 1 times larger than Î´ when  = MSX. In [5], dist(C, C0
) is defined in terms of
208
||Y T
Z âˆ’ Y 0
Z||2
F , and the loss is (closely related) to ||AÌ‚ âˆ’ SBM(G, C)||2
F . The bound does not take
209
into account the eigengap, that is, the stability of the subspace YÌ‚ itself.
210
Bootstrap for validating a clustering C was studied in [11] (see also references therein for earlier
211
work). In [3] the idea is to introduce a statistics, and large deviation bounds for it, conditioned on
212
sampling from a SBM (with covariates) and on a given C.
213
6 Experimental evaluation
214
Experiment Setup Given G, we obtain a clustering C0 by spectral clustering [16]. Then we
215
calculate clustering C by perturbing C0 with gradually increasing noise. For each C, we construct
216
PFM (C, G)and SBM(C, G) model, and further compute , Î´ and Î´0. If Î´ â‰¤ Î´0, C is guaranteed to be
217
stable by the theorems. In the remainder of this section, we describe the data generating process for
218
the simulated datasets and the results we obtained.
219
220
PFM Datasets We generate from PFM model with K = 5, n = 10000, Î»1:K =
221
(1, 0.875, 0.75, 0.625, 0.5). eigengap = 0.48, n1:K = (2000, 2000, 2000, 2000, 2000). The
222
stochastic matrix R and its stationary distribution Ï are shown below. We sample an adjacency
223
matrix AÌ‚ from A (shown below).
224
Ï =

25 .12 .17 .18 .28

R =
ï£®
ï£¯
ï£¯
ï£¯
ï£°
.79 .02 .06 .03 .10
.03 .71 .23 .00 .02
.09 .16 .69 .00 .06
.04 .00 .00 .80 .16
.10 .01 .03 .11 .76
ï£¹
ï£º
ï£º
ï£º
ï£»
A AÌ‚
225
Perturbed PFM Datasets A is obtained from the previous model by perturbing its principal
226
subspace (details in Supplement). Then we sample AÌ‚ from A.
227
228
Lancichinetti-Fortunato-Radicchi (LFR) simulated matrix [12] The LFR benchmark graphs
229
are widely used for community detection algorithms, due to heterogeneity in the distribution
230
of node degree and community size. A LFR matrix is simulated with n = 10000, K = 4,
231
nk = (2467, 2416, 2427, 2690) and Âµ = 0.2, where Âµ is the mixing parameter indicating the
232
fraction of edges shared between a node and the other nodes from outside its community.
233
234
Political Blogs Dataset A directed network ~
A of hyperlinks between weblogs on US politics,
235
compiled from online directories by Adamic and Glance [2], where each blog is assigned a political
236
leaning, liberal or conservative, based on its blog content. The network A contains 1490 blogs.
237
After erasing the disconnected nodes, n = 983. We study AÌ‚ = ( ~
AT ~
A)3
, which is a smoothed
238
undirected graph. For ~
AT ~
A we find no guarantees.
239
240
The first two data sets are expected to fit the PFM well, but not the SBM, while the LFR data is
241
expected to be a good fit for a SBM. Since all bounds can be computed on weighted graphs as well,
242
we have run the experiments also on the edge probability matrices A used to generate the PFM and
243
perturbed PFM graphs.
244
The results of these experiments are summarized in Figure 1. For all of the experiments, the cluster-
245
ing C is ensured to be stable by Theorem 4 as the unweighted error grows to a breaking point, then
246
the assumptions of the theorem fail. In particular, the C0 is always stable in the PFM framework.
247
7
Comparing Î´ from Theorem 9 to that from Theorem 4, we find that Theorem 9 (guarantees for SBM)
248
is much harder to satisfy. All Î´ values from Theorem 9 are above 1, and not shown.3
In particular,
249
for the SBM model class, the C cannot be proved stable even for the LFR data.
250
Note that part of the reason why with the PFM model very little difference from the clustering C0 can
251
be tolerated for a clustering to be stable is that the large eigengap makes PFM(G, C) differ from
252
PFM(G, C0) even for very small perturbations. By comparing the bounds for AÌ‚ with the bounds
253
for the â€œweighted graphsâ€ A, we can evaluate that the sampling noise on Î´ is approximately equal
254
to that of the clustering perturbation. Of course, the sampling noise varies with n, decreasing for
255
larger graphs. Moreover, from Political Blogs data, we see that â€œsmoothingâ€ a graph, by e.g. taking
256
powers of its adjacency matrix, has a stability inducing effect.
257
Figure 1: Quantities , Î´, Î´0 from Thm 4 plotted vs dist(C, C0) for various datasets: AÌ‚ denotes a simple graph, while A denotes a
weighted graph (i.e. a non-negative matrix). For the Political Blogs: Truth means C0 is true clustering of [2], spectral means C0 is obtained
from spectral clustering. For SBM, Î´ is always greater than Î´0.
7 Discussion
258
This paper makes several contributions. At a high level, it poses the problem of model free validation
259
in the area of community detection in networks. The stability paradigm is not entirely new, but
260
using it explicitly with model-based clustering (instead of cost-based) is. So is â€œturning aroundâ€ the
261
model-based recovery theorems to be used in a model-free framework.
262
All quantities in our theorems are computable from the data and the clustering C, i.e do not con-
263
tain undetermined constants, and do not depend on parameters that are not available. As with
264
distribution-free results in general, making fewer assumptions allows for less confidence in the con-
265
clusions, and the results are not always informative. Sometimes this should be so, e.g when the
266
data does not fit the model well. But it is also possible that the fit is good, but not good enough
267
to satisfy the conditions of the theorems as they are currently formulated. This happens with the
268
SBM bounds, and we believe tighter bounds are possible for this model. It would be particularly
269
interesting to study the non-spectral, sharp thresholds of [1] from the point of view of model-free
270
recovery. A complementary problem is to obtain negative guarantees (i.e that C is not unique up to
271
perturbations).
272
At the technical level, we obtain several different and model-specific stability results, that bound the
273
perturbation of a clustering by the perturbation of a model. They can be used both in model-free
274
and in existing or future model-based recovery guarantees, as we have shown in Section 3 and in the
275
experiments. The proof techniques that lead to these results are actually simpler, more direct, and
276
more elementary than the ones found in previous papers.
277
3
We also computed Î´RCY but the bounds were not informative.
8
References
278
[1] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models:
279
fundamental limits and efficient recovery algorithms. arXiv preprint arXiv:1503.00609, 2015.
280
[2] Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election:
281
divided they blog. In Proceedings of the 3rd international workshop on Link discovery, pages
282
36â€“43. ACM, 2005.
283
[3] Edoardo M. Airoldi, David S. Choi, and Patrick J. Wolfe. Confidence sets for network struc-
284
ture. Technical Report arXiv:1105.6245, 2011.
285
[4] Pranjal Awasthi. Clustering under stability assumptions. In Encyclopedia of Algorithms, pages
286
331â€“335. 2016.
287
[5] Francis Bach and Michael I. Jordan. Learning spectral clustering with applications to speech
288
separation. Journal of Machine Learning Research, 7:1963â€“2001, 2006.
289
[6] Maria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, and Shang-Hua
290
Teng. Finding endogenously formed communities. In Proceedings of the Twenty-Fourth An-
291
nual ACM-SIAM Symposium on Discrete Algorithms, pages 767â€“783. SIAM, 2013.
292
[7] Shai Ben-David. Computational feasibility of clustering under clusterability assumptions.
293
CoRR, abs/1501.00437, 2015.
294
[8] Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.
295
[9] Yonatan Bilu and Nathan Linial. Are stable instances easy? CoRR, abs/0906.3162, 2009.
296
[10] Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997.
297
[11] Brian Karrer, Elizaveta Levina, and M. E. J. Newman. Robustness of community structure in
298
networks. Phys. Rev. E, 77:046119, Apr 2008.
299
[12] Andrea Lancichinetti, Santo Fortunato, and Filippo Radicchi. Benchmark graphs for testing
300
community detection algorithms. Physical review E, 78(4):046110, 2008.
301
[13] Marina Meila and Jianbo Shi. Learning segmentation by random walks. In Neural Information
302
Processing Systems, 2001.
303
[14] Marina MeilaÌ†. Local equivalence of distances between clusterings â€“ a geometric perspective.
304
Machine Learning, 86(3):369â€“389, 2012.
305
[15] Marina MeilaÌ†, Susan Shortreed, and Liang Xu. Regularized spectral learning. In Robert Cow-
306
ell and Zoubin Ghahramani, editors, Proceedings of the Artificial Intelligence and Statistics
307
Workshop(AISTATS 05), 2005.
308
[16] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm.
309
In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information
310
Processing Systems 14, Cambridge, MA, 2002. MIT Press.
311
[17] Richard Peng, He Sun, and Luca Zanetti. Partitioning well-clustered graphs with k-means and
312
heat kernel. In Proceedings of the Annual Conference on Learning Theory (COLT), pages
313
1423â€“1455, 2015.
314
[18] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic
315
blockmodel. In Advances in Neural Information Processing Systems, pages 3120â€“3128, 2013.
316
[19] Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional
317
stochastic blockmodel. The Annals of Statistics, pages 1878â€“1915, 2011.
318
[20] Gilbert W Stewart and Ji-guang Sun. Matrix perturbation theory, volume 175. Academic press
319
New York, 1990.
320
[21] Yali Wan and Marina Meila. A class of network models recoverable by spectral clustering.
321
In Daniel Lee and Masashi Sugiyama, editors, Advances in Neural Information Processing
322
Systems (NIPS), page (to appear), 2015.
323
9
8 Supplementary Material for Graph Clustering: Block-models and model
324
free results
325
Proof of Proposition 2
326
1. Proof by verification.
327
2. LY = Y Î›Ì‚Y T
Y + (BBT
)LÌ‚(BBT
)Y = Y Î›Ì‚. Since B is the orthogonal complement of
328
Y , it follows that it is a stable subspace as well.
329
3. This is a well known result; see for example [20].
330
The celebrated Sinus Theorem is reproduced here for completeness.
331
Theorem 13 (Sinus Theorem of Davis-Kahan, from [20], Theorem V.3.6) Let LÌ‚ be a Hermi-
332
tian matrix with spectral resolution given by (4), Y be any n Ã— K matrix with orthonormal
333
columns, and M any symmetric K Ã— K matrix with eigenvalues Âµ1:K. Let R = LÌ‚Y âˆ’ Y M
334
and âˆ† = minÎ»âˆˆÎ»Ì‚K+1:n,ÂµâˆˆÂµ1:K
|Î» âˆ’ Âµ| > 0. Then, for any unitarily invariant norm || ||,
335
|| diag(sin Î¸1:K(YÌ‚ , Y ))|| â‰¤ ||R||
âˆ† , where Î¸1:K are the canonical angles between R(YÌ‚ ) and R(Y ).
336
Proof of Proposition 5 This is a corollary of Theorem 3.6 in [20]. If eigenvalues are sorted by their
absolute values, then Î»Ì‚K+1:n âˆˆ [âˆ’|Î»Ì‚K+1|, |Î»Ì‚K+1|] and Âµ1:K âˆˆ R\(âˆ’|Î»Ì‚K+1|âˆ’âˆ†, |Î»Ì‚K+1|+âˆ†). If
we set M = Î›Ì‚, so that Î»Ì‚1:K âˆˆ R \ (âˆ’|Î»Ì‚K+1| âˆ’ âˆ†, |Î»Ì‚K+1| + âˆ†). Now we view Y as a perturbation
of YÌ‚ , hence
R = LÌ‚Y âˆ’ Y Î›Ì‚ = LÌ‚Y âˆ’ LY + (LY âˆ’ Y Î›Ì‚) = (LÌ‚ âˆ’ L)Y (11)
||R|| = ||(LÌ‚ âˆ’ L)Y || â‰¤ ||LÌ‚ âˆ’ L||||Y || â‰¤ Îµ. (12)
From Theorem 13 the result follows. 2
337
Proof of Proposition 6 For 1:
||F||2
F = trace FFT
= trace UÎ£V T
V Î£UT
= trace UT
UÎ£V T
V Î£ = trace Î£2
= 1 +
K
X
k=2
cos2
Î¸k = 1 +
K
X
k=2
(1 âˆ’ sin2
Î¸k) = K âˆ’
K
X
k=2
sin2
Î¸k since Î¸1 = 0 (13)
â‰¥ K âˆ’ (K âˆ’ 1)Îµ02
(14)
For 2: Denote trace MÌ‚T
M =< MÌ‚, M >F . Then ||M âˆ’ MÌ‚||2
F = ||M||2
F + ||MÌ‚||2
F âˆ’ 2 <
338
MÌ‚, M >F â‰¤ K + K âˆ’ 2(K âˆ’ (K âˆ’ 1)Îµ02
) = 2(K âˆ’ 1)Îµ02
. 2
339
Proof of Proposition 7 We have that | < M âˆ’ MÌ‚, M0
âˆ’ MÌ‚ >F | â‰¤ ||M âˆ’ MÌ‚||F ||M0
âˆ’ MÌ‚||F .
From Proposition 6 the r.h.s is no larger than 2(K âˆ’ 1)Îµ02
.
âˆ’ < M âˆ’ MÌ‚, M0
âˆ’ MÌ‚ >F â‰¤ ||M âˆ’ MÌ‚||F ||M0
âˆ’ MÌ‚||F â‰¤ 2(K âˆ’ 1)Îµ02
(15)
âˆ’ < M, M0
>F + < MÌ‚, M >F + < MÌ‚, M0
>F âˆ’||MÌ‚||2
F â‰¤ 2(K âˆ’ 1)Îµ02
(16)
< M, M0
>F â‰¥ < MÌ‚, M >F + < MÌ‚, M0
>F âˆ’K âˆ’ 2(K âˆ’ 1)Îµ02
(17)
â‰¥ 2K âˆ’ 2(K âˆ’ 1)Îµ02
âˆ’ K âˆ’ 2(K âˆ’ 1)Îµ02
= K âˆ’ 4(K âˆ’ 1)Îµ02
2
(18)
Now, note that trace MM0
= trace Y Y T
Y 0
(Y 0
)T
= trace((Y 0
)T
Y ))(Y T
Y 0
) = ||Y T
Y 0
||2
F .
340
Moreover, by (7), YZ and Y differ by a unitary transformation. Since || ||F is unitarily invariant,
341
the result follows.
342
Proof of Theorem 4 We apply Theorem 9 of [14] with AX = Z, AX0 = Z0
, and AÌƒX = Y , AÌƒX0 =
343
Y 0
. It follows that pXYkk0 =
P
iâˆˆkâˆ©k0
Ë†
di/
Pn
i=1
Ë†
di. Hence, the point weights are proportional to
344
Ë†
d1:n. Also, evidently, pmin/pmax = Î´0, and the result follows.
345
Note that we use the fact that both PFMâ€™s have degrees equal to Ë†
d1:n to obtain this proof. 2
346
Proposition 14 Assumptions 3 and 4, imply || diag(sin Î¸1:K(YÌ‚ , Y ))|| â‰¤ Îµ/|Î»Ì‚A
K| = Îµ0
, where Î»Ì‚A
K
347
is the K-th eigenvalue of AÌ‚.
348
10
Proof of Proposition 14 We consider AÌ‚ a perturbation of A, its eigenvectors YÌ‚ as the perturbed
eigenvectors of A and M = Î›Ì‚. Then, R = AYÌ‚ âˆ’ YÌ‚ Î›Ì‚
||R|| = ||AYÌ‚ âˆ’ YÌ‚ Î›Ì‚|| (19)
= ||(AYÌ‚ âˆ’ AÌ‚YÌ‚ ) + (AÌ‚YÌ‚ âˆ’ YÌ‚ Î›Ì‚)|| (20)
â‰¤ ||(A âˆ’ AÌ‚)YÌ‚ || (21)
â‰¤ ||A âˆ’ AÌ‚||||YÌ‚ || â‰¤ Îµ. (22)
The separation between Î›Ì‚ and the residual spectrum of A is |Î»Ì‚K|. From the main Davis-Kahan
349
theorem 13 the result follows. 2
350
Proof of Proposition 8 The proofs of 1 and 2 are straightforward. To show 3, note that A =
351
ZCâˆ’1
ZT
AÌ‚ZCâˆ’1
ZT
= YZC1/2
BC1/2
Y T
Z = YZUÎ›UT
Y T
Z = Y Î›Y T
. The definition of B
352
above shows that this is the Maximum Likelihood estimator of B given the clustering C.
353
â‡” Bkl =
#edges from cluster k to cluster l
nknl
(23)
Proof of Theorem 9 We now follow the steps outlined in section 3 with Îµ0
from Proposition 14 to
354
obtain our main stability result.
355
Proof of Proposition 10 In the Proof of Proposition 7, we replace the bounds corresponding to
356
< MÌ‚, M >F , ||MÌ‚ âˆ’ M||F by the actual values computed from M, MÌ‚. We obtain
357
< M, M0
>F â‰¥< MÌ‚, M >F âˆ’(K âˆ’ 1)(Îµ0
)2
âˆ’ 2
p
2(K âˆ’ 1)Îµ0
||MÌ‚ âˆ’ M||F . (24)
Proof of Proposition 3
358
From the Proof of this theorem, we have that ||Lâˆ—
âˆ’ LÌ‚|| = o(1), ||(Dâˆ—
)1/2
âˆ’ DÌ‚1/2
|| = o(1),
359
||Î»âˆ—
âˆ’ Î›Ì‚|| = o(1), and ||YÌ‚ âˆ’ Y âˆ—
|| = o(1). Let Z be the indicator matrix of Câˆ—
. The principal
360
eigenvectors of Lâˆ—
are Y âˆ—
= (Dâˆ—
)1/2
Z(Câˆ—
)âˆ’1/2
. It follows then that ||ZT
DÌ‚Z âˆ’ ZT
Dâˆ—
Z|| =
361
o(1), and since C = ZT
DÌ‚Z, YZ = DÌ‚1/2
ZCâˆ’1/2
we have that ||YZ âˆ’ Y âˆ—
|| = o(1), ||Fâˆ—
âˆ’
362
F|| = o(1) where Fâˆ—
= Y T
Y âˆ—
. Moreover, since ||YÌ‚ âˆ’ Y âˆ—
|| = o(1), ||F âˆ’ I|| = o(1) Hence
363
||UV T
âˆ’ I|| = o(1). Since the choice of B depends only on R(YZ), it follows immediately that
364
||BBT
LÌ‚BT
B âˆ’ Bâˆ—
(Bâˆ—
)T
Lâˆ—
(Bâˆ—
)T
Bâˆ—
|| = o(1). Now, L = YZUV T
Î›Ì‚V UT
Y T
Z + BBT
LÌ‚BT
B,
365
and Lâˆ—
= Y âˆ—
Î›âˆ—
(Y âˆ—
)T
+ Bâˆ—
(Bâˆ—
)T
Lâˆ—
(Bâˆ—
)T
Bâˆ—
, which completes the proof. 2
366
perturbation of the PFM model To obtain a noisy PFM model A, we calculate the first K piecewise
367
constant [15] eigenvectors V of the transition matrix P = Dâˆ’1
A, from which we obtain V âˆ—
by
368
perturbing each entry in V with a noise  âˆ¼ unif(0, 10âˆ’4
). The perturbed similarity matrix A is
369
then obtained as A = D1/2
(D1/2
V âˆ—
Î›Ì‚V âˆ—T
D1/2
+ YÌ‚lowÎ›Ì‚lowYÌ‚ T
low)D1/2
. An adjacency matrix AÌ‚ is
370
generated from A. In figure 2, we show the perturbed graphs A and AÌ‚.
371
A AÌ‚
Figure 2: Left: the visualization of the perturbed A. Right: the visualization of the perturbed AÌ‚
11
