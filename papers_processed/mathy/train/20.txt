JournalofMachineLearningResearch11(2010)2057-2078Submitted6/09;Revised4/10;Published7/10MatrixCompletionfromNoisyEntriesRaghunandanH.KeshavanRAGHURAM@STANFORD.EDUAndreaMontanari∗MONTANARI@STANFORD.EDUSewoongOhSWOH@STANFORDKESHAVAN,MONTANARIANDOHTheaboveresultscruciallyrelyontheassumptionthatMisexactlyarankrmatrix.Formanyapplicationsofinterest,thisassumptionisunrealisticanditisthereforeimportanttoinvestigatetheirrobustness.Cantheaboveapproachesbegeneralizedwhentheunderlyingdatais‘wellap-proximated’byarankrmatrix?ThisquestionwasaddressedbyCandèsandPlan(2009)withintheconvexrelaxationapproachofCandèsandRecht(2008).ThepresentpaperprovesasimilarMATRIXCOMPLETIONFROMNOISYENTRIESOPTSPACE(matrixNE)1:TrimNE,andleteNEbetheoutput;2:Computetherank-rprojectionofeNEKESHAVAN,MONTANARIANDOH1.3SomeNotationsThematrixMtobereconstructedtakestheform(1)whereU∈Rm×r,V∈Rn×r.WewriteU=[u1,uMATRIXCOMPLETIONFROMNOISYENTRIESwithprobabilitylargerthan1−1/n3.Projectionontorank-rmatricesthroughSVDisaprettystandardtool,andisusedasfirstanalysismethodformanypracticalproblems.Atahigh-level,projectionontorank-rmatricescanbein-terpretedas‘treatmissingentriesaszeros’.Thistheoremshowsthatthisapproachisreasonablyrobustifthenumberofobservedentriesisaslargeasthenumberofdegreesoffreedom(whichisabout(m+n)r)timesalargeconstant.Theerrorboundisthesumoftwocontributions:thefirstonecanbeinterpretedasanundersamplingeffect(errorinducedbymissingentries)andthesecondasanoiseeffect.Letusstressthattrimmingiscrucialforachievingthisguarantee.1.4.2OPTSPACETheorem1.1helpstosetthestageforthekeypointofthispaper:amuchbetterapproximationisobtainedbyminimizingthecosteF(X,Y)(step3inthepseudocodeabove),providedMsatisfiesanappropriateincoherenceKESHAVAN,MONTANARIANDOHWorstcasemodel.InthismodelZisarbitrary,butwehaveanuniformboundonthesizeofitsentries:|Zij|≤Zmax.ThebasicparameterenteringourmainresultsistheoperatornormofeZE,whichisboundedasfollowsinthesetwonoisemodels.Theorem1.3IfZisarandommatrixdrawnaccordingtotheindependententriesmodel,thenforanysamplesize|E|thereisaconstantCsuchthat,keZEk2≤Cσ|E|lognn1/2,(4)withprobabilityatleast1−1/n3.FurtherthereexistsaconstantC′suchthat,ifthesamplesizeis|E|MATRIXCOMPLETIONFROMNOISYENTRIES00.20.40.60.810100200300400500600ConvexRelaxationLowerBoundrank-rprojectionOptSpace:1iteration2iterations3iterations10iterations|E|/nRMSEKESHAVAN,MONTANARIANDOH0.00010.0010.010.1105101520253035404550|E|/n=80,FiterrorRMSELowerBound|E|/n=160,FiterrorRMSELowerBoundIterationsErrorFigure3:Numericalsimulationwithrandomrank-2600×600matricesandnumberofobservedentries|E|/n=80and160.Thestandarddeviationofthei.i.d.Gaussiannoiseis0.001.FiterrorandrootmeansquareerrorachievedbyOPTSPACEareshownasfunctionsofthenumberoflineminimizations.Informationtheoreticlowerboundsarealsoshown.2),fromwhichwetookthedatapointsfortheconvexrelaxationapproach,aswellastheinforma-tiontheoreticlowerbounddescribedlaterinthissection.Afterafewiterations,OPTMATRIXCOMPLETIONFROMNOISYENTRIES(comparedtoourA1).Thereforetheassumptionsarenotdirectlycomparable.Asfarastheerrorboundisconcerned,CandèsandPlan(2009)provedthatthesemidefiniteprogrammingapproachreturnsanestimatebMwhichsatisfies1√mnkbMSDP−MkF≤7rn|E|kZEkF+2n√αkZEkF.(6)(Theconstantinfrontofthefirsttermisinfactslightlysmallerthan7inCandèsandPlan(2009),butinanycaselargerthan4√2.Wechoosetoquotearesultwhichisslightlylessaccuratebuteasiertoparse.)Theorem1.2improvesoverthisresultinseveralrespects:(1)Wedonothavethesecondtermontheright-handsideof(6),thatactuallyincreaseswiththenumberofobservedentries;(2)Ourerrordecreasesasn/|E|ratherthan(n/|E|)1/2;(3)ThenoiseentersTheorem1.2throughtheoperatornormkZEk2insteadofitsFrobeniusnormkZEkF≥kZEk2.ForEuniformlyrandom,oneexpectskZEkFtoberoughlyoforderkZEk2√n.Forinstance,withintheindependententriesmodelwithboundedvarianceσ,kZEkF=Θ(p|E|)whilekZEk2isoforderp|E|/n(uptologarithmicterms).Theorem1.2canalsobecomparedtoaninformationtheoreticlowerboundcomputedbyCandèsandPlan(2009).Suppose,forsimplicity,m=nandKESHAVAN,MONTANARIANDOH1.7RelatedWorkonGradientDescentLocaloptimizationtechniquessuchasgradientdescentofcoordinatedescenthavebeenintensivelystudiedinmachinelearning,withanumberofapplications.HerewewillbrieflyMATRIXCOMPLETIONFROMNOISYENTRIES1.8OntheSpectrumofSparseMatricesandtheRoleofTrimmingThetrimmingstepoftheOPTSPACEalgorithmissomewhatcounter-intuitiveinthatweseemtobewastinginformation.Inthissectionwewanttoclarifyitsrolethroughasimpleexample.Beforedescribingtheexample,letusstressonceagaintwofacts:(i)Inthelaststepofourthealgorithm,thetrimmedentriesareactuallyincorporatedinthecostfunctionandhencethefullinformationisexploited;(ii)Trimmingisnottheonlywaytotreatover-representedrows/columnsinME,andprobablynottheoptimalone.Onemightforinstancerescaletheentriesofsuchrows/columns.Westicktotrimmingbecausewecanproveitactuallyworks.Letusnowturntotheexample.Assume,forthesakeofsimplicity,thatm=n,thereisnonoiseintherevealedentries,andMistherankonematrixwithMij=1foralliandj.WithinKESHAVAN,MONTANARIANDOHProofForanymatrixA,letσq(A)denotetheqthsingularvalueofA.Then,σq(A+B)≤σq(A)+σ1(B),whenceσMATRIXCOMPLETIONFROMNOISYENTRIES3.1PreliminaryRemarksandDefinitionsGivenx1=(X1,Y1)andx2=(X2,Y2)∈M(m,n),twopointsonthismanifold,theirdistanceisdefinedasd(x1,x2)=pd(X1,X2)2+d(Y1,Y2)2,where,letting(cosθ1,...,cosθr)bethesingularvaluesofXT1X2/m,d(X1,X2)=kθk2.Thenextremarkboundsthedistancebetweentwopointsonthemanifold.Inparticular,wewillusethistoboundthedistancebetweentheoriginalmatrixM=UΣVKESHAVAN,MONTANARIANDOHLemma5ThereexistnumericalconstantsC0,C1,C2suchthatthefollowinghappens.Assumeε≥C0µ0r√α(Σmax/Σmin)2max{logn;µ0r√α(Σmax/Σmin)4}andδ≤Σmin/(C0Σmax).Then,kgradeF(x)k2≥C1nε2Σ4mind(x,u)−C2√rΣMATRIXCOMPLETIONFROMNOISYENTRIES1.xk∈K(4µ0)forallk.Firstwenoticethatwecanassumex0∈K(3µ0).Indeed,ifthisdoesnothold,KESHAVAN,MONTANARIANDOH3.3ProofofLemma4andCorollary3.1Proof(Lemma4)Theproofisbasedontheanalogousboundinthenoiselesscase,thatis,Lemma5.3inKeshavanetal.(2010).Forreaders’convenience,theresultisreportedinAppendixA,Lemma7.Fortheproofoftheselemmas,werefertoKeshavanetal.(2010).Inordertoprovethelowerbound,westartbynoticingthatF(u)≤12kPE(Z)k2MATRIXCOMPLETIONFROMNOISYENTRIESThesingularvaluebounds(10)and(11)followbytriangularinequality.Forinstanceσmin(S)≥Σmin−CΣmaxd(x,u)−C√rεkZEk2KESHAVAN,MONTANARIANDOHSinceXTX=mI,wegetkXSbQTk2F=mTr(STSbQTbQ)≤nασmax(S)2kbQk2F≤Cn2αΣmax+√rεkZEkF2d(x,u)2(24)≤4Cn2αΣ2maxd(x,u)2,where,ininequality(24),weusedCorollary3.1andinthelaststep,MATRIXCOMPLETIONFROMNOISYENTRIESToboundtheprobabilityoflargedeviation,weusetheresultonconcentrationinequalityforLipschitzfunctionsoni.i.d.sub-GaussianrandomvariablesduetoTalagrand(1996).Fora1-LipschitzfunctionKESHAVAN,MONTANARIANDOHwherethefirstinequalityfollowsfromthedefinitionofZkjasazeromeanrandomvariablewithsub-Gaussiantail,andthesecondinequalityfollowsfromlog(1+x)≤x.ByapplyingChernoffbound,Eq.(28)follows.NotethatananalogousresultholdsfortheEuclideannormontherowskZEi•k2.SubstitutingEq.(28)andPmaxjkZE•jk2≥z≤mPkZE•jk2≥zinEq.(27),wegetEmaxjkZE•jk2≤βσ2ε√α+8σ2m3e−38(β−3)ε√α.(29)Thesecondtermcanbemadearbitrarilysmallbytakingβ=ClognwithlargeenoughC.SinceEmaxjkZE•jk≤qEmaxjkZE•jk2,applyingEq.(29)withβ=ClogninEq.(26)givesEkZEk2≤Cσqε√αlogn.TogetherwithEq.(25),thisprovesthedesiredthesisforanysamplesize|E|.Inthecasewhen|E|≥nMATRIXCOMPLETIONFROMNOISYENTRIESAppendixA.ThreeLemmasontheNoiselessProblemLemma7ThereexistsnumericalconstantsC0,C1,C2suchthatthefollowinghappens.Assumeε≥C0µKESHAVAN,MONTANARIANDOHR.H.KeshavanandS.Oh.Optspace:Agradientdescentalgorithmonthegrassmanmanifoldformatrixcompletion.arXiv:0910.5260,2009.R.H.Keshavan,A.Montanari,andS.Oh.Matrixcompletionfromafewentries.IEEETrans.Inform.Theory,56(6):2980–2998,June2010.K.LeeandY.Bresler.Admira:Atomicdecompositionforminimumrankapproximation.arXiv:0905.0044,2009.S.Ma,D.Goldfarb,andL.Chen.FixedpointandBregmaniterativemethodsformatrixrankminimization.arXiv:0905.1643,2009.B.Recht,M.Fazel,