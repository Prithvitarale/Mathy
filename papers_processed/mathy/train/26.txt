741Reduced-RankHiddenMarkovModelsSajidM.SiddiqiByronBootsGeoffreyJ.GordonRoboticsInstituteCarnegieMellonUniversityPittsburgh,PA15213siddiqi@google.comMachineLearningDepartmentCarnegieMellonUniversityPittsburgh,PA15213beb@cs.cmu.eduMachineLearningDepartmentCarnegieMellonUniversityPittsburgh,PA15213ggordon@cs.cmu.eduAbstractHsuetal.(2009)recentlyproposedanef-ficient,accuratespectrallearningalgorithmforHiddenMarkovModels(HMMs).Inthispaperwerelaxtheirassumptionsandproveatighterfinite-sampleerrorboundforthecaseofReduced-RankHMMs,i.e.,HMMswithlow-ranktransition742Reduced-RankHiddenMarkovModelsequivalent,wewillrefertobothasPSRsbelow.)Recently,Hsu,KakadeandZhang(HKZforshort)proposedaspectralalgorithmwhichlearnsobservablerepresentationsofHMMs(Hsuetal.,2009).TheHKZalgorithmisfreeoflocaloptimaandstatisticallycon-sistent,withafinite-sampleboundonL1errorinjointprobabilityestimates.However,learninglarge-state-spaceHMMsisstilldifficult:thenumberofparame-tersgrowsprohibitivelywiththesizeofthestatespace.Inthispaper,we743SajidM.Siddiqi,ByronBoots,GeoffreyJ.GordonA.B.RkRmRnRSOsimplexsimplexxtltht...xththt+1xt+1h744Reduced-RankHiddenMarkovModelsstate(eqn.6(b)),andcomputeconditionalprobabili-ties(eqn.6(c))(proofsinSiddiqietal.(2009)):cPr[x1,...,xt]=bbT∞bBxt...bBx1bb1(6a)bbt+1=bBxtbbtbbT∞bBxtbbt(6b)cPr[xt|x1:t−1]=bbT∞bBxtbbtPxbbT∞bBxbbt(6c)3LearningReduced-RankHMMsWeestimatetheparametersoftheRR-HMMobserv-ablerepresentationfromdatausingSingularValueDe-composition(SVD)(Golub&VanLoan,1996).Thebasicalgorithmforestimatingrank-kRR-HMMsisequivalenttothespectrallearningalgorithmofHKZforlearningk-stateHMMs.However,ourrelaxationoftheirconditions(e.g.,HKZassumeafull-ranktran-sitionmatrix,withoutwhichtheirboundsarevacu-ous)leadstofinite-sampleperformanceguaranteesforrank-kRR-HMMs.Inaddition,weprovide(andana-lyze)generalizationstot-stepobservableRR-HMMs,toRR-HMMswithcontinuousobservations,toindica-tiveandcharacteristicfeatures(ratherthanevents),andtogeneralPSRs.So,ournewresultsallowustolearnamuchlargerclassofmodels.3.1TheAlgorithmThealgorithmtakesasinputthedesiredrankkratherthanthenumberofstatesm.Alternatively,givenathreshold,thealgorithmcanchoosetherankoftheHMMbyexaminingthesingularvaluesofbP2,1(whoserankiskintheabsenceofnoise)inStep2.ItassumesthatwearegivenNindependentlysampledobservationtripleshx1,x2,x3ifromtheHMM.Inpractice,wecanuseasinglelongsequenceofobserva-tionsaslongaswediscounttheboundonthenumberofsamplesbasedonthemixingrateoftheHMM(i.e.(1−secondeigenvalueofT)),inwhichcaseπmustcorrespondtothestationarydistributionoftheHMMtoallowestimationof~P1.ThealgorithmresultsinanestimatedobservablerepresentationoftheRR-HMM:Algorithm:Learn-RR-HMM(k,N)1.ComputeempiricalestimatesbP1,bP2,1,bP3,x,1of~P1,P2,1,P3,x,1(forx=1,...,n).2.UseSVDonbP2,1tocomputebU,thematrixofleftsingularvectorscorrespondingtotheklargestsingularvalues.3.Computemodelparameterestimates:(a)bb1=bUTbP1,(b)bb∞=(bPT2,1bU)+bP1,(c)bBx=bUTbP3,x,1(bUTbP2,1)+(x=1,...,n)EstimatedRR-HMMparameterscan,intheory,leadtonegativeprobabilityestimates,whichisanintrin-sicaspectoflinearPSRs(Wiewiora,2007).ThesearemostharmfulwhentheycausethenormalizersbbT∞bBxtbbtorPxbbT∞bBxbbttobenegative.However,inourexperiments,thelatterwasnevernegativeandtheformerwasveryrarelynegative;and,usingreal-valuedobservations(Section3.5)makesnegativenormalizersevenlesslikely,sinceinthiscasethenormalizerisaweightedsumofseveralestimatedprobabilities.Inpracticewerecommendthresholdingthenormalizerswithasmallpositivenumber,andnottrustingprob-abilityestimatesforafewstepsifthenormalizersfallbelowthethreshold.3.2TheoreticalGuaranteesTheorem2boundstheL1error745SajidM.Siddiqi,ByronBoots,GeoffreyJ.GordonSomeoftheassumptionsabovearesimilartocondi-tionsinHKZ.Others(startingwith“inaddition”)areuniquetothelow-ranksetting.Theconditiononrank(Sdiag(~π)OT746Reduced-RankHiddenMarkovModels3.4LearningwithSequencesandFeaturesTheprobabilitymatrixP2,1actsasacorrelationma-trixrelatingonepasttimesteptoonefuturetimestep.Itisusefulundertheassumptionthatthevectorof747SajidM.Siddiqi,ByronBoots,GeoffreyJ.Gordon123400.20.40.60.8112300.20.40.60.8112300.20.40.60.81eigenvaluesA.B.C.RR-HMM2-step-Obs.HMM2-step-Obs.RR-HMMTrue10000100000Figure2:LearningdiscreteRR-HMMs.Thethreefig-uresdepicttheactualeigenvaluesofthreedifferentRR-HMMtransitionmatrices,andtheestimatedeigenvalueswith95%errorbars,fortwodifferenttrainingsetsizes.−1C.RR-HMMB.StableLDSA.HMMFigure3:StatespacemanifoldandvideoframessimulatedbyaHMM,astableLDS,andaRR-HMMlearnedusingclockpendulumvideo(manifoldscalesarearbitrary).(A)10-stateHMM.(B)10-dimLDS.(C)Rank10RR-HMM.matrixTofthetruemodel.BisasimilaritytransformofSR,whichhasthesamenon-zeroeigenvaluesasT=RS,sotheestimatedeigenvaluesshouldconvergetothetrueeigenvalueswithenoughdata.SeeSiddiqietal.(2009)fortheHMMparameters.Example1:AnRR-HMM[m=3hiddenstates,n=3observations,k=2rank].InthisexampletheRR-HMMislow-rank.SeeFigure2(A).Example2:A2-step-ObservableHMM[m=3hid-denstates,n=2observations].Inthisexample,theHMMviolatesthem≤nconditionofHKZ.Thepa-rametersofthisHMMcannotbeestimatedwiththeoriginallearningalgorithm,sinceasingleobservationdoesnotprovideenoughinformationtodisambiguatestate.However,byconsidering2consecutiveobserva-tions(seeSection3.4),thespectrallearningalgorithmcanbeappliedsuccessfully.SeeFigure2(B).Example3:A2-step-ObservableRR-HMM[m=4hiddenstates,n=2observations,k=3rank].Inthisexample,theHMMislowrank,anditviolatesthem≤nconditionofHKZ.SeeFigure2(C).4.2CompetitiveInhibition+SmoothDynamicsWemodelaclockpendulumvideoconsistingof55frames(withaperiodof∼22frames)asa10-stateHMM,a10-dimensionalLDS,andarank10RR-HMM.Notethatwecouldeasilylearnmodelswithmorethan10latentstates/dimensions;welimitedthedimensionalityinordertodemonstratetherelativeex-pressivepowerofthedifferentmodels.FortheHMM,weconvertthecontinuousdatatodiscreteobserva-tionsby1-nearestneighboron25kernelcenterssam-pledsequentiallyfromthetrainingdata.WetrainedtheresultingdiscreteHMMusingEM.WelearnedA.ExampleImagesEnvironmentPathB.01020304050607080901003.54.55.56.57.58.5x106PredictionHorizonAvg.PredictionErr.RR-HMMLDSHMMMeanLastFigure4:(A)Sampleimagesfromtherobot’scamera.Thefigurebelowdepictsthehallwayenvironmentwithacentralobstacle(black)andthepaththattherobottookthroughtheenvironmentwhilecollectingdata(theredcounter-clockwiseellipse)(B)Squarederrorforpre-diction(1,...,100stepsoutinfuture)withdifferentesti-matedmodelsandbaselines,averagedoverdifferentinitialfilteringdurations(1,...,250).theLDSdirectlyfromthevideousingsubspaceIDwithstabilityconstraints(Siddiqietal.,2007)usingaHankelmatrixof10observations.WetrainedtheRR-HMMbyconsideringsequencesof4continuousmul-tivariateobservations,choosinganapproximaterankof10dimensions,andlearning25observableoperatorscorrespondingto25Gaussiankernelcenters.Wesim-ulateaseriesof500observationsfromthemodelandcomparethemanifoldsinthe10-dimensionalspaceandtheobservationsandframesfromthesimulatedvideos(Figure3).ThesmallnumberofstatesintheHMMisnotsufficienttocapturethesmoothevolu-tionoftheclock:thesimulatedvideoischaracterizedbyrealisticlookingframes,butexhibitsjerkyirregu-larmotion.FortheLDS,althoughthe10-dimensionalsubspacecapturessmoothevolutionofthesimulatedvideo,thesystemquicklydegeneratesandindividualframesofvideoaremodeledpoorly(resultinginsu-perpositionsofpendulumsingeneratedframes).FortheRR-HMM,thesimulatedvideobenefitsfrombothsmoothstateevolutionandcompetitiveinhibition.Thelow-dimensionalmanifoldissmoothandstruc-turedandthevideoisrealistic.Theresultsdemon-stratethattheRR-HMMhasthebenefitsofsmoothstateevolutionandcompactstatespaceofaLDSandthebenefitofcompetitiveinhibitionofaHMM.4.3Filtering,Prediction,andSimulationWecompareHMMs,LDSs,andRR-HMMsontheproblemofmodelingvideodatafromamobilerobotinanindoorenvironment.Avideoof2000frameswascollectedat6HzfromaPointGreyBumblebee2stereocameramountedonaBotricsObotd100mo-bilerobotplatformcirclingastationaryobstacle(Fig-ure4(A))and1500frameswereusedastrainingdataforeachmodel.Eachframefromthetrainingdatawasreducedto100dimensionsviaSVDonsingleob-servations.Usingthistrainingdata,wetrainedanRR-HMM(k=50,n=1500)usingspectrallearn-ingwithsequencesof20continuous748Reduced-RankHiddenMarkovModelsKDEwithGaussiankernelswith1500centers;a50-dimensionalLDSusingSubspaceIDwithHankelma-tricesof20timesteps;anda50-stateHMMwith1500discreteobservationsusingEMrununtilconvergence.Foreachmodel,weperformedfilteringfordifferentextentst1=100,101,...,250,thenpredictedanim-agewhichwasafurthert2stepsinthefuture,fort2=1,2...,100.Thesquarederrorofthispredic-tioninpixelspacewasrecorded,andaveragedoverallthedifferentfilteringextentst1toobtainmeanswhichareplottedinFigure4(B).Asbaselines,weplottheerrorobtainedbyusingthemeanoffiltereddataasapredictor(‘Mean’),andtheerrorobtainedbyusingthelastfilteredobservation(‘Last’).Bothbaselinesperformworsethananyofthemorecomplexalgorithms(thoughasexpected,the‘Last’predictorisagoodone-steppredictor),indicatingthatthisisanontrivialpredictionproblem.TheLDSdoeswellinitially(duetosmoothness),andtheHMMdoeswellinthelongerrun(duetocompetitiveinhibition),whiletheRR-HMMperformsaswellorbetteratbothtimescalessinceitmodelsboththesmoothstateevo-lutionandcompetitiveinhibitioninitspredictivedis-tribution.Inparticular,theRR-HMMyieldslowerpredictionerrorconsistentlyforthedurationofthepredictionhorizon(100steps,or1623seconds).5ConclusionWehavegeneralizedthespectrallearningalgorithmandboundsofHsuetal.(2009)toaccuratelylearnalargerclassofsequentialdatamodels(RR-HMMs)underalargerclassofobservationmodels(non-1-step-observabledomainsandmultivariatecontinuousob-servations).RR-HMMscombinedesirablepropertiesofHMMsandLDSs,allowingthemtomodelalargerclassofdynamicalsystems.Wehavealsoshownthatthealgorithmisconsistent