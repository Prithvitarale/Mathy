Journal of Machine Learning Research 15 (2014) 2773-2832 Submitted 2/13; Revised 3/14; Published 8/14
Tensor Decompositions for Learning Latent Variable Models
Animashree Anandkumar a.anandkumar@uci.edu
Electrical Engineering and Computer Science
University of California, Irvine
2200 Engineering Hall
Irvine, CA 92697
Rong Ge rongge@microsoft.com
Microsoft Research
One Memorial Drive
Cambridge, MA 02142
Daniel Hsu djhsu@cs.columbia.edu
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, #0401
New York, NY 10027
Sham M. Kakade skakade@microsoft.com
Microsoft Research
One Memorial Drive
Cambridge, MA 02142
Matus Telgarsky mtelgars@cs.ucsd.edu
Department of Statistics
Rutgers University
110 Frelinghuysen Road
Piscataway, NJ 08854
Editor: Benjamin Recht
Abstract
This work considers a computationally and statistically efficient parameter estimation
method for a wide class of latent variable modelsâ€”including Gaussian mixture models,
hidden Markov models, and latent Dirichlet allocationâ€”which exploits a certain tensor
structure in their low-order observable moments (typically, of second- and third-order).
Specifically, parameter estimation is reduced to the problem of extracting a certain (orthog-
onal) decomposition of a symmetric tensor derived from the moments; this decomposition
can be viewed as a natural generalization of the singular value decomposition for matrices.
Although tensor decompositions are generally intractable to compute, the decomposition
of these specially structured tensors can be efficiently obtained by a variety of approaches,
including power iterations and maximization approaches (similar to the case of matrices).
A detailed analysis of a robust tensor power method is provided, establishing an analogue
of Wedinâ€™s perturbation theorem for the singular vectors of matrices. This implies a ro-
bust and computationally tractable estimation approach for several popular latent variable
models.
c 2014 Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky.
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Keywords: latent variable models, tensor decompositions, mixture models, topic models,
method of moments, power method
1. Introduction
The method of moments is a classical parameter estimation technique (Pearson, 1894) from
statistics which has proved invaluable in a number of application domains. The basic
paradigm is simple and intuitive: (i) compute certain statistics of the dataâ€”often empirical
moments such as means and correlationsâ€”and (ii) find model parameters that give rise to
(nearly) the same corresponding population quantities. In a number of cases, the method of
moments leads to consistent estimators which can be efficiently computed; this is especially
relevant in the context of latent variable models, where standard maximum likelihood ap-
proaches are typically computationally prohibitive, and heuristic methods can be unreliable
and difficult to validate with high-dimensional data. Furthermore, the method of moments
can be viewed as complementary to the maximum likelihood approach; simply taking a
single step of Newton-Raphson on the likelihood function starting from the moment based
estimator (Le Cam, 1986) often leads to the best of both worlds: a computationally efficient
estimator that is (asymptotically) statistically optimal.
The primary difficulty in learning latent variable models is that the latent (hidden)
state of the data is not directly observed; rather only observed variables correlated with
the hidden state are observed. As such, it is not evident the method of moments should
fare any better than maximum likelihood in terms of computational performance: match-
ing the model parameters to the observed moments may involve solving computationally
intractable systems of multivariate polynomial equations. Fortunately, for many classes of
latent variable models, there is rich structure in low-order moments (typically second- and
third-order) which allow for this inverse moment problem to be solved efficiently (Cattell,
1944; Cardoso, 1991; Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar
et al., 2012c,a; Hsu and Kakade, 2013). What is more is that these decomposition problems
are often amenable to simple and efficient iterative methods, such as gradient descent and
the power iteration method.
1.1 Contributions
In this work, we observe that a number of important and well-studied latent variable
modelsâ€”including Gaussian mixture models, hidden Markov models, and Latent Dirichlet
allocationâ€”share a certain structure in their low-order moments, and this permits certain
tensor decomposition approaches to parameter estimation. In particular, this decompo-
sition can be viewed as a natural generalization of the singular value decomposition for
matrices.
While much of this (or similar) structure was implicit in several previous works (Chang,
1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c,a; Hsu and
Kakade, 2013), here we make the decomposition explicit under a unified framework. Specif-
ically, we express the observable moments as sums of rank-one terms, and reduce the pa-
rameter estimation task to the problem of extracting a symmetric orthogonal decomposition
of a symmetric tensor derived from these observable moments. The problem can then be
solved by a variety of approaches, including fixed-point and variational methods.
2774
Tensor Decompositions for Learning Latent Variable Models
One approach for obtaining the orthogonal decomposition is the tensor power method
of Lathauwer et al. (2000, Remark 3). We provide a convergence analysis of this method for
orthogonally decomposable symmetric tensors, as well as a detailed perturbation analysis
for a robust (and a computationally tractable) variant (Theorem 5.1). This perturbation
analysis can be viewed as an analogue of Wedinâ€™s perturbation theorem for singular vectors
of matrices (Wedin, 1972), providing a bound on the error of the recovered decomposition
in terms of the operator norm of the tensor perturbation. This analysis is subtle in at least
two ways. First, unlike for matrices (where every matrix has a singular value decomposi-
tion), an orthogonal decomposition need not exist for the perturbed tensor. Our robust
variant uses random restarts and deflation to extract an approximate decomposition in a
computationally tractable manner. Second, the analysis of the deflation steps is non-trivial;
a naÄ±Ìˆve argument would entail error accumulation in each deflation step, which we show can
in fact be avoided. When this method is applied for parameter estimation in latent variable
models previously discussed, improved sample complexity bounds (over previous work) can
be obtained using this perturbation analysis.
Finally, we also address computational issues that arise when applying the tensor de-
composition approaches to estimating latent variable models. Specifically, we show that the
basic operations of simple iterative approaches (such as the tensor power method) can be
efficiently executed in time linear in the dimension of the observations and the size of the
training data. For instance, in a topic modeling application, the proposed methods require
time linear in the number of words in the vocabulary and in the number of non-zero entries
of the term-document matrix. The combination of this computational efficiency and the
robustness of the tensor decomposition techniques makes the overall framework a promising
approach to parameter estimation for latent variable models.
1.2 Related Work
The connection between tensor decompositions and latent variable models has a long history
across many scientific and mathematical disciplines. We review some of the key works that
are most closely related to ours.
1.2.1 Tensor Decompositions
The role of tensor decompositions in the context of latent variable models dates back to early
uses in psychometrics (Cattell, 1944). These ideas later gained popularity in chemometrics,
and more recently in numerous science and engineering disciplines, including neuroscience,
phylogenetics, signal processing, data mining, and computer vision. A thorough survey of
these techniques and applications is given by Kolda and Bader (2009). Below, we discuss a
few specific connections to two applications in machine learning and statistics, independent
component analysis and latent variable models (between which there is also significant
overlap).
Tensor decompositions have been used in signal processing and computational neuro-
science for blind source separation and independent component analysis (ICA) (Comon and
Jutten, 2010). Here, statistically independent non-Gaussian sources are linearly mixed in
the observed signal, and the goal is to recover the mixing matrix (and ultimately, the orig-
inal source signals). A typical solution is to locate projections of the observed signals that
2775
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
correspond to local extrema of the so-called â€œcontrast functionsâ€ which distinguish Gaussian
variables from non-Gaussian variables. This method can be effectively implemented using
fast descent algorithms (Hyvarinen, 1999). When using the excess kurtosis (i.e., fourth-order
cumulant) as the contrast function, this method reduces to a generalization of the power
method for symmetric tensors (Lathauwer et al., 2000; Zhang and Golub, 2001; Kofidis and
Regalia, 2002). This case is particularly important, since all local extrema of the kurtosis
objective correspond to the true sources (under the assumed statistical model) (Delfosse
and Loubaton, 1995); the descent methods can therefore be rigorously analyzed, and their
computational and statistical complexity can be bounded (Frieze et al., 1996; Nguyen and
Regev, 2009; Arora et al., 2012b).
Higher-order tensor decompositions have also been used to develop estimators for com-
monly used mixture models, hidden Markov models, and other related latent variable mod-
els, often using the the algebraic procedure of R. Jennrich (as reported in the article of
Harshman, 1970), which is based on a simultaneous diagonalization of different ways of
flattening a tensor to matrices. Jennrichâ€™s procedure was employed for parameter estima-
tion of discrete Markov models by Chang (1996) via pair-wise and triple-wise probability
tables; and it was later used for other latent variable models such as hidden Markov models
(HMMs), latent trees, Gaussian mixture models, and topic models such as latent Dirichlet
allocation (LDA) by many others (Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar
et al., 2012c,a; Hsu and Kakade, 2013). In these contexts, it is often also possible to es-
tablish strong identifiability results, without giving an explicit estimators, by invoking the
non-constructive identifiability argument of Kruskal (1977)â€”see the article by Allman et al.
(2009) for several examples.
Related simultaneous diagonalization approaches have also been used for blind source
separation and ICA (as discussed above), and a number of efficient algorithms have been
developed for this problem (Bunse-Gerstner et al., 1993; Cardoso and Souloumiac, 1993;
Cardoso, 1994; Cardoso and Comon, 1996; Corless et al., 1997; Ziehe et al., 2004). A rather
different technique that uses tensor flattening and matrix eigenvalue decomposition has
been developed by Cardoso (1991) and later by De Lathauwer et al. (2007). A significant
advantage of this technique is that it can be used to estimate overcomplete mixtures, where
the number of sources is larger than the observed dimension.
The relevance of tensor analysis to latent variable modeling has been long recognized in
the field of algebraic statistics (Pachter and Sturmfels, 2005), and many works characterize
the algebraic varieties corresponding to the moments of various classes of latent variable
models (Drton et al., 2007; Sturmfels and Zwiernik, 2013). These works typically do not
address computational or finite sample issues, but rather are concerned with basic questions
of identifiability.
The specific tensor structure considered in the present work is the symmetric orthogo-
nal decomposition. This decomposition expresses a tensor as a linear combination of simple
tensor forms; each form is the tensor product of a vector (i.e., a rank-1 tensor), and the
collection of vectors form an orthonormal basis. An important property of tensors with
such decompositions is that they have eigenvectors corresponding to these basis vectors.
Although the concepts of eigenvalues and eigenvectors of tensors is generally significantly
more complicated than their matrix counterpartâ€”both algebraically (Qi, 2005; Cartwright
and Sturmfels, 2013; Lim, 2005) and computationally (Hillar and Lim, 2013; Kofidis and
2776
Tensor Decompositions for Learning Latent Variable Models
Regalia, 2002)â€”the special symmetric orthogonal structure we consider permits simple
algorithms to efficiently and stably recover the desired decomposition. In particular, a gen-
eralization of the matrix power method to symmetric tensors, introduced by Lathauwer
et al. (2000, Remark 3) and analyzed by Kofidis and Regalia (2002), provides such a de-
composition. This is in fact implied by the characterization of Zhang and Golub (2001),
which shows that iteratively obtaining the best rank-1 approximation of such orthogonally
decomposable tensors also yields the exact decomposition. We note that in general, ob-
taining such approximations for general (symmetric) tensors is NP-hard (Hillar and Lim,
2013).
1.2.2 Latent Variable Models
This work focuses on the particular application of tensor decomposition methods to estimat-
ing latent variable models, a significant departure from many previous approaches in the
machine learning and statistics literature. By far the most popular heuristic for parameter
estimation for such models is the Expectation-Maximization (EM) algorithm (Dempster
et al., 1977; Redner and Walker, 1984). Although EM has a number of merits, it may suffer
from slow convergence and poor quality local optima (Redner and Walker, 1984), requir-
ing practitioners to employ many additional heuristics to obtain good solutions. For some
models such as latent trees (Roch, 2006) and topic models (Arora et al., 2012a), maximum
likelihood estimation is NP-hard, which suggests that other estimation approaches may be
more attractive. More recently, algorithms from theoretical computer science and machine
learning have addressed computational and sample complexity issues related to estimating
certain latent variable models such as Gaussian mixture models and HMMs (Dasgupta,
1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004;
Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker
and Vempala, 2008; Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010;
Hsu and Kakade, 2013; Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar
et al., 2012c; Arora et al., 2012a; Anandkumar et al., 2012a). See the works by Anandku-
mar et al. (2012c) and Hsu and Kakade (2013) for a discussion of these methods, together
with the computational and statistical hardness barriers that they face. The present work
reviews a broad range of latent variables where a mild non-degeneracy condition implies
the symmetric orthogonal decomposition structure in the tensors of low-order observable
moments.
Notably, another class of methods, based on subspace identification (Overschee and
Moor, 1996) and observable operator models/multiplicity automata (SchuÌˆtzenberger, 1961;
Jaeger, 2000; Littman et al., 2001), have been proposed for a number of latent variable
models. These methods were successfully developed for HMMs by Hsu et al. (2012b), and
subsequently generalized and extended for a number of related sequential and tree Markov
models models (Siddiqi et al., 2010; Bailly, 2011; Boots et al., 2010; Parikh et al., 2011; Rodu
et al., 2013; Balle et al., 2012; Balle and Mohri, 2012), as well as certain classes of parse
tree models (Luque et al., 2012; Cohen et al., 2012; Dhillon et al., 2012). These methods
use low-order moments to learn an â€œoperatorâ€ representation of the distribution, which can
be used for density estimation and belief state updates. While finite sample bounds can be
given to establish the learnability of these models (Hsu et al., 2012b), the algorithms do not
2777
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
actually give parameter estimates (e.g., of the emission or transition matrices in the case of
HMMs).
1.3 Organization
The rest of the paper is organized as follows. Section 2 reviews some basic definitions of
tensors. Section 3 provides examples of a number of latent variable models which, after
appropriate manipulations of their low order moments, share a certain natural tensor struc-
ture. Section 4 reduces the problem of parameter estimation to that of extracting a certain
(symmetric orthogonal) decomposition of a tensor. We then provide a detailed analysis of
a robust tensor power method and establish an analogue of Wedinâ€™s perturbation theorem
for the singular vectors of matrices. The discussion in Section 6 addresses a number of
practical concerns that arise when dealing with moment matrices and tensors.
2. Preliminaries
We introduce some tensor notations borrowed from Lim (2005). A real p-th order tensor
A âˆˆ
Np
i=1 Rni is a member of the tensor product of Euclidean spaces Rni , i âˆˆ [p]. We
generally restrict to the case where n1 = n2 = Â· Â· Â· = np = n, and simply write A âˆˆ
Np
Rn.
For a vector v âˆˆ Rn, we use vâŠ—p := v âŠ— v âŠ— Â· Â· Â· âŠ— v âˆˆ
Np
Rn to denote its p-th tensor power.
As is the case for vectors (where p = 1) and matrices (where p = 2), we may identify a
p-th order tensor with the p-way array of real numbers [Ai1,i2,...,ip : i1, i2, . . . , ip âˆˆ [n]], where
Ai1,i2,...,ip is the (i1, i2, . . . , ip)-th coordinate of A (with respect to a canonical basis).
We can consider A to be a multilinear map in the following sense: for a set of matrices
{Vi âˆˆ RnÃ—mi : i âˆˆ [p]}, the (i1, i2, . . . , ip)-th entry in the p-way array representation of
A(V1, V2, . . . , Vp) âˆˆ Rm1Ã—m2Ã—Â·Â·Â·Ã—mp is
[A(V1, V2, . . . , Vp)]i1,i2,...,ip :=
X
j1,j2,...,jpâˆˆ[n]
Aj1,j2,...,jp [V1]j1,i1 [V2]j2,i2 Â· Â· Â· [Vp]jp,ip .
Note that if A is a matrix (p = 2), then
A(V1, V2) = V >
1 AV2.
Similarly, for a matrix A and vector v âˆˆ Rn, we can express Av as
A(I, v) = Av âˆˆ Rn
,
where I is the n Ã— n identity matrix. As a final example of this notation, observe
A(ei1 , ei2 , . . . , eip ) = Ai1,i2,...,ip ,
where {e1, e2, . . . , en} is the canonical basis for Rn.
Most tensors A âˆˆ
Np
Rn considered in this work will be symmetric (sometimes called
supersymmetric), which means that their p-way array representations are invariant to
permutations of the array indices: i.e., for all indices i1, i2, . . . , ip âˆˆ [n], Ai1,i2,...,ip =
AiÏ€(1),iÏ€(2),...,iÏ€(p)
for any permutation Ï€ on [p]. It can be checked that this reduces to the
usual definition of a symmetric matrix for p = 2.
2778
Tensor Decompositions for Learning Latent Variable Models
The rank of a p-th order tensor A âˆˆ
Np
Rn is the smallest non-negative integer k such
that A =
Pk
j=1 u1,j âŠ— u2,j âŠ— Â· Â· Â· âŠ— up,j for some ui,j âˆˆ Rn, i âˆˆ [p], j âˆˆ [k], and the symmetric
rank of a symmetric p-th order tensor A is the smallest non-negative integer k such that
A =
Pk
j=1 uâŠ—p
j for some uj âˆˆ Rn, j âˆˆ [k].1 The notion of rank readily reduces to the usual
definition of matrix rank when p = 2, as revealed by the singular value decomposition.
Similarly, for symmetric matrices, the symmetric rank is equivalent to the matrix rank as
given by the spectral theorem. A decomposition into such rank-one terms is known as a
canonical polyadic decomposition (Hitchcock, 1927a,b).
The notion of tensor (symmetric) rank is considerably more delicate than matrix (sym-
metric) rank. For instance, it is not clear a priori that the symmetric rank of a tensor
should even be finite (Comon et al., 2008). In addition, removal of the best rank-1 approx-
imation of a (general) tensor may increase the tensor rank of the residual (Stegeman and
Comon, 2010).
Throughout, we use kvk = (
P
i v2
i )1/2 to denote the Euclidean norm of a vector v, and
kMk to denote the spectral (operator) norm of a matrix. We also use kTk to denote the
operator norm of a tensor, which we define later.
3. Tensor Structure in Latent Variable Models
In this section, we give several examples of latent variable models whose low-order moments
can be written as symmetric tensors of low symmetric rank; some of these examples can be
deduced using the techniques developed in the text by McCullagh (1987). The basic form
is demonstrated in Theorem 3.1 for the first example, and the general pattern will emerge
from subsequent examples.
3.1 Exchangeable Single Topic Models
We first consider a simple bag-of-words model for documents in which the words in the
document are assumed to be exchangeable. Recall that a collection of random variables
x1, x2, . . . , x` are exchangeable if their joint probability distribution is invariant to permu-
tation of the indices. The well-known De Finettiâ€™s theorem (Austin, 2008) implies that such
exchangeable models can be viewed as mixture models in which there is a latent variable h
such that x1, x2, . . . , x` are conditionally i.i.d. given h (see Figure 1(a) for the corresponding
graphical model) and the conditional distributions are identical at all the nodes.
In our simplified topic model for documents, the latent variable h is interpreted as
the (sole) topic of a given document, and it is assumed to take only a finite number of
distinct values. Let k be the number of distinct topics in the corpus, d be the number of
distinct words in the vocabulary, and ` â‰¥ 3 be the number of words in each document. The
generative process for a document is as follows: the documentâ€™s topic is drawn according to
the discrete distribution specified by the probability vector w := (w1, w2, . . . , wk) âˆˆ âˆ†kâˆ’1.
This is modeled as a discrete random variable h such that
Pr[h = j] = wj, j âˆˆ [k].
1. For even p, the definition is slightly different (Comon et al., 2008).
2779
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Given the topic h, the documentâ€™s ` words are drawn independently according to the dis-
crete distribution specified by the probability vector Âµh âˆˆ âˆ†dâˆ’1. It will be convenient to
represent the ` words in the document by d-dimensional random vectors x1, x2, . . . , x` âˆˆ Rd.
Specifically, we set
xt = ei if and only if the t-th word in the document is i, t âˆˆ [`],
where e1, e2, . . . ed is the standard coordinate basis for Rd.
One advantage of this encoding of words is that the (cross) moments of these random
vectors correspond to joint probabilities over words. For instance, observe that
E[x1 âŠ— x2] =
X
1â‰¤i,jâ‰¤d
Pr[x1 = ei, x2 = ej] ei âŠ— ej
=
X
1â‰¤i,jâ‰¤d
Pr[1st word = i, 2nd word = j] ei âŠ— ej,
so the (i, j)-the entry of the matrix E[x1 âŠ— x2] is Pr[1st word = i, 2nd word = j]. More
generally, the (i1, i2, . . . , i`)-th entry in the tensor E[x1 âŠ— x2 âŠ— Â· Â· Â· âŠ— x`] is Pr[1st word =
i1, 2nd word = i2, . . . , `-th word = i`]. This means that estimating cross moments, say, of
x1 âŠ— x2 âŠ— x3, is the same as estimating joint probabilities of the first three words over all
documents. (Recall that we assume that each document has at least three words.)
The second advantage of the vector encoding of words is that the conditional expectation
of xt given h = j is simply Âµj, the vector of word probabilities for topic j:
E[xt|h = j] =
d
X
i=1
Pr[t-th word = i|h = j] ei =
d
X
i=1
[Âµj]i ei = Âµj, j âˆˆ [k]
(where [Âµj]i is the i-th entry in the vector Âµj). Because the words are conditionally inde-
pendent given the topic, we can use this same property with conditional cross moments,
say, of x1 and x2:
E[x1 âŠ— x2|h = j] = E[x1|h = j] âŠ— E[x2|h = j] = Âµj âŠ— Âµj, j âˆˆ [k].
This and similar calculations lead one to the following theorem.
Theorem 3.1 (Anandkumar et al., 2012c) If
M2 := E[x1 âŠ— x2]
M3 := E[x1 âŠ— x2 âŠ— x3],
then
M2 =
k
X
i=1
wi Âµi âŠ— Âµi
M3 =
k
X
i=1
wi Âµi âŠ— Âµi âŠ— Âµi.
2780
Tensor Decompositions for Learning Latent Variable Models
As we will see in Section 4.3, the structure of M2 and M3 revealed in Theorem 3.1 implies
that the topic vectors Âµ1, Âµ2, . . . , Âµk can be estimated by computing a certain symmetric
tensor decomposition. Moreover, due to exchangeability, all triples (resp., pairs) of words
in a documentâ€”and not just the first three (resp., two) wordsâ€”can be used in forming M3
(resp., M2); see Section 6.1.
3.2 Beyond Raw Moments
In the single topic model above, the raw (cross) moments of the observed words directly
yield the desired symmetric tensor structure. In some other models, the raw moments do
not explicitly have this form. Here, we show that the desired tensor structure can be found
through various manipulations of different moments.
3.2.1 Spherical Gaussian Mixtures: Common Covariance
We now consider a mixture of k Gaussian distributions with spherical covariances. We start
with the simpler case where all of the covariances are identical; this probabilistic model is
closely related to the (non-probabilistic) k-means clustering problem (MacQueen, 1967).
Let wi âˆˆ (0, 1) be the probability of choosing component i âˆˆ [k], {Âµ1, Âµ2, . . . , Âµk} âŠ‚ Rd
be the component mean vectors, and Ïƒ2I be the common covariance matrix. An observation
in this model is given by
x := Âµh + z,
where h is the discrete random variable with Pr[h = i] = wi for i âˆˆ [k] (similar to the ex-
changeable single topic model), and z âˆ¼ N(0, Ïƒ2I) is an independent multivariate Gaussian
random vector in Rd with zero mean and spherical covariance Ïƒ2I.
The Gaussian mixture model differs from the exchangeable single topic model in the way
observations are generated. In the single topic model, we observe multiple draws (words in
a particular document) x1, x2, . . . , x` given the same fixed h (the topic of the document). In
contrast, for the Gaussian mixture model, every realization of x corresponds to a different
realization of h.
Theorem 3.2 (Hsu and Kakade, 2013) Assume d â‰¥ k. The variance Ïƒ2 is the smallest
eigenvalue of the covariance matrix E[x âŠ— x] âˆ’ E[x] âŠ— E[x]. Furthermore, if
M2 := E[x âŠ— x] âˆ’ Ïƒ2
I
M3 := E[x âŠ— x âŠ— x] âˆ’ Ïƒ2
d
X
i=1
E[x] âŠ— ei âŠ— ei + ei âŠ— E[x] âŠ— ei + ei âŠ— ei âŠ— E[x]

,
then
M2 =
k
X
i=1
wi Âµi âŠ— Âµi
M3 =
k
X
i=1
wi Âµi âŠ— Âµi âŠ— Âµi.
2781
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
3.2.2 Spherical Gaussian Mixtures: Differing Covariances
The general case is where each component may have a different spherical covariance. An
observation in this model is again x = Âµh + z, but now z âˆˆ Rd is a random vector whose
conditional distribution given h = i (for some i âˆˆ [k]) is a multivariate Gaussian N(0, Ïƒ2
i I)
with zero mean and spherical covariance Ïƒ2
i I.
Theorem 3.3 (Hsu and Kakade, 2013) Assume d â‰¥ k. The average variance ÏƒÌ„2 :=
Pk
i=1 wiÏƒ2
i is the smallest eigenvalue of the covariance matrix E[x âŠ— x] âˆ’ E[x] âŠ— E[x]. Let v
be any unit norm eigenvector corresponding to the eigenvalue ÏƒÌ„2. If
M1 := E[x(v>
(x âˆ’ E[x]))2
]
M2 := E[x âŠ— x] âˆ’ ÏƒÌ„2
I
M3 := E[x âŠ— x âŠ— x] âˆ’
d
X
i=1
M1 âŠ— ei âŠ— ei + ei âŠ— M1 âŠ— ei + ei âŠ— ei âŠ— M1

,
then
M2 =
k
X
i=1
wi Âµi âŠ— Âµi
M3 =
k
X
i=1
wi Âµi âŠ— Âµi âŠ— Âµi.
As shown by Hsu and Kakade (2013), M1 =
Pk
i=1 wiÏƒ2
i Âµi. Note that for the common
covariance case, where Ïƒ2
i = Ïƒ2, we have that M1 = Ïƒ2E[x] (cf. Theorem 3.2).
3.2.3 Independent Component Analysis (ICA)
The standard model for ICA (Comon, 1994; Cardoso and Comon, 1996; HyvaÌˆrinen and
Oja, 2000; Comon and Jutten, 2010), in which independent signals are linearly mixed and
corrupted with Gaussian noise before being observed, is specified as follows. Let h âˆˆ Rk be
a latent random vector with independent coordinates, A âˆˆ RdÃ—k the mixing matrix, and z
be a multivariate Gaussian random vector. The random vectors h and z are assumed to be
independent. The observed random vector is
x := Ah + z.
Let Âµi denote the i-th column of the mixing matrix A.
Theorem 3.4 (Comon and Jutten, 2010) Define
M4 := E[x âŠ— x âŠ— x âŠ— x] âˆ’ T
where T is the fourth-order tensor with
[T]i1,i2,i3,i4 := E[xi1 xi2 ]E[xi3 xi4 ] + E[xi1 xi3 ]E[xi2 xi4 ]
+ E[xi1 xi4 ]E[xi2 xi3 ], 1 â‰¤ i1, i2, i3, i4 â‰¤ k
2782
Tensor Decompositions for Learning Latent Variable Models
( i.e., T is the fourth derivative tensor of the function v 7â†’ 8âˆ’1E[(v>
x)2]2, so M4 is the
fourth cumulant tensor). Let Îºi := E[h4
i ] âˆ’ 3 for each i âˆˆ [k]. Then
M4 =
k
X
i=1
Îºi Âµi âŠ— Âµi âŠ— Âµi âŠ— Âµi.
Note that Îºi corresponds to the excess kurtosis, a measure of non-Gaussianity as Îºi = 0 if
hi is a standard normal random variable. Furthermore, note that A is not identifiable if h
is a multivariate Gaussian.
We may derive forms similar to that of M2 and M3 from Theorem 3.1 using M4 by
observing that
M4(I, I, u, v) =
k
X
i=1
Îºi(Âµ>
i u)(Âµ>
i v) Âµi âŠ— Âµi,
M4(I, I, I, v) =
k
X
i=1
Îºi(Âµ>
i v) Âµi âŠ— Âµi âŠ— Âµi
for any vectors u, v âˆˆ Rd.
3.2.4 Latent Dirichlet Allocation (LDA)
An increasingly popular class of latent variable models are mixed membership models, where
each datum may belong to several different latent classes simultaneously. LDA is one such
model for the case of document modeling; here, each document corresponds to a mixture
over topics (as opposed to just a single topic). The distribution over such topic mixtures is a
Dirichlet distribution Dir(Î±) with parameter vector Î± âˆˆ Rk
++ with strictly positive entries;
its density over the probability simplex âˆ†kâˆ’1 := {v âˆˆ Rk : vi âˆˆ [0, 1]âˆ€i âˆˆ [k],
Pk
i=1 vi = 1}
is given by
pÎ±(h) =
Î“(Î±0)
Qk
i=1 Î“(Î±i)
k
Y
i=1
hÎ±iâˆ’1
i , h âˆˆ âˆ†kâˆ’1
where
Î±0 := Î±1 + Î±2 + Â· Â· Â· + Î±k.
As before, the k topics are specified by probability vectors Âµ1, Âµ2, . . . , Âµk âˆˆ âˆ†dâˆ’1. To
generate a document, we first draw the topic mixture h = (h1, h2, . . . , hk) âˆ¼ Dir(Î±), and
then conditioned on h, we draw ` words x1, x2, . . . , x` independently from the discrete
distribution specified by the probability vector
Pk
i=1 hiÂµi (i.e., for each xt, we independently
sample a topic j according to h and then sample xt according to Âµj). Again, we encode a
word xt by setting xt = ei iff the t-th word in the document is i.
The parameter Î±0 (the sum of the â€œpseudo-countsâ€) characterizes the concentration of
the distribution. As Î±0 â†’ 0, the distribution degenerates to a single topic model (i.e., the
limiting density has, with probability 1, exactly one entry of h being 1 and the rest are 0).
At the other extreme, if Î± = (c, c, . . . , c) for some scalar c > 0, then as Î±0 = ck â†’ âˆž, the
distribution of h becomes peaked around the uniform vector (1/k, 1/k, . . . , 1/k) (further-
more, the distribution behaves like a product distribution). We are typically interested in
2783
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
h
x1 x2 Â· Â· Â· x`
(a) Multi-view models
h1 h2 Â· Â· Â· h`
x1 x2 x`
(b) Hidden Markov model
Figure 1: Examples of latent variable models.
the case where Î±0 is small (e.g., a constant independent of k), whereupon h typically has
only a few large entries. This corresponds to the setting where the documents are mainly
comprised of just a few topics.
Theorem 3.5 (Anandkumar et al., 2012a) Define
M1 := E[x1]
M2 := E[x1 âŠ— x2] âˆ’
Î±0
Î±0 + 1
M1 âŠ— M1
M3 := E[x1 âŠ— x2 âŠ— x3]
âˆ’
Î±0
Î±0 + 2

E[x1 âŠ— x2 âŠ— M1] + E[x1 âŠ— M1 âŠ— x2] + E[M1 âŠ— x1 âŠ— x2]

+
2Î±2
0
(Î±0 + 2)(Î±0 + 1)
M1 âŠ— M1 âŠ— M1.
Then
M2 =
k
X
i=1
Î±i
(Î±0 + 1)Î±0
Âµi âŠ— Âµi
M3 =
k
X
i=1
2Î±i
(Î±0 + 2)(Î±0 + 1)Î±0
Âµi âŠ— Âµi âŠ— Âµi.
Note that Î±0 needs to be known to form M2 and M3 from the raw moments. This,
however, is a much weaker than assuming that the entire distribution of h is known (i.e.,
knowledge of the whole parameter vector Î±).
3.3 Multi-View Models
Multi-view models (also sometimes called naÄ±Ìˆve Bayes models) are a special class of Bayesian
networks in which observed variables x1, x2, . . . , x` are conditionally independent given a
latent variable h. This is similar to the exchangeable single topic model, but here we
do not require the conditional distributions of the xt, t âˆˆ [`] to be identical. Techniques
developed for this class can be used to handle a number of widely used models including
hidden Markov models (Mossel and Roch, 2006; Anandkumar et al., 2012c), phylogenetic
tree models (Chang, 1996; Mossel and Roch, 2006), certain tree mixtures (Anandkumar
et al., 2012b), and certain probabilistic grammar models (Hsu et al., 2012a).
2784
Tensor Decompositions for Learning Latent Variable Models
As before, we let h âˆˆ [k] be a discrete random variable with Pr[h = j] = wj for all j âˆˆ [k].
Now consider random vectors x1 âˆˆ Rd1 , x2 âˆˆ Rd2 , and x3 âˆˆ Rd3 which are conditionally
independent given h, and
E[xt|h = j] = Âµt,j, j âˆˆ [k], t âˆˆ {1, 2, 3}
where the Âµt,j âˆˆ Rdt are the conditional means of the xt given h = j. Thus, we allow the
observations x1, x2, . . . , x` to be random vectors, parameterized only by their conditional
means. Importantly, these conditional distributions may be discrete, continuous, or even a
mix of both.
We first note the form for the raw (cross) moments.
Proposition 3.1 We have that:
E[xt âŠ— xt0 ] =
k
X
i=1
wi Âµt,i âŠ— Âµt0,i, {t, t0
} âŠ‚ {1, 2, 3}, t 6= t0
E[x1 âŠ— x2 âŠ— x3] =
k
X
i=1
wi Âµ1,i âŠ— Âµ2,i âŠ— Âµ3,i.
The cross moments do not possess a symmetric tensor form when the conditional distri-
butions are different. Nevertheless, the moments can be â€œsymmetrizedâ€ via a simple linear
transformation of x1 and x2 (roughly speaking, this relates x1 and x2 to x3); this leads
to an expression from which the conditional means of x3 (i.e., Âµ3,1, Âµ3,2, . . . , Âµ3,k) can be
recovered. For simplicity, we assume d1 = d2 = d3 = k; the general case (with dt â‰¥ k) is
easily handled using low-rank singular value decompositions.
Theorem 3.6 (Anandkumar et al., 2012a) Assume that {Âµv,1, Âµv,2, . . . , Âµv,k} are lin-
early independent for each v âˆˆ {1, 2, 3}. Define
xÌƒ1 := E[x3 âŠ— x2]E[x1 âŠ— x2]âˆ’1
x1
xÌƒ2 := E[x3 âŠ— x1]E[x2 âŠ— x1]âˆ’1
x2
M2 := E[xÌƒ1 âŠ— xÌƒ2]
M3 := E[xÌƒ1 âŠ— xÌƒ2 âŠ— x3].
Then
M2 =
k
X
i=1
wi Âµ3,i âŠ— Âµ3,i
M3 =
k
X
i=1
wi Âµ3,i âŠ— Âµ3,i âŠ— Âµ3,i.
We now discuss three examples (taken mostly from Anandkumar et al., 2012c) where the
above observations can be applied. The first two concern mixtures of product distributions,
and the last one is the time-homogeneous hidden Markov model.
2785
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
3.3.1 Mixtures of Axis-Aligned Gaussians and Other Product Distributions
The first example is a mixture of k product distributions in Rn under a mild incoherence as-
sumption (Anandkumar et al., 2012c). Here, we allow each of the k component distributions
to have a different product distribution (e.g., Gaussian distribution with an axis-aligned co-
variance matrix), but require the matrix of component means A := [Âµ1|Âµ2| Â· Â· Â· |Âµk] âˆˆ RnÃ—k
to satisfy a certain (very mild) incoherence condition. The role of the incoherence condition
is explained below.
For a mixture of product distributions, any partitioning of the dimensions [n] into three
groups creates three (possibly asymmetric) â€œviewsâ€ which are conditionally independent
once the mixture component is selected. However, recall that Theorem 3.6 requires that
for each view, the k conditional means be linearly independent. In general, this may not
be achievable; consider, for instance, the case Âµi = ei for each i âˆˆ [k]. Such cases, where
the component means are very aligned with the coordinate basis, are precluded by the
incoherence condition.
Define coherence(A) := maxiâˆˆ[n]{e>
i Î Aei} to be the largest diagonal entry of the orthog-
onal projector to the range of A, and assume A has rank k. The coherence lies between k/n
and 1; it is largest when the range of A is spanned by the coordinate axes, and it is k/n when
the range is spanned by a subset of the Hadamard basis of cardinality k. The incoherence
condition requires, for some Îµ, Î´ âˆˆ (0, 1), coherence(A) â‰¤ (Îµ2/6)/ ln(3k/Î´). Essentially, this
condition ensures that the non-degeneracy of the component means is not isolated in just
a few of the n dimensions. Operationally, it implies the following.
Proposition 3.2 (Anandkumar et al., 2012c) Assume A has rank k, and
coherence(A) â‰¤
Îµ2/6
ln(3k/Î´)
for some Îµ, Î´ âˆˆ (0, 1). With probability at least 1âˆ’Î´, a random partitioning of the dimensions
[n] into three groups (for each i âˆˆ [n], independently pick t âˆˆ {1, 2, 3} uniformly at random
and put i in group t) has the following property. For each t âˆˆ {1, 2, 3} and j âˆˆ [k], let
Âµt,j be the entries of Âµj put into group t, and let At := [Âµt,1|Âµt,2| Â· Â· Â· |Âµt,k]. Then for each
t âˆˆ {1, 2, 3}, At has full column rank, and the k-th largest singular value of At is at least
p
(1 âˆ’ Îµ)/3 times that of A.
Therefore, three asymmetric views can be created by randomly partitioning the observed
random vector x into x1, x2, and x3, such that the resulting component means for each
view satisfy the conditions of Theorem 3.6.
3.3.2 Spherical Gaussian Mixtures, Revisited
Consider again the case of spherical Gaussian mixtures (cf. Section 3.2). As we shall see
in Section 4.3, the previous techniques (based on Theorem 3.2 and Theorem 3.3) lead to
estimation procedures when the dimension of x is k or greater (and when the k component
means are linearly independent). We now show that when the dimension is slightly larger,
say greater than 3k, a different (and simpler) technique based on the multi-view structure
can be used to extract the relevant structure.
2786
Tensor Decompositions for Learning Latent Variable Models
We again use a randomized reduction. Specifically, we create three views by (i) applying
a random rotation to x, and then (ii) partitioning x âˆˆ Rn into three views xÌƒ1, xÌƒ2, xÌƒ3 âˆˆ Rd
for d := n/3. By the rotational invariance of the multivariate Gaussian distribution, the
distribution of x after random rotation is still a mixture of spherical Gaussians (i.e., a
mixture of product distributions), and thus xÌƒ1, xÌƒ2, xÌƒ3 are conditionally independent given
h. What remains to be checked is that, for each view t âˆˆ {1, 2, 3}, the matrix of conditional
means of xÌƒt for each view has full column rank. This is true with probability 1 as long as
the matrix of conditional means A := [Âµ1|Âµ2| Â· Â· Â· |Âµk] âˆˆ RnÃ—k has rank k and n â‰¥ 3k. To
see this, observe that a random rotation in Rn followed by a restriction to d coordinates
is simply a random projection from Rn to Rd, and that a random projection of a linear
subspace of dimension k to Rd is almost surely injective as long as d â‰¥ k. Applying this
observation to the range of A implies the following.
Proposition 3.3 (Hsu and Kakade, 2013) Assume A has rank k and that n â‰¥ 3k. Let
R âˆˆ RnÃ—n be chosen uniformly at random among all orthogonal n Ã— n matrices, and set
xÌƒ := Rx âˆˆ Rn and AÌƒ := RA = [RÂµ1|RÂµ2| Â· Â· Â· |RÂµk] âˆˆ RnÃ—k. Partition [n] into three groups
of sizes d1, d2, d3 with dt â‰¥ k for each t âˆˆ {1, 2, 3}. Furthermore, for each t, define xÌƒt âˆˆ Rdt
(respectively, AÌƒt âˆˆ RdtÃ—k) to be the subvector of xÌƒ (resp., submatrix of AÌƒ) obtained by
selecting the dt entries (resp., rows) in the t-th group. Then xÌƒ1, xÌƒ2, xÌƒ3 are conditionally
independent given h; E[xÌƒt|h = j] = AÌƒtej for each j âˆˆ [k] and t âˆˆ {1, 2, 3}; and with
probability 1, the matrices AÌƒ1, AÌƒ2, AÌƒ3 have full column rank.
It is possible to obtain a quantitative bound on the k-th largest singular value of each At
in terms of the k-th largest singular value of A (analogous to Proposition 3.2). One avenue
is to show that a random rotation in fact causes AÌƒ to have low coherence, after which we
can apply Proposition 3.2. With this approach, it is sufficient to require n = O(k log k)
(for constant Îµ and Î´), which results in the k-th largest singular value of each At being
a constant fraction of the k-th largest singular value of A. We conjecture that, in fact,
n â‰¥ c Â· k for some c > 3 suffices.
3.3.3 Hidden Markov Models
Our last example is the time-homogeneous HMM for sequences of vector-valued observations
x1, x2, . . . âˆˆ Rd. Consider a Markov chain of discrete hidden states y1 â†’ y2 â†’ y3 â†’ Â· Â· Â·
over k possible states [k]; given a state yt at time t, the observation xt at time t (a random
vector taking values in Rd) is independent of all other observations and hidden states. See
Figure 1(b).
Let Ï€ âˆˆ âˆ†kâˆ’1 be the initial state distribution (i.e., the distribution of y1), and T âˆˆ RkÃ—k
be the stochastic transition matrix for the hidden state Markov chain: for all times t,
Pr[yt+1 = i|yt = j] = Ti,j, i, j âˆˆ [k].
Finally, let O âˆˆ RdÃ—k be the matrix whose j-th column is the conditional expectation of xt
given yt = j: for all times t,
E[xt|yt = j] = Oej, j âˆˆ [k].
2787
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Proposition 3.4 (Anandkumar et al., 2012c) Define h := y2, where y2 is the second
hidden state in the Markov chain. Then
â€¢ x1, x2, x3 are conditionally independent given h;
â€¢ the distribution of h is given by the vector w := TÏ€ âˆˆ âˆ†kâˆ’1;
â€¢ for all j âˆˆ [k],
E[x1|h = j] = O diag(Ï€)T>
diag(w)âˆ’1
ej
E[x2|h = j] = Oej
E[x3|h = j] = OTej.
Note the matrix of conditional means of xt has full column rank, for each t âˆˆ {1, 2, 3},
provided that: (i) O has full column rank, (ii) T is invertible, and (iii) Ï€ and TÏ€ have
positive entries.
4. Orthogonal Tensor Decompositions
We now show how recovering the Âµiâ€™s in our aforementioned problems reduces to the prob-
lem of finding a certain orthogonal tensor decomposition of a symmetric tensor. We start by
reviewing the spectral decomposition of symmetric matrices, and then discuss a generaliza-
tion to the higher-order tensor case. Finally, we show how orthogonal tensor decompositions
can be used for estimating the latent variable models from the previous section.
4.1 Review: The Matrix Case
We first build intuition by reviewing the matrix setting, where the desired decomposi-
tion is the eigendecomposition of a symmetric rank-k matrix M = V Î›V >
, where V =
[v1|v2| Â· Â· Â· |vk] âˆˆ RnÃ—k is the matrix with orthonormal eigenvectors as columns, and Î› =
diag(Î»1, Î»2, . . . , Î»k) âˆˆ RkÃ—k is diagonal matrix of non-zero eigenvalues. In other words,
M =
k
X
i=1
Î»i viv>
i =
k
X
i=1
Î»i vâŠ—2
i . (1)
Such a decomposition is guaranteed to exist for every symmetric matrix.
Recovery of the viâ€™s and Î»iâ€™s can be viewed at least two ways. First, each vi is fixed
under the mapping u 7â†’ Mu, up to a scaling factor Î»i:
Mvi =
k
X
j=1
Î»j(v>
j vi)vj = Î»ivi
as v>
j vi = 0 for all j 6= i by orthogonality. The viâ€™s are not necessarily the only such fixed
points. For instance, with the multiplicity Î»1 = Î»2 = Î», then any linear combination of v1
and v2 is similarly fixed under M. However, in this case, the decomposition in (1) is not
unique, as Î»1v1v>
1 + Î»2v2v>
2 is equal to Î»(u1u>
1 + u2u>
2 ) for any pair of orthonormal vectors,
2788
Tensor Decompositions for Learning Latent Variable Models
u1 and u2 spanning the same subspace as v1 and v2. Nevertheless, the decomposition is
unique when Î»1, Î»2, . . . , Î»k are distinct, whereupon the vjâ€™s are the only directions fixed
under u 7â†’ Mu up to non-trivial scaling.
The second view of recovery is via the variational characterization of the eigenvalues.
Assume Î»1 > Î»2 > Â· Â· Â· > Î»k; the case of repeated eigenvalues again leads to similar non-
uniqueness as discussed above. Then the Rayleigh quotient
u 7â†’
u>
Mu
u>
u
is maximized over non-zero vectors by v1. Furthermore, for any s âˆˆ [k], the maximizer of
the Rayleigh quotient, subject to being orthogonal to v1, v2, . . . , vsâˆ’1, is vs. Another way
of obtaining this second statement is to consider the deflated Rayleigh quotient
u 7â†’
u>

M âˆ’
Psâˆ’1
j=1 Î»jvjv>
j

u
u>
u
and observe that vs is the maximizer.
Efficient algorithms for finding these matrix decompositions are well studied (Golub
and van Loan, 1996, Section 8.2.3), and iterative power methods are one effective class of
algorithms.
We remark that in our multilinear tensor notation, we may write the maps u 7â†’ Mu
and u 7â†’ u>
Mu/kuk2
2 as
u 7â†’ Mu â‰¡ u 7â†’ M(I, u), (2)
u 7â†’
u>
Mu
u>
u
â‰¡ u 7â†’
M(u, u)
u>
u
. (3)
4.2 The Tensor Case
Decomposing general tensors is a delicate issue; tensors may not even have unique decom-
positions. Fortunately, the orthogonal tensors that arise in the aforementioned models have
a structure which permits a unique decomposition under a mild non-degeneracy condition.
We focus our attention to the case p = 3, i.e., a third order tensor; the ideas extend to
general p with minor modifications.
An orthogonal decomposition of a symmetric tensor T âˆˆ
N3
Rn is a collection of or-
thonormal (unit) vectors {v1, v2, . . . , vk} together with corresponding positive scalars Î»i > 0
such that
T =
k
X
i=1
Î»ivâŠ—3
i . (4)
Note that since we are focusing on odd-order tensors (p = 3), we have added the require-
ment that the Î»i be positive. This convention can be followed without loss of generality
since âˆ’Î»ivâŠ—p
i = Î»i(âˆ’vi)âŠ—p whenever p is odd. Also, it should be noted that orthogonal
decompositions do not necessarily exist for every symmetric tensor.
In analogy to the matrix setting, we consider two ways to view this decomposition: a
fixed-point characterization and a variational characterization. Related characterizations
based on optimal rank-1 approximations are given by Zhang and Golub (2001).
2789
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
4.2.1 Fixed-Point Characterization
For a tensor T, consider the vector-valued map
u 7â†’ T(I, u, u) (5)
which is the third-order generalization of (2). This can be explicitly written as
T(I, u, u) =
d
X
i=1
X
1â‰¤j,lâ‰¤d
Ti,j,l(e>
j u)(e>
l u)ei.
Observe that (5) is not a linear map, which is a key difference compared to the matrix case.
An eigenvector u for a matrix M satisfies M(I, u) = Î»u, for some scalar Î». We say a
unit vector u âˆˆ Rn is an eigenvector of T, with corresponding eigenvalue Î» âˆˆ R, if
T(I, u, u) = Î»u.
(To simplify the discussion, we assume throughout that eigenvectors have unit norm; oth-
erwise, for scaling reasons, we replace the above equation with T(I, u, u) = Î»kuku.) This
concept was originally introduced by Lim (2005) and Qi (2005). For orthogonally decom-
posable tensors T =
Pk
i=1 Î»ivâŠ—3
i ,
T(I, u, u) =
k
X
i=1
Î»i(u>
vi)2
vi .
By the orthogonality of the vi, it is clear that T(I, vi, vi) = Î»ivi for all i âˆˆ [k]. Therefore
each (vi, Î»i) is an eigenvector/eigenvalue pair.
There are a number of subtle differences compared to the matrix case that arise as a
result of the non-linearity of (5). First, even with the multiplicity Î»1 = Î»2 = Î», a linear
combination u := c1v1 + c2v2 may not be an eigenvector. In particular,
T(I, u, u) = Î»1c2
1v1 + Î»2c2
2v2 = Î»(c2
1v1 + c2
2v2)
may not be a multiple of c1v1 + c2v2. This indicates that the issue of repeated eigenvalues
does not have the same status as in the matrix case. Second, even if all the eigenvalues
are distinct, it turns out that the viâ€™s are not the only eigenvectors. For example, set
u := (1/Î»1)v1 + (1/Î»2)v2. Then,
T(I, u, u) = Î»1(1/Î»1)2
v1 + Î»2(1/Î»2)2
v2 = u,
so u/kuk is an eigenvector. More generally, for any subset S âŠ† [k], the vector
X
iâˆˆS
1
Î»i
Â· vi
is (proportional to) an eigenvector.
2790
Tensor Decompositions for Learning Latent Variable Models
As we now see, these additional eigenvectors can be viewed as spurious. We say a unit
vector u is a robust eigenvector of T if there exists an  > 0 such that for all Î¸ âˆˆ {u0 âˆˆ Rn :
ku0 âˆ’ uk â‰¤ }, repeated iteration of the map
Î¸Ì„ 7â†’
T(I, Î¸Ì„, Î¸Ì„)
kT(I, Î¸Ì„, Î¸Ì„)k
, (6)
starting from Î¸ converges to u. Note that the map (6) rescales the output to have unit
Euclidean norm. Robust eigenvectors are also called attracting fixed points of (6) (see, e.g.,
Kolda and Mayo, 2011).
The following theorem implies that if T has an orthogonal decomposition as given in (4),
then the set of robust eigenvectors of T are precisely the set {v1, v2, . . . vk}, implying that
the orthogonal decomposition is unique. (For even order tensors, the uniqueness is true up
to sign-flips of the vi.)
Theorem 4.1 Let T have an orthogonal decomposition as given in (4).
1. The set of Î¸ âˆˆ Rn which do not converge to some vi under repeated iteration of (6)
has measure zero.
2. The set of robust eigenvectors of T is equal to {v1, v2, . . . , vk}.
The proof of Theorem 4.1 is given in Appendix A.1, and follows readily from simple or-
thogonality considerations. Note that every vi in the orthogonal tensor decomposition is
robust, whereas for a symmetric matrix M, for almost all initial points, the map Î¸Ì„ 7â†’ MÎ¸Ì„
kMÎ¸Ì„k
converges only to an eigenvector corresponding to the largest magnitude eigenvalue. Also,
since the tensor order is odd, the signs of the robust eigenvectors are fixed, as each âˆ’vi is
mapped to vi under (6).
4.2.2 Variational Characterization
We now discuss a variational characterization of the orthogonal decomposition. The gener-
alized Rayleigh quotient (Zhang and Golub, 2001) for a third-order tensor is
u 7â†’
T(u, u, u)
(u>
u)3/2
,
which can be compared to (3). For an orthogonally decomposable tensor, the following
theorem shows that a non-zero vector u âˆˆ Rn is an isolated local maximizer (Nocedal and
Wright, 1999) of the generalized Rayleigh quotient if and only if u = vi for some i âˆˆ [k].
Theorem 4.2 Let T have an orthogonal decomposition as given in (4), and consider the
optimization problem
max
uâˆˆRn
T(u, u, u) s.t. kuk â‰¤ 1.
1. The stationary points are eigenvectors of T.
2. A stationary point u is an isolated local maximizer if and only if u = vi for some
i âˆˆ [k].
2791
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
The proof of Theorem 4.2 is given in Appendix A.2. It is similar to local optimality analysis
for ICA methods using fourth-order cumulants (e.g., Delfosse and Loubaton, 1995; Frieze
et al., 1996).
Again, we see similar distinctions to the matrix case. In the matrix case, the only local
maximizers of the Rayleigh quotient are the eigenvectors with the largest eigenvalue (and
these maximizers take on the globally optimal value). For the case of orthogonal tensor
forms, the robust eigenvectors are precisely the isolated local maximizers.
An important implication of the two characterizations is that, for orthogonally decom-
posable tensors T, (i) the local maximizers of the objective function u 7â†’ T(u, u, u)/(u>
u)3/2
correspond precisely to the vectors vi in the decomposition, and (ii) these local maximizers
can be reliably identified using a simple fixed-point iteration (i.e., the tensor analogue of
the matrix power method). Moreover, a second-derivative test based on T(I, I, u) can be
employed to test for local optimality and rule out other stationary points.
4.3 Estimation via Orthogonal Tensor Decompositions
We now demonstrate how the moment tensors obtained for various latent variable models
in Section 3 can be reduced to an orthogonal form. For concreteness, we take the specific
form from the exchangeable single topic model (Theorem 3.1):
M2 =
k
X
i=1
wi Âµi âŠ— Âµi,
M3 =
k
X
i=1
wi Âµi âŠ— Âµi âŠ— Âµi.
(The more general case allows the weights wi in M2 to differ in M3, but for simplicity
we keep them the same in the following discussion.) We now show how to reduce these
forms to an orthogonally decomposable tensor from which the wi and Âµi can be recovered.
See Appendix D for a discussion as to how previous approaches (Mossel and Roch, 2006;
Anandkumar et al., 2012c,a; Hsu and Kakade, 2013) achieved this decomposition through
a certain simultaneous diagonalization method.
Throughout, we assume the following non-degeneracy condition.
Condition 4.1 (Non-degeneracy) The vectors Âµ1, Âµ2, . . . , Âµk âˆˆ Rd are linearly indepen-
dent, and the scalars w1, w2, . . . , wk > 0 are strictly positive.
Observe that Condition 4.1 implies that M2  0 is positive semidefinite and has rank k.
This is often a mild condition in applications. When this condition is not met, learning
is conjectured to be generally hard for both computational (Mossel and Roch, 2006) and
information-theoretic reasons (Moitra and Valiant, 2010). As discussed by Hsu et al. (2012b)
and Hsu and Kakade (2013), when the non-degeneracy condition does not hold, it is often
possible to combine multiple observations using tensor products to increase the rank of the
relevant matrices. Indeed, this observation has been rigorously formulated in very recent
works of Bhaskara et al. (2014) and Anderson et al. (2014) using the framework of smoothed
analysis (Spielman and Teng, 2009).
2792
Tensor Decompositions for Learning Latent Variable Models
4.3.1 The Reduction
First, let W âˆˆ RdÃ—k be a linear transformation such that
M2(W, W) = W>
M2W = I
where I is the k Ã— k identity matrix (i.e., W whitens M2). Since M2  0, we may for
concreteness take W := UDâˆ’1/2, where U âˆˆ RdÃ—k is the matrix of orthonormal eigenvectors
of M2, and D âˆˆ RkÃ—k is the diagonal matrix of positive eigenvalues of M2. Let
ÂµÌƒi :=
âˆš
wi W>
Âµi.
Observe that
M2(W, W) =
k
X
i=1
W>
(
âˆš
wiÂµi)(
âˆš
wiÂµi)>
W =
k
X
i=1
ÂµÌƒiÂµÌƒ>
i = I,
so the ÂµÌƒi âˆˆ Rk are orthonormal vectors.
Now define f
M3 := M3(W, W, W) âˆˆ RkÃ—kÃ—k, so that
f
M3 =
k
X
i=1
wi (W>
Âµi)âŠ—3
=
k
X
i=1
1
âˆš
wi
ÂµÌƒâŠ—3
i .
As the following theorem shows, the orthogonal decomposition of f
M3 can be obtained by
identifying its robust eigenvectors, upon which the original parameters wi and Âµi can be
recovered. For simplicity, we only state the result in terms of robust eigenvector/eigenvalue
pairs; one may also easily state everything in variational form using Theorem 4.2.
Theorem 4.3 Assume Condition 4.1 and take f
M3 as defined above.
1. The set of robust eigenvectors of f
M3 is equal to {ÂµÌƒ1, ÂµÌƒ2, . . . , ÂµÌƒk}.
2. The eigenvalue corresponding to the robust eigenvector ÂµÌƒi of f
M3 is equal to 1/
âˆš
wi,
for all i âˆˆ [k].
3. If B âˆˆ RdÃ—k is the Moore-Penrose pseudoinverse of W>
, and (v, Î») is a robust eigen-
vector/eigenvalue pair of f
M3, then Î»Bv = Âµi for some i âˆˆ [k].
The theorem follows by combining the above discussion with the robust eigenvector charac-
terization of Theorem 4.1. Recall that we have taken as convention that eigenvectors have
unit norm, so the Âµi are exactly determined from the robust eigenvector/eigenvalue pairs of
f
M3 (together with the pseudoinverse of W>
); in particular, the scale of each Âµi is correctly
identified (along with the corresponding wi). Relative to previous works on moment-based
estimators for latent variable models (e.g., Anandkumar et al., 2012c,a; Hsu and Kakade,
2013), Theorem 4.3 emphasizes the role of the special tensor structure, which in turn makes
transparent the applicability of methods for orthogonal tensor decomposition.
2793
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
4.3.2 Local Maximizers of (Cross Moment) Skewness
The variational characterization provides an interesting perspective on the robust eigen-
vectors for these latent variable models. Consider the exchangeable single topic models
(Theorem 3.1), and the objective function
u 7â†’
E[(x>
1 u)(x>
2 u)(x>
3 u)]
E[(x>
1 u)(x>
2 u)]3/2
=
M3(u, u, u)
M2(u, u)3/2
.
In this case, every local maximizer uâˆ— satisfies M2(I, uâˆ—) =
âˆš
wiÂµi for some i âˆˆ [k]. The
objective function can be interpreted as the (cross moment) skewness of the random vectors
x1, x2, x3 along direction u.
5. Tensor Power Method
In this section, we consider the tensor power method of Lathauwer et al. (2000, Remark 3)
for orthogonal tensor decomposition. We first state a simple convergence analysis for an
orthogonally decomposable tensor T.
When only an approximation TÌ‚ to an orthogonally decomposable tensor T is available
(e.g., when empirical moments are used to estimate population moments), an orthogonal
decomposition need not exist for this perturbed tensor (unlike for the case of matrices),
and a more robust approach is required to extract the approximate decomposition. Here,
we propose such a variant in Algorithm 1 and provide a detailed perturbation analysis. We
note that alternative approaches such as simultaneous diagonalization can also be employed
(see Appendix D).
5.1 Convergence Analysis for Orthogonally Decomposable Tensors
The following lemma establishes the quadratic convergence of the tensor power methodâ€”
i.e., repeated iteration of (6)â€”for extracting a single component of the orthogonal decom-
position. Note that the initial vector Î¸0 determines which robust eigenvector will be the
convergent point. Computation of subsequent eigenvectors can be computed with deflation,
i.e., by subtracting appropriate terms from T.
Lemma 5.1 Let T âˆˆ
N3
Rn have an orthogonal decomposition as given in (4). For a vector
Î¸0 âˆˆ Rn, suppose that the set of numbers |Î»1v>
1 Î¸0|, |Î»2v>
2 Î¸0|, . . . , |Î»kv>
k Î¸0| has a unique largest
element. Without loss of generality, say |Î»1v>
1 Î¸0| is this largest value and |Î»2v>
2 Î¸0| is the
second largest value. For t = 1, 2, . . . , let
Î¸t :=
T(I, Î¸tâˆ’1, Î¸tâˆ’1)
kT(I, Î¸tâˆ’1, Î¸tâˆ’1)k
.
Then
kv1 âˆ’ Î¸tk2
â‰¤

2Î»2
1
k
X
i=2
Î»âˆ’2
i

Â·
Î»2v>
2 Î¸0
Î»1v>
1 Î¸0
2t+1
.
That is, repeated iteration of (6) starting from Î¸0 converges to v1 at a quadratic rate.
2794
Tensor Decompositions for Learning Latent Variable Models
To obtain all eigenvectors, we may simply proceed iteratively using deflation, executing
the power method on T âˆ’
P
j Î»jvâŠ—3
j after having obtained robust eigenvector / eigenvalue
pairs {(vj, Î»j)}.
Proof Let Î¸0, Î¸1, Î¸2, . . . be the sequence given by Î¸0 := Î¸0 and Î¸t := T(I, Î¸tâˆ’1, Î¸tâˆ’1) for
t â‰¥ 1. Let ci := v>
i Î¸0 for all i âˆˆ [k]. It is easy to check that (i) Î¸t = Î¸t/kÎ¸tk, and
(ii) Î¸t =
Pk
i=1 Î»2tâˆ’1
i c2t
i vi. (Indeed, Î¸t+1 =
Pk
i=1 Î»i(v>
i Î¸t)2vi =
Pk
i=1 Î»i(Î»2tâˆ’1
i c2t
i )2vi =
Pk
i=1 Î»2t+1âˆ’1
i c2t+1
i vi.) Then
1 âˆ’ (v>
1 Î¸t)2
= 1 âˆ’
(v>
1 Î¸t)2
kÎ¸tk2
= 1 âˆ’
Î»2t+1âˆ’2
1 c2t+1
1
Pk
i=1 Î»2t+1âˆ’2
i c2t+1
i
â‰¤
Pk
i=2 Î»2t+1âˆ’2
i c2t+1
i
Pk
i=1 Î»2t+1âˆ’2
i c2t+1
i
â‰¤ Î»2
1
k
X
i=2
Î»âˆ’2
i Â·
Î»2c2
Î»1c1
2t+1
.
Since Î»1 > 0, we have v>
1 Î¸t > 0 and hence kv1 âˆ’ Î¸tk2 = 2(1 âˆ’ v>
1 Î¸t) â‰¤ 2(1 âˆ’ (v>
1 Î¸t)2) as
required.
5.2 Perturbation Analysis of a Robust Tensor Power Method
Now we consider the case where we have an approximation TÌ‚ to an orthogonally decom-
posable tensor T. Here, a more robust approach is required to extract an approximate
decomposition. We propose such an algorithm in Algorithm 1, and provide a detailed per-
turbation analysis. For simplicity, we assume the tensor TÌ‚ is of size k Ã— k Ã— k as per the
reduction from Section 4.3. In some applications, it may be preferable to work directly with
a n Ã— n Ã— n tensor of rank k â‰¤ n (as in Lemma 5.1); our results apply in that setting with
little modification.
Algorithm 1 Robust tensor power method
input symmetric tensor TÌƒ âˆˆ RkÃ—kÃ—k, number of iterations L, N.
output the estimated eigenvector/eigenvalue pair; the deflated tensor.
1: for Ï„ = 1 to L do
2: Draw Î¸
(Ï„)
0 uniformly at random from the unit sphere in Rk.
3: for t = 1 to N do
4: Compute power iteration update
Î¸
(Ï„)
t :=
TÌƒ(I, Î¸
(Ï„)
tâˆ’1, Î¸
(Ï„)
tâˆ’1)
kTÌƒ(I, Î¸
(Ï„)
tâˆ’1, Î¸
(Ï„)
tâˆ’1)k
(7)
5: end for
6: end for
7: Let Ï„âˆ— := arg maxÏ„âˆˆ[L]{TÌƒ(Î¸
(Ï„)
N , Î¸
(Ï„)
N , Î¸
(Ï„)
N )}.
8: Do N power iteration updates (7) starting from Î¸
(Ï„âˆ—)
N to obtain Î¸Ì‚, and set Î»Ì‚ := TÌƒ(Î¸Ì‚, Î¸Ì‚, Î¸Ì‚).
9: return the estimated eigenvector/eigenvalue pair (Î¸Ì‚, Î»Ì‚); the deflated tensor TÌƒ âˆ’Î»Ì‚ Î¸Ì‚âŠ—3.
2795
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Assume that the symmetric tensor T âˆˆ RkÃ—kÃ—k is orthogonally decomposable, and that
TÌ‚ = T + E, where the perturbation E âˆˆ RkÃ—kÃ—k is a symmetric tensor with small operator
norm:
kEk := sup
kÎ¸k=1
|E(Î¸, Î¸, Î¸)|.
In our latent variable model applications, TÌ‚ is the tensor formed by using empirical mo-
ments, while T is the orthogonally decomposable tensor derived from the population mo-
ments for the given model. In the context of parameter estimation (as in Section 4.3), E
must account for any error amplification throughout the reduction, such as in the whitening
step (see, e.g., Hsu and Kakade, 2013, for such an analysis).
The following theorem is similar to Wedinâ€™s perturbation theorem for singular vectors
of matrices (Wedin, 1972) in that it bounds the error of the (approximate) decomposition
returned by Algorithm 1 on input TÌ‚ in terms of the size of the perturbation, provided that
the perturbation is small enough.
Theorem 5.1 Let TÌ‚ = T + E âˆˆ RkÃ—kÃ—k, where T is a symmetric tensor with orthogonal
decomposition T =
Pk
i=1 Î»ivâŠ—3
i where each Î»i > 0, {v1, v2, . . . , vk} is an orthonormal basis,
and E is a symmetric tensor with operator norm kEk â‰¤ . Define Î»min := min{Î»i : i âˆˆ [k]},
and Î»max := max{Î»i : i âˆˆ [k]}. There exists universal constants C1, C2, C3 > 0 such that
the following holds. Pick any Î· âˆˆ (0, 1), and suppose
 â‰¤ C1 Â·
Î»min
k
, N â‰¥ C2 Â·

log(k) + log log
Î»max


,
and
s
ln(L/ log2(k/Î·))
ln(k)
Â· 1 âˆ’
ln(ln(L/ log2(k/Î·))) + C3
4 ln(L/ log2(k/Î·))
âˆ’
s
ln(8)
ln(L/ log2(k/Î·))
!
â‰¥ 1.02 1 +
s
ln(4)
ln(k)
!
.
(Note that the condition on L holds with L = poly(k) log(1/Î·).) Suppose that Algorithm 1
is iteratively called k times, where the input tensor is TÌ‚ in the first call, and in each
subsequent call, the input tensor is the deflated tensor returned by the previous call. Let
(vÌ‚1, Î»Ì‚1), (vÌ‚2, Î»Ì‚2), . . . , (vÌ‚k, Î»Ì‚k) be the sequence of estimated eigenvector/eigenvalue pairs re-
turned in these k calls. With probability at least 1 âˆ’ Î·, there exists a permutation Ï€ on [k]
such that
kvÏ€(j) âˆ’ vÌ‚jk â‰¤ 8/Î»Ï€(j), |Î»Ï€(j) âˆ’ Î»Ì‚j| â‰¤ 5, âˆ€j âˆˆ [k],
and
T âˆ’
k
X
j=1
Î»Ì‚jvÌ‚âŠ—3
j â‰¤ 55.
The proof of Theorem 5.1 is given in Appendix B.
One important difference from Wedinâ€™s theorem is that this is an algorithm dependent
perturbation analysis, specific to Algorithm 1 (since the perturbed tensor need not have an
2796
Tensor Decompositions for Learning Latent Variable Models
orthogonal decomposition). Furthermore, note that Algorithm 1 uses multiple restarts to
ensure (approximate) convergenceâ€”the intuition is that by restarting at multiple points,
we eventually start at a point in which the initial contraction towards some eigenvector
dominates the error E in our tensor. The proof shows that we find such a point with high
probability within L = poly(k) trials. It should be noted that for large k, the required
bound on L is very close to linear in k.
We note that it is also possible to design a variant of Algorithm 1 that instead uses
a stopping criterion to determine if an iterate has (almost) converged to an eigenvector.
For instance, if TÌƒ(Î¸, Î¸, Î¸) > max{kTÌƒkF /
âˆš
2r, kTÌƒ(I, I, Î¸)kF /1.05}, where kTÌƒkF is the tensor
Frobenius norm (vectorized Euclidean norm), and r is the expected rank of the unperturbed
tensor (r = k âˆ’ # of deflation steps), then it can be shown that Î¸ must be close to one of
the eigenvectors, provided that the perturbation is small enough. Using such a stopping
criterion can reduce the number of random restarts when a good initial point is found early
on. See Appendix C for details.
In general, it is possible, when run on a general symmetric tensor (e.g., TÌ‚), for the
tensor power method to exhibit oscillatory behavior (Kofidis and Regalia, 2002, Example
1). This is not in conflict with Theorem 5.1, which effectively bounds the amplitude of
these oscillations; in particular, if TÌ‚ = T + E is a tensor built from empirical moments, the
error term E (and thus the amplitude of the oscillations) can be driven down by drawing
more samples. The practical value of addressing these oscillations and perhaps stabilizing
the algorithm is an interesting direction for future research (Kolda and Mayo, 2011).
A final consideration is that for specific applications, it may be possible to use domain
knowledge to choose better initialization points. For instance, in the topic modeling appli-
cations (cf. Section 3.1), the eigenvectors are related to the topic word distributions, and
many documents may be primarily composed of words from just single topic. Therefore,
good initialization points can be derived from these single-topic documents themselves, as
these points would already be close to one of the eigenvectors.
6. Discussion
In this section, we discuss some practical and application-oriented issues related to the
tensor decomposition approach to learning latent variable models.
6.1 Practical Implementation Considerations
A number of practical concerns arise when dealing with moment matrices and tensors.
Below, we address two issues that are especially pertinent to topic modeling applica-
tions (Anandkumar et al., 2012c,a) or other settings where the observations are sparse.
6.1.1 Efficient Moment Representation for Exchangeable Models
In an exchangeable bag-of-words model, it is assumed that the words x1, x2, . . . , x` in a
document are conditionally i.i.d. given the topic h. This allows one to estimate p-th order
moments using just p words per document. The estimators obtained via Theorem 3.1 (single
topic model) and Theorem 3.5 (LDA) use only up to third-order moments, which suggests
that each document only needs to have three words.
2797
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
In practice, one should use all of the words in a document for efficient estimation of the
moments. One way to do this is to average over all `
3

Â· 3! ordered triples of words in a
document of length `. At first blush, this seems computationally expensive (when ` is large),
but as it turns out, the averaging can be done implicitly, as shown by Zou et al. (2013).
Let c âˆˆ Rd be the word count vector for a document of length `, so ci is the number of
occurrences of word i in the document, and
Pd
i=1 ci = `. Note that c is a sufficient statistic
for the document. Then, the contribution of this document to the empirical third-order
moment tensor is given by
1
`
3
 Â·
1
3!
Â·

c âŠ— c âŠ— c + 2
d
X
i=1
ci (ei âŠ— ei âŠ— ei)
âˆ’
d
X
i=1
d
X
j=1
cicj (ei âŠ— ei âŠ— ej) âˆ’
d
X
i=1
d
X
j=1
cicj (ei âŠ— ej âŠ— ei) âˆ’
d
X
i=1
d
X
j=1
cicj (ei âŠ— ej âŠ— ej)

. (8)
It can be checked that this quantity is equal to
1
`
3
 Â·
1
3!
Â·
X
ordered word triple (x, y, z)
ex âŠ— ey âŠ— ez
where the sum is over all ordered word triples in the document. A similar expression is
easily derived for the contribution of the document to the empirical second-order moment
matrix:
1
`
2
 Â·
1
2!
Â·

c âŠ— c âˆ’ diag(c)

. (9)
Note that the word count vector c is generally a sparse vector, so this representation allows
for efficient multiplication by the moment matrices and tensors in time linear in the size of
the document corpus (i.e., the number of non-zero entries in the term-document matrix).
6.1.2 Dimensionality Reduction
Another serious concern regarding the use of tensor forms of moments is the need to op-
erate on multidimensional arrays with â„¦(d3) values (it is typically not exactly d3 due to
symmetry). When d is large (e.g., when it is the size of the vocabulary in natural language
applications), even storing a third-order tensor in memory can be prohibitive. Sparsity is
one factor that alleviates this problem. Another approach is to use efficient linear dimen-
sionality reduction. When this is combined with efficient techniques for matrix and tensor
multiplication that avoid explicitly constructing the moment matrices and tensors (such as
the procedure described above), it is possible to avoid any computational scaling more than
linear in the dimension d and the training sample size.
Consider for concreteness the tensor decomposition approach for the exchangeable single
topic model as discussed in Section 4.3. Using recent techniques for randomized linear
algebra computations (e.g., Halko et al., 2011), it is possible to efficiently approximate the
whitening matrix W âˆˆ RdÃ—k from the second-moment matrix M2 âˆˆ RdÃ—d. To do this, one
first multiplies M2 by a random matrix R âˆˆ RdÃ—k0
for some k0 â‰¥ k, and then computes the
top k singular vectors of the product M2R. This provides a basis U âˆˆ RdÃ—k whose span
2798
Tensor Decompositions for Learning Latent Variable Models
is approximately the range of M2. From here, an approximate SVD of U>
M2U is used to
compute the approximate whitening matrix W. Note that both matrix products M2R and
U>
M2U may be performed via implicit access to M2 by exploiting (9), so that M2 need
not be explicitly formed. With the whitening matrix W in hand, the third-moment tensor
f
M3 = M3(W, W, W) âˆˆ RkÃ—kÃ—k can be implicitly computed via (8). For instance, the core
computation in the tensor power method Î¸0 := f
M3(I, Î¸, Î¸) is performed by (i) computing
Î· := WÎ¸, (ii) computing Î·0 := M3(I, Î·, Î·), and finally (iii) computing Î¸0 := W>
Î·0. Using the
fact that M3 is an empirical third-order moment tensor, these steps can be computed with
O(dk + N) operations, where N is the number of non-zero entries in the term-document
matrix (Zou et al., 2013).
6.2 Computational Complexity
It is interesting to consider the computational complexity of the tensor power method in the
dense setting where T âˆˆ RkÃ—kÃ—k is orthogonally decomposable but otherwise unstructured.
Each iteration requires O(k3) operations, and assuming at most k1+Î´ random restarts for
extracting each eigenvector (for some small Î´ > 0) and O(log(k)+log log(1/)) iterations per
restart, the total running time is O(k5+Î´(log(k)+log log(1/))) to extract all k eigenvectors
and eigenvalues.
An alternative approach to extracting the orthogonal decomposition of T is to reorganize
T into a matrix M âˆˆ RkÃ—k2
by flattening two of the dimensions into one. In this case, if
T =
Pk
i=1 Î»ivâŠ—3
i , then M =
Pk
i=1 Î»ivi âŠ— vec(vi âŠ— vi). This reveals the singular value
decomposition of M (assuming the eigenvalues Î»1, Î»2, . . . , Î»k are distinct), and therefore can
be computed with O(k4) operations. Therefore it seems that the tensor power method is
less efficient than a pure matrix-based approach via singular value decomposition. However,
it should be noted that this matrix-based approach fails to recover the decomposition when
eigenvalues are repeated, and can be unstable when the gap between eigenvalues is smallâ€”
see Appendix D for more discussion.
It is worth noting that the running times differ by roughly a factor of Î˜(k1+Î´), which
can be accounted for by the random restarts. This gap can potentially be alleviated or
removed by using a more clever method for initialization. Moreover, using special structure
in the problem (as discussed above) can also improve the running time of the tensor power
method.
6.3 Sample Complexity Bounds
Previous work on using linear algebraic methods for estimating latent variable models cru-
cially rely on matrix perturbation analysis for deriving sample complexity bounds (Mossel
and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c,a; Hsu and Kakade, 2013).
The learning algorithms in these works are plug-in estimators that use empirical moments in
place of the population moments, and then follow algebraic manipulations that result in the
desired parameter estimates. As long as these manipulations can tolerate small perturba-
tions of the population moments, a sample complexity bound can be obtained by exploiting
the convergence of the empirical moments to the population moments via the law of large
numbers. As discussed in Appendix D, these approaches do not directly lead to practical
2799
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
algorithms due to a certain amplification of the error (a polynomial factor of k, which is
observed in practice).
Using the perturbation analysis for the tensor power method, improved sample complex-
ity bounds can be obtained for all of the examples discussed in Section 3. The underlying
analysis remains the same as in previous works (e.g., Anandkumar et al., 2012a; Hsu and
Kakade, 2013), the main difference being the accuracy of the orthogonal tensor decompo-
sition obtained via the tensor power method. Relative to the previously cited works, the
sample complexity bound will be considerably improved in its dependence on the rank pa-
rameter k, as Theorem 5.1 implies that the tensor estimation error (e.g., error in estimating
f
M3 from Section 4.3) is not amplified by any factor explicitly depending on k (there is
a requirement that the error be smaller than some factor depending on k, but this only
contributes to a lower-order term in the sample complexity bound). See Appendix D for
further discussion regarding the stability of the techniques from these previous works.
6.4 Other Perspectives
The tensor power method is simply one approach for extracting the orthogonal decomposi-
tion needed in parameter estimation. The characterizations from Section 4.2 suggest that a
number of fixed point and variational techniques may be possible (and Appendix D provides
yet another perspective based on simultaneous diagonalization). One important consider-
ation is that the model is often misspecified, and therefore approaches with more robust
guarantees (e.g., for convergence) are desirable. Our own experience with the tensor power
method (as applied to exchangeable topic modeling) is that while model misspecification
does indeed affect convergence, the results can be very reasonable even after just a dozen
or so iterations (Anandkumar et al., 2012a). Nevertheless, robustness is likely more impor-
tant in other applications, and thus the stabilization approaches (Kofidis and Regalia, 2002;
Regalia and Kofidis, 2003; Erdogan, 2009; Kolda and Mayo, 2011) may be advantageous.
Acknowledgments
We thank Boaz Barak, Dean Foster, Jon Kelner, and Greg Valiant for helpful discussions.
We are also grateful to Hanzhang Hu, Drew Bagnell, and Martial Hebert for alerting us
of an issue with Theorem 4.2 and suggesting a simple fix. This work was completed while
DH was a postdoctoral researcher at Microsoft Research New England, and partly while
AA, RG, and MT were visiting the same lab. AA is supported in part by the NSF Award
CCF-1219234, AFOSR Award FA9550-10-1-0310 and the ARO Award W911NF-12-1-0404.
Appendix A. Fixed-Point and Variational Characterizations of
Orthogonal Tensor Decompositions
We give detailed proofs of Theorems 4.1 and 4.2 in this section for completeness.
A.1 Proof of Theorem 4.1
Theorem A.1 Let T have an orthogonal decomposition as given in (4).
2800
Tensor Decompositions for Learning Latent Variable Models
1. The set of Î¸ âˆˆ Rn which do not converge to some vi under repeated iteration of (6)
has measure zero.
2. The set of robust eigenvectors of T is {v1, v2, . . . , vk}.
Proof For a random choice of Î¸ âˆˆ Rn (under any distribution absolutely continuous with
respect to Lebesgue measure), the values |Î»1v>
1 Î¸|, |Î»2v>
2 Î¸|, . . . , |Î»kv>
k Î¸| will be distinct with
probability 1. Therefore, there exists a unique largest value, say |Î»iv>
i Î¸| for some i âˆˆ [k],
and by Lemma 5.1, we have convergence to vi under repeated iteration of (6). Thus the
first claim holds.
We now prove the second claim. First, we show that every vi is a robust eigenvector.
Pick any i âˆˆ [k], and note that for a sufficiently small ball around vi, we have that for all Î¸
in this ball, Î»iv>
i Î¸ is strictly greater than Î»jv>
j Î¸ for j âˆˆ [k] \ {i}. Thus by Lemma 5.1, vi is
a robust eigenvector. Now we show that the vi are the only robust eigenvectors. Suppose
there exists some robust eigenvector u not equal to vi for any i âˆˆ [k]. Then there exists a
positive measure set around u such that all points in this set converge to u under repeated
iteration of (6). This contradicts the first claim.
A.2 Proof of Theorem 4.2
Theorem A.2 Let T have an orthogonal decomposition as given in (4), and consider the
optimization problem
max
uâˆˆRn
T(u, u, u) s.t. kuk â‰¤ 1.
1. The stationary points are eigenvectors of T.
2. A stationary point u is an isolated local maximizer if and only if u = vi for some
i âˆˆ [k].
Proof Consider the Lagrangian form of the corresponding constrained maximization prob-
lem over unit vectors u âˆˆ Rn:
L(u, Î») := T(u, u, u) âˆ’
3
2
Î»(u>
u âˆ’ 1).
Since
âˆ‡uL(u, Î») = âˆ‡u
 k
X
i=1
Î»i(v>
i u)3
âˆ’
3
2
Î»(u>
u âˆ’ 1)

= 3

T(I, u, u) âˆ’ Î»u

,
the stationary points u âˆˆ Rn (with kuk â‰¤ 1) satisfy
T(I, u, u) = Î»u
for some Î» âˆˆ R, i.e., (u, Î») is an eigenvector/eigenvalue pair of T.
Now we characterize the isolated local maximizers. Observe that if u 6= 0 and T(I, u, u) =
Î»u for Î» < 0, then T(u, u, u) < 0. Therefore u0 = (1 âˆ’ Î´)u for any Î´ âˆˆ (0, 1) satisfies
T(u0, u0, u0) = (1 âˆ’ Î´)3T(u, u, u) > T(u, u, u). So such a u cannot be a local maximizer.
2801
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Moreover, if kuk < 1 and T(I, u, u) = Î»u for Î» > 0, then u0 = (1 + Î´)u for a small enough
Î´ âˆˆ (0, 1) satisfies ku0k â‰¤ 1 and T(u0, u0, u0) = (1 + Î´)3T(u, u, u) > T(u, u, u). Therefore a
local maximizer must have T(I, u, u) = Î»u for some Î» â‰¥ 0, and kuk = 1 whenever Î» > 0.
Extend {v1, v2, . . . , vk} to an orthonormal basis {v1, v2, . . . , vn} of Rn. Now pick any
stationary point u =
Pn
i=1 civi with Î» := T(u, u, u) = u>
T(I, u, u). Then
Î»ic2
i = Î»i(u>
vi)2
= v>
i T(I, u, u) = Î»v>
i u = Î»ci â‰¥ 0, i âˆˆ [k],
and thus
âˆ‡2
uL(u, Î») = 6
k
X
i=1
Î»ici viv>
i âˆ’ 3Î»I = 3Î»

2
X
iâˆˆâ„¦
viv>
i âˆ’ I

where â„¦ := {i âˆˆ [k] : ci 6= 0}. This implies that for any unit vector w âˆˆ Rn,
w>
âˆ‡2
uL(u, Î»)w = 3Î»

2
X
iâˆˆâ„¦
(v>
i w)2
âˆ’ 1

.
The point u is an isolated local maximum if the above quantity is strictly negative for all
unit vectors w orthogonal to u. We now consider three cases depending on the cardinality
of â„¦ and the sign of Î».
â€¢ Case 1: |â„¦| = 1 and Î» > 0. This means u = vi for some i âˆˆ [k] (as u = âˆ’vi implies
Î» = âˆ’Î»i < 0). In this case,
w>
âˆ‡2
uL(u, Î»)w = 3Î»i(2(v>
i w)2
âˆ’ 1) = âˆ’3Î»i < 0
for all w âˆˆ Rn satisfying (u>
w)2 = (v>
i w)2 = 0. Hence u is an isolated local maximizer.
â€¢ Case 2: |â„¦| â‰¥ 2 and Î» > 0. Since |â„¦| â‰¥ 2, we may pick a strict non-empty subset
S ( â„¦ and set
w :=
1
Z

1
ZS
X
iâˆˆS
civi âˆ’
1
ZSc
X
iâˆˆâ„¦\S
civi

where ZS :=
P
iâˆˆS c2
i , ZSc :=
P
iâˆˆâ„¦\S c2
i , and Z :=
p
1/ZS + 1/ZSc . It is easy to
check that kwk2 =
P
iâˆˆâ„¦(v>
i w)2 = 1 and u>
w = 0. Consider any open neighborhood
U of u, and pick Î´ > 0 small enough so that uÌƒ :=
âˆš
1 âˆ’ Î´2u + Î´w is contained in
U. Set u0 :=
âˆš
1 âˆ’ Î´2u. By Taylorâ€™s theorem, there exists  âˆˆ [0, Î´] such that, for
2802
Tensor Decompositions for Learning Latent Variable Models
uÌ„ := u0 + w, we have
T(uÌƒ, uÌƒ, uÌƒ) = T(u0, u0, u0) + âˆ‡uT(u, u, u)>
(uÌƒ âˆ’ u0)
u=u0
+
1
2
(uÌƒ âˆ’ u0)>
âˆ‡2
uT(u, u, u)(uÌƒ âˆ’ u0)
u=uÌ„
= (1 âˆ’ Î´2
)3/2
Î» + Î´(1 âˆ’ Î´2
)Î»u>
w +
1
2
Î´2
w>
âˆ‡2
uT(u, u, u)w
u=uÌ„
= (1 âˆ’ Î´2
)3/2
Î» + 0 + 3Î´2
k
X
i=1
Î»i(v>
i (u0 + w))(v>
i w)2
= (1 âˆ’ Î´2
)3/2
Î» + 3Î´2
p
1 âˆ’ Î´2
k
X
i=1
Î»ici(v>
i w)2
+ 3Î´2

k
X
i=1
Î»i(v>
i w)3
= (1 âˆ’ Î´2
)3/2
Î» + 3Î´2
p
1 âˆ’ Î´2Î»
X
iâˆˆâ„¦
(v>
i w)2
+ 3Î´2

k
X
i=1
Î»i(v>
i w)3
= (1 âˆ’ Î´2
)3/2
Î» + 3Î´2
p
1 âˆ’ Î´2Î» + 3Î´2

k
X
i=1
Î»i(v>
i w)3
=

1 âˆ’
3
2
Î´2
+ O(Î´4
)

Î» + 3Î´2
p
1 âˆ’ Î´2Î» + 3Î´2

k
X
i=1
Î»i(v>
i w)3
.
Since  â‰¤ Î´, for small enough Î´, the RHS is strictly greater than Î». This implies that
u is not an isolated local maximizer.
â€¢ Case 3: |â„¦| = 0 or Î» = 0. Note that if |â„¦| = 0, then Î» = 0, so we just consider Î» = 0.
Consider any open neighborhood U of u, and pick j âˆˆ [n] and Î´ > 0 small enough so
that uÌƒ :=
âˆš
1 âˆ’ Î´2u + Î´vj is contained in U. Then
T(uÌƒ, uÌƒ, uÌƒ) = (1 âˆ’ Î´2
)3/2
T(u, u, u) + 3Î»j(1 âˆ’ Î´2
)Î´c2
j + 3Î»i
p
1 âˆ’ Î´2Î´2
cj + Î´3
> 0 = Î»
for sufficiently small Î´. Thus u is not an isolated local maximizer.
From these exhaustive cases, we conclude that a stationary point u is an isolated local
maximizer if and only if u = vi for some i âˆˆ [k].
We are grateful to Hanzhang Hu, Drew Bagnell, and Martial Hebert for alerting us of
an issue with our original statement of Theorem 4.2 and its proof, and for suggesting a
simple fix. The original statement used the optimization constraint kuk = 1 (rather than
kuk â‰¤ 1), but the characterization of the decomposition with this constraint is then only
given by isolated local maximizers u with the additional constraint that T(u, u, u) > 0â€”that
is, there can be isolated local maximizers with T(u, u, u) â‰¤ 0 that are not vectors in the
decomposition. The suggested fix of Hu, Bagnell, and Herbert is to relax to kuk â‰¤ 1, which
eliminates isolated local maximizers with T(u, u, u) â‰¤ 0; this way, the characterization of
the decomposition is simply the isolated local maximizers under the relaxed constraint.
2803
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Appendix B. Analysis of Robust Power Method
In this section, we prove Theorem 5.1. The proof is structured as follows. In Appendix B.1,
we show that with high probability, at least one out of L random vectors will be a good
initializer for the tensor power iterations. An initializer is good if its projection onto an
eigenvector is noticeably larger than its projection onto other eigenvectors. We then analyze
in Appendix B.2 the convergence behavior of the tensor power iterations. Relative to the
proof of Lemma 5.1, this analysis is complicated by the tensor perturbation. We show that
there is an initial slow convergence phase (linear rate rather than quadratic), but as soon
as the projection of the iterate onto an eigenvector is large enough, it enters the quadratic
convergence regime until the perturbation dominates. Finally, we show how errors accrue
due to deflation in Appendix B.3, which is rather subtle and different from deflation with
matrix eigendecompositions. This is because when some initial set of eigenvectors and
eigenvalues are accurately recovered, the additional errors due to deflation are effectively
only lower-order terms. These three pieces are assembled in Appendix B.4 to complete the
proof of Theorem 5.1.
B.1 Initialization
Consider a set of non-negative numbers Î»Ìƒ1, Î»Ìƒ2, . . . , Î»Ìƒk â‰¥ 0. For Î³ âˆˆ (0, 1), we say a unit
vector Î¸0 âˆˆ Rk is Î³-separated relative to iâˆ— âˆˆ [k] if
Î»Ìƒiâˆ— |Î¸iâˆ—,0| âˆ’ max
iâˆˆ[k]\{iâˆ—}
Î»Ìƒi|Î¸i,0| â‰¥ Î³Î»Ìƒi|Î¸iâˆ—,0|
(the dependence on Î»Ìƒ1, Î»Ìƒ2, . . . , Î»Ìƒk is implicit).
The following lemma shows that for any constant Î³, with probability at least 1 âˆ’ Î·,
at least one out of poly(k) log(1/Î·) i.i.d. random vectors (uniformly distributed over the
unit sphere Skâˆ’1) is Î³-separated relative to arg maxiâˆˆ[k] Î»Ìƒi. (For small enough Î³ and large
enough k, the polynomial is close to linear in k.)
Lemma B.1 There exists an absolute constant c > 0 such that if positive integer L â‰¥ 2
satisfies
s
ln(L)
ln(k)
Â· 1 âˆ’
ln(ln(L)) + c
4 ln(L)
âˆ’
s
ln(8)
ln(L)
!
â‰¥
1
1 âˆ’ Î³
Â· 1 +
s
ln(4)
ln(k)
!
, (10)
the following holds. With probability at least 1/2 over the choice of L i.i.d. random vectors
drawn uniformly distributed over the unit sphere Skâˆ’1 in Rk, at least one of the vectors is
Î³-separated relative to arg maxiâˆˆ[k] Î»Ìƒi. Moreover, with the same c, L, and for any Î· âˆˆ (0, 1),
with probability at least 1 âˆ’ Î· over L Â· log2(1/Î·) i.i.d. uniform random unit vectors, at least
one of the vectors is Î³-separated.
Proof Without loss of generality, assume arg maxiâˆˆ[k] Î»Ìƒi = 1. Consider a random matrix
Z âˆˆ RkÃ—L whose entries are independent N(0, 1) random variables; we take the j-th column
of Z to be comprised of the random variables used for the j-th random vector (before
normalization). Specifically, for the j-th random vector,
Î¸i,0 :=
Zi,j
qPk
i0=1 Z2
i0,j
, i âˆˆ [n].
2804
Tensor Decompositions for Learning Latent Variable Models
It suffices to show that with probability at least 1/2, there is a column jâˆ— âˆˆ [L] such that
|Z1,jâˆ— | â‰¥
1
1 âˆ’ Î³
max
iâˆˆ[k]\{1}
|Zi,jâˆ— |.
Since maxjâˆˆ[L] |Z1,j| is a 1-Lipschitz function of L independent N(0, 1) random variables,
it follows that
Pr

max
jâˆˆ[L]
|Z1,j| âˆ’ median
h
max
jâˆˆ[L]
|Z1,j|
i
>
p
2 ln(8)

â‰¤ 1/4.
Moreover,
median
h
max
jâˆˆ[L]
|Z1,j|
i
â‰¥ median
h
max
jâˆˆ[L]
Z1,j
i
=: m.
Observe that the cumulative distribution function of maxjâˆˆ[L] Z1,j is given by F(z) = Î¦(z)L,
where Î¦ is the standard Gaussian CDF. Since F(m) = 1/2, it follows that m = Î¦âˆ’1(2âˆ’1/L).
It can be checked that
Î¦âˆ’1
(2âˆ’1/L
) â‰¥
p
2 ln(L) âˆ’
ln(ln(L)) + c
2
p
2 ln(L)
for some absolute constant c > 0. Also, let jâˆ— := arg maxjâˆˆ[L] |Z1,j|.
Now for each j âˆˆ [L], let |Z2:k,j| := max{|Z2,j|, |Z3,j|, . . . , |Zk,j|}. Again, since |Z2:k,j| is
a 1-Lipschitz function of k âˆ’ 1 independent N(0, 1) random variables, it follows that
Pr

|Z2:k,j| > E
h
|Z2:k,j|
i
+
p
2 ln(4)

â‰¤ 1/4.
Moreover, by a standard argument,
E
h
|Z2:k,j|
i
â‰¤
p
2 ln(k).
Since |Z2:k,j| is independent of |Z1,j| for all j âˆˆ [L], it follows that the previous two displayed
inequalities also hold with j replaced by jâˆ—.
Therefore we conclude with a union bound that with probability at least 1/2,
|Z1,jâˆ— | â‰¥
p
2 ln(L) âˆ’
ln(ln(L)) + c
2
p
2 ln(L)
âˆ’
p
2 ln(8) and |Z2:k,jâˆ— | â‰¤
p
2 ln(k) +
p
2 ln(4).
Since L satisfies (10) by assumption, in this event, the jâˆ—-th random vector is Î³-separated.
B.2 Tensor Power Iterations
Recall the update rule used in the power method. Let Î¸t =
Pk
i=1 Î¸i,tvi âˆˆ Rk be the unit
vector at time t. Then
Î¸t+1 =
k
X
i=1
Î¸i,t+1vi := TÌƒ(I, Î¸t, Î¸t)/kTÌƒ(I, Î¸t, Î¸t)k.
2805
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
In this subsection, we assume that TÌƒ has the form
TÌƒ =
k
X
i=1
Î»ÌƒivâŠ—3
i + EÌƒ (11)
where {v1, v2, . . . , vk} is an orthonormal basis, and, without loss of generality,
Î»Ìƒ1|Î¸1,t| = max
iâˆˆ[k]
Î»Ìƒi|Î¸i,t| > 0.
Also, define
Î»Ìƒmin := min{Î»Ìƒi : i âˆˆ [k], Î»Ìƒi > 0}, Î»Ìƒmax := max{Î»Ìƒi : i âˆˆ [k]}.
We further assume the error EÌƒ is a symmetric tensor such that, for some constant p > 1,
kEÌƒ(I, u, u)k â‰¤ Ëœ
, âˆ€u âˆˆ Skâˆ’1
; (12)
kEÌƒ(I, u, u)k â‰¤ Ëœ
/p, âˆ€u âˆˆ Skâˆ’1
s.t. (u>
v1)2
â‰¥ 1 âˆ’ (3Ëœ
/Î»Ìƒ1)2
. (13)
In the next two propositions (Propositions B.1 and B.2) and next two lemmas (Lemmas B.2
and B.3), we analyze the power method iterations using TÌƒ at some arbitrary iterate Î¸t using
only the property (12) of EÌƒ. But throughout, the quantity Ëœ
 can be replaced by Ëœ
/p if Î¸t
satisfies (Î¸>
t v1)2 â‰¥ 1 âˆ’ (3Ëœ
/Î»Ìƒ1)2 as per property (13).
Define
RÏ„ :=

Î¸2
1,Ï„
1 âˆ’ Î¸2
1,Ï„
1/2
, ri,Ï„ :=
Î»Ìƒ1Î¸1,Ï„
Î»Ìƒi|Î¸i,Ï„ |
,
Î³Ï„ := 1 âˆ’
1
mini6=1 |ri,Ï„ |
, Î´Ï„ :=
Ëœ

Î»Ìƒ1Î¸2
1,Ï„
, Îº :=
Î»Ìƒmax
Î»Ìƒ1
(14)
for Ï„ âˆˆ {t, t + 1}.
Proposition B.1
min
i6=1
|ri,t| â‰¥
Rt
Îº
, Î³t â‰¥ 1 âˆ’
Îº
Rt
, Î¸2
1,t =
R2
t
1 + R2
t
.
Proposition B.2
ri,t+1 â‰¥ r2
i,t Â·
1 âˆ’ Î´t
1 + ÎºÎ´tr2
i,t
=
1 âˆ’ Î´t
1
r2
i,t
+ ÎºÎ´t
, i âˆˆ [k], (15)
Rt+1 â‰¥ Rt Â·
1 âˆ’ Î´t
1 âˆ’ Î³t + Î´tRt
â‰¥
1 âˆ’ Î´t
Îº
R2
t
+ Î´t
. (16)
Proof Let Î¸ÌŒt+1 := TÌƒ(I, Î¸t, Î¸t), so Î¸t+1 = Î¸ÌŒt+1/kÎ¸ÌŒt+1k. Since Î¸ÌŒi,t+1 = TÌƒ(vi, Î¸t, Î¸t) =
T(vi, Î¸t, Î¸t) + E(vi, Î¸t, Î¸t), we have
Î¸ÌŒi,t+1 = Î»ÌƒiÎ¸2
i,t + E(vi, Î¸t, Î¸t), i âˆˆ [k].
2806
Tensor Decompositions for Learning Latent Variable Models
Using the triangle inequality and the fact kE(vi, Î¸t, Î¸t)k â‰¤ Ëœ
, we have
Î¸ÌŒi,t+1 â‰¥ Î»ÌƒiÎ¸2
i,t âˆ’ Ëœ
 â‰¥ |Î¸i,t| Â·

Î»Ìƒi|Î¸i,t| âˆ’ Ëœ
/|Î¸i,t|

(17)
and
|Î¸ÌŒi,t+1| â‰¤ |Î»ÌƒiÎ¸2
i,t| + Ëœ
 â‰¤ |Î¸i,t| Â·

Î»Ìƒi|Î¸i,t| + Ëœ
/|Î¸i,t|

(18)
for all i âˆˆ [k]. Combining (17) and (18) gives
ri,t+1 =
Î»Ìƒ1Î¸1,t+1
Î»Ìƒi|Î¸i,t+1|
=
Î»Ìƒ1Î¸ÌŒ1,t+1
Î»Ìƒi|Î¸ÌŒi,t+1|
â‰¥ r2
i,t Â·
1 âˆ’ Î´t
1 + Ëœ

Î»ÌƒiÎ¸2
i,t
= r2
i,t Â·
1 âˆ’ Î´t
1 + (Î»Ìƒi/Î»Ìƒ1)Î´tr2
i,t
â‰¥ r2
i,t Â·
1 âˆ’ Î´t
1 + ÎºÎ´tr2
i,t
.
Moreover, by the triangle inequality and HoÌˆlderâ€™s inequality,
 n
X
i=2
[Î¸ÌŒi,t+1]2
1/2
=
 n
X
i=2

Î»ÌƒiÎ¸2
i,t + E(vi, Î¸t, Î¸t)
2
1/2
â‰¤
 n
X
i=2
Î»Ìƒ2
i Î¸4
i,t
1/2
+
 n
X
i=2
E(vi, Î¸t, Î¸t)2
1/2
â‰¤ max
i6=1
Î»Ìƒi|Î¸i,t|
 n
X
i=2
Î¸2
i,t
1/2
+ Ëœ

= (1 âˆ’ Î¸2
1,t)1/2
Â·

max
i6=1
Î»Ìƒi|Î¸i,t| + Ëœ
/(1 âˆ’ Î¸2
1,t)1/2

. (19)
Combining (17) and (19) gives
|Î¸1,t+1|
(1 âˆ’ Î¸2
1,t+1)1/2
=
|Î¸ÌŒ1,t+1|
Pn
i=2[Î¸ÌŒi,t+1]2
1/2
â‰¥
|Î¸1,t|
(1 âˆ’ Î¸2
1,t)1/2
Â·
Î»Ìƒ1|Î¸1,t| âˆ’ Ëœ
/|Î¸1,t|
maxi6=1 Î»Ìƒi|Î¸i,t| + Ëœ
/(1 âˆ’ Î¸2
1,t)1/2
.
In terms of Rt+1, Rt, Î³t, and Î´t, this reads
Rt+1 â‰¥
1 âˆ’ Î´t
(1 âˆ’ Î³t)
1âˆ’Î¸2
1,t
Î¸2
1,t
1/2
+ Î´t
= Rt Â·
1 âˆ’ Î´t
1 âˆ’ Î³t + Î´tRt
=
1 âˆ’ Î´t
1âˆ’Î³t
Rt
+ Î´t
â‰¥
1 âˆ’ Î´t
Îº
R2
t
+ Î´t
where the last inequality follows from Proposition B.1.
Lemma B.2 Fix any Ï > 1. Assume
0 â‰¤ Î´t < min
n 1
2(1 + 2ÎºÏ2)
,
1 âˆ’ 1/Ï
1 + ÎºÏ
o
and Î³t > 2(1 + 2ÎºÏ2)Î´t.
1. If r2
i,t â‰¤ 2Ï2, then ri,t+1 â‰¥ |ri,t| 1 + Î³t
2

.
2807
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
2. If Ï2 < r2
i,t, then ri,t+1 â‰¥ min{r2
i,t/Ï, 1âˆ’Î´tâˆ’1/Ï
ÎºÎ´t
}.
3. Î³t+1 â‰¥ min{Î³t, 1 âˆ’ 1/Ï}.
4. If mini6=1 r2
i,t > (Ï(1 âˆ’ Î´t) âˆ’ 1)/(ÎºÎ´t), then Rt+1 > 1âˆ’Î´tâˆ’1/Ï
ÎºÎ´t
Â· Î»Ìƒmin
Î»Ìƒ1
Â· 1
âˆš
k
.
5. If Rt â‰¤ 1 + 2ÎºÏ2, then Rt+1 â‰¥ Rt 1 + Î³t
3

, Î¸2
1,t+1 â‰¥ Î¸2
1,t, and Î´t+1 â‰¤ Î´t.
Proof Consider two (overlapping) cases depending on r2
i,t.
â€¢ Case 1: r2
i,t â‰¤ 2Ï2. By (15) from Proposition B.2,
ri,t+1 â‰¥ r2
i,t Â·
1 âˆ’ Î´t
1 + ÎºÎ´tr2
i,t
â‰¥ |ri,t| Â·
1
1 âˆ’ Î³t
Â·
1 âˆ’ Î´t
1 + 2ÎºÏ2Î´t
â‰¥ |ri,t|

1 +
Î³t
2

where the last inequality uses the assumption Î³t > 2(1+2ÎºÏ2)Î´t. This proves the first
claim.
â€¢ Case 2: Ï2 < r2
i,t. We split into two sub-cases. Suppose r2
i,t â‰¤ (Ï(1 âˆ’ Î´t) âˆ’ 1)/(ÎºÎ´t).
Then, by (15),
ri,t+1 â‰¥ r2
i,t Â·
1 âˆ’ Î´t
1 + ÎºÎ´tr2
i,t
â‰¥ r2
i,t Â·
1 âˆ’ Î´t
1 + ÎºÎ´t
Ï(1âˆ’Î´t)âˆ’1
ÎºÎ´t
=
r2
i,t
Ï
.
Now suppose instead r2
i,t > (Ï(1 âˆ’ Î´t) âˆ’ 1)/(ÎºÎ´t). Then
ri,t+1 â‰¥
1 âˆ’ Î´t
ÎºÎ´t
Ï(1âˆ’Î´t)âˆ’1 + ÎºÎ´t
=
1 âˆ’ Î´t âˆ’ 1/Ï
ÎºÎ´t
. (20)
Observe that if mini6=1 r2
i,t â‰¤ (Ï(1 âˆ’ Î´t) âˆ’ 1)/(ÎºÎ´t), then ri,t+1 â‰¥ |ri,t| for all i âˆˆ [k], and
hence Î³t+1 â‰¥ Î³t. Otherwise we have Î³t+1 > 1 âˆ’ ÎºÎ´t
1âˆ’Î´tâˆ’1/Ï > 1 âˆ’ 1/Ï. This proves the third
claim.
If mini6=1 r2
i,t > (Ï(1 âˆ’ Î´t) âˆ’ 1)/(ÎºÎ´t), then we may apply the inequality (20) from the
second sub-case of Case 2 above to get
Rt+1 =
1
P
i6=1(Î»Ìƒ1/Î»Ìƒi)2/r2
i,t+1
1/2
>

1 âˆ’ Î´t âˆ’ 1/Ï
ÎºÎ´t

Â·
Î»Ìƒmin
Î»Ìƒ1
Â·
1
âˆš
k
.
This proves the fourth claim.
Finally, for the last claim, if Rt â‰¤ 1 + 2ÎºÏ2, then by (16) from Proposition B.2 and the
assumption Î³t > 2(1 + 2ÎºÏ2)Î´t,
Rt+1 â‰¥ Rt Â·
1 âˆ’ Î´t
1 âˆ’ Î³t + Î´tRt
â‰¥ Rt Â·
1 âˆ’ Î³t
2(1+2ÎºÏ2)
1 âˆ’ Î³t/2
â‰¥ Rt

1 + Î³t Â·
ÎºÏ2
1 + 2ÎºÏ2

â‰¥ Rt

1 +
Î³t
3

.
This in turn implies that Î¸2
1,t+1 â‰¥ Î¸2
1,t via Proposition B.1, and thus Î´t+1 â‰¤ Î´t.
2808
Tensor Decompositions for Learning Latent Variable Models
Lemma B.3 Assume 0 â‰¤ Î´t < 1/2 and Î³t > 0. Pick any Î² > Î± > 0 such that
Î±
(1 + Î±)(1 + Î±2)
â‰¥
Ëœ

Î³tÎ»Ìƒ1
,
Î±
2(1 + Î±)(1 + Î²2)
â‰¥
Ëœ

Î»Ìƒ1
.
1. If Rt â‰¥ 1/Î±, then Rt+1 â‰¥ 1/Î±.
2. If 1/Î± > Rt â‰¥ 1/Î², then Rt+1 â‰¥ min{R2
t /(2Îº), 1/Î±}.
Proof Observe that for any c > 0,
Rt â‰¥
1
c
â‡” Î¸2
1,t â‰¥
1
1 + c2
â‡” Î´t â‰¤
(1 + c2)Ëœ

Î»Ìƒ1
. (21)
Now consider the following cases depending on Rt.
â€¢ Case 1: Rt â‰¥ 1/Î±. In this case, we have
Î´t â‰¤
(1 + Î±2)Ëœ

Î»Ìƒ1
â‰¤
Î±Î³t
1 + Î±
by (21) (with c = Î±) and the condition on Î±. Combining this with (16) from Propo-
sition B.2 gives
Rt+1 â‰¥
1 âˆ’ Î´t
1âˆ’Î³t
Rt
+ Î´t
â‰¥
1 âˆ’ Î±Î³t
1+Î±
(1 âˆ’ Î³t)Î± + Î±Î³t
1+Î±
=
1
Î±
.
â€¢ Case 2: 1/Î² â‰¤ Rt < 1/Î±. In this case, we have
Î´t â‰¤
(1 + Î²2)Ëœ

Î»Ìƒ1
â‰¤
Î±
2(1 + Î±)
by (21) (with c = Î²) and the conditions on Î± and Î². If Î´t â‰¥ 1/(2 + R2
t /Îº), then (16)
implies
Rt+1 â‰¥
1 âˆ’ Î´t
Îº
R2
t
+ Î´t
â‰¥
1 âˆ’ 2Î´t
2Î´t
â‰¥
1 âˆ’ Î±
1+Î±
Î±
1+Î±
=
1
Î±
.
If instead Î´t < 1/(2 + R2
t /Îº), then (16) implies
Rt+1 â‰¥
1 âˆ’ Î´t
Îº
R2
t
+ Î´t
>
1 âˆ’ 1
2+R2
t /Îº
Îº
R2
t
+ 1
2+R2
t /Îº
=
R2
t
2Îº
.
2809
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
B.2.1 Approximate Recovery of a Single Eigenvector
We now state the main result regarding the approximate recovery of a single eigenvector
using the tensor power method on TÌƒ. Here, we exploit the special properties of the error
EÌƒâ€”both (12) and (13).
Lemma B.4 There exists a universal constant C > 0 such that the following holds. Let
iâˆ— := arg maxiâˆˆ[k] Î»Ìƒi|Î¸i,0|. If
Ëœ
 <
Î³0
2(1 + 8Îº)
Â· Î»Ìƒmin Â· Î¸2
iâˆ—,0 and N â‰¥ C Â·

log(kÎº)
Î³0
+ log log
pÎ»Ìƒiâˆ—
Ëœ


,
then after t â‰¥ N iterations of the tensor power method on tensor TÌƒ as defined in (11) and
satisfying (12) and (13), the final vector Î¸t satisfies
Î¸iâˆ—,t â‰¥
s
1 âˆ’

3Ëœ

pÎ»Ìƒiâˆ—
2
, kÎ¸t âˆ’ viâˆ— k â‰¤
4Ëœ

pÎ»Ìƒiâˆ—
, |TÌƒ(Î¸t, Î¸t, Î¸t) âˆ’ Î»Ìƒiâˆ— | â‰¤

27Îº
 Ëœ

pÎ»iâˆ—
2
+ 2

Ëœ

p
.
Proof Assume without loss of generality that iâˆ— = 1. We consider three phases: (i)
iterations before the first time t such that Rt > 1 + 2ÎºÏ2 = 1 + 8Îº (using Ï := 2), (ii) the
subsequent iterations before the first time t such that Rt â‰¥ 1/Î± (where Î± will be defined
below), and finally (iii) the remaining iterations.
We begin by analyzing the first phase, i.e., the iterates in T1 := {t â‰¥ 0 : Rt â‰¤ 1+2ÎºÏ2 =
1 + 8Îº}. Observe that the condition on Ëœ
 implies
Î´0 =
Ëœ

Î»Ìƒ1Î¸2
1,0
<
Î³0
2(1 + 8Îº)
Â·
Î»Ìƒmin
Î»Ìƒ1
â‰¤ min

Î³0
2(1 + 2ÎºÏ2)
,
1 âˆ’ 1/Ï
2(1 + 2ÎºÏ2)

,
and hence the preconditions on Î´t and Î³t of Lemma B.2 hold for t = 0. For all t âˆˆ T1
satisfying the preconditions, Lemma B.2 implies that Î´t+1 â‰¤ Î´t and Î³t+1 â‰¥ min{Î³t, 1âˆ’1/Ï},
so the next iteration also satisfies the preconditions. Hence by induction, the preconditions
hold for all iterations in T1. Moreover, for all i âˆˆ [k], we have
|ri,0| â‰¥
1
1 âˆ’ Î³0
;
and while t âˆˆ T1: (i) |ri,t| increases at a linear rate while r2
i,t â‰¤ 2Ï2, and (ii) |ri,t| increases
at a quadratic rate while Ï2 â‰¤ r2
i,t â‰¤ 1âˆ’Î´tâˆ’1/Ï
ÎºÎ´t
. (The specific rates are given, respectively,
in Lemma B.2, claims 1 and 2.) Since 1âˆ’Î´tâˆ’1/Ï
ÎºÎ´t
â‰¤ Î»Ìƒ1
2ÎºËœ
 , it follows that mini6=1 r2
i,t â‰¤ 1âˆ’Î´tâˆ’1/Ï
ÎºÎ´t
for at most
2
Î³0
ln
p
2Ï2
1
1âˆ’Î³0

+ ln

ln Î»Ìƒ1
2ÎºËœ

ln
âˆš
2

= O

1
Î³0
+ log log
Î»Ìƒ1
Ëœ


(22)
iterations in T1. As soon as mini6=1 r2
i,t > 1âˆ’Î´tâˆ’1/Ï
ÎºÎ´t
, we have that in the next iteration,
Rt+1 >
1 âˆ’ Î´t âˆ’ 1/Ï
ÎºÎ´t
Â·
Î»Ìƒmin
Î»Ìƒ1
Â·
1
âˆš
k
â‰¥
7
âˆš
k
;
2810
Tensor Decompositions for Learning Latent Variable Models
and all the while Rt is growing at a linear rate (given in Lemma B.2, claim 5). Therefore,
there are at most an additional
1 +
3
Î³0
ln

1 + 8Îº
7/
âˆš
k

= O

log(kÎº)
Î³0

(23)
iterations in T1 over that counted in (22). Therefore, by combining the counts in (22)
and (23), we have that the number of iterations in the first phase satisfies
|T1| = O

log log
Î»Ìƒ1
Ëœ

+
log(kÎº)
Î³0

.
We now analyze the second phase, i.e., the iterates in T2 := {t â‰¥ 0 : t /
âˆˆ T1, Rt < 1/Î±}.
Define
Î± :=
3Ëœ

Î»Ìƒ1
, Î² :=
1
1 + 2ÎºÏ2
=
1
1 + 8Îº
.
Note that for the initial iteration t0 := min T2, we have that Rt0 â‰¥ 1 + 2ÎºÏ2 = 1 + 8Îº = 1/Î²,
and by Proposition B.1, Î³t0 â‰¥ 1 âˆ’ Îº/(1 + 8Îº) > 7/8. It can be checked that Î´t, Î³t, Î±,
and Î² satisfy the preconditions of Lemma B.3 for this initial iteration t0. For all t âˆˆ T2
satisfying these preconditions, Lemma B.3 implies that Rt+1 â‰¥ min{Rt, 1/Î±}, Î¸2
1,t+1 â‰¥
min{Î¸2
1,t, 1/(1+Î±2)} (via Proposition B.1), Î´t+1 â‰¤ max{Î´t, (1+Î±)2Ëœ
/Î»Ìƒ1} (using the definition
of Î´t), and Î³t+1 â‰¥ min{Î³t, 1 âˆ’ Î±Îº} (via Proposition B.1). Hence the next iteration t + 1
also satisfies the preconditions, and by induction, so do all iterations in T2. To bound the
number of iterations in T2, observe that Rt increases at a quadratic rate until Rt â‰¥ 1/Î±, so
|T2| â‰¤ ln

ln(1/Î±)
ln((1/Î²)/(2Îº))

< ln

ln Î»Ìƒ1
3Ëœ

ln 4

= O

log log
Î»Ìƒ1
Ëœ


. (24)
Therefore the total number of iterations before Rt â‰¥ 1/Î± is
O

log(kÎº)
Î³0
+ log log
Î»Ìƒ1
Ëœ


.
After Rt00 â‰¥ 1/Î± (for t00 := max(T1 âˆª T2) + 1), we have
Î¸2
1,t00 â‰¥
1/Î±2
1 + 1/Î±2
â‰¥ 1 âˆ’ Î±2
â‰¥ 1 âˆ’

3Ëœ

Î»Ìƒ1
2
.
Therefore, the vector Î¸t00 satisfies the condition for property (13) of EÌƒ to hold. Now we
apply Lemma B.3 using Ëœ
/p in place of Ëœ
, including in the definition of Î´t (which we call Î´t):
Î´t :=
Ëœ

pÎ»Ìƒ1Î¸2
1,t
;
we also replace Î± and Î² with Î± and Î², which we set to
Î± :=
3Ëœ

pÎ»Ìƒ1
, Î² :=
3Ëœ

Î»Ìƒ1
.
2811
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
It can be checked that Î´t00 âˆˆ (0, 1/2), Î³t00 â‰¥ 1 âˆ’ 3Ëœ
Îº/Î»1 > 0,
Î±
(1 + Î±)(1 + Î±2)
â‰¥
Ëœ

p(1 âˆ’ 3Ëœ
Îº/Î»1)Î»Ìƒ1
â‰¥
Ëœ

pÎ³t00 Î»Ìƒ1
,
Î±
2(1 + Î±)(1 + Î²
2
)
â‰¥
Ëœ

pÎ»Ìƒ1
.
Therefore, the preconditions of Lemma B.3 are satisfied for the initial iteration t00 in this
final phase, and by the same arguments as before, the preconditions hold for all subsequent
iterations t â‰¥ t00. Initially, we have Rt00 â‰¥ 1/Î± â‰¥ 1/Î², and by Lemma B.3, we have that Rt
increases at a quadratic rate in this final phase until Rt â‰¥ 1/Î±. So the number of iterations
before Rt â‰¥ 1/Î± can be bounded as
ln

ln(1/Î±)
ln((1/Î²)/(2Îº))

= ln

ln pÎ»Ìƒ1
3Ëœ

ln

Î»1
3Ëœ
 Â· 1
2Îº


â‰¤ ln ln
pÎ»Ìƒ1
3Ëœ

= O

log log
pÎ»Ìƒ1
Ëœ


.
Once Rt â‰¥ 1/Î±, we have
Î¸2
1,t â‰¥ 1 âˆ’

3Ëœ

pÎ»Ìƒ1
2
.
Since sign(Î¸1,t) = r1,t â‰¥ r2
1,tâˆ’1 Â· (1 âˆ’ Î´tâˆ’1)/(1 + ÎºÎ´tâˆ’1r2
1,tâˆ’1) = (1 âˆ’ Î´tâˆ’1)/(1 + ÎºÎ´tâˆ’1) > 0 by
Proposition B.2, we have Î¸1,t > 0. Therefore we can conclude that
kÎ¸t âˆ’ v1k =
q
2(1 âˆ’ Î¸1,t) â‰¤
s
2

1 âˆ’
q
1 âˆ’ (3Ëœ
/(pÎ»Ìƒ1))2

â‰¤ 4Ëœ
/(pÎ»Ìƒ1).
Finally,
|TÌƒ(Î¸t, Î¸t, Î¸t) âˆ’ Î»Ìƒ1| = Î»Ìƒ1(Î¸3
1,t âˆ’ 1) +
k
X
i=2
Î»ÌƒiÎ¸3
i,t + EÌƒ(Î¸t, Î¸t, Î¸t)
â‰¤ Î»Ìƒ1|Î¸3
1,t âˆ’ 1| +
k
X
i=2
Î»Ìƒi|Î¸i,t|Î¸2
i,t + kEÌƒ(I, Î¸t, Î¸t)k
â‰¤ Î»Ìƒ1 1 âˆ’ Î¸1,t + |Î¸1,t(1 âˆ’ Î¸2
1,t)|

+ max
i6=1
Î»Ìƒi|Î¸i,t|
k
X
i=2
Î¸2
i,t + kEÌƒ(I, Î¸t, Î¸t)k
â‰¤ Î»Ìƒ1 1 âˆ’ Î¸1,t + |Î¸1,t(1 âˆ’ Î¸2
1,t)|

+ max
i6=1
Î»Ìƒi
q
1 âˆ’ Î¸2
1,t
k
X
i=2
Î¸2
i,t + kEÌƒ(I, Î¸t, Î¸t)k
= Î»Ìƒ1 1 âˆ’ Î¸1,t + |Î¸1,t(1 âˆ’ Î¸2
1,t)|

+ max
i6=1
Î»Ìƒi(1 âˆ’ Î¸2
1,t)3/2
+ kEÌƒ(I, Î¸t, Î¸t)k
â‰¤ Î»Ìƒ1 Â· 3

3Ëœ

pÎ»Ìƒ1
2
+ ÎºÎ»Ìƒ1 Â·

3Ëœ

pÎ»Ìƒ1
3
+
Ëœ

p
â‰¤
(27Îº Â· (Ëœ
/pÎ»Ìƒ1)2 + 2)Ëœ

p
.
2812
Tensor Decompositions for Learning Latent Variable Models
B.3 Deflation
Lemma B.5 Fix some Ëœ
 â‰¥ 0. Let {v1, v2, . . . , vk} be an orthonormal basis for Rk, and
Î»1, Î»2, . . . , Î»k â‰¥ 0 with Î»min := miniâˆˆ[k] Î»i. Also, let {vÌ‚1, vÌ‚2, . . . , vÌ‚k} be a set of unit vectors
in Rk (not necessarily orthogonal), Î»Ì‚1, Î»Ì‚2, . . . , Î»Ì‚k â‰¥ 0 be non-negative scalars, and define
Ei := Î»ivâŠ—3
i âˆ’ Î»Ì‚ivÌ‚âŠ—3
i , i âˆˆ [k].
Pick any t âˆˆ [k]. If
|Î»Ì‚i âˆ’ Î»i| â‰¤ Ëœ
,
kvÌ‚i âˆ’ vik â‰¤ min{
âˆš
2, 2Ëœ
/Î»i}
for all i âˆˆ [t], then for any unit vector u âˆˆ Skâˆ’1,
t
X
i=1
Ei(I, u, u)
2
2
â‰¤

4(5 + 11Ëœ
/Î»min)2
+ 128(1 + Ëœ
/Î»min)2
(Ëœ
/Î»min)2

Ëœ
2
t
X
i=1
(u>
vi)2
+ 64(1 + Ëœ
/Î»min)2
Ëœ
2
t
X
i=1
(Ëœ
/Î»i)2
+ 2048(1 + Ëœ
/Î»min)2
Ëœ
2
 t
X
i=1
(Ëœ
/Î»i)3
2
.
In particular, for any âˆ† âˆˆ (0, 1), there exists a constant âˆ†0 > 0 (depending only on âˆ†) such
that Ëœ
 â‰¤ âˆ†0Î»min/
âˆš
k implies
t
X
i=1
Ei(I, u, u)
2
2
â‰¤

âˆ† + 100
t
X
i=1
(u>
vi)2

Ëœ
2
.
Proof For any unit vector u and i âˆˆ [t], the error term
Ei(I, u, u) = Î»i(u>
vi)2
vi âˆ’ Î»Ì‚i(u>
vÌ‚i)2
vÌ‚i
lives in span{vi, vÌ‚i}; this space is the same as span{vi, vÌ‚âŠ¥
i }, where
vÌ‚âŠ¥
i := vÌ‚i âˆ’ (v>
i vÌ‚i)vi
is the projection of vÌ‚i onto the subspace orthogonal to vi. Since kvÌ‚i âˆ’ vik2 = 2(1 âˆ’ v>
i vÌ‚i), it
follows that
ci := v>
i vÌ‚i = 1 âˆ’ kvÌ‚i âˆ’ vik2
/2 â‰¥ 0
(the inequality follows from the assumption kvÌ‚iâˆ’vik â‰¤
âˆš
2, which in turn implies 0 â‰¤ ci â‰¤ 1).
By the Pythagorean theorem and the above inequality for ci,
kvÌ‚âŠ¥
i k2
= 1 âˆ’ c2
i â‰¤ kvÌ‚i âˆ’ vik2
.
Later, we will also need the following bound, which is easily derived from the above inequal-
ities and the triangle inequality:
|1 âˆ’ c3
i | = |1 âˆ’ ci + ci(1 âˆ’ c2
i )| â‰¤ 1 âˆ’ ci + |ci(1 âˆ’ c2
i )| â‰¤ 1.5kvÌ‚i âˆ’ vik2
.
2813
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
We now express Ei(I, u, u) in terms of the coordinate system defined by vi and vÌ‚âŠ¥
i ,
depicted below. Define
ai := u>
vi and bi := u>
vÌ‚âŠ¥
i /kvÌ‚âŠ¥
i k

.
(Note that the part of u living in span{vi, vÌ‚âŠ¥
i }âŠ¥ is irrelevant for analyzing Ei(I, u, u).) We
have
Ei(I, u, u) = Î»i(u>
vi)2
vi âˆ’ Î»Ì‚i(u>
vÌ‚i)2
vÌ‚i
= Î»ia2
i vi âˆ’ Î»Ì‚i aici + kvÌ‚âŠ¥
i kbi
2
civi + vÌ‚âŠ¥
i

= Î»ia2
i vi âˆ’ Î»Ì‚i a2
i c2
i + 2kvÌ‚âŠ¥
i kaibici + kvÌ‚âŠ¥
i k2
b2
i

civi âˆ’ Î»Ì‚i aici + kvÌ‚âŠ¥
i kbi
2
vÌ‚âŠ¥
i
=

(Î»i âˆ’ Î»Ì‚ic3
i )a2
i âˆ’ 2Î»Ì‚ikvÌ‚âŠ¥
i kaibic2
i âˆ’ Î»Ì‚ikvÌ‚âŠ¥
i k2
b2
i ci

| {z }
=:Ai
vi âˆ’ Î»Ì‚ikvÌ‚âŠ¥
i k aici + kvÌ‚âŠ¥
i kbi
2
| {z }
=:Bi
vÌ‚âŠ¥
i /kvÌ‚âŠ¥
i k

= Aivi âˆ’ Bi vÌ‚âŠ¥
i /kvÌ‚âŠ¥
i k

.
The overall error can also be expressed in terms of the Ai and Bi:
t
X
i=1
Ei(I, u, u)
2
2
=
t
X
i=1
Aivi âˆ’
t
X
i=1
Bi(vÌ‚âŠ¥
i /kvÌ‚âŠ¥
i k)
2
2
â‰¤ 2
t
X
i=1
Aivi
2
+ 2
t
X
i=1
Bi(vÌ‚âŠ¥
i /kvÌ‚âŠ¥
i k)
2
2
â‰¤ 2
t
X
i=1
A2
i + 2
 t
X
i=1
|Bi|
2
(25)
where the first inequality uses the fact (x + y)2 â‰¤ 2(x2 + y2) and the triangle inequality,
and the second inequality uses the orthonormality of the vi and the triangle inequality.
It remains to bound A2
i and |Bi| in terms of |ai|, Î»i, and Ëœ
. The first term, A2
i , can be
bounded using the triangle inequality and the various bounds on |Î»i âˆ’ Î»Ì‚i|, kvÌ‚i âˆ’ vik, kvÌ‚âŠ¥
i k,
and ci:
|Ai| â‰¤ (|Î»i âˆ’ Î»Ì‚i|c3
i + Î»i|c3
i âˆ’ 1|)a2
i + 2(Î»i + |Î»i âˆ’ Î»Ì‚i|)kvÌ‚âŠ¥
i k|aibi|c2
i + (Î»i + |Î»i âˆ’ Î»Ì‚i|)kvÌ‚âŠ¥
i k2
b2
i ci
â‰¤ (|Î»i âˆ’ Î»Ì‚i| + 1.5Î»ikvÌ‚i âˆ’ vik2
+ 2(Î»i + |Î»i âˆ’ Î»Ì‚i|)kvÌ‚i âˆ’ vik)|ai| + (Î»i + |Î»i âˆ’ Î»Ì‚i|)kvÌ‚i âˆ’ vik2
â‰¤ (Ëœ
 + 7Ëœ
2
/Î»i + 4Ëœ
 + 4Ëœ
2
/Î»i)|ai| + 4Ëœ
2
/Î»i + Ëœ
3
/Î»2
i
= (5 + 11Ëœ
/Î»i)Ëœ
|ai| + 4(1 + Ëœ
/Î»i)Ëœ
2
/Î»i,
and therefore (via (x + y)2 â‰¤ 2(x2 + y2))
A2
i â‰¤ 2(5 + 11Ëœ
/Î»i)2
Ëœ
2
a2
i + 32(1 + Ëœ
/Î»i)2
Ëœ
4
/Î»2
i .
The second term, |Bi|, is bounded similarly:
|Bi| â‰¤ 2(Î»i + |Î»i âˆ’ Î»Ì‚i|)kvÌ‚âŠ¥
i k2
(a2
i + kvÌ‚âŠ¥
i k2
)
â‰¤ 2(Î»i + |Î»i âˆ’ Î»Ì‚i|)kvÌ‚i âˆ’ vik2
(a2
i + kvÌ‚i âˆ’ vik2
)
â‰¤ 8(1 + Ëœ
/Î»i)(Ëœ
2
/Î»i)a2
i + 32(1 + Ëœ
/Î»i)Ëœ
4
/Î»3
i .
2814
Tensor Decompositions for Learning Latent Variable Models
Therefore, using the inequality from (25) and again (x + y)2 â‰¤ 2(x2 + y2),
t
X
i=1
Ei(I, u, u)
2
2
â‰¤ 2
t
X
i=1
A2
i + 2
 t
X
i=1
|Bi|
2
â‰¤ 4(5 + 11Ëœ
/Î»min)2
Ëœ
2
t
X
i=1
a2
i + 64(1 + Ëœ
/Î»min)2
Ëœ
2
t
X
i=1
(Ëœ
/Î»i)2
+ 2

8(1 + Ëœ
/Î»min)(Ëœ
2
/Î»min)
t
X
i=1
a2
i + 32(1 + Ëœ
/Î»min)Ëœ

t
X
i=1
(Ëœ
/Î»i)3
2
â‰¤ 4(5 + 11Ëœ
/Î»min)2
Ëœ
2
t
X
i=1
a2
i + 64(1 + Ëœ
/Î»min)2
Ëœ
2
t
X
i=1
(Ëœ
/Î»i)2
+ 128(1 + Ëœ
/Î»min)2
(Ëœ
/Î»min)2
Ëœ
2
t
X
i=1
a2
i
+ 2048(1 + Ëœ
/Î»min)2
Ëœ
2
 t
X
i=1
(Ëœ
/Î»i)3
2
=

4(5 + 11Ëœ
/Î»min)2
+ 128(1 + Ëœ
/Î»min)2
(Ëœ
/Î»min)2

Ëœ
2
t
X
i=1
a2
i
+ 64(1 + Ëœ
/Î»min)2
Ëœ
2
t
X
i=1
(Ëœ
/Î»i)2
+ 2048(1 + Ëœ
/Î»min)2
Ëœ
2
 t
X
i=1
(Ëœ
/Î»i)3
2
.
B.4 Proof of the Main Theorem
Theorem B.1 Let TÌ‚ = T + E âˆˆ RkÃ—kÃ—k, where T is a symmetric tensor with orthogonal
decomposition T =
Pk
i=1 Î»ivâŠ—3
i where each Î»i > 0, {v1, v2, . . . , vk} is an orthonormal basis,
and E has operator norm  := kEk. Define Î»min := min{Î»i : i âˆˆ [k]}, and Î»max := max{Î»i :
i âˆˆ [k]}. There exists universal constants C1, C2, C3 > 0 such that the following holds. Pick
any Î· âˆˆ (0, 1), and suppose
 â‰¤ C1 Â·
Î»min
k
, N â‰¥ C2 Â·

log(k) + log log
Î»max


,
and
s
ln(L/ log2(k/Î·))
ln(k)
Â· 1 âˆ’
ln(ln(L/ log2(k/Î·))) + C3
4 ln(L/ log2(k/Î·))
âˆ’
s
ln(8)
ln(L/ log2(k/Î·))
!
â‰¥ 1.02 1 +
s
ln(4)
ln(k)
!
.
2815
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
(Note that the condition on L holds with L = poly(k) log(1/Î·).) Suppose that Algorithm 1
is iteratively called k times, where the input tensor is TÌ‚ in the first call, and in each
subsequent call, the input tensor is the deflated tensor returned by the previous call. Let
(vÌ‚1, Î»Ì‚1), (vÌ‚2, Î»Ì‚2), . . . , (vÌ‚k, Î»Ì‚k) be the sequence of estimated eigenvector/eigenvalue pairs re-
turned in these k calls. With probability at least 1 âˆ’ Î·, there exists a permutation Ï€ on [k]
such that
kvÏ€(j) âˆ’ vÌ‚jk â‰¤ 8/Î»Ï€(j), |Î»Ï€(j) âˆ’ Î»Ì‚j| â‰¤ 5, âˆ€j âˆˆ [k],
and
T âˆ’
k
X
j=1
Î»Ì‚jvÌ‚âŠ—3
j â‰¤ 55.
Proof We prove by induction that for each i âˆˆ [k] (corresponding to the i-th call to
Algorithm 1), with probability at least 1 âˆ’ iÎ·/k, there exists a permutation Ï€ on [k] such
that the following assertions hold.
1. For all j â‰¤ i, kvÏ€(j) âˆ’ vÌ‚jk â‰¤ 8/Î»Ï€(j) and |Î»Ï€(j) âˆ’ Î»Ì‚j| â‰¤ 12.
2. The error tensor
EÌƒi+1 :=

TÌ‚ âˆ’
X
jâ‰¤i
Î»Ì‚jvÌ‚âŠ—3
j

âˆ’
X
jâ‰¥i+1
Î»Ï€(j)vâŠ—3
Ï€(j) = E +
X
jâ‰¤i

Î»Ï€(j)vâŠ—3
Ï€(j) âˆ’ Î»Ì‚jvÌ‚âŠ—3
j

satisfies
kEÌƒi+1(I, u, u)k â‰¤ 56, âˆ€u âˆˆ Skâˆ’1
; (26)
kEÌƒi+1(I, u, u)k â‰¤ 2, âˆ€u âˆˆ Skâˆ’1
s.t. âˆƒj â‰¥ i + 1  (u>
vÏ€(j))2
â‰¥ 1 âˆ’ (168/Î»Ï€(j))2
.
(27)
We actually take i = 0 as the base case, so we can ignore the first assertion, and just observe
that for i = 0,
EÌƒ1 = TÌ‚ âˆ’
k
X
j=1
Î»ivâŠ—3
i = E.
We have kEÌƒ1k = kEk = , and therefore the second assertion holds.
Now fix some i âˆˆ [k], and assume as the inductive hypothesis that, with probability at
least 1 âˆ’ (i âˆ’ 1)Î·/k, there exists a permutation Ï€ such that two assertions above hold for
i âˆ’ 1 (call this Eventiâˆ’1). The i-th call to Algorithm 1 takes as input
TÌƒi := TÌ‚ âˆ’
X
jâ‰¤iâˆ’1
Î»Ì‚jvÌ‚âŠ—3
j ,
which is intended to be an approximation to
Ti :=
X
jâ‰¥i
Î»Ï€(j)vâŠ—3
Ï€(j).
2816
Tensor Decompositions for Learning Latent Variable Models
Observe that
TÌƒi âˆ’ Ti = EÌƒi,
which satisfies the second assertion in the inductive hypothesis. We may write Ti =
Pk
l=1 Î»ÌƒlvâŠ—3
l where Î»Ìƒl = Î»l whenever Ï€âˆ’1(l) â‰¥ i, and Î»Ìƒl = 0 whenever Ï€âˆ’1(l) â‰¤ i âˆ’ 1. This
form is used when referring to TÌƒ or the Î»Ìƒi in preceding lemmas (in particular, Lemma B.1
and Lemma B.4).
By Lemma B.1, with conditional probability at least 1âˆ’Î·/k given Eventiâˆ’1, at least one
of Î¸
(Ï„)
0 for Ï„ âˆˆ [L] is Î³-separated relative to Ï€(jmax), where jmax := arg maxjâ‰¥i Î»Ï€(j), (for
Î³ = 0.01; call this Event0
i; note that the application of Lemma B.1 determines C3). Therefore
Pr[Eventiâˆ’1 âˆ© Event0
i] = Pr[Event0
i|Eventiâˆ’1] Pr[Eventiâˆ’1] â‰¥ (1 âˆ’ Î·/k)(1 âˆ’ (i âˆ’ 1)Î·/k) â‰¥
1 âˆ’ iÎ·/k. It remains to show that Eventiâˆ’1 âˆ© Event0
i âŠ† Eventi; so henceforth we condition
on Eventiâˆ’1 âˆ© Event0
i.
Set
C1 := min

(56 Â· 9 Â· 102)âˆ’1
, (100 Â· 168)âˆ’1
, âˆ†0
from Lemma B.5 with âˆ† = 1/50 . (28)
For all Ï„ âˆˆ [L] such that Î¸
(Ï„)
0 is Î³-separated relative to Ï€(jmax), we have (i) |Î¸
(Ï„)
jmax,0| â‰¥ 1/
âˆš
k,
and (ii) that by Lemma B.4 (using Ëœ
/p := 2, Îº := 1, and iâˆ— := Ï€(jmax), and providing C2),
|TÌƒi(Î¸
(Ï„)
N , Î¸
(Ï„)
N , Î¸
(Ï„)
N ) âˆ’ Î»Ï€(jmax)| â‰¤ 5
(notice by definition that Î³ â‰¥ 1/100 implies Î³0 â‰¥ 1 âˆ’ /(1 + Î³) â‰¥ 1/101, thus it follows
from the bounds on the other quantities that Ëœ
 = 2p â‰¤ 56C1 Â· Î»min
k < Î³0
2(1+8Îº) Â· Î»Ìƒmin Â· Î¸2
iâˆ—,0 as
necessary). Therefore Î¸N := Î¸
(Ï„âˆ—)
N must satisfy
TÌƒi(Î¸N , Î¸N , Î¸N ) = max
Ï„âˆˆ[L]
TÌƒi(Î¸
(Ï„)
N , Î¸
(Ï„)
N , Î¸
(Ï„)
N ) â‰¥ max
jâ‰¥i
Î»Ï€(j) âˆ’ 5 = Î»Ï€(jmax) âˆ’ 5.
On the other hand, by the triangle inequality,
TÌƒi(Î¸N , Î¸N , Î¸N ) â‰¤
X
jâ‰¥i
Î»Ï€(j)Î¸3
Ï€(j),N + |EÌƒi(Î¸N , Î¸N , Î¸N )|
â‰¤
X
jâ‰¥i
Î»Ï€(j)|Î¸Ï€(j),N |Î¸2
Ï€(j),N + 56
â‰¤ Î»Ï€(jâˆ—)|Î¸Ï€(jâˆ—),N | + 56
where jâˆ— := arg maxjâ‰¥i Î»Ï€(j)|Î¸Ï€(j),N |. Therefore
Î»Ï€(jâˆ—)|Î¸Ï€(jâˆ—),N | â‰¥ Î»Ï€(jmax) âˆ’ 5 âˆ’ 56 â‰¥
4
5
Î»Ï€(jmax).
Squaring both sides and using the fact that Î¸2
Ï€(jâˆ—),N + Î¸2
Ï€(j),N â‰¤ 1 for any j 6= jâˆ—,
Î»Ï€(jâˆ—)Î¸Ï€(jâˆ—),N
2
â‰¥
16
25
Î»Ï€(jmax)Î¸Ï€(jâˆ—),N
2
+
16
25
Î»Ï€(jmax)Î¸Ï€(j),N
2
â‰¥
16
25
Î»Ï€(jâˆ—)Î¸Ï€(jâˆ—),N
2
+
16
25
Î»Ï€(j)Î¸Ï€(j),N
2
2817
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
which in turn implies
Î»Ï€(j)|Î¸Ï€(j),N | â‰¤
3
4
Î»Ï€(jâˆ—)|Î¸Ï€(jâˆ—),N |, j 6= jâˆ—
.
This means that Î¸N is (1/4)-separated relative to Ï€(jâˆ—). Also, observe that
|Î¸Ï€(jâˆ—),N | â‰¥
4
5
Â·
Î»Ï€(jmax)
Î»Ï€(jâˆ—)
â‰¥
4
5
,
Î»Ï€(jmax)
Î»Ï€(jâˆ—)
â‰¤
5
4
.
Therefore by Lemma B.4 (using Ëœ
/p := 2, Î³ := 1/4, and Îº := 5/4), executing another N
power iterations starting from Î¸N gives a vector Î¸Ì‚ that satisfies
kÎ¸Ì‚ âˆ’ vÏ€(jâˆ—)k â‰¤
8
Î»Ï€(jâˆ—)
, |Î»Ì‚ âˆ’ Î»Ï€(jâˆ—)| â‰¤ 5.
Since vÌ‚i = Î¸Ì‚ and Î»Ì‚i = Î»Ì‚, the first assertion of the inductive hypothesis is satisfied, as we
can modify the permutation Ï€ by swapping Ï€(i) and Ï€(jâˆ—) without affecting the values of
{Ï€(j) : j â‰¤ i âˆ’ 1} (recall jâˆ— â‰¥ i).
We now argue that EÌƒi+1 has the required properties to complete the inductive step. By
Lemma B.5 (using Ëœ
 := 5 and âˆ† := 1/50, the latter providing one upper bound on C1 as
per (28)), we have for any unit vector u âˆˆ Skâˆ’1,
X
jâ‰¤i

Î»Ï€(j)vâŠ—3
Ï€(j) âˆ’ Î»Ì‚jvÌ‚âŠ—3
j

(I, u, u) â‰¤

1/50 + 100
i
X
j=1
(u>
vÏ€(j))2
1/2
5 â‰¤ 55. (29)
Therefore by the triangle inequality,
kEÌƒi+1(I, u, u)k â‰¤ kE(I, u, u)k +
X
jâ‰¤i

Î»Ï€(j)vâŠ—3
Ï€(j) âˆ’ Î»Ì‚jvÌ‚âŠ—3
j

(I, u, u) â‰¤ 56.
Thus the bound (26) holds.
To prove that (27) holds, pick any unit vector u âˆˆ Skâˆ’1 such that there exists j0 â‰¥ i + 1
with (u>
vÏ€(j0))2 â‰¥ 1 âˆ’ (168/Î»Ï€(j0))2. We have, via the second bound on C1 in (28) and the
corresponding assumed bound  â‰¤ C1 Â· Î»min
k ,
100
i
X
j=1
(u>
vÏ€(j))2
â‰¤ 100

1 âˆ’ (u>
vÏ€(j0))2

â‰¤ 100

168
Î»Ï€(j0)
2
â‰¤
1
50
,
and therefore

1/50 + 100
i
X
j=1
(u>
vÏ€(j))2
1/2
5 â‰¤ (1/50 + 1/50)1/2
5 â‰¤ .
By the triangle inequality, we have kEÌƒi+1(I, u, u)k â‰¤ 2. Therefore (27) holds, so the
second assertion of the inductive hypothesis holds. Thus Eventiâˆ’1 âˆ© Event0
i âŠ† Eventi, and
Pr[Eventi] â‰¥ Pr[Eventiâˆ’1 âˆ© Event0
i] â‰¥ 1 âˆ’ iÎ·/k. We conclude that by the induction principle,
2818
Tensor Decompositions for Learning Latent Variable Models
there exists a permutation Ï€ such that two assertions hold for i = k, with probability at
least 1 âˆ’ Î·.
From the last induction step (i = k), it is also clear from (29) that kT âˆ’
Pk
j=1 Î»Ì‚jvÌ‚âŠ—3
j k â‰¤
55 (in Eventkâˆ’1 âˆ© Event0
k). This completes the proof of the theorem.
Appendix C. Variant of Robust Power Method that uses a Stopping
Condition
In this section we analyze a variant of Algorithm 1 that uses a stopping condition. The
variant is described in Algorithm 2. The key difference is that the inner for-loop is repeated
until a stopping condition is satisfied (rather than explicitly L times). The stopping condi-
tion ensures that the power iteration is converging to an eigenvector, and it will be satisfied
within poly(k) random restarts with high probability. The condition depends on one new
quantity, r, which should be set to r := k âˆ’ # deflation steps so far (i.e., the first call to
Algorithm 2 uses r = k, the second call uses r = k âˆ’ 1, and so on).
Algorithm 2 Robust tensor power method with stopping condition
input symmetric tensor TÌƒ âˆˆ RkÃ—kÃ—k, number of iterations N, expected rank r.
output the estimated eigenvector/eigenvalue pair; the deflated tensor.
1: repeat
2: Draw Î¸0 uniformly at random from the unit sphere in Rk.
3: for t = 1 to N do
4: Compute power iteration update
Î¸t :=
TÌƒ(I, Î¸tâˆ’1, Î¸tâˆ’1)
kTÌƒ(I, Î¸tâˆ’1, Î¸tâˆ’1)k
(30)
5: end for
6: until the following stopping condition is satisfied:
|TÌƒ(Î¸N , Î¸N , Î¸N )| â‰¥ max

1
2
âˆš
r
kTÌƒkF ,
1
1.05
kTÌƒ(I, I, Î¸N )kF

.
7: Do N power iteration updates (30) starting from Î¸N to obtain Î¸Ì‚, and set Î»Ì‚ := TÌƒ(Î¸Ì‚, Î¸Ì‚, Î¸Ì‚).
8: return the estimated eigenvector/eigenvalue pair (Î¸Ì‚, Î»Ì‚); the deflated tensor TÌƒ âˆ’Î»Ì‚ Î¸Ì‚âŠ—3.
C.1 Stopping Condition Analysis
For a matrix A, we use kAkF := (
P
i,j A2
i,j)1/2 to denote its Frobenius norm. For a third-
order tensor A, we use kAkF := (
P
i kA(I, I, ei)k2
F )1/2 = (
P
i kA(I, I, vi)k2
F )1/2.
Define TÌƒ as before in (11):
TÌƒ :=
k
X
i=1
Î»ÌƒivâŠ—3
i + EÌƒ.
2819
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
We assume EÌƒ is a symmetric tensor such that, for some constant p > 1,
kEÌƒ(I, u, u)k â‰¤ Ëœ
, âˆ€u âˆˆ Skâˆ’1
;
kEÌƒ(I, u, u)k â‰¤ Ëœ
/p, âˆ€u âˆˆ Skâˆ’1
s.t. (u>
v1)2
â‰¥ 1 âˆ’ (3Ëœ
/Î»Ìƒ1)2
;
kEÌƒkF â‰¤ Ëœ
F .
Assume that not all Î»Ìƒi are zero, and define
Î»Ìƒmin := min{Î»Ìƒi : i âˆˆ [k], Î»Ìƒi > 0}, Î»Ìƒmax := max{Î»Ìƒi : i âˆˆ [k]},
` := |{i âˆˆ [k] : Î»Ìƒi > 0}|, Î»Ìƒavg :=

1
`
k
X
i=1
Î»Ìƒ2
i
1/2
.
We show in Lemma C.1 that if the stopping condition is satisfied by a vector Î¸, then
it must be close to an eigenvector of TÌƒ. Then in Lemma C.2, we show that the stopping
condition is satisfied by Î¸N when Î¸0 is a good starting point (as per the conditions of
Lemma B.4).
Lemma C.1 Fix any vector Î¸ =
Pk
i=1 Î¸ivi, and let iâˆ— := arg maxiâˆˆ[k] Î»Ìƒi|Î¸i|. Assume that
` â‰¥ 1 and that for some Î± âˆˆ (0, 1/20) and Î² â‰¥ 2Î±/
âˆš
k,
Ëœ
 â‰¤ Î± Â·
Î»Ìƒmin
âˆš
k
, Ëœ
F â‰¤
âˆš
`
1
2
âˆ’
Î±
Î²
âˆš
k

Â· Î»Ìƒavg.
If the stopping condition
|TÌƒ(Î¸, Î¸, Î¸)| â‰¥ max

Î²
âˆš
`
kTÌƒkF ,
1
1 + Î±
kTÌƒ(I, I, Î¸)kF

(31)
holds, then
1. Î»Ìƒiâˆ— â‰¥ Î²Î»Ìƒavg/2 and Î»Ìƒiâˆ— |Î¸iâˆ— | > 0;
2. maxi6=iâˆ— Î»Ìƒi|Î¸i| â‰¤
âˆš
7Î± Â· Î»Ìƒiâˆ— |Î¸iâˆ— |;
3. Î¸iâˆ— â‰¥ 1 âˆ’ 2Î±.
Proof Without loss of generality, assume iâˆ— = 1. First, we claim that Î»Ìƒ1|Î¸1| > 0. By the
triangle inequality,
|TÌƒ(Î¸, Î¸, Î¸)| â‰¤
k
X
i=1
Î»ÌƒiÎ¸3
i + |EÌƒ(Î¸, Î¸, Î¸)| â‰¤
k
X
i=1
Î»Ìƒi|Î¸i|Î¸2
i + Ëœ
 â‰¤ Î»Ìƒ1|Î¸1| + Ëœ
.
2820
Tensor Decompositions for Learning Latent Variable Models
Moreover,
kTÌƒkF â‰¥
k
X
i=1
Î»ÌƒivâŠ—3
i
F
âˆ’ kEÌƒkF
=
 k
X
j=1
k
X
i=1
Î»Ìƒiviv>
i (v>
i vj)
2
F
1/2
âˆ’ kEÌƒkF
=
 k
X
j=1
Î»Ìƒjvjv>
j
2
F
1/2
âˆ’ kEÌƒkF
=
 k
X
j=1
Î»Ìƒ2
j
1/2
âˆ’ kEÌƒkF
â‰¥
âˆš
`Î»Ìƒavg âˆ’ Ëœ
F .
By assumption, |TÌƒ(Î¸, Î¸, Î¸)| â‰¥ (Î²/
âˆš
`)kTÌƒkF , so
Î»Ìƒ1|Î¸1| â‰¥ Î²Î»Ìƒavg âˆ’
Î²
âˆš
`
Ëœ
F âˆ’ Ëœ
 â‰¥ Î²Î»Ìƒavg âˆ’ Î²
1
2
âˆ’
Î±
Î²
âˆš
k

Î»Ìƒavg âˆ’
Î±
âˆš
k
Î»Ìƒmin â‰¥
Î²
2
Î»Ìƒavg
where the second inequality follows from the assumptions on Ëœ
 and Ëœ
F . Since Î² > 0, Î»Ìƒavg > 0,
and |Î¸1| â‰¤ 1, it follows that
Î»Ìƒ1 â‰¥
Î²
2
Î»Ìƒavg, Î»Ìƒ1|Î¸1| > 0.
This proves the first claim.
Now we prove the second claim. Define MÌƒ := TÌƒ(I, I, Î¸) =
Pk
i=1 Î»ÌƒiÎ¸iviv>
i + EÌƒ(I, I, Î¸) (a
symmetric k Ã— k matrix), and consider its eigenvalue decomposition
MÌƒ =
k
X
i=1
Ï†iuiu>
i
where, without loss of generality, |Ï†1| â‰¥ |Ï†2| â‰¥ Â· Â· Â· â‰¥ |Ï†k| and {u1, u2, . . . , uk} is an or-
thonormal basis. Let M :=
Pk
i=1 Î»ÌƒiÎ¸iviv>
i , so MÌƒ = M + EÌƒ(I, I, Î¸). Note that the Î»Ìƒi|Î¸i| and
|Ï†i| are the singular values of M and MÌƒ, respectively. We now show that the assumption
on |TÌƒ(Î¸, Î¸, Î¸)| implies that almost all of the energy in M is contained in its top singular
component.
By Weylâ€™s theorem,
|Ï†1| â‰¤ Î»Ìƒ1|Î¸1| + kMÌƒ âˆ’ Mk â‰¤ Î»Ìƒ1|Î¸1| + Ëœ
.
Next, observe that the assumption kTÌƒ(I, I, Î¸)kF â‰¤ (1 + Î±)TÌƒ(Î¸, Î¸, Î¸) is equivalent to (1 +
Î±)Î¸>
MÌƒÎ¸ â‰¥ kMÌƒkF . Therefore, using the fact that |Ï†1| = maxuâˆˆSkâˆ’1 |u>
MÌƒu|, the triangle
2821
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
inequality, and the fact kAkF â‰¤
âˆš
kkAk for any matrix A âˆˆ RkÃ—k,
(1 + Î±)|Ï†1| â‰¥ (1 + Î±)Î¸>
MÌƒÎ¸ â‰¥ kMÌƒkF (32)
â‰¥
k
X
i=1
Î»ÌƒiÎ¸iviv>
i
F
âˆ’ EÌƒ(I, I, Î¸) F
â‰¥
 k
X
i=1
Î»Ìƒ2
i Î¸2
i
1/2
âˆ’
âˆš
kkEÌƒ(I, I, Î¸)k
â‰¥
 k
X
i=1
Î»Ìƒ2
i Î¸2
i
1/2
âˆ’
âˆš
kËœ
.
Combining these bounds on |Ï†1| gives
Î»Ìƒ1|Î¸1| + Ëœ
 â‰¥
1
1 + Î±
" k
X
i=1
Î»Ìƒ2
i Î¸2
i
1/2
âˆ’
âˆš
kËœ

#
. (33)
The assumption Ëœ
 â‰¤ Î±Î»Ìƒmin/
âˆš
k implies that
âˆš
kËœ
 â‰¤ Î±Î»Ìƒmin â‰¤ Î±
 k
X
i=1
Î»Ìƒ2
i Î¸2
i
1/2
.
Moreover, since Î»Ìƒ1|Î¸1| > 0 (by the first claim) and Î»Ìƒ1|Î¸1| = maxiâˆˆ[k] Î»Ìƒi|Î¸i|, it follows that
Î»Ìƒ1|Î¸1| â‰¥ Î»Ìƒmin max
iâˆˆ[k]
|Î¸i| â‰¥
Î»Ìƒmin
âˆš
k
, (34)
so we also have
Ëœ
 â‰¤ Î±Î»Ìƒ1|Î¸1|.
Applying these bounds on Ëœ
 to (33), we obtain
Î»Ìƒ1|Î¸1| â‰¥
1 âˆ’ Î±
(1 + Î±)2
 k
X
i=1
Î»Ìƒ2
i Î¸2
i
1/2
â‰¥
1 âˆ’ Î±
(1 + Î±)2

Î»Ìƒ2
1Î¸2
1 + max
i6=1
Î»Ìƒ2
i Î¸2
i
1/2
which in turn implies (for Î± âˆˆ (0, 1/20))
max
i6=1
Î»Ìƒ2
i Î¸2
i â‰¤

(1 + Î±)4
(1 âˆ’ Î±)2
âˆ’ 1

Â· Î»Ìƒ2
1Î¸2
1 â‰¤ 7Î± Â· Î»Ìƒ2
1Î¸2
1.
Therefore maxi6=1 Î»Ìƒi|Î¸i| â‰¤
âˆš
7Î± Â· Î»Ìƒ1|Î¸1|, proving the second claim.
Now we prove the final claim. This is done by (i) showing that Î¸ has a large projection
onto u1, (ii) using an SVD perturbation argument to show that Â±u1 is close to v1, and (iii)
concluding that Î¸ has a large projection onto v1.
We begin by showing that (u>
1 Î¸)2 is large. Observe that from (32), we have (1+Î±)2Ï†2
1 â‰¥
kMÌƒk2
F â‰¥ Ï†2
1 + maxi6=1 Ï†2
i , and therefore
max
i6=1
|Ï†i| â‰¤
p
2Î± + Î±2 Â· |Ï†1|.
2822
Tensor Decompositions for Learning Latent Variable Models
Moreover, by the triangle inequality,
|Î¸>
MÌƒÎ¸| â‰¤
k
X
i=1
|Ï†i|(u>
i Î¸)2
â‰¤ |Ï†1|(u>
1 Î¸)2
+ max
i6=1
|Ï†i| 1 âˆ’ (u>
1 Î¸)2

= (u>
1 Î¸)2
|Ï†1| âˆ’ max
i6=1
|Ï†i|

+ max
i6=1
|Ï†i|.
Using (32) once more, we have |Î¸>
MÌƒÎ¸| â‰¥ kMÌƒkF /(1 + Î±) â‰¥ |Ï†1|/(1 + Î±), so
(u>
1 Î¸)2
â‰¥
1
1+Î± âˆ’ maxi6=1
|Ï†i|
|Ï†1|
1 âˆ’ maxi6=1
|Ï†i|
|Ï†1|
= 1 âˆ’
Î±
(1 + Î±)

1 âˆ’ maxi6=1
|Ï†i|
|Ï†1|
 â‰¤ 1 âˆ’
Î±
(1 + Î±)(1 âˆ’
âˆš
2Î± + Î±2)
.
Now we show that (u>
1 v1)2 is also large. By the second claim, the assumption on Ëœ
, and (34),
Î»Ìƒ1|Î¸1| âˆ’ max
i6=1
Î»Ìƒi|Î¸i| > (1 âˆ’
âˆš
7Î±) Â· Î»Ìƒ1|Î¸1| â‰¥ (1 âˆ’
âˆš
7Î±) Â· Î»Ìƒmin/
âˆš
k.
Combining this with Weylâ€™s theorem gives
|Ï†1| âˆ’ max
i6=1
Î»Ìƒi|Î¸i| â‰¥ Î»Ìƒ1|Î¸1| âˆ’ Ëœ
 âˆ’ max
i6=1
Î»Ìƒi|Î¸i| â‰¥ (1 âˆ’ (Î± +
âˆš
7Î±)) Â· Î»Ìƒmin/
âˆš
k,
so we may apply Wedinâ€™s theorem to obtain
(u>
1 v1)2
â‰¥ 1 âˆ’

kEÌƒ(I, I, Î¸)k
|Ï†1| âˆ’ maxi6=1 Î»Ìƒi|Î¸i|
2
â‰¥ 1 âˆ’

Î±
1 âˆ’ (Î± +
âˆš
7Î±)
2
.
It remains to show that Î¸1 = v>
1 Î¸ is large. Indeed, by the triangle inequality, Cauchy-
Schwarz, and the above inequalities on (u>
1 v1)2 and (u>
1 Î¸)2,
|v>
1 Î¸| =
k
X
i=1
(u>
i v1)(u>
i Î¸)
â‰¥ |u>
1 v1||u>
1 Î¸| âˆ’
k
X
i=2
|u>
i v1||u>
i Î¸|
â‰¥ |u>
1 v1||u>
1 Î¸| âˆ’
 k
X
i=2
(u>
i v1)2
1/2 k
X
i=2
(u>
i Î¸)2
1/2
= |u>
1 v1||u>
1 Î¸| âˆ’

1 âˆ’ (u>
i v1)2

1 âˆ’ (u>
i Î¸)2
1/2
â‰¥

1 âˆ’
Î±
(1 + Î±)(1 âˆ’
âˆš
2Î± + Î±2)

1 âˆ’

Î±
1 âˆ’ (Î± +
âˆš
7Î±)
2!1/2
âˆ’
Î±
(1 + Î±)(1 âˆ’
âˆš
2Î± + Î±2)
Â·

Î±
1 âˆ’ (Î± +
âˆš
7Î±)
2
!1/2
â‰¥ 1 âˆ’ 2Î±
2823
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
for Î± âˆˆ (0, 1/20). Moreover, by assumption we have TÌƒ(Î¸, Î¸, Î¸) â‰¥ 0, and
TÌƒ(Î¸, Î¸, Î¸) =
k
X
i=1
Î»ÌƒiÎ¸3
i + EÌƒ(Î¸, Î¸, Î¸)
= Î»Ìƒ1Î¸3
1 +
k
X
i=2
Î»ÌƒiÎ¸3
i + EÌƒ(Î¸, Î¸, Î¸)
â‰¤ Î»Ìƒ1Î¸3
1 + max
i6=1
Î»Ìƒi|Î¸i|
k
X
i=2
Î¸2
i + Ëœ

â‰¤ Î»Ìƒ1Î¸3
1 +
âˆš
7Î±Î»Ìƒ1|Î¸1|(1 âˆ’ Î¸2
1) + Ëœ
 (by the second claim)
â‰¤ Î»Ìƒ1|Î¸1|3

sign(Î¸1) +
âˆš
7Î±
(1 âˆ’ 2Î±)2
âˆ’
âˆš
7Î± +
Î±
(1 âˆ’ 2Î±)3

(since |Î¸1| â‰¥ 1 âˆ’ 2Î±)
< Î»Ìƒ1|Î¸1|3

sign(Î¸1) + 1

so sign(Î¸1) > âˆ’1, meaning Î¸1 > 0. Therefore Î¸1 = |Î¸1| â‰¥ 1 âˆ’ 2Î±. This proves the final
claim.
Lemma C.2 Fix Î±, Î² âˆˆ (0, 1). Assume Î»Ìƒiâˆ— = maxiâˆˆ[k] Î»Ìƒi and
Ëœ
 â‰¤ min

Î±
5
âˆš
k + 7
,
1 âˆ’ Î²
7

Â· Î»Ìƒiâˆ— , Ëœ
F â‰¤
âˆš
` Â·
1 âˆ’ Î²
2Î²
Â· Î»Ìƒiâˆ— .
To the conclusion of Lemma B.4, it can be added that the stopping condition (31) is satisfied
by Î¸ = Î¸t.
Proof Without loss of generality, assume iâˆ— = 1. By the triangle inequality and Cauchy-
Schwarz,
kTÌƒ(I, I, Î¸t)kF â‰¤ Î»Ìƒ1|Î¸1,t| +
X
i6=1
Î»i|Î¸i,t| + kEÌƒ(I, I, Î¸t)kF â‰¤ Î»Ìƒ1|Î¸1,t| + Î»Ìƒ1
âˆš
k
X
i6=1
Î¸2
i,t
1/2
+
âˆš
kËœ

â‰¤ Î»Ìƒ1|Î¸1,t| +
3
âˆš
kËœ

p
+
âˆš
kËœ
.
where the last step uses the fact that Î¸2
1,t â‰¥ 1 âˆ’ (3Ëœ
/(pÎ»Ìƒ1))2. Moreover,
TÌƒ(Î¸t, Î¸t, Î¸t) â‰¥ Î»Ìƒ1 âˆ’

27
 Ëœ

pÎ»1
2
+ 2

Ëœ

p
.
Combining these two inequalities with the assumption on Ëœ
 implies that
TÌƒ(Î¸t, Î¸t, Î¸t) â‰¥
1
1 + Î±
kTÌƒ(I, I, Î¸t)kF .
2824
Tensor Decompositions for Learning Latent Variable Models
Using the definition of the tensor Frobenius norm, we have
1
âˆš
`
kTÌƒkF â‰¤
1
âˆš
`
k
X
i=1
Î»ÌƒivâŠ—3
i
F
+
1
âˆš
`
kEÌƒkF = Î»Ìƒavg +
1
âˆš
`
kEÌƒkF â‰¤ Î»Ìƒavg +
1
âˆš
`
Ëœ
F .
Combining this with the above inequality implies
TÌƒ(I, I, Î¸t) â‰¥
Î²
âˆš
`
kTÌƒkF .
Therefore the stopping condition (31) is satisfied.
C.2 Sketch of Analysis of Algorithm 2
The analysis of Algorithm 2 is very similar to the proof of Theorem 5.1 for Algorithm 1, so
here we just sketch the essential differences.
First, the guarantee afforded to Algorithm 2 is somewhat different than Theorem 5.1.
Specifically, it is of the following form: (i) under appropriate conditions, upon termination,
the algorithm returns an accurate decomposition, and (ii) the algorithm terminates after
poly(k) random restarts with high probability.
The conditions on  and N are the same (but for possibly different universal constants
C1, C2). In Lemma C.1 and Lemma C.2, there is reference to a condition on the Frobenius
norm of E, but we may use the inequality kEkF â‰¤ kkEk â‰¤ k so that the condition is
subsumed by the  condition.
Now we outline the differences relative to the proof of Theorem 5.1. The basic structure
of the induction argument is the same. In the induction step, we argue that (i) if the
stopping condition is satisfied, then by Lemma C.1 (with Î± = 0.05 and Î² = 1/2), we have
a vector Î¸N such that, for some jâˆ— â‰¥ i,
1. Î»Ï€(jâˆ—) â‰¥ Î»Ï€(jmax)/(4
âˆš
k);
2. Î¸N is (1/4)-separated relative to Ï€(jâˆ—);
3. Î¸Ï€(jâˆ—),N â‰¥ 4/5;
and (ii) the stopping condition is satisfied within poly(k) random restarts (via Lemma B.1
and Lemma C.2) with high probability. We now invoke Lemma B.4 to argue that executing
another N power iterations starting from Î¸N gives a vector Î¸Ì‚ that satisfies
kÎ¸Ì‚ âˆ’ vÏ€(jâˆ—)k â‰¤
8
Î»Ï€(jâˆ—)
, |Î»Ì‚ âˆ’ Î»Ï€(jâˆ—)| â‰¤ 5.
The main difference here, relative to the proof of Theorem 5.1, is that we use Îº := 4
âˆš
k
(rather than Îº = O(1)), but this ultimately leads to the same guarantee after taking into
consideration the condition  â‰¤ C1Î»min/k. The remainder of the analysis is essentially the
same as the proof of Theorem 5.1.
2825
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
Appendix D. Simultaneous Diagonalization for Tensor Decomposition
As discussed in the introduction, another standard approach to certain tensor decomposition
problems is to simultaneously diagonalize a collection of similar matrices obtained from the
given tensor. We now examine this approach in the context of our latent variable models,
where
M2 =
k
X
i=1
wi Âµi âŠ— Âµi
M3 =
k
X
i=1
wi Âµi âŠ— Âµi âŠ— Âµi.
Let V := [Âµ1|Âµ2| Â· Â· Â· |Âµk] and D(Î·) := diag(Âµ>
1 Î·, Âµ>
2 Î·, . . . , Âµ>
k Î·), so
M2 = V diag(w1, w2, . . . wk)V >
M3(I, I, Î·) = V diag(w1, w2, . . . wk)D(Î·)V >
Thus, the problem of determining the Âµi can be cast as a simultaneous diagonalization
problem: find a matrix X such that X>
M2X and X>
M3(I, I, Î·)X (for all Î·) are diagonal.
It is easy to see that if the Âµi are linearly independent, then the solution X>
= V â€  is unique
up to permutation and rescaling of the columns.
With exact moments, a simple approach is as follows. Assume for simplicity that d = k,
and define
M(Î·) := M3(I, I, Î·)Mâˆ’1
2 = V D(Î·)V âˆ’1
.
Observe that if the diagonal entries of D(Î·) are distinct, then the eigenvectors of M(Î·) are
the columns of V (up to permutation and scaling). This criterion is satisfied almost surely
when Î· is chosen randomly from a continuous distribution over Rk.
The above technique (or some variant thereof) was previously used to give the efficient
learnability results, where the computational and sample complexity bounds were polyno-
mial in relevant parameters of the problem, including the rank parameter k (Mossel and
Roch, 2006; Anandkumar et al., 2012c,a; Hsu and Kakade, 2013). However, the specific
polynomial dependence on k was rather large due to the need for the diagonal entries of
D(Î·) to be well-separated. This is because with finite samples, M(Î·) is only known up
to some perturbation, and thus the sample complexity bound depends inversely in (some
polynomial of) the separation of the diagonal entries of D(Î·). With Î· drawn uniformly
at random from the unit sphere in Rk, the separation was only guaranteed to be roughly
1/k2.5 (Anandkumar et al., 2012c) (while this may be a loose estimate, the instability is ob-
served in practice). In contrast, using the tensor power method to approximately recover V
(and hence the model parameters Âµi and wi) requires only a mild, lower-order dependence
on k.
It should be noted, however, that the use of a single random choice of Î· is quite restric-
tive, and it is easy to see that a simultaneous diagonalization of M(Î·) for several choices
of Î· can be beneficial. While the uniqueness of the eigendecomposition of M(Î·) is only
guaranteed when the diagonal entries of D(Î·) are distinct, the simultaneous diagonaliza-
tion of M(Î·(1)), M(Î·(2)), . . . , M(Î·(m)) for vectors Î·(1), Î·(2), . . . , Î·(m) is unique as long as the
2826
Tensor Decompositions for Learning Latent Variable Models
columns of ï£®
ï£¯
ï£¯
ï£¯
ï£°
Âµ>
1 Î·(1) Âµ>
2 Î·(1) Â· Â· Â· Âµ>
k Î·(1)
Âµ>
1 Î·(2) Âµ>
2 Î·(2) Â· Â· Â· Âµ>
k Î·(2)
.
.
.
.
.
.
...
.
.
.
Âµ>
1 Î·(m) Âµ>
2 Î·(m) Â· Â· Â· Âµ>
k Î·(m)
ï£¹
ï£º
ï£º
ï£º
ï£»
are distinct (i.e., for each pair of column indices i, j, there exists a row index r such that
the (r, i)-th and (r, j)-th entries are distinct). This is a much weaker requirement for
uniqueness, and therefore may translate to an improved perturbation analysis. In fact, using
the techniques discussed in Section 4.3, we may even reduce the problem to an orthogonal
simultaneous diagonalization, which may be easier to obtain. Furthermore, a number of
robust numerical methods for (approximately) simultaneously diagonalizing collections of
matrices have been proposed and used successfully in the literature (e.g., Bunse-Gerstner
et al., 1993; Cardoso and Souloumiac, 1993; Cardoso, 1994; Cardoso and Comon, 1996;
Ziehe et al., 2004). Another alternative and a more stable approach compared to full
diagonalization is a Schur-like method which finds a unitary matrix U which simultaneously
triangularizes the respective matrices (Corless et al., 1997). It is an interesting open question
whether these techniques can yield similar improved learnability results and also enjoy the
attractive computational properties of the tensor power method.
References
D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In
Eighteenth Annual Conference on Learning Theory, pages 458â€“469, 2005.
E. S. Allman, C. Matias, and J. A. Rhodes. Identifiability of parameters in latent structure
models with many observed variables. The Annals of Statistics, 37(6A):3099â€“3132, 2009.
A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y.-K. Liu. A spectral algorithm
for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25,
2012a.
A. Anandkumar, D. Hsu, F. Huang, and S. M. Kakade. Learning mixtures of tree graphical
models. In Advances in Neural Information Processing Systems 25, 2012b.
A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models
and hidden Markov models. In Twenty-Fifth Annual Conference on Learning Theory,
volume 23, pages 33.1â€“33.34, 2012c.
J. Anderson, M. Belkin, N. Goyal, L. Rademacher, and J. Voss. The more, the merrier:
the blessing of dimensionality for learning large Gaussian mixtures. In Twenty-Seventh
Annual Conference on Learning Theory, 2014.
S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. The
Annals of Applied Probability, 15(1A):69â€“92, 2005.
S. Arora, R. Ge, and A. Moitra. Learning topic models â€” going beyond SVD. In Fifty-Third
IEEE Annual Symposium on Foundations of Computer Science, pages 1â€“10, 2012a.
2827
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
S. Arora, R. Ge, A. Moitra, and S. Sachdeva. Provable ICA with unknown Gaussian
noise, and implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems 25, 2012b.
T. Austin. On exchangeable random variables and the statistics of large graphs and hyper-
graphs. Probab. Survey, 5:80â€“145, 2008.
R. Bailly. Quadratic weighted automata: Spectral algorithm and likelihood maximization.
Journal of Machine Learning Research, 2011.
B. Balle and M. Mohri. Spectral learning of general weighted automata via constrained
matrix completion. In Advances in Neural Information Processing Systems 25, 2012.
B. Balle, A. Quattoni, and X. Carreras. Local loss optimization in operator models: A new
insight into spectral learning. In Twenty-Ninth International Conference on Machine
Learning, 2012.
M. Belkin and K. Sinha. Polynomial learning of distribution families. In Fifty-First Annual
IEEE Symposium on Foundations of Computer Science, pages 103â€“112, 2010.
A. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaraghavan. Smoothed analysis of ten-
sor decompositions. In Proceedings of the 46th Annual ACM Symposium on Theory of
Computing, 2014.
B. Boots, S. M. Siddiqi, and G. J. Gordon. Closing the learning-planning loop with predic-
tive state representations. In Proceedings of the Robotics Science and Systems Conference,
2010.
S. C. Brubaker and S. Vempala. Isotropic PCA and affine-invariant clustering. In Forty-
Ninth Annual IEEE Symposium on Foundations of Computer Science, 2008.
A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simultaneous
diagonalization. SIAM Journal on Matrix Analysis and Applications, 14(4):927â€“949, 1993.
J.-F. Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. blind
identification of more sources than sensors. In Acoustics, Speech, and Signal Processing,
1991. ICASSP-91., 1991 International Conference on, pages 3109â€“3112. IEEE, 1991.
J.-F. Cardoso. Perturbation of joint diagonalizers. Technical Report 94D027, Signal De-
partment, TeÌleÌcom Paris, 1994.
J.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic
methods. In IEEE International Symposium on Circuits and Systems, pages 93â€“96, 1996.
J.-F. Cardoso and A. Souloumiac. Blind beamforming for non Gaussian signals. IEE
Proceedings-F, 140(6):362â€“370, 1993.
D. Cartwright and B. Sturmfels. The number of eigenvalues of a tensor. Linear Algebra
Appl., 438(2):942â€“952, 2013.
2828
Tensor Decompositions for Learning Latent Variable Models
R. B. Cattell. Parallel proportional profiles and other principles for determining the choice
of factors by rotation. Psychometrika, 9(4):267â€“283, 1944.
J. T. Chang. Full reconstruction of Markov models on evolutionary trees: Identifiability
and consistency. Mathematical Biosciences, 137:51â€“73, 1996.
K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations
and independence. In Twenty-First Annual Conference on Learning Theory, pages 9â€“20,
2008.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. Spectral learning of
latent-variable PCFGs. In Fiftieth Annual Meeting of the Association for Computational
Linguistics, 2012.
P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):
287â€“314, 1994.
P. Comon and C. Jutten. Handbook of Blind Source Separation: Independent Component
Analysis and Applications. Academic Press. Elsevier, 2010.
P. Comon, G. Golub, L.-H. Lim, and B. Mourrain. Symmetric tensors and symmetric tensor
rank. SIAM Journal on Matrix Analysis Appl., 30(3):1254â€“1279, 2008.
R. M. Corless, P. M. Gianni, and B. M. Trager. A reordered Schur factorization method
for zero-dimensional polynomial systems with multiple roots. In Proceedings of the 1997
International Symposium on Symbolic and Algebraic Computation, pages 133â€“140. ACM,
1997.
S. Dasgupta. Learning mixtures of Gaussians. In Fortieth Annual IEEE Symposium on
Foundations of Computer Science, pages 634â€“644, 1999.
S. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated,
spherical Gaussians. Journal of Machine Learning Research, 8(Feb):203â€“226, 2007.
L. De Lathauwer, J. Castaing, and J.-F. Cardoso. Fourth-order cumulant-based blind
identification of underdetermined mixtures. Signal Processing, IEEE Transactions on,
55(6):2965â€“2973, 2007.
N. Delfosse and P. Loubaton. Adaptive blind separation of independent sources: a deflation
approach. Signal processing, 45(1):59â€“83, 1995.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum-likelihood from incomplete data
via the EM algorithm. J. Royal Statist. Soc. Ser. B, 39:1â€“38, 1977.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. Ungar. Spectral dependency parsing
with latent variables. In Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, 2012.
M. Drton, B. Sturmfels, and S. Sullivant. Algebraic factor analysis: tetrads, pentads and
beyond. Probability Theory and Related Fields, 138(3):463â€“493, 2007.
2829
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
A. T. Erdogan. On the convergence of ICA algorithms with symmetric orthogonalization.
IEEE Transactions on Signal Processing, 57:2209â€“2221, 2009.
A. M. Frieze, M. Jerrum, and R. Kannan. Learning linear transformations. In Thirty-
Seventh Annual Symposium on Foundations of Computer Science, pages 359â€“368, 1996.
G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins University Press,
1996.
N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Prob-
abilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2), 2011.
R. Harshman. Foundations of the PARAFAC procedure: model and conditions for an
â€˜explanatoryâ€™ multi-mode factor analysis. Technical report, UCLA Working Papers in
Phonetics, 1970.
C. J. Hillar and L.-H. Lim. Most tensor problems are NP-hard. J. ACM, 60(6):45:1â€“45:39,
November 2013. ISSN 0004-5411. doi: 10.1145/2512329.
F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of
Mathematics and Physics, 6:164â€“189, 1927a.
F. L. Hitchcock. Multiple invariants and generalized rank of a p-way matrix or tensor.
Journal of Mathematics and Physics, 7:39â€“79, 1927b.
D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and
spectral decompositions. In Fourth Innovations in Theoretical Computer Science, 2013.
D. Hsu, S. M. Kakade, and P. Liang. Identifiability and unmixing of latent parse trees. In
Advances in Neural Information Processing Systems 25, 2012a.
D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov
models. Journal of Computer and System Sciences, 78(5):1460â€“1480, 2012b.
A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis.
Neural Networks, IEEE Transactions on, 10(3):626â€“634, 1999.
A. HyvaÌˆrinen and E. Oja. Independent component analysis: algorithms and applications.
Neural Networks, 13(4â€“5):411â€“430, 2000.
H. Jaeger. Observable operator models for discrete stochastic time series. Neural Comput.,
12(6), 2000.
A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In
Forty-second ACM Symposium on Theory of Computing, pages 553â€“562, 2010.
R. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models.
SIAM Journal on Computing, 38(3):1141â€“1156, 2008.
2830
Tensor Decompositions for Learning Latent Variable Models
E. Kofidis and P. A. Regalia. On the best rank-1 approximation of higher-order super-
symmetric tensors. SIAM Journal on Matrix Analysis and Applications, 23(3):863â€“884,
2002.
T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51
(3):455, 2009.
T. G. Kolda and J. R. Mayo. Shifted power method for computing tensor eigenpairs. SIAM
Journal on Matrix Analysis and Applications, 32(4):1095â€“1124, October 2011.
J. B. Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with
application to arithmetic complexity and statistics. Linear Algebra and Appl., 18(2):
95â€“138, 1977.
L. D. Lathauwer, B. D. Moor, and J. Vandewalle. On the best rank-1 and rank-
(R1, R2, ..., Rn) approximation and applications of higher-order tensors. SIAM J. Matrix
Anal. Appl., 21(4):1324â€“1342, 2000.
L. Le Cam. Asymptotic Methods in Statistical Decision Theory. Springer, 1986.
L.-H. Lim. Singular values and eigenvalues of tensors: a variational approach. Proceedings of
the IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive
Processing, 1:129â€“132, 2005.
M. Littman, R. Sutton, and S. Singh. Predictive representations of state. In Advances in
Neural Information Processing Systems 14, pages 1555â€“1561, 2001.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. Spectral learning for non-deterministic
dependency parsing. In Conference of the European Chapter of the Association for Com-
putational Linguistics, 2012.
J. B. MacQueen. Some methods for classification and analysis of multivariate observa-
tions. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and
Probability, volume 1, pages 281â€“297. University of California Press, 1967.
P. McCullagh. Tensor Methods in Statistics. Chapman and Hall, 1987.
A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In
Fifty-First Annual IEEE Symposium on Foundations of Computer Science, pages 93â€“102,
2010.
E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models.
Annals of Applied Probability, 16(2):583â€“614, 2006.
P. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU
signatures. Journal of Cryptology, 22(2):139â€“160, 2009.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.
P. V. Overschee and B. D. Moor. Subspace Identification of Linear Systems. Kluwer Aca-
demic Publishers, 1996.
2831
Anandkumar, Ge, Hsu, Kakade, and Telgarsky
L. Pachter and B. Sturmfels. Algebraic Statistics for Computational Biology, volume 13.
Cambridge University Press, 2005.
A. Parikh, L. Song, and E. P. Xing. A spectral algorithm for latent tree graphical models.
In Twenty-Eighth International Conference on Machine Learning, 2011.
K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Trans-
actions of the Royal Society, London, A., page 71, 1894.
L. Qi. Eigenvalues of a real supersymmetric tensor. Journal of Symbolic Computation, 40
(6):1302â€“1324, 2005.
R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM
algorithm. SIAM Review, 26(2):195â€“239, 1984.
P. A. Regalia and E. Kofidis. Monotonic convergence of fixed-point algorithms for ICA.
IEEE Transactions on Neural Networks, 14:943â€“949, 2003.
S. Roch. A short proof that phylogenetic tree reconstruction by maximum likelihood is
hard. IEEE/ACM Trans. Comput. Biol. Bioinformatics, 3(1), 2006.
J. Rodu, D. P. Foster, W. Wu, and L. H. Ungar. Using regression for spectral estimation
of HMMs. In Statistical Language and Speech Processing, pages 212â€“223, 2013.
M. P. SchuÌˆtzenberger. On the definition of a family of automata. Inf. Control, 4:245â€“270,
1961.
S. M. Siddiqi, B. Boots, and G. J. Gordon. Reduced-rank hidden Markov models. In
Thirteenth International Conference on Artificial Intelligence and Statistics, 2010.
D. A. Spielman and S. H. Teng. Smoothed analysis: An attempt to explain the behavior of
algorithms in practice. Communications of the ACM, pages 76â€“84, 2009.
A. Stegeman and P. Comon. Subtracting a best rank-1 approximation may increase tensor
rank. Linear Algebra and Its Applications, 433:1276â€“1300, 2010.
B. Sturmfels and P. Zwiernik. Binary cumulant varieties. Ann. Comb., (17):229â€“250, 2013.
S. Vempala and G. Wang. A spectral algorithm for learning mixtures models. Journal of
Computer and System Sciences, 68(4):841â€“860, 2004.
P. Wedin. Perturbation bounds in connection with singular value decomposition. BIT
Numerical Mathematics, 12(1):99â€“111, 1972.
T. Zhang and G. Golub. Rank-one approximation to high order tensors. SIAM Journal on
Matrix Analysis and Applications, 23:534â€“550, 2001.
A. Ziehe, P. Laskov, G. Nolte, and K. R. MuÌˆller. A fast algorithm for joint diagonaliza-
tion with non-orthogonal transformations and its application to blind source separation.
Journal of Machine Learning Research, 5:777â€“800, 2004.
J. Zou, D. Hsu, D. Parkes, and R. P. Adams. Contrastive learning using spectral methods.
In Advances in Neural Information Processing Systems 26, 2013.
2832
