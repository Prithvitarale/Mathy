arXiv:1306.3917v1
[stat.ML]
17
Jun
2013
On Finding the Largest Mean Among Many
Kevin Jamieson†
, Matthew Malloy†∗
, Robert Nowak†
, and Sébastien Bubeck‡
†
Department of Electrical and Computer Engineering,
University of Wisconsin-Madison
‡
Princeton University,
Department of Operations Research and Financial Engineering
Abstract
Sampling from distributions to find the one with the largest mean arises in a broad range of applica-
tions, and it can be mathematically modeled as a multi-armed bandit problem in which each distribution
is associated with an arm. This paper studies the sample complexity of identifying the best arm (largest
mean) in a multi-armed bandit problem. Motivated by large-scale applications, we are especially in-
terested in identifying situations where the total number of samples that are necessary and sufficient to
find the best arm scale linearly with the number of arms. We present a single-parameter multi-armed
bandit model that spans the range from linear to superlinear sample complexity. We also give a new
algorithm for best arm identification, called PRISM, with linear sample complexity for a wide range of
mean distributions. The algorithm, like most exploration procedures for multi-armed bandits, is adaptive
in the sense that the next arms to sample are selected based on previous samples. We compare the sam-
ple complexity of adaptive procedures with simpler non-adaptive procedures using new lower bounds.
For many problem instances, the increased sample complexity required by non-adaptive procedures is a
polynomial factor of the number of arms.
1 Introduction
This paper studies the sample complexity of finding the best arm in a multi-armed bandit problem. Consider
n + 1 arms with mean payoffs µ0 > µ1 > · · · > µn. The mean values and the ordering of the arms are
unknown. The goal is to identify the arm with the largest mean (i.e., the “best arm”) by sampling the arms.
A sample of arm i is an independent realization of a random variable Xi ∈ [0, 1] with mean µi ∈ [0, 1]
(it is straightforward to extend all results presented in this paper to sub-Gaussian realizations with bounded
means and variances).
The main focus of this paper is identifying necessary and sufficient conditions under which the sample
complexity (total number of samples) of finding the best arm grows linearly in the number of arms. This
is motivated by applications involving very large numbers of arms, such as virus replication experiments
testing thousands of cell strains [3], cognitive radio problems searching over hundreds of communication
channels [1, 2], and network surveillance of large social networks. These applications are time-consuming
and/or costly, so minimizing the number of samples required to find the most influential genes, best chan-
nels, or malicious agents is crucial. This paper quantifies the minimum number of samples needed in such
applications and gives a new algorithm called PRISM that succeeds using a total number of samples within
a negligible factor of the minimum.
∗
The first two authors are listed in alphabetical order as both contributed equally.
1
Mannor and Tsitsiklis [4] showed that for any procedure that finds the best arm with probability at least
1−δ requires on the order of H log(1/δ) samples, where H :=
Pn
i=1(µ0 −µi)−2. This lower bound shows
that the sample complexity can be much greater than the number of arms. For example, if the gap between
µ0 and µ1 is 1/n, then H, and the sample complexity, are at least O(n2). On the other hand, scenarios can
arise in which H grows linearly with n. For instance, if µ0 − µi is greater than a positive constant for all i,
then H = O(n). This case is “sparse” in the sense that µ0 is bounded away from all others, and recent work
has shown that O(n) samples are sufficient in such cases [5].
Most bandit exploration algorithms are sequential and adaptive in the sense that the selection of the arms
to sample next is based on previous samples. This is necessary in order to achieve linear sample complexity
in the sparse case mentioned above. Non-adaptive methods, in which every arm is sampled an equal number
of times, require at least O(n log n)1 samples [6]. The factor of log n is significant if n is large, which is
one motivation for adaptive strategies like the one used in [3]. Contrasting the differences between adaptive
and non-adaptive sampling is a second focus of this paper.
Of particular interest here is the scaling of the sample complexity as a function of the number of arms
and the behavior of the gaps between their means. The sparse model discussed above is an enlightening
idealization, but unlikely to arise in practice. A smoothly decaying distribution of means may be a more
reasonable model for the biological and radio applications discussed above. Fig. 1 depicts the means in
three different arm configurations. The left plot (a) represents a sparse model in which µ0 is bounded
away from all others means by a fixed constant (all gaps greater than a fixed constant). In this case, the
sample complexities of non-adaptive and adaptive strategies differs by a factor of log n. The other two plots
represent cases in which the gaps between means are shrinking as n increases. In Figure 1(b) µ0 − µi =
( i
n ).49 and in (c) µ0 − µi = i
n . The difference between the sample complexities is much more significant
for these non-sparse cases. Also note that there are non-sparse cases in which adaptive strategies can find
the best arm in O(n) samples like the one shown in Fig. 1(b).
adaptive – O(n)
non-adaptive – O(n logn)
adaptive – O(n)
non-adaptive – O(n1.98
)
adaptive – O(n2
)
non-adaptive – O(n3
)
(a) (b) (c)
Figure 1: Three different configurations of means, each ordered µ0 > µ1 > · · · > µn. In (a) µ0 − µi ≥ 0.95, in (b)
µ0 − µi = ( i
n ).49
, and in (c) µ0 − µi = i
n , for i = 1, . . . , n. The necessary sample complexities (sufficient to within
log log n factors) of non-adaptive and adaptive strategies are indicated in each case. Finding the best arm becomes
increasingly difficult as the gaps between the means decrease, but in all cases adaptive strategies have significantly
lower sample complexities.
1
In this paper our focus is on how the number of samples scales with n, not necessarily the probability of failure δ. For instance,
if we were to say an algorithm requires just O(n) samples then it is understood that this many samples suffices to find the best arm
with a fixed probability of error. However, in the theorem statements the dependence on δ is explicit.
2
1.1 Contributions and Organization
The paper is organized as follows. In Sec. 2, we present a single-parameter model for the distribution of
means that spans the range from linear to superlinear sample complexity. In Sec. 3 we present an algorithm
for best arm identification with sample complexity O(H log(1/δ)) for a wide range of mean distributions.
In particular, we show that the algorithm has linear sample complexity for all of the single-parameter dis-
tributions satisfying H = O(n). Our bounds apply to the PAC (probably approximately correct) setting
(generalizing the algorithm to the fixed budget case of [7, 8] is a challenging open problem). While com-
pleting this paper2, we became aware of independent work on the best arm problem to appear at ICML 2013
[9]. The algorithm and theoretical analysis in that paper are essentially the same as ours, but in fact the
upper bound on sample complexity bound given in [9] is slightly tighter.
Sec. 4 discusses the limitations of non-adaptive sampling strategies and shows that any non-adaptive
sampling strategy may require drastically more samples than our adaptive algorithm. Using new lower
bounds for non-adaptive sampling procedures, we show that there exist problems in which the difference
between the sample complexities of non-adaptive and adaptive procedures grows polynomially with the
number of arms. This is somewhat surprising, as the as the advantage of adaptivity in the sparse setting
(where the gaps are bound by a fixed constant) is known to be a factor of log n at best. To be more concrete,
consider the following. We demonstrate problem instances where adaptive procedures, for example, suc-
ceed with just O(n) samples, but all non-adaptive procedures fail without at least O(n1.98) samples. This
observation is crucial since it shows that adaptive designs can be vastly superior to simpler non-adaptive
methods often used in practice (e.g., biological applications mentioned above). The take-away message is
that the added implementational burden of adaptive sampling methods may be well worth the investment.
Notation, in general, follows convention. Since the order and means are unknown, we denote the index
of the best arm as i∗ throughout the paper. An estimate of the best arm is denoted as b
i. Proofs of all
Theorems are found in the Appendix.
2 A Single-Parameter Family of Mean Distributions
A lower bound on the sample complexity of finding the best arm follows in a straightforward way from [4,
Theorem 5]; see Theorem 3 in Sec. 4 for the derivation. To find the best arm with probability at least 1 − δ
requires at least
c1H log(1/δ)
samples, where c1 > 0 is a universal constant. The quantity H, refereed to as the hardness of the problem,
is given by
H =
n
X
i=1
∆−2
i . (1)
where
∆i = µ0 − µi (2)
is the gap between the best arm and the ith arm.
2
The main results of this paper were presented in a lecture (but not as a publication) at the Information Theory and Applications
(ITA) Workshop in San Diego in February 2013.
3
In this paper we focus on a specific parametric family that spans the hardness of the best arm problem
with a single parameter. Consider a model in which the means are given by
µi = µ0 − (i/n)α
(3)
for i = 1, . . . , n and some α ≥ 0. We refer to this model as an α-parameterization. Under this model,
∆i = (i/n)α. The α-parameterization spans the range from “hard” problems in which the gaps ∆i are
shrinking quickly as n grows, to “easy” or sparse cases in which the gaps are greater than a constant (when
α = 0). Theorem 3 in Sec. 4 yields the following lower bounds on the sample complexity of identifying the
best arm (ignoring constant and log(1/δ) factors):
sample complexity ≥









n if α < 1/2
n log n if α = 1/2
n2α if α > 1/2.
(4)
The focus of this paper is on finding sufficient conditions under which the best arm can be found with
O(n) samples. If the gaps follow the α-parameterization with α ≥ 1/2, (4) implies that the best arm cannot
be found with O(n) samples. Conversely, if the gaps satisfy ∆i ≥ C( i
n)α, for α < 1/2, the lower bound
in (4) does not preclude the possibility that order n samples are sufficient to find the best arm. In the next
section, we show that when α < 1/2, order n samples are indeed sufficient.
3 PRISM Algorithm for Best Arm Identification
To show that a linear number of pulls is sufficient for a number of problem instances, we propose and analyze
the algorithm for best arm identification outlined in Fig. 2. The algorithm follow a multi-phase approach,
with a specific allocation of confidence and sampling budgets across phases. The algorithm relies on the
output of Median Elimination [10] to establish a threshold on each phase. We mention again independent
work to appear at ICML 2013 [9], which proposes and analyzes essentially the same algorithm.
Input δ. Let A1 = {0, 1, . . . , n}, nℓ = ℓ2ℓ, and εℓ =
q
log(1/δ)
2ℓ .
For each phase ℓ = 1, 2, . . . ,
(1) Let iℓ be the output of Median Elimination [10] run on Aℓ with accuracy εℓ, δℓ

.
(2) For each arm i ∈ Aℓ, sample nℓ times arm i and let b
µi(ℓ) be the corresponding average.
(3) Let
Aℓ+1 = {i ∈ Aℓ : b
µi(ℓ) ≥ b
µiℓ
− 2εℓ} .
Stop when Aℓ contains a unique elementb
i and outputb
i.
Figure 2: PRISM algorithm for the best arm identification problem.
The following theorem is our main result. The sample complexity of identifying the best arm with
probability at least 1 − δ is bounded in terms of H and a novel measure of complexity denoted by G :=
4
Pn
i=1 ∆−2
i log2(∆−2
i ). In general H ≤ G ≤ H log(H) but in many cases, G = H and the bound implies
that the best arm can be found using O(H) samples with a fixed probability of error. This is the best known
bound for the best arm problem. The proof is left to the appendix.
Theorem 1. Let δ ∈ (0, 1). Let H =
Pn
i=1 ∆−2
i where ∆1 is the minimum gap. Then with probability at
least 1 − 3δ2
1−δ2 − δ
1−δ − 4δ2
(1−δ2)2 , the PRISM algorithm of Fig. 2 stops after at most
O log(1/δ)
"
H log(log(1/δ)) +
n
X
i=1
∆−2
i log2(∆−2
i )
#!
samples and outputs armb
i = i∗.
Corollary 1. Consider the problem instance µ0 = 1 and µi = 1 − (i/n)α, for i = 1, . . . , n and some
0 < α. Then for a fixed probability error, using the PRISM algorithm of Fig. 2 we have that
number of total samples =









O(n) if α < 1/2
O(n log2
n) if α = 1/2
O(n2α log(n)) if α > 1/2.
(5)
and the algorithm outputs armb
i = i∗.
Proof. For ∆i = i
n
α
we have the relevant quantities given in the following table (ignoring lower order
terms):
H
Pn
i=1 ∆−2
i log2(∆−2
i )
α > 1/2 2α
2α−1 n2α (2α)2
2α−1 n2α log(n)
α = 1/2 n log(n) 2αn log2
(n)
α < 1/2 2α
1−2α n 2α
(1−2α)2 n
The result follows from plugging the above quantities into Theorem 1.
Conservative PRISM. Consider the algorithm of Fig. 2, but in the first line set nℓ = 2ℓ and εℓ =
p
log(ℓ2/δ)/2ℓ and for item (2), run Median Elimination with input (εℓ, δ/ℓ2).
Theorem 2. Let δ ∈ (0, 0.6]. With probability at least 1 − 2δ − 6δ2 − 6δ4, Conservative PRISM stops after
at most O

H log

log(H)
δ

pulls and outputsb
i = i∗.
Theorem 1 matches the lower bound when α < 1/2 and comes within a log(n) factor for α ≥ 1/2. On
the other hand, Theorem 2 comes within a factor of log log n of the lower bound for all α > 0. We note that
the upper bound of [9] also comes no closer than log log n of the lower bound for α ≥ 1/2.
4 Lower Bounds on the Sample Complexity of Non-Adaptive Algorithms
Here we examine the limitations of non-adaptive sampling strategies (which sample all arms an equal num-
ber of times), since these simpler procedures are not uncommon in applications like the biological problems
that partially motivate this paper. A non-adaptive procedure is any procedure that samples each arm m
5
times, where m is fixed a-priori, and outputs a single arm as an estimate of the best arm. We show that all
non-adaptive methods may require drastically more samples than PRISM. The take-away message is that
the small added difficulty (for the practitioner) in applying PRISM may be well worth the investment.
We begin by formally stating the adaptive lower bound developed in [4].
Theorem 3. Adaptive Lower Bound [4, Theorem 5]. For every set of means, {µi}n
i=0, µi ∈ (3/8, 1/2],
there exists a joint distribution on the arms such that the arms take mean values {µ0, . . . , µn}, each arm is
sub-Gaussian, and any adaptive procedure with fewer than
c1H log
1
8δ
(6)
samples in expectation has P(b
i 6= i∗) ≥ δ for any δ ∈ (0, e−8/8) and some constant c1.
Note that the restriction on the means to (3/8, 1/2] in Theorem 3 can be relaxed (see [4] for details). We
proceed with the non-adaptive lower bounds which allow us to compare the best non-adaptive procedures
against adaptive procedures.
Theorem 4. Non-Adaptive Lower Bound. Consider any δ ∈ (0, e−3/24). For every set of means
{µ1, . . . , µn}, there exists a joint distribution on the arms such that the arms take mean values {µ1, . . . , µn},
each arm is sub-Guassian, and any non-adaptive procedure with fewer than
H log
 n
25δ

samples in expectation has P(b
i 6= i∗) ≥ δ. Moreover, for any value of H there exists a joint sub-Gaussian
distribution over arms with means {µ0, . . . , µn} satisfying
Pn
i=1(µ0 − µi)−2 = H, such that any non-
adaptive procedure with fewer than
Hn
2
log

1
24δ

samples in expectation has P(î 6= i∗) ≥ δ.
The lower bound of Theorem 4 consists of two statements, the first which implies that for any set
of means, the sample complexity must be at least order H log n. The second statement, on the other hand,
implies the existence of particular problem instances that are especially difficult for non-adaptive procedures,
requiring order Hn samples. Inspecting the proofs of the two parts of Theorem 4 one sees that the minimum
gap ∆1, not H, is governing the query complexity for non-adaptive procedures. Using this fact we have the
following Theorem which is proved in the appendix.
Theorem 5. Non-adaptive Lower Bound, α-parameterization. Consider arms with mean values accord-
ing to the parameterization of (3) for some α ≥ 0. There exists a joint sub-Gaussian distribution on the
arms such that any non-adaptive procedure with fewer than
(
n log n
25δ

if α = 0
n2α+1 log 1
24δ

if α > 0
samples to find the best arm with a fixed probability of failure.
6
To see that Theorem 5 is indeed tight, it is straightforward to show that the non-adaptive procedure
which chooses the arm with the largest empirical mean after sampling each arm the same number of times
does indeed meet the lower bound. Letting m be the number of times each arm is sampled. Then
P

b
i 6= i∗

≤
X
i6=i∗
P (b
µi∗ ≤ b
µi) ≤
X
i6=i∗
(P (b
µi∗ ≤ µi∗ − ∆i/2) + P (b
µi ≥ µi + ∆i/2))
≤
X
i6=i∗
2 exp(−m∆2
i ) =
X
i6=i∗
2 exp −m

i
n
2α
!
which follow from a union bound and Hoeffding’s inequality. For α 6= 0, if m ≥ n2α (which implies the
total number of samples is greater than n2α+1) the above sum is convergent, and the probability that the
wrong arm is returned is controlled. The case where α = 0 is also controlled if m ≥ log n.
We conclude that for α ∈ (0, 1/2), when compared to adaptive procedures that require just O(n) sam-
ples, any non-adaptive procedure requires a factor of n2α more samples to identify the best arm. The
implications of this observation can be somewhat surprising: for many problem instances, the improvement
in the sample complexity resulting from adaptivity is polynomial in n, compared with the typical log(n)
improvement observed for sparse problems (α = 0).
References
[1] Simon Haykin. Cognitive radio: brain-empowered wireless communications. Selected Areas in Com-
munications, IEEE Journal on, 23(2):201–220, 2005.
[2] David López-Pérez, Alvaro Valcarce, Guillaume De La Roche, and Jie Zhang. OFDMA femtocells: A
roadmap on interference avoidance. Communications Magazine, IEEE, 47(9):41–48, 2009.
[3] L. Hao, A. Sakurai, T. Watanabe, E. Sorensen, C. Nidom, M. Newton, P. Ahlquist, and Y. Kawaoka.
Drosophila RNAi screen identifies host genes important for influenza virus replication. Nature, page
8903, 2008.
[4] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
problem. The Journal of Machine Learning Research, 5:623–648, 2004.
[5] Matthew L Malloy and Robert Nowak. Sequential testing for sparse recovery. arXiv preprint
arXiv:1212.1801, 2012.
[6] Matt Malloy and Robert Nowak. On the limits of sequential testing in high dimensions. In Signals,
Systems and Computers (ASILOMAR), 2011 Conference Record of the Forty Fifth Asilomar Conference
on, pages 1245–1249. IEEE, 2011.
[7] J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In Proceed-
ings of the 23rd Annual Conference on Learning Theory (COLT), 2010.
[8] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings
of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.
[9] Z. Karnin, T. Koren, and O. Somekh. Almost optimal exploration in multi-armed bandits. Proceedings
of the 30th International Conference on Machine Learning, June 2013.
7
[10] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research,
7:1079–1105, 2006.
[11] G. Abreu. Very simple tight bounds on the q-function. Communications, IEEE Transactions on,
60(9):2415–2420, 2012.
[12] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer Science and Business Media,
2006.
A Appendix
A.1 Proof of Theorem 1
Proof. It will be useful to consider the following ‘slicing’ of arms:
Ωs = {i ∈ [n] : 5
√
2 εs+1 < ∆i ≤ 5
√
2 εs}, s ≥ 1.
Note that
25 log (1/δ)
X
i∈Ωs
∆−2
i ≤ 2s
|Ωs| ≤ 50 log (1/δ)
X
i∈Ωs
∆−2
i . (7)
Step 1: A good event. In this step we describe the event of probability 1 − δ on which we will prove the
result. We want the following to hold:
b
µi∗ (ℓ) − µ∗
≥ −εℓ , ∀ℓ ≥ 1 , (8)
|b
µiℓ
(ℓ) − µiℓ
| ≤ εℓ , ∀ℓ ≥ 1 , (9)
max
j∈Aℓ
µj − µiℓ
≤ εℓ , ∀ℓ ≥ 1 , (10)

i ∈ Aℓ ∩ Ωs : b
µi(ℓ) ≥ b
µiℓ
− 2εℓ ≤
|Aℓ ∩ Ωs|
4
, ∀ℓ ≥ s ≥ 1. (11)
We first bound the probability that the above events do not hold. By Hoeffding’s inequality we have
P (µ̂i(ℓ) − µi ≥ εℓ) ≤ exp(−2nℓε2
ℓ ) = δ2ℓ
for any i and note that an analogous inequality holds for the deviation away from its mean in the other
direction. Thus, applying Hoeffding’s and a union bound we have that the probability that 8) is not satisfied
is less than δ2
1−δ2 . The probability that (9) is not satisfied is bound in the exact same manner with an additional
factor of two to satisfy both inequality directions. The only subtlety is that after the union bound one needs
to condition on the value of iℓ before using Hoeffding’s inequality, and this is possible since the random
variables obtain in Step (2) of the algorithm are independent of iℓ. By the properties of median elimination
and a union bound we have that the probability that (10) is not satisfied is less than δ
1−δ . Observe that by (8)
and (9) one always has:
b
µi∗ (ℓ) ≥ µ∗
− εℓ ≥ µiℓ
− εℓ ≥ b
µiℓ
(ℓ) − 2εℓ.
which implies that the best arm is never removed from the set.
8
It remains to bound the probability that (11) is not satisfied while (8), (9) and (10) are satisfied.
P


i ∈ Aℓ ∩ Ωs : b
µi(ℓ) ≥ b
µiℓ
− 2εℓ >
|Aℓ ∩ Ωs|
4
|(8), (9), (10), Aℓ, iℓ

≤ P


i ∈ Aℓ ∩ Ωs : b
µi(ℓ) ≥ µ∗
− 4εℓ >
|Aℓ ∩ Ωs|
4
|Aℓ

≤
4
|Aℓ ∩ Ωs|
E


i ∈ Aℓ ∩ Ωs : b
µi(ℓ) ≥ µ∗
− 4εℓ | Aℓ

=
4
|Aℓ ∩ Ωs|
X
i∈Aℓ∩Ωs
P (b
µi(ℓ) ≥ µ∗
− 4 εℓ | Aℓ)
≤ 4 exp(−2nℓε2
ℓ ) = 4δ2ℓ
.
where the last inequality follows since all elements of Ωs have gaps greater than 5
√
2εs+1 = 5εs. Summing
over all s ≤ ℓ and then over ℓ ≥ 1 gives the probability that (11) is not satisfied:
P∞
ℓ=1
P
s≤ℓ 4δ2ℓ = 4δ2
(1−δ2)2 .
In the next steps we will assume that (8)-(9)-(10)-(11) are satisfied as they all hold with probability at least
1 − 3δ2
1−δ2 − δ
1−δ − 4δ2
(1−δ2)2 .
Step 2: Bound on the total number of phases. It suffices to bound the number of phases given that (8), (9),
(10) and (11) hold. Let L denote the first phase such that |Aℓ| = 1 (if there is no such phase then L = +∞).
Observe that using (11) one can show by induction that
|Aℓ| ≤ 1 +
ℓ
X
s=1
|Ωs|
4ℓ−s
+
+∞
X
s=ℓ+1
|Ωs|. (12)
Define s∗ = log2(∆−2
1 log(1/δ)) so that Ωs = ∅ for all s > s∗. By definition, when ℓ ≥ s∗ the third term in
the equation immediately above is equal to zero so that
|Aℓ| ≤ 1 + 2−ℓ
s∗
X
s=1
2s−s∗
2s
|Ωs| ∀ℓ ≥ s∗
where we have
s∗
X
s=1
2s−s∗
2s
|Ωs| =
∆2
1
log(1/δ)
∞
X
s=1
22s
|Ωs|
=
∆2
1
log(1/δ)
∞
X
s=1
22s
n
X
i=1
1
(
5
√
2
r
log(1/δ)
2s+1
< ∆i ≤ 5
√
2
r
log(1/δ)
2s
)
≤
∆2
1
log(1/δ)
n
X
i=1
502 log(1/δ)2
∆2
i
= 502
log(1/δ)H∆2
1.
We conclude that for
L := 1 + max

s∗
, log2(502
log(1/δ)H∆2
1) = log2(2 log(1/δ)) + max

log2(∆−2
1 ), log2(502
H∆2
1)
we have that |Aℓ| < 2 whenever ℓ ≥ L. Hence, L is an upper bound on the stopping time.
Step 3: Bound on the total number of pulls. Recall that Median Elimination applied to a set Aℓ with
parameters εℓ, δℓ takes no more than cME
ε2
ℓ
|Aℓ|ℓ log(1/δ) = cMEℓ2ℓ|Aℓ| pulls. Thus, the total number of pulls
9
on phase ℓ is bounded by cℓ2ℓ|Aℓ| pulls with c = cME + 1. Using the results from the previous step (in
particular (12) and the stopping time L) one has that the total number of pulls is bounded from above by
L
X
ℓ=1
cℓ2ℓ
|Aℓ| ≤
L
X
ℓ=1
cℓ2ℓ
1 +
ℓ
X
s=1
|Ωs|
4ℓ−s
+
∞
X
s=ℓ+1
|Ωs|
!
≤ cL2L+1
+ c
+∞
X
ℓ=1
ℓ
+∞
X
s=1

2s
2ℓ
1s≤ℓ +
2ℓ
2s
1s>ℓ

2s
|Ωs|
≤ cL2L+1
+ 3c
+∞
X
s=1
s2s
|Ωs| + 2c(n − 1)
= cL2L+1
+ 150c log(1/δ) [log2(50 log(1/δ))H + G] + 2c(n − 1)
where
H ≤ G :=
n
X
i=1
∆−2
i log2 ∆−2
i

≤ H log(H)
which follows directly from
∞
X
s=1
s2s
|Ωs| =
∞
X
s=1
s2s
n
X
i=1
1
(
5
√
2
r
log(1/δ)
2s+1
< ∆i ≤ 5
√
2
r
log(1/δ)
s2s
)
≤ 50 log(1/δ)
n
X
i=1
1
∆2
i
log2

50 log(1/δ)
∆2
i

.
Evaluating L2L+1 and collecting terms obtains the result.
A.2 Proof of Theorem 3
Proof. Assume some procedure has P(b
i 6= i∗) ≤ δ and requires fewer than c1H log 1
8δ samples for some
{µi}n
i=0, µi ∈ (3/8, 1/2]. This procedure is by definition (ε, δ) PAC (probably approximately correct) for
any ε ∈ (0, ∆1). [4, Theorem 5] implies any (ε, δ), procedure, ε ∈ (0, ∆1), requires more than
c1
X
i∈N
1
µi∗ − µi
log
1
8δ
samples in expectation, where
N =
(
i : µi ≤ µi∗ − ε, µi ≥
ε + µi∗
1 +
p
1/2
)
Since µi ∈ (3/8, 1/2], N := [n]. Any procedure requires more than
ciH log
1
8δ
samples in expectation. This negates the original assumption.
10
A.3 Proof of Theorem 4
Proof. We restrict our attention to reward distributions of the form N(µi, 1). Assume that µi, i = 0, . . . , n,
are know up to a permutation, and let each arm be assigned a mean uniformly at random. We first show
that the test with minimum average probability of error simply picks the largest empirical mean among all
arms, i.e., b
i = arg maxi b
µi, where b
µi = 1/m
Pm
j=1 Xi,j, and Xi,j represents the reward of arm i on the
jth play of that arm, and m is the total number of samples of each arm. This can be seen by considering
the maximum a-posteriori (MAP) estimator of the best arm, which by definition has the smallest probably
of error. Under the assumption that the arms are assigned means uniformly at random, the MAP estimator
reduces to the maximum likelihood (ML) estimator:
b
iMAP = b
iML = arg max
i
P(Xm
0 , . . . , Xm
n |Hi),
where Xm
i = Xi,1, . . . , Xi,m and Hi is event that arm i is the best arm. Consider comparing between events
Hi and Hi′ , i 6= i′: the ML test is P(Xm
0 , . . . , Xm
n |Hi) ≶i′
i P(Xm
0 , . . . , Xm
n |Hi′ ). By the independence
across arms, it is straightforward to show this test reduces to:
P(Xm
i |Hi)P(Xm
i′ |Hi) ≶i′
i P(Xm
i |Hi′ )P(Xm
i′ |Hi′ ) (13)
The distribution of Xm
i , given Hi, is simply
P(Xm
i |Hi) =
1
√
2π
exp

− ||Xm
i − µi∗ 1||2
/2

(14)
where Xm
i = [Xi,1, . . . , Xi,m]T ∈ Rm. The marginal distribution on arm i′, given that arm i is the largest,
follows a mixture distribution:
P(Xm
i′ |Hi) =
1
n
√
2π
n
X
j=1
exp

− ||Xm
i′ − µj1||2
/2

. (15)
Combining (13), (14), and (15), after a number of straightforward manipulations, excluded for brevity, it can
be shown that the ML estimate prefers arm i to i′ if and only if
Pm
ℓ=1 Xi,ℓ >
Pm
ℓ=1 Xi′,ℓ, or equivalently,
b
µi > b
µi′ . The estimate with minimum probability of error is simplyb
i = arg maxi b
µi.
We continue by bounding the probability of error of the maximum likelihood test. For any estimator,
P(b
i 6= i∗
) ≥ P


[
i6=i∗
b
µi∗ − b
µi ≤ 0


= P(b
µi∗ ≥ µi∗ ) P


[
i6=i∗
b
µi∗ − b
µi ≤ 0 b
µi∗ ≥ µi∗


+ P(b
µi∗ < µi∗ ) P


[
i6=i∗
b
µi∗ − b
µi ≤ 0 b
µi∗ < µi∗


≥
1
2
P


[
i6=i∗
b
µi ≥ µi∗

 =
1
2

1 − P


\
i6=i∗
b
µi ≤ µi∗




=
1
2

1 −
Y
i6=i∗
FN
q
m∆2
i


 ≥
1
2

1 −
Y
i6=i∗

1 −
1
12
exp −m∆2
i



 (16)
≥ min
∆1,...,∆n:
P
i
1
∆2
i
=H
1
2

1 −
Y
i6=i∗

1 −
1
12
exp −m∆2
i



 (17)
11
where FN (x) is the standard Gaussian CDF. The inequality in (16) follows since FN (x) ≤ 1−exp(−x2)/12
for x ≥ 0 [11, Eqn. 13]. The next step in the proof will be showing that error probabilities smaller than a
fixed constant, (17) is minimized when the gaps are equal, i.e., when ∆1 = ∆2 = ... =
p
n/H. First define
z ∈ Rn
+ with elements zi := 1/(m∆2
i ). We can recover the minimum of (17) by solving
argmax
z∈Rn
+:1T z=H/m
n
X
i=1
log

1 −
1
12
exp (−1/zi)

. (18)
Define the Lagrangian of (18) as
L(z, λ) = −
n
X
i=1
log

1 −
1
12
exp z−1
i


− λ(1T
z − H/m).
From [12, p. 321], any z that maximizes (18) necessarily satisfies
∂L
∂zi
=
z−2
i
12 exp(z−1
i ) − 1
− λzi = 0 ∀ i (19)
1T
z = H.
The above system of equations is satisfied by pairs (z, λ) that satisfy

zi : λ =
z−3
i
12 exp(z−1
i ) − 1

∀ i (20)
and 1T z = H simultaneously. Differentiation of (20) shows the function λ(zi) is monotonically increasing
in zi for zi ≤ 1/3. First, consider a solution to (19) which has one or more zi ≥ 1/3. This would imply
m∆2
i ≤ 3 for some i, and from (17), P(b
i 6= i∗) ≥ 1
24 exp(−3). For any λ, (20) is satisfied by at most
one zi ∈ (0, 1/3] by the monotonicity of the function on this range; this implies implies either 1) the z that
maximizes (18) has the form z1 = z2 = · · · = zn, or 2) P(b
i 6= i∗) ≥ 1
24 exp(−3). We focus our attention
on the case when z1 = · · · = zn (and thus ∆1 = · · · = ∆n). Since
P
i 1/∆2
i = H, ∆i =
p
n/H for all i.
(17) gives
P(b
i 6= i∗
) ≥
1
2

1 −

1 −
1
12
exp

−
mn
H
n
.
Recall the total number of samples is given by nm. If mn ≤ H(log n + log (25δ)−1)

, then for δ ∈
(0, e−3/24)
P(b
i 6= i∗
) ≥
1
2

1 −

1 −
25δ
12n
n
(21)
≥
1
2

1 − exp

−
25δ
12

for all n ≥ 1 (22)
≥ δ (23)
which completes the proof of the first statement of the theorem.
To prove the second statement of the theorem, consider the following set of gaps –
∆i =



q
2
H i = 1
q
2(n−1)
H i > 1.
12
Note that {∆1, ..., ∆n} satisfy
Pn
i=1 1/∆2
i = H. From (16), and by considering only the arm with the
smallest gap,
P(b
i 6= i) ≥
1
2

1 −
Y
i6=i∗

1 −
1
12
exp −m∆2
i



 ≥
1
24
exp

−
2m
H

. (24)
If m ≤ H
2 log 1
24δ

, we have P

b
i 6= i∗

≥ δ. This implies that if the total number of measurements is less
than Hn
2 log 1
24δ

, then P(b
i 6= i∗) ≥ δ, completing the proof of the second statement of Thm. 4.
A.4 Proof of Corollary 2
Proof. When α = 0, Theorem 4 implies the result. When α > 0, we can bound (16) by dropping all terms
in the product except the term corresponding to the smallest gap. This gives
P(b
i 6= i) ≥
1
24
exp −m∆2
1

=
1
24
exp −mn−2α

(25)
Setting m ≤ n2α log 1
24δ

implies the result.
13
