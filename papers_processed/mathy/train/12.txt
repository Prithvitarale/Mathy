MachineLearning,29,103–130(1997)c°1997KluwerAcademicPublishers.ManufacturedinTheNetherlands.OntheOptimalityoftheSimpleBayesianClassifierunderZero-OneLossPEDRODOMINGOSpedrod@ics.uci.eduMICHAELPAZZANIpazzani@ics.uci.eduDepartmentofInformationandComputerScience,UniversityofCalifornia,Irvine,CA92697Editor:GregoryProvanAbstract.ThesimpleBayesianclassifierisknowntobeoptimalwhenattributesareindependentgiventheclass,butthequestionofwhetherothersufficientconditionsforitsoptimalityexisthassofarnotbeenexplored.Empiricalresultsshowingthatitperformssurprisinglywellinmanydomainscontainingclearattributedependencessuggestthattheanswertothisquestionmaybepositive.Thisarticleshowsthat,althoughtheBayesianclassifier’sprobabilityestimatesareonlyoptimalunderquadraticlossiftheindependenceassumptionholds,theclassifieritselfcanbeoptimalunderzero-oneloss(misclassificationrate)evenwhenthisassumptionisviolatedbyawidemargin.Theregionofquadratic-lossoptimalityoftheBayesianclassifierisinfactasecond-orderinfinitesimalfractionoftheregionofzero-oneoptimality.ThisimpliesthattheBayesianclassifierhasamuchgreaterrangeofapplicabilitythanpreviouslythought.Forexample,inthisarticleitisshowntobeoptimalforlearningconjunctionsanddisjunctions,eventhoughtheyviolatetheindependenceassumption.Further,studiesinartificialdomainsshowthatitwilloftenoutperformmorepowerfulclassifiersforcommontrainingsetsizesandnumbersofattributes,evenifitsbiasisapriorimuchlessappropriatetothedomain.Thisarticle’sresultsalsoimplythatdetectingattributedependenceisnotnecessarilythebestwaytoextendtheBayesianclassifier,andthisisalsoverifiedempirically.Keywords:SimpleBayesianclassifier,naiveBayesianclassifier,zero-oneloss,optimalclassification,inductionwithattributedependences1.IntroductionInclassificationlearningproblems,thelearnerisgivenasetoftrainingexamplesandthecorrespondingclasslabels,andoutputsaclassifier.Theclassifiertakesanunlabeledexampleandassignsittoaclass.Manyclassifierscanbeviewedascomputingasetofdiscriminantfunctionsoftheexample,oneforeachclass,andassigningtheexampletotheclasswhosefunctionismaximum(Duda&Hart,1973).IfEistheexample,andfi(E)isthediscriminantfunctioncorrespondingtotheithclass,thechosenclassCkistheoneforwhich1fk(E)>fi(E)∀i6=k.(1)Supposeanexampleisavectorofaattributes,asistypicallythecaseinclassificationap-plications.LetvjkbethevalueofattributeAjintheexample,P(X)denotetheprobabilityofX,andP(Y|X)denotetheconditionalprobabilityofYgivenX.Thenonepossible104P.DOMINGOSANDM.PAZZANITheclassifierobtainedbyusingthissetofdiscriminantfunctions,andestimatingtherelevantprobabilitiesfromthetrainingset,isoftencalledthenaiveBayesianclassifier.Thisisbecause,ifthe“naive”assumptionismadethattheattributesareindependentgiventheclass,thisclassifiercaneasilybeshowntobeoptimal,inthesenseofminimizingthemisclassificationrateorzero-oneloss,byadirectapplicationofBayes’theorem,asfollows.IfP(Ci|E)istheprobabilitythatexampleEisofclassCi,zero-onelossisminimizedif,andonlyif,EisassignedtotheclassCkforwhichP(Ck|E)ismaximum(Duda&Hart,1973).Inotherwords,usingP(Ci|E)asthediscriminantfunctionsfi(E)istheoptimalclassificationprocedure.ByBayes’theorem,P(Ci|E)=P(Ci)P(E|Ci)P(E).(3)P(E)canbeignored,sinceitisthesameforallclasses,anddoesnotaffecttherelativevaluesoftheirprobabilities.Iftheattributesareindependentgiventheclass,P(E|Ci)canbedecomposedintotheproductP(A1=v1k|Ci)...P(Aa=vak|Ci),leadingtoP(Ci|E)=fi(E),asdefinedinEquation2,Q.E.D.Inpractice,attributesareseldomindependentgiventheclass,whichiswhythisassump-tionis“naive.”However,thequestionarisesofwhethertheBayesianclassifier,asdefinedbyEquations1and2,canbeoptimalevenwhentheassumptionofattributeindependencedoesnothold,andthereforeP(Ci|E)6=fi(E).Inthesesituations,theBayesianclassifiercannolongerbesaidtocomputeclassprobabilitiesgiventheexample,butthediscriminantfunctionsdefinedbyEquation2maystillminimizemisclassificationerror.Thequestionofwhetherthesesituationsexisthaspracticalrelevance,sincetheBayesianclassifierhasmanydesirableproperties(simplicity,lowtimeandmemoryrequirements,etc.),andthusmaywellbetheclassifierofchoiceforsuchsituations(i.e.,itwillbechosenoverotherclassifiersthatarealsooptimal,butdifferinotherrespects).However,eventhoughtheBayesianclassifierhasbeenknownforseveraldecades,toourknowledgethisquestionhassofarnotbeenexplored;thetacitassumptionhasalwaysbeenthattheBayesianclassifierwillnotbeoptimalwhenattributeindependencedoesnothold.Inspiteofthisrestrictiveviewofitsapplicability,inrecentyearstherehasbeenagradualrecognitionamongmachinelearningresearchersthattheBayesianclassifiercanperformquitewellinawidevarietyofdomains,includingmanywhereclearattributedependencesexist.EvidenceoftheBayesianclassifier’ssurprisingpracticalvaluehasalsoledtoattemptstoextenditbyincreasingitstoleranceofattributeindependenceinvariousways,butthesuccessoftheseattemptshasbeenuneven.Thisisdescribedinmoredetailinthenextsection.ThisarticlederivesthemostgeneralconditionsfortheBayesianclassifier’soptimality,givingapositiveanswertothequestionofwhetheritcanstillbeoptimalwhenattributesarenotindependentgiventheclass.AcorollaryoftheseresultsisthattheBayesianclassifier’strueregionofoptimalperformanceisinfactfargreaterthanthatimpliedbytheattributeindependenceassumption,andthatitsrangeofapplicabilityisthusmuchbroaderthanpreviouslythought.ThistoleranceofattributedependencealsohelpstoexplainwhyextendingtheBayesianOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER105Theremainderofthearticleelaboratesontheseideas.Section2reviewspreviousempir-icalresultsontheBayesianclassifierinthemachinelearningliterature,andrecentattemptstoextendit.Section3106P.DOMINGOSANDM.PAZZANILangleyandSage(1994)arguedthat,whentwoattributesarecorrelated,itmightbebettertodeleteoneattributethantoassumethetwoareconditionallyindependent.Theyfoundthatanalgorithmforfeaturesubsetselection(forwardsequentialselection)improvedaccuracyonsomedatasets,buthadlittleornoeffectinothers.Inarelatedapproach,Kubat,Flotzinger,andPfurtscheller(1993)foundthatusingadecision-treelearnertoselectfeaturesforuseintheBayesianclassifiergavegoodresultsinthedomainofEEGsignalclassification.Kononenko(1991)proposedsuccessivelyjoiningdependentattributevalues,usingasta-tisticaltesttojudgewhethertwoattributevaluesaresignificantlydependent.Experimentalresultswiththismethodwerenotencouraging.Ontwodomains,themodifiedBayesianclassifierhadtheOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER1073.EmpiricalevidenceWhenevertheoreticalexpectationsandempiricalobservationsdisagree,eithercouldbeatfault.Ontheempiricalside,twopotentialsourcesoferrorcanbereadilyidentified.Theresultsofpreviousauthorscouldbeafluke,duetounusualcharacteristicsofthedatasetsused(especiallysince,inseveralcases,thenumberofdatasetsusedwasrelativelysmall).Alternatively,thesedatasetsmightcontainnosignificantattributedependences,andinthiscasetheBayesianclassifierwouldindeedbeexpectedtoperformwell.Inordertotestboththesehypotheses,weconductedanempiricalstudyon28datasets,comparingtheBayesianclassifierwithotherlearners,andmeasuringthedegreeofattributedependenceinthedatasets.Thelearnersusedwerestate-of-theartrepresentativesofthreemajorapproachestoclassificationlearning:decisiontreeinduction(C4.5release8,Quinlan,1993),instance-basedlearning(PEBLS2.1,Cost&Salzberg,1993)andruleinduction(CN2version6.1,Clark&Boswell,1991).AsimpleBayesianclassifierwasimplementedfortheseexperiments.Threemainissuesarisehere:howtohandlenumericattributes,zerocounts,andmissingvalues.Wedealwitheachinturn.•Numericattributeswerediscretizedintotenequal-lengthintervals(oroneperobservedvalue,whicheverwasleast).AlthoughDoughertyetal.(1995)foundthisapproachtobeslightlylessaccuratethanamoreinformedone,ithastheadvantageofsimplicity,andissufficientforverifyingthattheBayesianclassifierperformsaswellas,orbetterthan,otherlearners.AversionincorporatingtheconventionalassumptionofGaussiandistributionswasalsoimplemented,forpurposesofcomparisonwiththediscretizedone.•Zerocountsareobtainedwhenagivenclassandattributevalueneveroccurtogetherinthetrainingset,andcanbeproblematicbecausetheresultingzeroprobabilitieswillwipeouttheinformationinalltheotherprobabilitiesP(Aj=vjk|Ci)whentheyaremultipliedaccordingtoEquation2.Aprincipledsolutiontothisproblemistoincorporateasmall-samplecorrectionintoallprobabilities,suchastheLaplacecorrection(Niblett,1987).IfnijkisthenumberoftimesclassCiandvaluevjkofattributeAjoccurtogether,andniisthetotalnumberoftimesclassCioccursinthetrainingset,theuncorrectedestimateofP(Aj=vjk|Ci)isnijk/ni,andtheLaplace-correctedestimateisP(Aj=vjk|Ci)=(nijk+f)/(ni+fnj),wherenjisthenumberofvaluesofattributeAj(e.g.,2foraBooleanattribute),andfisamultiplicativefactor.FollowingKohavi,Becker,andSommerfield(1997),theLaplacecorrectionwasusedwithf=1/n,wherenisthenumberofexamplesinthetrainingset.•Missingvalueswereignored,bothwhencomputingcountsfortheprobabilityestimatesandwhenclassifyingatestexample.ThisensurestheBayesianclassifierdoesnotinadvertentlyhaveaccesstomoreinformationthantheotheralgorithms,andifanythingbiasestheresultsagainstit.Twenty-eightdatasetsfromtheUCIrepository(Merz,Murphy&Aha,1997)wereusedinthestudy.Twentyrunswere108P.DOMINGOSANDM.PAZZANITable1.Classificationaccuraciesandsamplestandarddeviations,averagedover20randomtraining/testsplits.“Bayes”istheBayesianclassifierwithdiscretizationand“Gauss”istheBayesianclassifierwithGaussiandistributions.SuperscriptsdenoteconfidencelevelsOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER109Table2.Summaryofaccuracyresults.MeasureBayesGaussC4.5PEBLSCN2No.wins-12-719-916-1120-8No.signif.wins-6-512-812-616-6Signtest-75.096.075.098.0Wilcoxontest-70.096.094.099.8Average79.177.877.277.676.2Rank2.432.753.143.213.46classifierismoreaccuratethanC4.5andCN2,ifthissampleofdatasetsisassumedtoberepresentative.ThefourthlineshowstheconfidencelevelsobtainedbyapplyingthemoresensitiveWilcoxontest(DeGroot,1986)tothe28averageaccuracydifferencesobtained,andresultsinhighconfidencethattheBayesianclassifierismoreaccuratethaneachoftheotherlearners.Thefifthlineshowstheaverageaccuracyacrossalldatasets,andagaintheBayesianclassifierperformsthebest.Thelastlineshowstheaveragerankofeachalgorithm,computedforeachdomainbyassigningrank1tothemostaccuratealgorithm,rank2tothesecondbest,andsoon.TheBayesianclassifieristhebest-rankedofallalgorithms,indicatingthatwhenitdoesnotwinitstilltendstobeoneofthebest.ThecomparativeresultsofthediscretizedandGaussianversionsalsoconfirmtheadvan-tageofdiscretization,althoughonthislargerensembleofdatasetsthedifferenceislesspronouncedthanthatfoundbyDoughertyetal.(1995),andtheGaussianversionalsodoesquitewellcomparedtothenon-Bayesianlearners.Insummary,thepresentlarge-scalestudyconfirmspreviousauthors’observationsonsmallerensemblesofdatasets;infact,thecurrentresultsareevenmorefavorabletotheBayesianclassifier.However,thisdoesnotbyitselfdisprovethenotionthattheBayesianclassifierwillonlydowellwhenattributesareindependentgiventheclass(ornearlyso).Aspointedoutabove,theBayesianclassifier’sgoodperformancecouldsimplybeduetotheabsenceofsignificantattributedependencesinthedata.Toinvestigatethis,weneedtomeasurethedegreeofattributedependenceinthedatainsomeway.Measuringhigh-orderdependenciesisdifficult,becausetherelevantprobabilitiesareapttobeverysmall,andnotreliablyrepresentedinthedata.However,afirstandfeasibleapproachconsistsinmeasuringpairwisedependencies(i.e.,dependenciesbetweenpairsofattributesgiventheclass).GivenattributesAmandAnandtheclassvariableC,apossiblemeasureofthedegreeofpairwisedependencebetweenAmandAngivenC(Wan&Wong,1989;Kononenko,1991)isD(Am,An|C)=H(Am|C)+H(An|C)−H(AmAn|C),(4)whereAmAnrepresentstheCartesianproductof110P.DOMINGOSANDM.PAZZANITable3.Empiricalmeasuresofattributedependence.DataSetRankMax.D%D>0.2Avg.DBreastcancer10.54866.70.093Credit20.79046.70.063Chessendgames40.38325.0OPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER111account.Giventhisempiricalevidence,itisclearthatanewtheoreticalunderstandingoftheBayesianclassifierisneeded.Wenowturntothis.4.AnexampleofoptimalitywithoutindependenceConsideraBooleanconcept,describedbythreeattributesA,BandC.Assumethatthetwoclasses,denotedby+and−,areequiprobable(P(+)=P(−)=12).GivenanexampleE,letP(A|+)beashorthandforP(A=aE|+),aEbeingthevalueofattributeAintheinstance,andsimilarlyfortheotherattributes.LetAandCbeindependent,andletA=B(i.e.,AandBarecompletelydependent).ThereforeBshouldbeignored,andtheoptimalclassificationprocedureforatestinstanceistoassignittoclass+ifP(A|+)P(C|+)−P(A|−)P(C|−)>0,toclass−iftheinequalityhastheoppositesign,andtoanarbitraryclassifthetwosidesareequal.Ontheotherhand,theBayesianclassifierwilltakeBintoaccountasifitwasindependentfromA,andthiswillbeequivalenttocountingAtwice.Thus,theBayesianclassifierwillassigntheinstancetoclass+ifP(A|+)2P(C|+)−P(A|−)2P(C|−)>0,andto−otherwise.ApplyingBayes’theorem,P(A|+)canbereexpressedasP(A)P(+|A)/P(+),andsimilarlyfortheotherprobabilities.SinceP(+)=P(−),aftercancelingliketermsthisleadstotheequivalentexpressionsP(+|A)P(+|C)−P(−|A)P(−|C)>0fortheoptimaldecision,andP(+|A)2P(+|C)−P(−|A)2P(−|C)>0fortheBayesianclassifier.LetP(+|A)=pandP(+|C)=q.Thenclass+shouldbeselectedwhenpq−(1−p)(1−q)>0,whichisequivalenttoq>1−p.WiththeBayesianclassifier,itwillbeselectedwhenp2q−(1−p)2(1−q)>0,whichisequivalenttoq>(1−p)2p2+(1−p)2.ThetwocurvesareshowninFigure1.Theremarkablefactisthat,eventhoughtheindependenceassumptionisdecisivelyviolatedbecauseB=A,theBayesianclassifierdisagreeswiththeoptimalprocedureonlyinthetwonarrowregionsthatareaboveoneofthecurvesandbelowtheother;everywhereelseitperformsthecorrectclassification.Thus,forallproblemswhere(p,q)doesnotfallinthosetwosmallregions,theBayesianclassifieriseffectivelyoptimal.Bycontrast,accordingtotheindependenceassumptionitshouldbeoptimalonlywhenthetwoexpressionsareidentical,i.e.atthethreeisolatedpointswherethecurvescross:(0,1),(12,12)and(1,0).ThisshowsthattheBayesianclassifier’srangeofapplicabilitymayinfactbemuchbroaderthanpreviouslythought.Inthenextsectionweexaminethegeneralcaseandformalizethisresult.5.LocaloptimalityWebeginwithsomenecessarydefinitions.Definition1LetC(E)betheactualclassofexampleE,andletCX(E)betheclassassignedto112P.DOMINGOSANDM.PAZZANI00.20.40.60.8100.20.40.60.81qpSimpleBayesOptimalFigure1.DecisionboundariesfortheBayesianclassifierandtheoptimalclassifier.Zero-onelossisanappropriatemeasureofperformancewhenthetaskisclassification,anditisthemostfrequentlyusedone.Itsimplyassignsacost(loss)ofonetothefailuretoguessthecorrectclass.Insomesituations,differenttypesofmisclassificationhavedifferentcostsassociatedwiththem,andtheuseofafullcostmatrix,specifyingalossvalueforeach(C(E),CX(E))pair,willthenbeappropriate.(Forexample,inmedicaldiagnosisthecostofdiagnosinganillpatientashealthyisgenerallydifferentfromthatofdiagnosingahealthypatientasill.)Inpractice,itoftenoccursthatexampleswithexactlythesameattributevalueshavedifferentclasses.Thisreflectsthefactthatthoseattributesdonotcontainalltheinformationnecessarytouniquelydeterminetheclass.Ingeneral,then,anexampleEwillnotbeassociatedwithasingleclass,butratherwithavectorofclassprobabilitiesP(Ci|E),wheretheithcomponentrepresentsthefractionoftimesthatEappearswithclassCi.Thezero-onelossormisclassificationrateofXonEisthenmoregenerallydefinedasLX(E)=1−P(CX|E),(7)whereCX(E),theclassassignedbyXtoE,isabbreviatedtoCXforsimplicity.P(CX|E)istheaccuracyofXonE.ThisdefinitionOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER113Definition3Aclassifierislocallyoptimalforagivenexampleiffitszero-onelossonthatexampleisequaltotheBayesrate.Definition4Aclassifierisgloballyoptimalforagivensample(dataset)iffitislocallyoptimalforeveryexampleinthatsample.Aclassifierisgloballyoptimalforagivenproblem(domain)iffitisgloballyoptimalforallpossiblesamplesofthatproblem(i.e.,foralldatasetsextractedfromthatdomain).114P.DOMINGOSANDM.PAZZANIEquation2,i.e.,whenr>s.Thusifp>12∧r>stheBayesianclassifierisoptimal.Conversely,whenp=P(+|E)<12,theminimumzero-onelossisp,andisobtainedbyassigningEtoclass−,whichtheBayesianclassifierdoeswhenr<s.ThustheBayesianclassifierisoptimalwhenp<12∧r<s.Whenp=12,eitherdecisionisoptimal,sotheinequalitiescanbegeneralizedasshown.Notethatthisisnotanasymptoticresult:itisvalidevenwhentheprobabilityestimatesusedtocomputerandsareobtainedfromfinitesamples.Corollary1TheBayesianclassifierislocallyoptimalunderzero-onelossinhalfthevolumeofthespaceofpossiblevaluesof(p,r,s).Proof:Sincepisaprobability,andrandsareproductsofprobabilities,(p,r,s)onlytakesvaluesintheunitcube[0,1]3.TheregionofthiscubesatisfyingtheconditioninTheorem1isshownshadedinFigure2;itcaneasilybeseentooccupyhalfofthetotalvolumeofthecube.However,notall(r,s)pairscorrespondtovalidprobabilitycombinations.Sincepisunconstrained,theprojectionOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER115pp<1/2p>1/2rrs00ss(0,0,0)r(1,1,1)Figure2.RegionofoptimalityofthesimpleBayesianclassifier.Proof:ByDefinition4andTheorem1.However,verifyingthisconditiondirectlyonatestsamplewillingeneralnotbepossible,sinceitinvolvesfindingthetrueclassprobabilitiesforallexamplesinthesample.Further,verifyingitforagivendomain(i.e,forallpossiblesamplesextractedfromthatdomain)willingeneralinvolveacomputationofsizeproportionaltothenumberofpossibleexamples,whichisexponentialinthenumberofattributes,andthereforecomputationallyinfeasible.ThustheremainderofthissectionisdedicatedtoinvestigatingmoreconcreteconditionsfortheglobaloptimalityoftheBayesianclassifier,somenecessaryandsomesufficient.Azero-onelossfunctionisassumedthroughout.6.1.NecessaryconditionsLetabethenumberofattributes,asbefore,letcbethenumberofclasses,letvbethemaximumnumberofvaluesper116P.DOMINGOSANDM.PAZZANITheorem3TheBayesianclassifiercannotbegloballyoptimalformorethandc(av+1)differentproblems.Proof:SincetheBayesianclassifier’sstateiscomposedofc(av+1)probabilities,andeachprobabilitycanonlyhaveddifferentvalues,theBayesianclassifiercanonlybeinatmostdc(av+1)states,andthusitcannotdistinguishbetweenmorethanthisnumberofconcepts.Eventhoughdc(av+1)canbeverylarge,thisisasignificantrestrictionbecausemanyconceptclasseshavesizedoublyexponentialina(e.g.,arbitraryDNFformulasinBooleandomains),andduetotheextremelyrapidgrowthofthisfunctiontheBayesianclassifier’scapacitywillbeexceededevenforcommonly-occurringvaluesofa.Ontheotherhand,thisrestrictioniscompatiblewithconceptclasseswhosesizegrowsonlyexponentiallywitha(e.g.,conjunctions).ThisresultreflectstheBayesianclassifier’slimitedcapacityforinformationstorage,andshouldbecontrastedwiththecaseofclassifiers(likeinstance-based,ruleanddecisiontreelearners)whosememorysizecanbeproportionaltothesamplesize.ItalsoOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER117Thisisnotasufficientcondition,becausetheBayesianclassifiercannotlearnsomelinearlyseparableconcepts.Forexample,itfailsforsomem-of-nconcepts,eventhoughtheyarelinearlyseparable.Anm-of-nconcept118P.DOMINGOSANDM.PAZZANIDiff(17,25,j)isnegativeforallj≥19andtheBayesianclassifiermakesfalsenegativeerrorswhenthereare17and18attributesthataretrue.However,asimplemodificationoftheBayesianclassifierwillallowittoperfectlydiscriminateallpositiveexamplesfromnegatives:addingaconstanttothediscriminantfunctionfortheconcept,orsubtractingthesameconstantfromthediscriminantfunctionforitsnegation(Equation1).WehaveimplementedanextensiontotheBayesianclassifierOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER119Theorem7TheBayesianclassifierisgloballyoptimalforlearningconjunctionsofliterals.Proof:SupposetherearenliteralsLjintheconjunction.AliteralmaybeaBooleanattributeoritsnegation.Inaddition,theremaybea−nirrelevantattributes;theysimplycauseeachrowinthetruthtabletobecome2a−nrowswiththesamevaluesfortheclassandallrelevantattributes,eachofthoserowscorrespondingtoapossiblecombinationoftheirrelevantattributes.Forsimplicity,theywillbeignoredfromhereon(i.e.,n=awillbeassumedwithoutlossofgenerality).Recallthat,inthetruthtableforconjunction,theclassCis0(false)forallbutL0=L1=···=Ln=1(true).Thus,usingabartodenotenegation,P(C)=12n,P(C)=2n−12n,P(Lj|C)=1,P(Lj|C)=0,P(Lj|C)=2n−12n−1(thenumberoftimestheliteralis0inthetruthtable,dividedbythenumberoftimestheclassis0),andP(Lj|C)=2n−1−12n−1(thenumberoftimestheliteralis1minustheonetimeitcorrespondstoC,dividedbythenumberoftimestheclassis0).LetEbeanarbitraryexample,andletmoftheconjunction’sliteralsbetrueinE.Forsimplicity,thefactor1/P(E)willbeomittedfromallprobabilities.ThenwehaveP(C|E)=P(C)Pm(Lj|C)Pn−m(Lj|C)=½12nifm=n0otherwiseandP(C|E)=P(C)Pm(Lj|C)Pn−m(Lj|C)=2n−12nµ2n−1−12n−1¶mµ2n−12n−1¶n−m.Noticethat2n−1−12n−1<12foralln.Thus,form=n,P(C|E)=P(C)³2n−1−12n−1´n<P(C)(12)n<12n=P(C|E),andclass1wins.Forallm<n,P(C|E)=0andP(C|E)>0,andthusclass0wins.ThereforetheBayesianclassifieralwaysmakesthecorrectdecision,i.e.,itisgloballyoptimal.Conjunctiveconceptssatisfytheindependenceassumptionforclass1,butnotforclass0.(Forexample,ifC=A0∧A1,P(A1|C)=136=P(A1|C,A0)=0,byinspectionofthetruthtable.)ThusconjunctionsareanexampleofaclassofconceptswheretheBayesianclassifierisinfactoptimal,butwouldnotbeifitrequiredattributeindependence.Thisanalysisassumesthatthewholetruthtableisknown,andthatallexamplesareequallylikely.Whatwillhappenifeitheroftheserestrictionsisremoved?Considerfirstthecasewhereexamplesarenotdistributeduniformly.Form<n,theBayesianclassifieralwaysproducesthecorrectclass,givenasufficientsample.Form=n,theresultwill,ingeneral,dependonthedistribution.ThemoreinterestingandpracticalcaseoccurswhenP(C)>12n,andinthiscaseonecaneasilyverifythattheBayesianclassifiercontinuestogivethecorrectanswers(and,infact,isnowmorerobustwithrespecttosamplefluctuations).ItwillfailifP(C)<12n,butthisisaveryartificialsituation:inpractice,120P.DOMINGOSANDM.PAZZANIexamplesofsuchaconjunctionwouldneverappearinthedata,ortheywouldappearsoinfrequentlythatlearningtheconjunctionwouldbeoflittleornorelevancetotheaccuracy.Atfirstsight,theBayesianclassifiercanalsofailiftheprobabilitiesP(Lj|C)aresuchthattheproductofallnsuchprobabilitiesisgreaterthan12n(or,moreprecisely,greaterthanP(C|E)/P(C)).P(Lj|C)canbeincreasedbyincreasingOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER121tothisquestion,butsomeelementscanbegleanedfromtheresultsinthisarticle,andfromtheliterature.Itiswellknownthatsquarederrorlosscanbedecomposedintothreeadditivecomponents(Friedman,1996):theintrinsicerrorduetonoiseinthesample,thestatisticalbias(system-aticcomponentoftheapproximationerror,orerrorforaninfinitesample)andthevariance(componentoftheerrorduetotheapproximation’ssensitivitytothesample,orerrorduetothesample’sfinitesize).Atrade-offexistsbetweenbiasandvariance,andknowledgeofitcanoftenhelpinunderstandingtherelativebehaviorofestimationalgorithms:thosewithgreaterrepresentationalpower,andthusgreaterabilitytorespondtothesample,tendtohavelowerbias,butalsohighervariance.Recently,severalauthors(Kong&Dietterich,1995;Kohavi&Wolpert,1996;Tibshirani,1996;Breiman,1996;Friedman,1996)haveproposedsimilarbias-variancedecomposi-tionsforzero-onelossfunctions.Inparticular,Friedman(1996)hasshown,usingnormalapproximationstotheclassprobabilities,thatthebias-varianceinteractionnowtakesaverydifferentform.Zero-onelosscanbehighlyinsensitivetosquared-errorbiasintheclassi-fier’sprobabilityestimates,asTheorem1implies,8but,crucially,willingeneralstillbesensitivetoestimationvariance.Thus,aslongasTheorem1’spreconditions122P.DOMINGOSANDM.PAZZANIintermediatecomplexity(d=1wouldproducethesimplestconcepts,andd=athemostcomplexones).Onehundreddifferentdomainsweregeneratedatrandomforeach(n,a)pair.Foreachdomain,nexamplesweregeneratedfortraining,and1000fortesting.Test-setaccuracywasthenaveragedacrossdomains.TheC4.5RULESpostprocessor,whichconvertsdecisiontreestorulesandthusbettermatchesthetargetconceptclass,wasused,andfoundtoindeedincreaseaccuracy,byasmuchOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER1235055606570758010100100010000Accuracy(%)No.ofexamplesBayesC4.55055606570758010100100010000Accuracy(%)No.ofexamplesBayesC4.55055606570758010100100010000Accuracy(%)No.ofexamplesBayesC4.5Figure3.AccuracyoftheBayesianclassifierandC4.5RULESasafunctionofthenumberofexamples,given16attributes(upper),32attributes(middle),and124P.DOMINGOSANDM.PAZZANIzero-oneloss.Overall,Ben-Bassat,Klove,andWeil(1980)haveshownthattheBayesianclassifierisquiterobustwithrespecttoerrorsinprobabilityestimatesduetosmallsamplesize;thisisnotsurprising,sinceitcanbeattributedtothesamefactorsthatmakeitrobustwithrespecttoviolationsoftheindependenceassumption.8.HowistheBayesianclassifierbestextended?OnesignificantconsequenceoftheBayesianclassifier’soptimalityevenwhenstrongat-tributedependencesarepresentisthatdetectingtheseisnotnecessarilythebestwaytoimproveperformance.ThissectionempiricallyteststhisclaimbycomparingPazzani’s(1996)extensionwithonethatdiffersfromitsolelybyusingthemethodforattributede-pendencedetectiondescribedin(Kononenko,1991)and(Wan&Wong,1989).Ineachcase,thealgorithmfindsthesinglebestpairofattributestojoinbyconsideringallpossiblejoins.Twomeasuresfordeterminingthebestpairwerecompared.FollowingPazzani(1996),thefirstmeasurewasestimatedaccuracy,asdeterminedbyleave-one-outcrossval-idationonthetrainingset.Inthesecondmeasure,Equation4wasusedtofindtheattributesthathadthelargestviolationoftheconditionalindependenceassumption.Toconductanexperimenttocomparethesetwoapproaches,amethodisalsorequiredtodecidewhentostopjoiningattributes.OPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER125Table4.AcomparisonoftwoapproachestoextendingtheBayesianclassifier.DataSetTrainingBayesAccuracyEntropyAccuracyEntropySizeOnceOnceRepeatedOptimalExclusiveOR12846.1100.0100.0100.0100.0126P.DOMINGOSANDM.PAZZANI899091929394909192939495Test-setaccuracy(%)Cross-validationaccuracy(%)8990919293940.10.20.3Test-setaccuracy(%)DegreeofcorrelationFigure4.Upper:TherelationshipbetweenaccuracyonthetestsetandusingaccuracyestimationonthetrainingsettodecidewhichCartesianproductattributetoform,plottedforallpairsofattributesinthechessdataset(R2=0.497).Lower:TherelationshipbetweenaccuracyOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER127classifierwasonly92.8%accurateonthisproblem.Whenusingtheentropy-basedapproachtofindingapairofattributestojoin,A1andA2werealwayschosen,andtheclassifierwas128P.DOMINGOSANDM.PAZZANIAcknowledgmentsThefirstauthorwaspartlysupportedbyPRAXISXXIandNATOscholarships.TheauthorsaregratefultothecreatorsoftheC4.5,PEBLSandCN2systems,andtoallthosewhoprovidedtheOPTIMALITYOFTHESIMPLEBAYESIANCLASSIFIER129Clark,P.,&Boswell,R.(1991).RuleinductionwithCN2:Somerecentimprovements.ProceedingsoftheSixthEuropeanWorkingSessiononLearning(pp.151–163).Porto,Portugal:Springer-Verlag.Clark,P.,&Niblett,T.(1989).TheCN2inductionalgorithm.MachineLearning,3,261–283.Cost,S.,&Salzberg,S.(1993).Aweightednearestneighboralgorithmforlearningwithsymbolicfeatures.MachineLearning,10,57–78.DeGroot,M.H.(1986).Probabilityandstatistics(2nded.).Reading,MA:Addison-Wesley.Dietterich,T.(1996).Statisticaltestsfor130P.DOMINGOSANDM.PAZZANIPazzani,M.J.(1996).SearchingfordependenciesinBayesianclassifiers.InD.Fisher&H.-J.Lenz(Eds.),Learningfromdata:ArtificialintelligenceandstatisticsV(pp.239–248).NewYork,NY:Springer-Verlag.Pazzani,M.,Muramatsu,J.,&Billsus,D.(1996).Syskill&Webert:Identifyinginterestingwebsites.ProceedingsoftheThirteenthNationalConferenceonArtificialIntelligence(pp.54–61).Portland,OR:AAAIPress.Pazzani,M.,&Sarrett,W.(1990).Aframeworkforaveragecaseanalysisofconjunctivelearningalgorithms.MachineLearning,9,349–372.Quinlan,J.R.(1993).C4.5:Programsformachinelearning.SanMateo,CA:MorganKaufmann.Russek,E.,Kronmal,R.A.,&Fisher,L.D.(1983).TheeffectofassumingindependenceinapplyingBayes’theoremtoriskestimationandclassificationindiagnosis.ComputersandBiomedicalResearch,16,