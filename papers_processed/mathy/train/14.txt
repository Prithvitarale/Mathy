MachLearn(2006)62:107‚Äì136DOI10.1007/s10994-006-5833-1MarkovlogicnetworksMatthewRichardson¬∑PedroDomingosReceived:11August2004/Revised:31January2005/Accepted:5July2005/Publishedonline:27January2006CSpringerScience+BusinessMedia,Inc.2006AbstractWeproposeasimpleapproachtocombiningfirst-orderlogicandprobabilisticgraphicalmodelsinasinglerepresentation.AMarkovlogicnetwork(MLN)isafirst-orderknowledgebasewithaweightattachedtoeachformula(orclause).Togetherwithasetofconstantsrepresentingobjectsinthedomain,itspecifiesagroundMarkovnetworkcontainingonefeatureforeachpossiblegroundingofafirst-orderformulaintheKB,withthecorrespondingweight.InferenceinMLNsisperformedbyMCMCovertheminimalsubsetofthegroundnetworkrequiredforansweringthequery.Weightsareefficientlylearnedfromrelationaldatabasesbyiterativelyoptimizingapseudo-likelihoodmeasure.Optionally,additionalclausesarelearnedusinginductivelogicprogrammingtechniques.Experimentswithareal-worlddatabaseandknowledgebaseinauniversitydomainillustratethepromiseofthisapproach.KeywordsStatisticalrelationallearning.Markovnetworks.Markovrandomfields.Log-linearmodels.Graphicalmodels.First-orderlogic.Satisfiability.Inductivelogicprogramming.Knowledge-basedmodelconstruction.MarkovchainMonteCarlo.Pseudo-likelihood.Linkprediction1.IntroductionCombiningprobabilityandfirst-orderlogicinasinglerepresentationhaslongbeenagoalofAI.Probabilisticgraphicalmodelsenableustoefficientlyhandleuncertainty.First-orderlogicenablesustocompactlyrepresentawidevarietyofknowledge.ManyEditors:HendrikBlockeel,DavidJensenandStefanKramerM.Richardson¬∑P.Domingos()DepartmentofComputerScienceandEngineering,UniversityofWashington,Seattle,WA98195-2350,USAe-mail:pedrod@cs.washington.edu108MachLearn(2006)62:107‚Äì136(ifnotmost)applicationsrequireboth.Interestinthisproblemhasgrowninrecentyearsduetoitsrelevancetostatisticalrelationallearning(Getoor&Jensen,2000,2003;Dietterichetal.,2003),alsoknownasmulti-relationaldatamining(DzÃåeroski&DeRaedt,2003;DzÃåeroskietal.,2002,2003;DzÃåeroski&Blockeel,2004).Currentpropos-alstypicallyfocusoncombiningprobabilitywithrestrictedsubsetsoffirst-orderlogic,likeHornclauses(e.g.,Wellmanetal.,1992;Poole,1993;Muggleton,1996;Ngo&Haddawy,1997;Sato&Kameya,1997;Cussens,1999;Kersting&DeRaedt,2001;SantosCostaetal.,2003),frame-basedsystems(e.g.,Friedmanetal.,1999;Pasula&Russell,2001;Cumby&Roth,2003),ordatabasequerylanguages(e.g.,Taskaretal.,2002;Popescul&Ungar,2003).Theyareoftenquitecomplex.Inthispaper,weintroduceMarkovlogicnetworks(MLNs),arepresentationthatisquitesimple,yetcombinesprobabilityandfirst-orderlogicwithnorestrictionsotherthanfinitenessofthedomain.WedevelopefficientalgorithmsforinferenceandlearningMachLearn(2006)62:107‚Äì136109oftenconvenientlyrepresentedaslog-linearmodels,witheachcliquepotentialreplacedbyanexponentiatedweightedsumoffeaturesofthestate,leadingtoP(X=x)=1ZexpÔ£´Ô£≠jwjfj(x)Ô£∂Ô£∏(2)Afeaturemaybeanyreal-valuedfunctionofthestate.Thispaperwillfocusonbinaryfeatures,fj(x)‚àà{0,1}.Inthemostdirecttranslationfromthepotential-functionform(Eq.(1)),there110MachLearn(2006)62:107‚Äì136constructedfromatomicformulasusinglogicalconnectivesandquantifiers.IfF1andF2areformulas,thefollowingarealsoformulas:¬¨F1(negation),whichistrueiffF1MachLearn(2006)62:107‚Äì136111Table1Exampleofafirst-orderknowledgebaseandMLN.Fr()isshortforFriends(),Sm()forSmokes(),andCa()forCancer()EnglishFirst-orderlogicClausalformWeightFriendsoffriendsarefriends‚àÄx‚àÄy‚àÄzFr(x,y)‚àßFr(y,z)‚áíFr(x,z)¬¨Fr(x,y)‚à®¬¨Fr(y,z)‚à®Fr(x,z)0.7Friendlesspeoplesmoke‚àÄx(¬¨(‚àÉyFr(x,y))‚áíSm(x))Fr(x,g(x))‚à®Sm(x)2.3Smokingcausescancer‚àÄxSm(x)‚áíCa(x)¬¨Sm(x)‚à®Ca(x)1.5If112MachLearn(2006)62:107‚Äì136Fig.1.GroundMarkovnetworkobtainedbyapplyingthelasttwoformulasinTable1totheconstantsAnna(A)andBob(B)thesecondequalityaboveshows.Thiswillbethemostconvenientapproachindomainswithamixtureofhardandsoftconstraints(i.e.,wheresomeformulasholdwithcertainty,leadingtozeroprobabilitiesforsomeworlds).ThegraphicalstructureofML,CfollowsfromDefinition4.1:thereisanedgebetweentwonodesofML,CMachLearn(2006)62:107‚Äì136113Table2.Constructionofallgroundingsofafirst-orderformulaunderAssumptions1‚Äì3functionGround(F,C)inputs:F,aformulainfirst-orderlogicC,asetofconstantsoutput:GF,asetofgroundformulascalls:CNF(F,C),whichconvertsFtoconjunctivenormalform,replacingexistentiallyquantifiedformulasbydisjunctionsoftheirgroundingsoverCF‚ÜêCNF(F,C)GF=‚àÖforeachclauseFj‚ààFGj={F114MachLearn(2006)62:107‚Äì136MLNs.However,ifwerestrictthelevelofnestingtosomemaximum,theresultingMLNisstillfinite.Tosummarize,Assumptions1‚Äì3canberemovedaslongasthedomainisfinite.WebelieveitispossibletoextendMLNstoinfinitedomains(seeJaeger,1998),butthisisanissueofchieflytheoreticalinterest,andweleaveitforfuturework.IntheremainderofthispaperweproceedunderAssumptions1‚Äì3,exceptwherenoted.Afirst-orderKBcanbetransformedintoanMLNsimplybyassigningaweighttoeachformula.Forexample,theclausesandweightsinthelasttwocolumnsofTable1constituteanMLN.AccordingtothisMLN,otherthingsbeingequal,aworldwherenfriendlesspeoplearenon-smokersise(2.3)ntimeslessprobablethanaworldwhereallfriendlesspeoplesmoke.NoticethatalltheformulasinTable1arefalseintherealworldasuniversallyquantifiedlogicalstatements,butcaptureusefulinformationonfriendshipsandsmokinghabits,whenviewedasfeaturesofaMarkovnetwork.Forexample,itiswellknownthatteenagefriendstendtohavesimilarsmokinghabits(Lloyd-Richardsonetal.,2002).Infact,anMLNliketheoneinTable1succinctlyrepresentsatypeofmodelthatisastapleofsocialnetworkanalysis(Wasserman&Faust,1994).ItiseasytoseethatMLNssubsumeessentiallyallpropositionalprobabilisticmodels,asdetailedbelow.Proposition4.2.Everyprobabilitydistributionoverdiscreteorfinite-precisionnumericvariablescanberepresentedasaMarkovlogicnetwork.Proof:ConsiderfirstthecaseofBooleanvariables(X1,X2,...,Xn).DefineapredicateofzeroarityRhforeachvariableXh,andincludeintheMLNLaformulaforeachpos-siblestateof(X1,X2,...,Xn).Thisformulaisaconjunctionofnliterals,withthehthliteralbeingRh()ifXhistrueinthestate,and¬¨Rh()otherwise.Theformula‚ÄôsweightisMachLearn(2006)62:107‚Äì136115probabilityassignedtoa(setof)possibleworld(s)xbyML,C,XKBbethesetofworldsthatsatisfyKB,andFbeanarbitraryformulainfirst-order116MachLearn(2006)62:107‚Äì136However,ifFsharesvariableswithotherformulas,aswilltypicallybethecase,itmaynotbepossibletokeepthoseformulas‚ÄôstruthvaluesunchangedwhilereversingF‚Äôs.InthisMachLearn(2006)62:107‚Äì136117Table3.NetworkconstructionforinferenceinMLNsfunctionConstructNetwork(F1,F2,L,C)inputs:F1,asetofgroundatomswithunknowntruthvalues(the‚Äúquery‚Äù)F2,asetofgroundatomswithknowntruthvalues(the‚Äúevidence‚Äù)L,aMarkovlogicnetworkC,asetofconstantsoutput:M,agroundMarkovnetworkcalls:MB(q),theMarkovblanketofqinML,CG‚ÜêF1whileF1=√òforallq‚ààF1ifq/‚ààF2F1‚ÜêF1‚à™(MB(q)\G)G‚ÜêG‚à™MB(q)F1‚ÜêF1\{q}returnM,thegroundMarkov118MachLearn(2006)62:107‚Äì136whereFlisthesetofgroundformulasthatXlappearsin,andfi(Xl=xl,Bl=bl)istheMachLearn(2006)62:107‚Äì136119disjoiningthenegationsofalltheR(xi,xj),andthuscanbecountedbycountingthenumberoftruegroundingsofthisclauseandsubtractingitfromthetotalnumberofgroundings.Inlargedomains,thenumberoftruegroundingsofaformulamaybecountedapproxi-mately,byuniformlysamplinggroundingsoftheformulaandcheckingwhethertheyaretrueinthedata.Insmallerdomains,andinourexperimentsbelow,weuseanefficientrecursivealgorithmtofindtheexactcount.AsecondproblemwithEq.(6)isthatcomputingtheexpectednumberoftruegroundingsisalsointractable,requiringinferenceoverthemodel.Further,efficientoptimizationmethodsalsorequirecomputingthelog-likelihooditself(Eq.(3)),andthusthepartitionfunctionZ.ThiscanbedoneapproximatelyusingaMonteCarlomaximumlikelihoodestimator(MC-MLE)(Geyer&Thompson,1992).However,inourexperimentstheGibbssamplingusedtocomputetheMC-MLEsandgradientsdidnotconvergeinreasonabletime,andusingthesamplesfromtheunconvergedchainsyieldedpoorresults.Amoreefficientalternative,widelyusedinareaslikespatialstatistics,socialnetworkmodelingandlanguageprocessing,istooptimizeinsteadthepseudo-likelihood(Besag,1975)P‚àów(X=x)=nl=1Pw(Xl=xl|MBx(Xl))(7)whereMBx(Xl)isthestateoftheMarkovblanketofXlinthedata.Thegradientofthepseudo-log-likelihoodis‚àÇ‚àÇwilogP‚àów(X=x)=nl=1ni(x)‚àíPw(Xl=0|MBx(Xl))nix[Xl=0]‚àíPw(Xl=1|MBx(Xl))nix[Xl=1](8)whereni(x[Xl=0])isthenumberoftruegroundingsoftheithformulawhenweforceXl=0andleavetheremainingdataunchanged,andsimilarlyforni(x[Xl=1]).Computingthisexpression(orEq.(7))doesnotrequireinferenceoverthemodel.Weoptimizethepseudo-log-likelihoodusingthelimited-memoryBFGSalgorithm(Liu&Nocedal,1989).Thecomputationcanbemademoreefficientinseveralways:‚ÄìThesuminEq.(8).canbegreatlyspedupby120MachLearn(2006)62:107‚Äì136learnonlyHornclauses,CLAUDIENisabletolearnarbitraryfirst-orderclauses,makingitwellsuitedtoMLNs.Also,byconstructingaparticularlanguagebias,weareabletodirectCLAUDIENMachLearn(2006)62:107‚Äì136121Info).Inbothcases,wemeasuredtheaverageconditionallog-likelihoodofallpossiblegroundingsofAdvisedBy(x,y)overallareas,drewprecision/recallcurves,andcomputedtheareaunderthecurve.Thistaskisaninstanceoflinkprediction,aproblemthathasbeentheobjectofmuchinterestinstatisticalrelationallearning(seeSection8).AllKBswereconvertedtoclausalform.Timingresultsareona2.8GHzPentium4machine.7.1.122MachLearn(2006)62:107‚Äì136involvescharacteristicsofindividualconstantsinthequerypredicate,andthelatterinvolvescharacteristicsofrelationsbetweentheconstantsinthequerypredicate.Fortheorder-1attributes,wedefinedonevariableMachLearn(2006)62:107‚Äì136123bias.Wedidthisby,foreachclauseintheKB,allowingCLAUDIENto(1)removeanynumberoftheliterals,(2)adduptovnewvariables,and(3)adduptolnewliterals.WeranCLAUDIENfor24hoursonaSun-Blade1000foreach(v,l)intheset{(1,2),(2,3),(3,4)}.Allthreegavenearlyidenticalresults;wereporttheresultswithv=3andl=4.7.1.4.MLNsOurresultscomparetheabovesystemstoMarkovlogicnetworks.TheMLNsweretrainedusingaGaussianweightpriorwithzeromeanandunitvariance,andwiththeweightsinitial-izedatthemodeoftheprior(zero).Foroptimization,weusedtheFortran124MachLearn(2006)62:107‚Äì1367.2.2.Trainingwithpseudo-likelihoodIncontrasttoMC-MLE,pseudo-likelihoodtrainingwasquitefast.AsdiscussedinSection6,eachiterationoftrainingmaybedonequitequicklyoncetheinitialclauseandgroundatomsatisfiabilitycountsarecomplete.Onaverage(overthefivetestsets),findingthesecountstook2.5minutes.Fromthere,trainingtook,onaverage,255iterationsofL-BFGS,foratotalof16minutes.7.2.3.InferenceInferencewasalsoquitequick.InferringtheprobabilityofallAdvisedBy(x,y)atomsintheAllInfocasetook3.3minutesintheAItestset(4624atoms),24.4ingraphics(3721),1.8inprogramminglanguages(784),10.4insystems(5476),and1.6intheory(2704).ThenumberofGibbspassesrangedfrom4270to500,000,andaveraged124,000.Thisamountsto18msperGibbspassandapproximately200,000‚Äì500,000Gibbsstepspersecond.TheaveragetimetoperforminferenceinthePartialInfocasewas14.8minutes(vs.8.3intheAllInfocase).7.2.4.ComparisonofsystemsWecomparedtwelvesystems:theoriginalKB(KB);CLAUDIEN(CL);CLAUDIENwiththeoriginalKBaslanguagebias(CLB);theunionoftheoriginalKBandCLAUDIEN‚Äôsout-putinbothcases(KB+CLandKB+CLB);anMLNwitheachoftheaboveKBs(MLN(KB),MLN(CL),MLN(KB+CL),andMLN(KB+CLB));naiveBayes(NB);andaBayesiannet-worklearner(BN).Add-onesmoothingofprobabilitieswasusedinallcases.Table4summarizestheresults.Figure2showsprecision/recallcurvesforallareas(i.e.,averagedoverallAdvisedBy(x,y)atoms),andFigures3to7showprecision/recallcurvesforthefiveindividualareas.MLNsareclearlymoreaccuratethanthealternatives,showingthepromiseofthisapproach.Thepurelylogicalandpurelyprobabilisticmethodsoftensufferwhenintermediatepredicateshavetobeinferred,whileMLNsarelargelyunaffected.NaiveBayesperformswellinAUCinsometestsets,butverypoorlyinothers;itsCLLsareuniformlypoor.CLAUDIENperformspoorlyonitsown,andproducesnoimprovementwhenaddedtotheKBintheMLN.UsingCLAUDIENtorefinetheKBtypicallyperformsworseinAUCbutbetterinCLLthanusingCLAUDIENfromscratch;overall,thebest-performinglogicalmethodisKB+CLB,butitsresultsfallwellshortofthebestMLNs‚Äô.Thegeneraldrop-offinprecisionaround50%recallisattributabletothefactthatthedatabaseisveryincomplete,andonlyallowsidentifyingaminorityoftheAdvisedByrelations.Inspectionrevealsthattheoccasionalsmallerdrop-offsinprecisionatverylowrecallsareduetostudentswhograduatedorchangedadvisorsafterco-authoringmanypublicationswiththem.8.StatisticalrelationallearningtasksManySRLtaskscanbeconciselyformulatedinthelanguageofMLNs,allowingthealgo-rithmsintroducedinthispapertobedirectlyappliedtothem.Inthissectionweexemplifythiswithfivekeytasks:collectiveclassification,linkprediction,link-basedclustering,socialnetworkmodeling,andobjectidentification.SpringerMachLearn(2006)62:107‚Äì136125Fig.2.Precisionandrecallforallareas:AllInfo(leftgraph)andPartialInfo(rightgraph)Fig.3.PrecisionandrecallfortheAIarea:AllInfo(leftgraph)andPartialInfo126MachLearn(2006)62:107‚Äì136Fig.5.Precisionandrecallfortheprogramminglanguagesarea:AllInfo(leftgraph)andPartialInfo(rightgraph)Fig.6.Precisionandrecallforthesystemsarea:AllInfo(leftgraph)andPartialInfo(rightgraph).ThecurvesfornaiveMachLearn(2006)62:107‚Äì136127Table4.ExperimentalresultsforpredictingAdvisedBy(x,y)whenallotherpredicatesareknown(AllInfo)andwhenStudent(x)andProfessor(x)areunknown(PartialInfo).CLListheaverageconditionallog-likelihood,andAUCistheareaundertheprecision-recallcurve.Theresultsareaveragesoverallatomsinthefivetestsetsandtheirstandarddeviations(Seehttp://www.cs.washington.edu/ai/mlnfordetailsonhowthestandarddeviationsoftheAUCswerecomputed)AllInfoPartialInfoSystemAUCCLLAUCCLLMLN(KB)0.215¬±0.0172‚àí0.052¬±0.0040.224¬±0.0185‚àí0.048¬±0.004MLN(KB+CL)0.152¬±0.0165‚àí0.058¬±0.0050.203¬±0.0196‚àí0.045¬±0.004MLN(KB+CLB)0.011¬±0.0003‚àí3.905¬±0.0480.011¬±0.0003‚àí3.958¬±0.048MLN(CL)0.035¬±0.0008‚àí2.315¬±0.0300.032¬±0.0009‚àí2.478¬±0.030MLN(CLB)0.003¬±0.0000‚àí0.052¬±0.0050.023¬±0.0003‚àí0.338¬±0.002KB0.059¬±0.0081‚àí0.135¬±0.0050.048¬±0.0058‚àí0.063¬±0.004KB+CL0.037¬±0.0012‚àí0.202¬±0.0080.028¬±0.0012‚àí0.122¬±0.006KB+CLB0.084¬±0.0100‚àí0.056¬±0.0040.044¬±0.0064‚àí0.051¬±0.005CL0.048¬±0.0009‚àí0.434¬±0.0120.037¬±0.0001‚àí0.836¬±0.017CLB0.003¬±0.0000‚àí0.052¬±0.0050.010¬±0.0001‚àí0.598¬±0.003NB0.054¬±0.0006‚àí1.214¬±128MachLearn(2006)62:107‚Äì136Crangesoverclusters,andP(C|X)isX‚ÄôsdegreeofmembershipinclusterC.Inlink-basedclustering,objectsareclusteredaccordingtotheirlinks(e.g.,objectsthataremorecloselyrelatedMachLearn(2006)62:107‚Äì136129transitiveclosureisincorporatedbyaddingtheformula‚àÄx‚àÄy‚àÄzx=y‚àßy=z‚áíx=z,withaweightthatcanbelearnedfromdata.9.RelatedworkThereisaverylargeliteraturerelatinglogicandprobability;herewewillfocusonlyontheapproachesmostrelevanttostatisticalrelationallearning,anddiscusshowtheyrelatetoMLNs.9.1.EarlyworkAttemptstocombinelogicandprobabilityinAIdatebacktoatleastNilsson(1986).Bacchus(1990),Halpern(1990)andcoworkers(e.g.,Bacchusetal.,1996)studiedtheprobleminde-tailfromatheoreticalstandpoint.Theymadeadistinctionbetweenstatisticalstatements(e.g.,‚Äú65%ofthestudentsinourdepartmentareundergraduate‚Äù)andstatementsaboutpossibleworlds(e.g.,‚ÄúTheprobabilitythatAnnaisanundergraduateis65%‚Äù),andprovidedmethodsforcomputingthelatterfromtheformer.Intheirapproach,aKBdidnotspecifyacompleteanduniquedistributionoverpossibleworlds,requiringadditionalassumptions130MachLearn(2006)62:107‚Äì136theformulaisw=log[p/(1‚àíp)],wherepistheconditionalprobabilityofthechildpredicatewhenthecorrespondingconjunctionofparentliteralsistrue,accordingtothecombinationfunctionused.Ifthecombinationfunctionislogisticregression,itcanberepresentedusingonlyalinearnumberofformulas,takingadvantageofthefactthatalogisticregressionmodelisa(conditional)Markovnetworkwithabinarycliquebetweeneachpredictorandtheresponse.NoisyORMachLearn(2006)62:107‚Äì136131MLNbywritingdownaformulaforeachlineofeach(class-level)conditionalprobabilitytable(CPT)andvalueofthechildattribute.Theformulaisaconjunctionofliteralsstatingtheparentvaluesandaliteralstatingthechildvalue,anditsweightisthelogarithmofP(x|Parents(x)),thecorrespondingentryintheCPT.Inaddition,theMLNcontainsformulaswithinfiniteweightstatingthateachattributemusttakeexactlyonevalue.ThisapproachhandlesalltypesofuncertaintyinPRMs(attribute,referenceandexistenceuncertainty).AsTaskaretal.(2002)pointout,theneedtoavoidcyclesinPRMscausessignificantrepresentationalandcomputationaldifficulties.InferenceinPRMsisdonebycreatingthecompletegroundnetwork,whichlimitstheirscalability.PRMsrequirespecifyingacompleteconditionalmodelforeachattributeofeachclass,whichinlargecomplexdomainscanbequiteburdensome.Incontrast,MLNscreateacompletejointdistributionfromwhatevernumberoffirst-orderfeaturestheuserchoosestospecify.9.5.RelationalMarkovnetworksRelationalMarkovnetworks(RMNs)usedatabasequeriesascliquetemplates,andhaveafeatureforeachstateofaclique(Taskaretal.,2002).MLNsgeneralizeRMNsbyprovidingamorepowerfullanguageforconstructingfeatures(first-orderlogicinsteadofconjunctivequeries),andbyallowinguncertaintyoverarbitraryrelations(notjustattributesofindividualobjects).RMNsareexponentialincliquesize,whileMLNsallowtheuser(orlearner)todeterminethenumberoffeatures,makingitpossibletoscaletomuchlargercliquesizes.RMNsaretraineddiscriminatively,anddonotspecifyacompletejoint132MachLearn(2006)62:107‚Äì136individualsandtheirrelationstobeexplicitlyrepresented(seeCussens,2003),andcontext-specificindependenciestobecompactlywrittendown,insteadofleftimplicitinthenodemodels.Morerecently,HeckermanMachLearn(2006)62:107‚Äì136133Learning:WeplantodevelopalgorithmsforlearningandrevisingthestructureofMLNsbydirectlyoptimizing(pseudo)likelihood,studyalternateapproachestoweightlearn-ing,trainMLNsdiscriminatively,learnMLNsfromincompletedata,useMLNsforlink-basedclustering,anddevelopmethodsforprobabilisticpredicatediscovery.Applications:WewouldliketoapplyMLNsinavarietyofdomains,includinginformationextractionandintegration,naturallanguageprocessing,vision,socialnetworkanalysis,computationalbiology,etc.11.ConclusionMarkovlogicnetworks(MLNs)areasimplewaytocombineprobabilityandfirst-orderlogicinfinitedomains.AnMLNisobtainedbyattachingweightstotheformulas(orclauses)inafirst-orderknowledgebase,andcanbeviewedasatemplateforconstructingordinaryMarkovnetworks.EachpossiblegroundingofaformulaintheKByieldsafeatureintheconstructednetwork.InferenceisperformedbygroundingtheminimalsubsetofthenetworkrequiredforansweringthequeryandrunningaGibbssampleroverthissubnetwork,withinitialstatesfoundbyMaxWalkSat.Weightsarelearnedbyoptimizingapseudo-likelihoodmeasureusingtheL-BFGSalgorithm,andclausesarelearnedusingtheCLAUDIENsys-tem.Empiricaltestswithreal-worlddataandknowledgeinauniversitydomainillustratethepromiseofMLNs.SourcecodeforlearningandinferenceinMLNsisavailableathttp://www.cs.washington.edu/ai/alchemy.AcknowledgmentsWearegratefultoJulianBesag,VitorSantosCosta,JamesCussens,NileshDalvi,AlanFern,AlonHalevy,MarkHandcock,HenryKautz,KristianKersting,TianSang,BartSelman,DanSuciu,JeremyTantrum,andWeiWeiforhelpfuldiscussions.ThisresearchwaspartlysupportedbyONRgrantN00014-02-1-0408andbyaSloanFellowshipawardedtothesecondauthor.WeusedtheVFMLlibraryinourexperiments(http://www.cs.washington.edu/dm/vfml/).ReferencesBacchus,F.(1990).Representingandreasoningwithprobabilisticknowledge.Cambridge,MA:MITPress.Bacchus,F.,Grove,A.J.,Halpern,J.Y.,&Koller,D.(1996).Fromstatisticalknowledgebasestodegreesofbelief.ArtificialIntelligence,87,75‚Äì143.Bergadano,F.,&Giordana,A.(1988).Aknowledge-intensiveapproachtoconceptinduction.ProceedingsoftheFifthInternationalConferenceonMachineLearning(pp.305‚Äì317).AnnArbor,MI:MorganKaufmann.Berners-Lee,T.,Hendler,J.,&Lassila,O.(2001).TheSemanticWeb.ScientificAmerican,284:5,34‚Äì43.Besag,J.(1975).Statisticalanalysisofnon-latticedata.TheStatistician,24,179‚Äì195.Buntine,W.(1994).Operationsforlearningwithgraphicalmodels.JournalofArtificialIntelligenceResearch,2,159‚Äì225.Byrd,R.H.,Lu,P.,&Nocedal,J.(1995).Alimitedmemoryalgorithmforboundconstrainedoptimization.SIAMJournalonScientificandStatisticalComputing,16,1190‚Äì1208.Chakrabarti,S.,Dom,B.,&Indyk,P.(1998).Enhancedhypertextcategorizationusinghyperlinks.Pro-ceedingsofthe1998ACMSIGMODInternationalConferenceonManagementofData(pp.307‚Äì318).Seattle,WA:ACMPress.Collins,M.(2002).DiscriminativetrainingmethodsforhiddenMarkovmodels:Theoryandexperimentswithperceptronalgorithms.InProceedingsofthe2002ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Philadelphia,PA.Cumby,C.,&Roth,D.(2003).Featureextractionlanguagesforpropositionalized134MachLearn(2006)62:107‚Äì136Cussens,J.(1999).Loglinearmodelsforfirst-orderprobabilisticreasoning.InProceedingsoftheFifteenthConferenceonUncertaintyinArtificialIntelligence(pp.126‚Äì133).Stockholm,Sweden:MorganKaufmann.Cussens,J.(2003).Individuals,relationsMachLearn(2006)62:107‚Äì136135Kautz,H.,Selman,B.,&Jiang,Y.(1997).Ageneralstochasticapproachtosolvingproblemswithhardandsoftconstraints.InD.Gu,J.Du&P.Pardalos(Eds.),Thesatisfiabilityproblem:Theoryandapplications,(pp.573‚Äì586).NewYork,NY:AmericanMathematicalSociety.Kersting,K.,&DeRaedt,L.(2001).TowardscombininginductivelogicprogrammingwithBayesiannetworks.InProceedingsoftheEleventhInternationalConferenceonInductiveLogicProgramming(pp.118‚Äì131).Strasbourg,France:Springer.Laffar,J.,&Lassez,J.(1987).Constraintlogicprogramming.ProceedingsoftheFourteenthACMConferenceonPrinciplesofProgrammingLanguages(pp.111‚Äì119).Munich,Germany:ACMPress.LavracÃå,N.,&DzÃåeroski,S.(1994).InductiveLogicProgramming:TechniquesandApplications.Chichester,UK:EllisHorwood.Liu,D.C.,&Nocedal,J.(1989).136MachLearn(2006)62:107‚Äì136SantosCosta,V.,Page,D.,Qazi,M.,,&Cussens,J.(2003).CLP(BN):Constraintlogicprogrammingforprobabilisticknowledge.InProceedingsoftheNineteenthConferenceonUncertaintyinArtificialIntelligence(pp.517‚Äì524).Acapulco,Mexico:MorganKaufmann.Sato,T.,&Kameya,Y.(1997).PRISM:Asymbolic-statisticalmodelinglanguage.InProceedingsoftheFifteenthInternationalJointConferenceonArtificialIntelligence(pp.1330‚Äì1335).Nagoya,Japan:MorganKaufmann.Taskar,B.,Abbeel,P.,&Koller,D.(2002).Discriminativeprobabilisticmodelsforrelational