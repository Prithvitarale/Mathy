MachLearn(2006)62:107â€“136DOI10.1007/s10994-006-5833-1MarkovlogicnetworksMatthewRichardsonÂ·PedroDomingosReceived:11August2004/Revised:31January2005/Accepted:5July2005/Publishedonline:27January2006CSpringerScience+BusinessMedia,Inc.2006AbstractWeproposeasimpleapproachtocombiningfirst-orderlogicandprobabilisticgraphicalmodelsinasinglerepresentation.AMarkovlogicnetwork(MLN)isafirst-orderknowledgebasewithaweightattachedtoeachformula(orclause).Togetherwithasetofconstantsrepresentingobjectsinthedomain,itspecifiesagroundMarkovnetworkcontainingonefeatureforeachpossiblegroundingofafirst-orderformulaintheKB,withthecorrespondingweight.InferenceinMLNsisperformedbyMCMCovertheminimalsubsetofthegroundnetworkrequiredforansweringthequery.Weightsareefficientlylearnedfromrelationaldatabasesbyiterativelyoptimizingapseudo-likelihoodmeasure.Optionally,additionalclausesarelearnedusinginductivelogicprogrammingtechniques.Experimentswithareal-worlddatabaseandknowledgebaseinauniversitydomainillustratethepromiseofthisapproach.KeywordsStatisticalrelationallearning.Markovnetworks.Markovrandomfields.Log-linearmodels.Graphicalmodels.First-orderlogic.Satisfiability.Inductivelogicprogramming.Knowledge-basedmodelconstruction.MarkovchainMonteCarlo.Pseudo-likelihood.Linkprediction1.IntroductionCombiningprobabilityandfirst-orderlogicinasinglerepresentationhaslongbeenagoalofAI.Probabilisticgraphicalmodelsenableustoefficientlyhandleuncertainty.First-orderlogicenablesustocompactlyrepresentawidevarietyofknowledge.ManyEditors:HendrikBlockeel,DavidJensenandStefanKramerM.RichardsonÂ·P.Domingos()DepartmentofComputerScienceandEngineering,UniversityofWashington,Seattle,WA98195-2350,USAe-mail:pedrod@cs.washington.edu108MachLearn(2006)62:107â€“136(ifnotmost)applicationsrequireboth.Interestinthisproblemhasgrowninrecentyearsduetoitsrelevancetostatisticalrelationallearning(Getoor&Jensen,2000,2003;Dietterichetal.,2003),alsoknownasmulti-relationaldatamining(DzÌŒeroski&DeRaedt,2003;DzÌŒeroskietal.,2002,2003;DzÌŒeroski&Blockeel,2004).Currentpropos-alstypicallyfocusoncombiningprobabilitywithrestrictedsubsetsoffirst-orderlogic,likeHornclauses(e.g.,Wellmanetal.,1992;Poole,1993;Muggleton,1996;Ngo&Haddawy,1997;Sato&Kameya,1997;Cussens,1999;Kersting&DeRaedt,2001;SantosCostaetal.,2003),frame-basedsystems(e.g.,Friedmanetal.,1999;Pasula&Russell,2001;Cumby&Roth,2003),ordatabasequerylanguages(e.g.,Taskaretal.,2002;Popescul&Ungar,2003).Theyareoftenquitecomplex.Inthispaper,weintroduceMarkovlogicnetworks(MLNs),arepresentationthatisquitesimple,yetcombinesprobabilityandfirst-orderlogicwithnorestrictionsotherthanfinitenessofthedomain.WedevelopefficientalgorithmsforinferenceandlearningMachLearn(2006)62:107â€“136109oftenconvenientlyrepresentedaslog-linearmodels,witheachcliquepotentialreplacedbyanexponentiatedweightedsumoffeaturesofthestate,leadingtoP(X=x)=1Zexpï£«ï£­jwjfj(x)ï£¶ï£¸(2)Afeaturemaybeanyreal-valuedfunctionofthestate.Thispaperwillfocusonbinaryfeatures,fj(x)âˆˆ{0,1}.Inthemostdirecttranslationfromthepotential-functionform(Eq.(1)),there110MachLearn(2006)62:107â€“136constructedfromatomicformulasusinglogicalconnectivesandquantifiers.IfF1andF2areformulas,thefollowingarealsoformulas:Â¬F1(negation),whichistrueiffF1MachLearn(2006)62:107â€“136111Table1Exampleofafirst-orderknowledgebaseandMLN.Fr()isshortforFriends(),Sm()forSmokes(),andCa()forCancer()EnglishFirst-orderlogicClausalformWeightFriendsoffriendsarefriendsâˆ€xâˆ€yâˆ€zFr(x,y)âˆ§Fr(y,z)â‡’Fr(x,z)Â¬Fr(x,y)âˆ¨Â¬Fr(y,z)âˆ¨Fr(x,z)0.7Friendlesspeoplesmokeâˆ€x(Â¬(âˆƒyFr(x,y))â‡’Sm(x))Fr(x,g(x))âˆ¨Sm(x)2.3Smokingcausescancerâˆ€xSm(x)â‡’Ca(x)Â¬Sm(x)âˆ¨Ca(x)1.5If112MachLearn(2006)62:107â€“136Fig.1.GroundMarkovnetworkobtainedbyapplyingthelasttwoformulasinTable1totheconstantsAnna(A)andBob(B)thesecondequalityaboveshows.Thiswillbethemostconvenientapproachindomainswithamixtureofhardandsoftconstraints(i.e.,wheresomeformulasholdwithcertainty,leadingtozeroprobabilitiesforsomeworlds).ThegraphicalstructureofML,CfollowsfromDefinition4.1:thereisanedgebetweentwonodesofML,CMachLearn(2006)62:107â€“136113Table2.Constructionofallgroundingsofafirst-orderformulaunderAssumptions1â€“3functionGround(F,C)inputs:F,aformulainfirst-orderlogicC,asetofconstantsoutput:GF,asetofgroundformulascalls:CNF(F,C),whichconvertsFtoconjunctivenormalform,replacingexistentiallyquantifiedformulasbydisjunctionsoftheirgroundingsoverCFâ†CNF(F,C)GF=âˆ…foreachclauseFjâˆˆFGj={F114MachLearn(2006)62:107â€“136MLNs.However,ifwerestrictthelevelofnestingtosomemaximum,theresultingMLNisstillfinite.Tosummarize,Assumptions1â€“3canberemovedaslongasthedomainisfinite.WebelieveitispossibletoextendMLNstoinfinitedomains(seeJaeger,1998),butthisisanissueofchieflytheoreticalinterest,andweleaveitforfuturework.IntheremainderofthispaperweproceedunderAssumptions1â€“3,exceptwherenoted.Afirst-orderKBcanbetransformedintoanMLNsimplybyassigningaweighttoeachformula.Forexample,theclausesandweightsinthelasttwocolumnsofTable1constituteanMLN.AccordingtothisMLN,otherthingsbeingequal,aworldwherenfriendlesspeoplearenon-smokersise(2.3)ntimeslessprobablethanaworldwhereallfriendlesspeoplesmoke.NoticethatalltheformulasinTable1arefalseintherealworldasuniversallyquantifiedlogicalstatements,butcaptureusefulinformationonfriendshipsandsmokinghabits,whenviewedasfeaturesofaMarkovnetwork.Forexample,itiswellknownthatteenagefriendstendtohavesimilarsmokinghabits(Lloyd-Richardsonetal.,2002).Infact,anMLNliketheoneinTable1succinctlyrepresentsatypeofmodelthatisastapleofsocialnetworkanalysis(Wasserman&Faust,1994).ItiseasytoseethatMLNssubsumeessentiallyallpropositionalprobabilisticmodels,asdetailedbelow.Proposition4.2.Everyprobabilitydistributionoverdiscreteorfinite-precisionnumericvariablescanberepresentedasaMarkovlogicnetwork.Proof:ConsiderfirstthecaseofBooleanvariables(X1,X2,...,Xn).DefineapredicateofzeroarityRhforeachvariableXh,andincludeintheMLNLaformulaforeachpos-siblestateof(X1,X2,...,Xn).Thisformulaisaconjunctionofnliterals,withthehthliteralbeingRh()ifXhistrueinthestate,andÂ¬Rh()otherwise.Theformulaâ€™sweightisMachLearn(2006)62:107â€“136115probabilityassignedtoa(setof)possibleworld(s)xbyML,C,XKBbethesetofworldsthatsatisfyKB,andFbeanarbitraryformulainfirst-order116MachLearn(2006)62:107â€“136However,ifFsharesvariableswithotherformulas,aswilltypicallybethecase,itmaynotbepossibletokeepthoseformulasâ€™struthvaluesunchangedwhilereversingFâ€™s.InthisMachLearn(2006)62:107â€“136117Table3.NetworkconstructionforinferenceinMLNsfunctionConstructNetwork(F1,F2,L,C)inputs:F1,asetofgroundatomswithunknowntruthvalues(theâ€œqueryâ€)F2,asetofgroundatomswithknowntruthvalues(theâ€œevidenceâ€)L,aMarkovlogicnetworkC,asetofconstantsoutput:M,agroundMarkovnetworkcalls:MB(q),theMarkovblanketofqinML,CGâ†F1whileF1=Ã˜forallqâˆˆF1ifq/âˆˆF2F1â†F1âˆª(MB(q)\G)Gâ†GâˆªMB(q)F1â†F1\{q}returnM,thegroundMarkov118MachLearn(2006)62:107â€“136whereFlisthesetofgroundformulasthatXlappearsin,andfi(Xl=xl,Bl=bl)istheMachLearn(2006)62:107â€“136119disjoiningthenegationsofalltheR(xi,xj),andthuscanbecountedbycountingthenumberoftruegroundingsofthisclauseandsubtractingitfromthetotalnumberofgroundings.Inlargedomains,thenumberoftruegroundingsofaformulamaybecountedapproxi-mately,byuniformlysamplinggroundingsoftheformulaandcheckingwhethertheyaretrueinthedata.Insmallerdomains,andinourexperimentsbelow,weuseanefficientrecursivealgorithmtofindtheexactcount.AsecondproblemwithEq.(6)isthatcomputingtheexpectednumberoftruegroundingsisalsointractable,requiringinferenceoverthemodel.Further,efficientoptimizationmethodsalsorequirecomputingthelog-likelihooditself(Eq.(3)),andthusthepartitionfunctionZ.ThiscanbedoneapproximatelyusingaMonteCarlomaximumlikelihoodestimator(MC-MLE)(Geyer&Thompson,1992).However,inourexperimentstheGibbssamplingusedtocomputetheMC-MLEsandgradientsdidnotconvergeinreasonabletime,andusingthesamplesfromtheunconvergedchainsyieldedpoorresults.Amoreefficientalternative,widelyusedinareaslikespatialstatistics,socialnetworkmodelingandlanguageprocessing,istooptimizeinsteadthepseudo-likelihood(Besag,1975)Pâˆ—w(X=x)=nl=1Pw(Xl=xl|MBx(Xl))(7)whereMBx(Xl)isthestateoftheMarkovblanketofXlinthedata.Thegradientofthepseudo-log-likelihoodisâˆ‚âˆ‚wilogPâˆ—w(X=x)=nl=1ni(x)âˆ’Pw(Xl=0|MBx(Xl))nix[Xl=0]âˆ’Pw(Xl=1|MBx(Xl))nix[Xl=1](8)whereni(x[Xl=0])isthenumberoftruegroundingsoftheithformulawhenweforceXl=0andleavetheremainingdataunchanged,andsimilarlyforni(x[Xl=1]).Computingthisexpression(orEq.(7))doesnotrequireinferenceoverthemodel.Weoptimizethepseudo-log-likelihoodusingthelimited-memoryBFGSalgorithm(Liu&Nocedal,1989).Thecomputationcanbemademoreefficientinseveralways:â€“ThesuminEq.(8).canbegreatlyspedupby120MachLearn(2006)62:107â€“136learnonlyHornclauses,CLAUDIENisabletolearnarbitraryfirst-orderclauses,makingitwellsuitedtoMLNs.Also,byconstructingaparticularlanguagebias,weareabletodirectCLAUDIENMachLearn(2006)62:107â€“136121Info).Inbothcases,wemeasuredtheaverageconditionallog-likelihoodofallpossiblegroundingsofAdvisedBy(x,y)overallareas,drewprecision/recallcurves,andcomputedtheareaunderthecurve.Thistaskisaninstanceoflinkprediction,aproblemthathasbeentheobjectofmuchinterestinstatisticalrelationallearning(seeSection8).AllKBswereconvertedtoclausalform.Timingresultsareona2.8GHzPentium4machine.7.1.122MachLearn(2006)62:107â€“136involvescharacteristicsofindividualconstantsinthequerypredicate,andthelatterinvolvescharacteristicsofrelationsbetweentheconstantsinthequerypredicate.Fortheorder-1attributes,wedefinedonevariableMachLearn(2006)62:107â€“136123bias.Wedidthisby,foreachclauseintheKB,allowingCLAUDIENto(1)removeanynumberoftheliterals,(2)adduptovnewvariables,and(3)adduptolnewliterals.WeranCLAUDIENfor24hoursonaSun-Blade1000foreach(v,l)intheset{(1,2),(2,3),(3,4)}.Allthreegavenearlyidenticalresults;wereporttheresultswithv=3andl=4.7.1.4.MLNsOurresultscomparetheabovesystemstoMarkovlogicnetworks.TheMLNsweretrainedusingaGaussianweightpriorwithzeromeanandunitvariance,andwiththeweightsinitial-izedatthemodeoftheprior(zero).Foroptimization,weusedtheFortran124MachLearn(2006)62:107â€“1367.2.2.Trainingwithpseudo-likelihoodIncontrasttoMC-MLE,pseudo-likelihoodtrainingwasquitefast.AsdiscussedinSection6,eachiterationoftrainingmaybedonequitequicklyoncetheinitialclauseandgroundatomsatisfiabilitycountsarecomplete.Onaverage(overthefivetestsets),findingthesecountstook2.5minutes.Fromthere,trainingtook,onaverage,255iterationsofL-BFGS,foratotalof16minutes.7.2.3.InferenceInferencewasalsoquitequick.InferringtheprobabilityofallAdvisedBy(x,y)atomsintheAllInfocasetook3.3minutesintheAItestset(4624atoms),24.4ingraphics(3721),1.8inprogramminglanguages(784),10.4insystems(5476),and1.6intheory(2704).ThenumberofGibbspassesrangedfrom4270to500,000,andaveraged124,000.Thisamountsto18msperGibbspassandapproximately200,000â€“500,000Gibbsstepspersecond.TheaveragetimetoperforminferenceinthePartialInfocasewas14.8minutes(vs.8.3intheAllInfocase).7.2.4.ComparisonofsystemsWecomparedtwelvesystems:theoriginalKB(KB);CLAUDIEN(CL);CLAUDIENwiththeoriginalKBaslanguagebias(CLB);theunionoftheoriginalKBandCLAUDIENâ€™sout-putinbothcases(KB+CLandKB+CLB);anMLNwitheachoftheaboveKBs(MLN(KB),MLN(CL),MLN(KB+CL),andMLN(KB+CLB));naiveBayes(NB);andaBayesiannet-worklearner(BN).Add-onesmoothingofprobabilitieswasusedinallcases.Table4summarizestheresults.Figure2showsprecision/recallcurvesforallareas(i.e.,averagedoverallAdvisedBy(x,y)atoms),andFigures3to7showprecision/recallcurvesforthefiveindividualareas.MLNsareclearlymoreaccuratethanthealternatives,showingthepromiseofthisapproach.Thepurelylogicalandpurelyprobabilisticmethodsoftensufferwhenintermediatepredicateshavetobeinferred,whileMLNsarelargelyunaffected.NaiveBayesperformswellinAUCinsometestsets,butverypoorlyinothers;itsCLLsareuniformlypoor.CLAUDIENperformspoorlyonitsown,andproducesnoimprovementwhenaddedtotheKBintheMLN.UsingCLAUDIENtorefinetheKBtypicallyperformsworseinAUCbutbetterinCLLthanusingCLAUDIENfromscratch;overall,thebest-performinglogicalmethodisKB+CLB,butitsresultsfallwellshortofthebestMLNsâ€™.Thegeneraldrop-offinprecisionaround50%recallisattributabletothefactthatthedatabaseisveryincomplete,andonlyallowsidentifyingaminorityoftheAdvisedByrelations.Inspectionrevealsthattheoccasionalsmallerdrop-offsinprecisionatverylowrecallsareduetostudentswhograduatedorchangedadvisorsafterco-authoringmanypublicationswiththem.8.StatisticalrelationallearningtasksManySRLtaskscanbeconciselyformulatedinthelanguageofMLNs,allowingthealgo-rithmsintroducedinthispapertobedirectlyappliedtothem.Inthissectionweexemplifythiswithfivekeytasks:collectiveclassification,linkprediction,link-basedclustering,socialnetworkmodeling,andobjectidentification.SpringerMachLearn(2006)62:107â€“136125Fig.2.Precisionandrecallforallareas:AllInfo(leftgraph)andPartialInfo(rightgraph)Fig.3.PrecisionandrecallfortheAIarea:AllInfo(leftgraph)andPartialInfo126MachLearn(2006)62:107â€“136Fig.5.Precisionandrecallfortheprogramminglanguagesarea:AllInfo(leftgraph)andPartialInfo(rightgraph)Fig.6.Precisionandrecallforthesystemsarea:AllInfo(leftgraph)andPartialInfo(rightgraph).ThecurvesfornaiveMachLearn(2006)62:107â€“136127Table4.ExperimentalresultsforpredictingAdvisedBy(x,y)whenallotherpredicatesareknown(AllInfo)andwhenStudent(x)andProfessor(x)areunknown(PartialInfo).CLListheaverageconditionallog-likelihood,andAUCistheareaundertheprecision-recallcurve.Theresultsareaveragesoverallatomsinthefivetestsetsandtheirstandarddeviations(Seehttp://www.cs.washington.edu/ai/mlnfordetailsonhowthestandarddeviationsoftheAUCswerecomputed)AllInfoPartialInfoSystemAUCCLLAUCCLLMLN(KB)0.215Â±0.0172âˆ’0.052Â±0.0040.224Â±0.0185âˆ’0.048Â±0.004MLN(KB+CL)0.152Â±0.0165âˆ’0.058Â±0.0050.203Â±0.0196âˆ’0.045Â±0.004MLN(KB+CLB)0.011Â±0.0003âˆ’3.905Â±0.0480.011Â±0.0003âˆ’3.958Â±0.048MLN(CL)0.035Â±0.0008âˆ’2.315Â±0.0300.032Â±0.0009âˆ’2.478Â±0.030MLN(CLB)0.003Â±0.0000âˆ’0.052Â±0.0050.023Â±0.0003âˆ’0.338Â±0.002KB0.059Â±0.0081âˆ’0.135Â±0.0050.048Â±0.0058âˆ’0.063Â±0.004KB+CL0.037Â±0.0012âˆ’0.202Â±0.0080.028Â±0.0012âˆ’0.122Â±0.006KB+CLB0.084Â±0.0100âˆ’0.056Â±0.0040.044Â±0.0064âˆ’0.051Â±0.005CL0.048Â±0.0009âˆ’0.434Â±0.0120.037Â±0.0001âˆ’0.836Â±0.017CLB0.003Â±0.0000âˆ’0.052Â±0.0050.010Â±0.0001âˆ’0.598Â±0.003NB0.054Â±0.0006âˆ’1.214Â±128MachLearn(2006)62:107â€“136Crangesoverclusters,andP(C|X)isXâ€™sdegreeofmembershipinclusterC.Inlink-basedclustering,objectsareclusteredaccordingtotheirlinks(e.g.,objectsthataremorecloselyrelatedMachLearn(2006)62:107â€“136129transitiveclosureisincorporatedbyaddingtheformulaâˆ€xâˆ€yâˆ€zx=yâˆ§y=zâ‡’x=z,withaweightthatcanbelearnedfromdata.9.RelatedworkThereisaverylargeliteraturerelatinglogicandprobability;herewewillfocusonlyontheapproachesmostrelevanttostatisticalrelationallearning,anddiscusshowtheyrelatetoMLNs.9.1.EarlyworkAttemptstocombinelogicandprobabilityinAIdatebacktoatleastNilsson(1986).Bacchus(1990),Halpern(1990)andcoworkers(e.g.,Bacchusetal.,1996)studiedtheprobleminde-tailfromatheoreticalstandpoint.Theymadeadistinctionbetweenstatisticalstatements(e.g.,â€œ65%ofthestudentsinourdepartmentareundergraduateâ€)andstatementsaboutpossibleworlds(e.g.,â€œTheprobabilitythatAnnaisanundergraduateis65%â€),andprovidedmethodsforcomputingthelatterfromtheformer.Intheirapproach,aKBdidnotspecifyacompleteanduniquedistributionoverpossibleworlds,requiringadditionalassumptions130MachLearn(2006)62:107â€“136theformulaisw=log[p/(1âˆ’p)],wherepistheconditionalprobabilityofthechildpredicatewhenthecorrespondingconjunctionofparentliteralsistrue,accordingtothecombinationfunctionused.Ifthecombinationfunctionislogisticregression,itcanberepresentedusingonlyalinearnumberofformulas,takingadvantageofthefactthatalogisticregressionmodelisa(conditional)Markovnetworkwithabinarycliquebetweeneachpredictorandtheresponse.NoisyORMachLearn(2006)62:107â€“136131MLNbywritingdownaformulaforeachlineofeach(class-level)conditionalprobabilitytable(CPT)andvalueofthechildattribute.Theformulaisaconjunctionofliteralsstatingtheparentvaluesandaliteralstatingthechildvalue,anditsweightisthelogarithmofP(x|Parents(x)),thecorrespondingentryintheCPT.Inaddition,theMLNcontainsformulaswithinfiniteweightstatingthateachattributemusttakeexactlyonevalue.ThisapproachhandlesalltypesofuncertaintyinPRMs(attribute,referenceandexistenceuncertainty).AsTaskaretal.(2002)pointout,theneedtoavoidcyclesinPRMscausessignificantrepresentationalandcomputationaldifficulties.InferenceinPRMsisdonebycreatingthecompletegroundnetwork,whichlimitstheirscalability.PRMsrequirespecifyingacompleteconditionalmodelforeachattributeofeachclass,whichinlargecomplexdomainscanbequiteburdensome.Incontrast,MLNscreateacompletejointdistributionfromwhatevernumberoffirst-orderfeaturestheuserchoosestospecify.9.5.RelationalMarkovnetworksRelationalMarkovnetworks(RMNs)usedatabasequeriesascliquetemplates,andhaveafeatureforeachstateofaclique(Taskaretal.,2002).MLNsgeneralizeRMNsbyprovidingamorepowerfullanguageforconstructingfeatures(first-orderlogicinsteadofconjunctivequeries),andbyallowinguncertaintyoverarbitraryrelations(notjustattributesofindividualobjects).RMNsareexponentialincliquesize,whileMLNsallowtheuser(orlearner)todeterminethenumberoffeatures,makingitpossibletoscaletomuchlargercliquesizes.RMNsaretraineddiscriminatively,anddonotspecifyacompletejoint132MachLearn(2006)62:107â€“136individualsandtheirrelationstobeexplicitlyrepresented(seeCussens,2003),andcontext-specificindependenciestobecompactlywrittendown,insteadofleftimplicitinthenodemodels.Morerecently,HeckermanMachLearn(2006)62:107â€“136133Learning:WeplantodevelopalgorithmsforlearningandrevisingthestructureofMLNsbydirectlyoptimizing(pseudo)likelihood,studyalternateapproachestoweightlearn-ing,trainMLNsdiscriminatively,learnMLNsfromincompletedata,useMLNsforlink-basedclustering,anddevelopmethodsforprobabilisticpredicatediscovery.Applications:WewouldliketoapplyMLNsinavarietyofdomains,includinginformationextractionandintegration,naturallanguageprocessing,vision,socialnetworkanalysis,computationalbiology,etc.11.ConclusionMarkovlogicnetworks(MLNs)areasimplewaytocombineprobabilityandfirst-orderlogicinfinitedomains.AnMLNisobtainedbyattachingweightstotheformulas(orclauses)inafirst-orderknowledgebase,andcanbeviewedasatemplateforconstructingordinaryMarkovnetworks.EachpossiblegroundingofaformulaintheKByieldsafeatureintheconstructednetwork.InferenceisperformedbygroundingtheminimalsubsetofthenetworkrequiredforansweringthequeryandrunningaGibbssampleroverthissubnetwork,withinitialstatesfoundbyMaxWalkSat.Weightsarelearnedbyoptimizingapseudo-likelihoodmeasureusingtheL-BFGSalgorithm,andclausesarelearnedusingtheCLAUDIENsys-tem.Empiricaltestswithreal-worlddataandknowledgeinauniversitydomainillustratethepromiseofMLNs.SourcecodeforlearningandinferenceinMLNsisavailableathttp://www.cs.washington.edu/ai/alchemy.AcknowledgmentsWearegratefultoJulianBesag,VitorSantosCosta,JamesCussens,NileshDalvi,AlanFern,AlonHalevy,MarkHandcock,HenryKautz,KristianKersting,TianSang,BartSelman,DanSuciu,JeremyTantrum,andWeiWeiforhelpfuldiscussions.ThisresearchwaspartlysupportedbyONRgrantN00014-02-1-0408andbyaSloanFellowshipawardedtothesecondauthor.WeusedtheVFMLlibraryinourexperiments(http://www.cs.washington.edu/dm/vfml/).ReferencesBacchus,F.(1990).Representingandreasoningwithprobabilisticknowledge.Cambridge,MA:MITPress.Bacchus,F.,Grove,A.J.,Halpern,J.Y.,&Koller,D.(1996).Fromstatisticalknowledgebasestodegreesofbelief.ArtificialIntelligence,87,75â€“143.Bergadano,F.,&Giordana,A.(1988).Aknowledge-intensiveapproachtoconceptinduction.ProceedingsoftheFifthInternationalConferenceonMachineLearning(pp.305â€“317).AnnArbor,MI:MorganKaufmann.Berners-Lee,T.,Hendler,J.,&Lassila,O.(2001).TheSemanticWeb.ScientificAmerican,284:5,34â€“43.Besag,J.(1975).Statisticalanalysisofnon-latticedata.TheStatistician,24,179â€“195.Buntine,W.(1994).Operationsforlearningwithgraphicalmodels.JournalofArtificialIntelligenceResearch,2,159â€“225.Byrd,R.H.,Lu,P.,&Nocedal,J.(1995).Alimitedmemoryalgorithmforboundconstrainedoptimization.SIAMJournalonScientificandStatisticalComputing,16,1190â€“1208.Chakrabarti,S.,Dom,B.,&Indyk,P.(1998).Enhancedhypertextcategorizationusinghyperlinks.Pro-ceedingsofthe1998ACMSIGMODInternationalConferenceonManagementofData(pp.307â€“318).Seattle,WA:ACMPress.Collins,M.(2002).DiscriminativetrainingmethodsforhiddenMarkovmodels:Theoryandexperimentswithperceptronalgorithms.InProceedingsofthe2002ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Philadelphia,PA.Cumby,C.,&Roth,D.(2003).Featureextractionlanguagesforpropositionalized134MachLearn(2006)62:107â€“136Cussens,J.(1999).Loglinearmodelsforfirst-orderprobabilisticreasoning.InProceedingsoftheFifteenthConferenceonUncertaintyinArtificialIntelligence(pp.126â€“133).Stockholm,Sweden:MorganKaufmann.Cussens,J.(2003).Individuals,relationsMachLearn(2006)62:107â€“136135Kautz,H.,Selman,B.,&Jiang,Y.(1997).Ageneralstochasticapproachtosolvingproblemswithhardandsoftconstraints.InD.Gu,J.Du&P.Pardalos(Eds.),Thesatisfiabilityproblem:Theoryandapplications,(pp.573â€“586).NewYork,NY:AmericanMathematicalSociety.Kersting,K.,&DeRaedt,L.(2001).TowardscombininginductivelogicprogrammingwithBayesiannetworks.InProceedingsoftheEleventhInternationalConferenceonInductiveLogicProgramming(pp.118â€“131).Strasbourg,France:Springer.Laffar,J.,&Lassez,J.(1987).Constraintlogicprogramming.ProceedingsoftheFourteenthACMConferenceonPrinciplesofProgrammingLanguages(pp.111â€“119).Munich,Germany:ACMPress.LavracÌŒ,N.,&DzÌŒeroski,S.(1994).InductiveLogicProgramming:TechniquesandApplications.Chichester,UK:EllisHorwood.Liu,D.C.,&Nocedal,J.(1989).136MachLearn(2006)62:107â€“136SantosCosta,V.,Page,D.,Qazi,M.,,&Cussens,J.(2003).CLP(BN):Constraintlogicprogrammingforprobabilisticknowledge.InProceedingsoftheNineteenthConferenceonUncertaintyinArtificialIntelligence(pp.517â€“524).Acapulco,Mexico:MorganKaufmann.Sato,T.,&Kameya,Y.(1997).PRISM:Asymbolic-statisticalmodelinglanguage.InProceedingsoftheFifteenthInternationalJointConferenceonArtificialIntelligence(pp.1330â€“1335).Nagoya,Japan:MorganKaufmann.Taskar,B.,Abbeel,P.,&Koller,D.(2002).Discriminativeprobabilisticmodelsforrelational