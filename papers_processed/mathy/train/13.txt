Matrix Completion from a Few Entries
Raghunandan H. Keshavanâˆ—
, Andrea Montanariâˆ—â€ 
, and Sewoong Ohâˆ—
March 17, 2009
Abstract
Let M be a random nÎ± Ã— n matrix of rank r â‰ª n, and assume that a uniformly random
subset E of its entries is observed. We describe an efficient algorithm that reconstructs M from
|E| = O(r n) observed entries with relative root mean square error
RMSE â‰¤ C(Î±)

nr
|E|
1/2
.
Further, if r = O(1), M can be reconstructed exactly from |E| = O(n log n) entries. These results
apply beyond random matrices to general low-rank incoherent matrices.
This settles (in the case of bounded rank) a question left open by CandeÌ€s and Recht and
improves over the guarantees for their reconstruction algorithm. The complexity of our algorithm
is O(|E|r log n), which opens the way to its use for massive data sets. In the process of proving
these statements, we obtain a generalization of a celebrated result by Friedman-Kahn-SzemereÌdi
and Feige-Ofek on the spectrum of sparse random matrices.
1 Introduction
Imagine that each of m customers watches and rates a subset of the n movies available through a
movie rental service. This yields a dataset of customer-movie pairs (i, j) âˆˆ E âŠ† [m] Ã— [n] and, for
each such pair, a rating Mij âˆˆ R. The objective of collaborative filtering is to predict the rating for
the missing pairs in such a way as to provide targeted suggestions.1 The general question we address
here is: Under which conditions do the known ratings provide sufficient information to infer the
unknown ones? Can this inference problem be solved efficiently? The second question is particularly
important in view of the massive size of actual data sets.
1.1 Model definition
A simple mathematical model for such data assumes that the (unknown) matrix of ratings has rank
r â‰ª m, n. More precisely, we denote by M the matrix whose entry (i, j) âˆˆ [m] Ã— [n] corresponds
to the rating user i would assign to movie j. We assume that there exist matrices U, of dimensions
m Ã— r, and V , of dimensions n Ã— r, and a diagonal matrix Î£, of dimensions r Ã— r such that
M = UÎ£V T
. (1)
âˆ—
Department of Electrical Engineering, Stanford University
â€ 
Departments of Statistics, Stanford University
1
Indeed, in 2006, Netflix made public such a dataset with m â‰ˆ 5 Â· 105
, n â‰ˆ 2 Â· 104
and |E| â‰ˆ 108
and challenged
the research community to predict the missing ratings with root mean square error below 0.8563 [Net].
1
For justification of these assumptions and background on the use of low rank matrices in information
retrieval, we refer to [BDJ99]. Since we are interested in very large data sets, we shall focus on the
limit m, n â†’ âˆž with m/n = Î± bounded away from 0 and âˆž.
We further assume that the factors U, V are unstructured. This notion is formalized by the
incoherence condition introduced by CandeÌs and Recht [CR08], and defined in Section 2. In particular
the incoherence condition is satisfied with high probability if M = UÎ£V T with U and V uniformly
random matrices with UT U = m1 and V T V = n1. Alternatively, incoherence holds if the entries of
U and V are i.i.d. bounded random variables.
Out of the m Ã— n entries of M, a subset E âŠ† [m] Ã— [n] (the user/movie pairs for which a rating
is available) is revealed. We let ME be the m Ã— n matrix that contains the revealed entries of M,
and is filled with 0â€™s in the other positions
ME
i,j =

Mi,j if (i, j) âˆˆ E ,
0 otherwise.
(2)
The set E will be uniformly random given its size |E|.
1.2 Algorithm
A naive algorithm consists of the following projection operation.
Projection. Compute the singular value decomposition (SVD) of ME (with Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ 0)
ME
=
min(m,n)
X
i=1
ÏƒixiyT
i , (3)
and return the matrix Tr(ME) = (mn/|E|)
Pr
i=1 ÏƒixiyT
i obtained by setting to 0 all but the r largest
singular values. Notice that, apart from the rescaling factor (mn/|E|), Tr(ME) is the orthogonal
projection of ME onto the set of rank-r matrices. The rescaling factor compensates the smaller
average size of the entries of ME with respect to M.
It turns out that, if |E| = Î˜(n), this algorithm performs very poorly. The reason is that the
matrix ME contains columns and rows with Î˜(log n/ log log n) non-zero (revealed) entries. The
largest singular values of ME are of order Î˜(
p
log n/ log log n). The corresponding singular vectors
are highly concentrated on high-weight column or row indices (respectively, for left and right singular
vectors). Such singular vectors are an artifact of the high-weight columns/rows and do not provide
useful information about the hidden entries of M. This motivates the definition of the following
operation (hereafter the degree of a column or of a row is the number of its revealed entries).
Trimming. Set to zero all columns in ME with degree larger that 2|E|/n. Set to 0 all rows with
degree larger than 2|E|/m.
Figure 1 shows the singular value distributions of ME and f
ME for a random rank-3 matrix M.
The surprise is that trimming (which amounts to â€˜throwing out informationâ€™) makes the underlying
rank-3 structure much more apparent. This effect becomes even more important when the number
of revealed entries per row/column follows a heavy tail distribution, as for real data.
In terms of the above routines, our algorithm has the following structure.
Spectral Matrix Completion( matrix ME )
1: Trim ME, and let f
ME be the output;
2: Project f
ME to Tr(f
ME);
3: Clean residual errors by minimizing the discrepancy F(X, Y ).
2
0 10 20 30 40 50 60
0
5
10
15
20
25
30
35
40
0 5 10 15 20 25 30 35 40 45
0
5
10
15
20
25
30
35
Ïƒ4 Ïƒ3 Ïƒ2 Ïƒ1
Ïƒ3 Ïƒ2 Ïƒ1
Figure 1: Histogram of the singular values of a partially revealed matrix ME
before trimming (left) and after
trimming (right) for 104
Ã—104
random rank-3 matrix M with Ç« = 30 and Î£ = diag(1, 1.1, 1.2). After trimming
the underlying rank-3 structure becomes clear. Here the number of revealed entries per row follows a heavy
tail distribution with P{N = k} = const./k3
.
The last step of the above algorithm allows to reduce (or eliminate) small discrepancies between
Tr(f
ME) and M, and is described below.
Cleaning. Various implementation are possible, but we found the following one particularly appeal-
ing. Given X âˆˆ RmÃ—r, Y âˆˆ RnÃ—r with XT X = m1 and Y T Y = n1, we define
F(X, Y ) â‰¡ min
SâˆˆRrÃ—r
F(X, Y, S) , (4)
F(X, Y, S) â‰¡
1
2
X
(i,j)âˆˆE
(Mij âˆ’ (XSY T
)ij)2
. (5)
The cleaning step consists in writing Tr(f
ME) = X0S0Y T
0 and minimizing F(X, Y ) locally with initial
condition X = X0, Y = Y0.
Notice that F(X, Y ) is easy to evaluate since it is defined by minimizing the quadratic function
S 7â†’ F(X, Y, S) over the low-dimensional matrix S. Further it depends on X and Y only through
their column spaces. In geometric terms, F is a function defined over the cartesian product of
two Grassmann manifolds (we refer to Section 5 for background and references). Optimization
over Grassmann manifolds is a well understood topic [EAS99] and efficient algorithms (in particular
Newton and conjugate gradient) can be applied. To be definite, we assume that gradient descent
with line search is used to minimize F(X, Y ).
Finally, the implementation proposed here implicitly assumes that the rank r is known. In
practice this is a non-issue. Since r â‰ª n, a loop over the value of r can be added at little extra cost.
For instance, in collaborative filtering applications, r ranges between 10 and 30.
1.3 Main results
Notice that computing Tr(f
ME) only requires to find the first r singular vectors of a sparse matrix.
Our main result establishes that this simple procedure achieves arbitrarily small relative root mean
3
square error from O(nr) revealed entries. We define the relative root mean square error as
RMSE â‰¡

1
mnr
||M âˆ’ Tr(f
ME
)||2
F
1/2
. (6)
where we denote by ||A||F the Frobenius norm of matrix A. Notice that the factor (1/mn) corresponds
to the usual normalization by the number of entries. The factor (1/r) is instead necessary because
(as described in Section 2), the typical size of the entries of M is
âˆš
r.
Theorem 1.1. Assume M to be a rank r â‰¤ n1/2 matrix that satisfies the incoherence condition A2
(in particular, this is the case for random orthogonal matrices U, V ). Then with high probability
1
mnr
||M âˆ’ Tr(f
ME
)||2
F â‰¤ C(Î±)
nr
|E|
. (7)
The proof is provided in Section 3.
Notice that the top r singular values and singular vectors of the sparse matrix f
ME can be
computed efficiently by subspace iteration [Ber92]. Each iteration requires O(|E|r) operations. As
proved in Section 3, the (r + 1)-th singular value is smaller than one half of the r-th one. As a
consequence, subspace iteration converges exponentially. A simple calculation shows that O(log n)
iterations are sufficient to ensure the error bound mentioned.
The â€˜cleaningâ€™ step in the above pseudocode improves systematically over Tr(f
ME) and, for large
enough |E|, reconstructs M exactly.
Theorem 1.2. Assume M to be a rank r â‰¤ n1/2 matrix that satisfies the incoherence conditions A1
and A2. Further, assume Î£min â‰¤ Î£1, . . . , Î£r â‰¤ Î£max with Î£min, Î£max bounded away from 0 and âˆž.
Then there exists Câ€²(Î±) such that, if
|E| â‰¥ Câ€²
(Î±)nr max{log n, r} , (8)
then the cleaning procedure in Spectral Matrix Completion converges, with high probability, to
the matrix M.
The proof is provided in Section 5. The basic intuition is that, for |E| â‰¥ Câ€²(Î±)nr max{log n, r},
Tr(f
ME) is so close to M that the cost function is well approximated by a quadratic function.
Theorem 1.1 is optimal: the number of degrees of freedom in M is of order nr, without the same
number of observations is impossible to fix them. The extra log n factor in Theorem 1.2 is due to a
coupon-collector effect [CR08, KMO08, KOM09]: it is necessary that E contains at least one entry
per row and one per column and this happens only for |E| â‰¥ Cn log n. As a consequence, for rank r
bounded, Theorem 1.2 is optimal. It is suboptimal by a polylogarithmic factor for r = O(log n).
1.4 Related work
Beyond collaborative filtering, low rank models are used for clustering, information retrieval, machine
learning, and image processing. In [Faz02], the NP-hard problem of finding a matrix of minimum
rank satisfying a set of affine constraints was addresses through convex relaxation. This problem is
analogous to the problem of finding the sparsest vector satisfying a set of affine constraints, which
is at the heart of compressed sensing [Don06, CRT06]. The connection with compressed sensing was
emphasized in [RFP07], that provided performance guarantees under appropriate conditions on the
constraints.
4
In the case of collaborative filtering, we are interested in finding a matrix M of minimum rank
that matches the known entries {Mij : (i, j) âˆˆ E}. Each known entry thus provides an affine
constraint. CandeÌ€s and Recht [CR08] introduced the incoherent model for M. Within this model,
they proved that, if E is random, the convex relaxation correctly reconstructs M as long as |E| â‰¥
C r n6/5 log n. On the other hand, from a purely information theoretic point of view (i.e. disregarding
algorithmic considerations), it is clear that |E| = O(n r) observations should allow to reconstruct M
with arbitrary precision. Indeed this point was raised in [CR08] and proved in [KMO08], through a
counting argument.
The present paper describes an efficient algorithm that reconstructs a rank-r matrix from O(n r)
random observations. The most complex component of our algorithm is the SVD in step 2. We were
able to treat realistic data sets with n â‰ˆ 105. This must be compared with the O(n4) complexity of
semidefinite programming [CR08].
Cai, CandeÌ€s and Shen [CCS08] recently proposed a low-complexity procedure to solve the convex
program posed in [CR08]. Our spectral method is akin to a single step of this procedure, with the
important novelty of the trimming step that improves significantly its performances. Our analysis
techniques might provide a new tool for characterizing the convex relaxation as well.
Theorem 1.1 can also be compared with a copious line of work in the theoretical computer science
literature [FKV04, AFK+01, AM07]. An important motivation in this context is the development of
fast algorithms for low-rank approximation. In particular, Achlioptas and McSherry [AM07] prove
a theorem analogous to 1.1, but holding only for |E| â‰¥ (8 log n)4n (in the case of square matrices).
A short account of our results was submitted to the 2009 International Symposium on Information
Theory [KOM09]. While the present paper was under completion, CaÌndes and Tao posted online
a preprint proving a theorem analogous to 1.2 [CT09]. Once more, their approach is substantially
different from ours.
1.5 Open problems and future directions
It is worth pointing out some limitations of our results, and interesting research directions:
1. Optimal RMSE with O(n) entries. Numerical simulations with the Spectral Matrix Com-
pletion algorithm suggest that the RMSE decays much faster with the number of observations
per degree of freedom (|E|/nr), than indicated by Eq. (7). This improved behavior is a product of
the cleaning step in the algorithm. It would be important to characterize the decay of RMSE with
(|E|/nr).
2. Threshold for exact completion. As pointed out, Theorem 1.2 is order optimal for r bounded.
It would nevertheless be useful to derive quantitatively sharp estimates in this regime. A systematic
numerical study was initiated in [KMO08]. It appears that available theoretical estimates (including
the recent ones in [CT09]) are for larger values of the rank, we expect that our arguments can be
strenghtened to prove exact reconstruction for |E| â‰¥ Câ€²(Î±)nr log n for all values of r.
3. More general models. The model studied here and introduced in [CR08] presents obvious
limitations. In applications to collaborative filtering, the subset of observed entries E is far from
uniformly random. A recent paper [SC09] investigates the uniqueness of the solution of the matrix
completion problem for general sets E. In applications to fast low-rank approximation, it would be
desirable to consider non-incoherent matrices as well (as in [AM07]).
5
2 Incoherence property and some notations
The matrix M to be reconstructed takes the form (1) where U âˆˆ RmÃ—r, V âˆˆ RnÃ—r. We write
U = [u1, u2, . . . , ur] and V = [v1, v2, . . . , vr] for the columns of the two factors, with ||ui|| =
âˆš
m,
||vi|| =
âˆš
n and uT
i uj = 0, vT
i vj = 0 for i 6= j (there is no loss of generality in this, since normalizations
can be adsorbed by redefining Î£). We shall further write Î£ = diag(Î£1, . . . , Î£r) with Î£1 â‰¥ Î£2 â‰¥
Â· Â· Â· â‰¥ Î£r â‰¥ 0.
The matrices U, V and Î£ will be said to be incoherent if they satisfy the following properties:
A1. There exists a constant Âµ0 > 0 such that for all i âˆˆ [m], j âˆˆ [n] we have
Pr
k=1 U2
i,k â‰¤ Âµ0r,
Pr
k=1 V 2
i,k â‰¤ Âµ0r.
A2. There exists Âµ1 such that |
Pr
k=1 Ui,kÎ£kVj,k| â‰¤ Âµ1r1/2.
Apart from difference in normalization, these assumptions coincide with the ones in [CR08].
In the proof of Theorem 1.1, we only require the second incoherence assumption A2. In the
following, whenever we write that a property A holds with high probability (w.h.p.), we mean that
there exists a function f(n) = f(n; Î±, Âµ1) such that P(A) â‰¥ 1 âˆ’ f(n) and f(n) â†’ 0 for Âµ1 bounded
away from 0 and âˆž. In the case of exact completion (i.e. in the proof of Theorem 1.2) f( Â· ) can also
depend on Âµ0, Î£min, Î£max, and f(n) â†’ 0 for Âµ0, Î£min, Î£max bounded away from 0 and âˆž.
Probability is taken with respect to the uniformly random subset E âŠ† [m] Ã— [n]. It is convenient
to work with a model in which each entry is revealed independently with probability Ç«/
âˆš
mn. Since,
with high probability |E| âˆˆ [Ç«
âˆš
Î± nâˆ’A
âˆš
n log n, Ç«
âˆš
Î± nâˆ’A
âˆš
n log n], any guarantee on the algorithm
performances that holds within one model, holds within the other model as well if we allow for a
vanishing shift in Ç«. Finally, we will use C, Câ€² etc. to denote generic constants that depend uniquely
on Î±, Âµ1 and, when proving Theorem 1.2, Âµ0, Î£min, Î£max.
Given a vector x âˆˆ Rn, ||x|| will denote its Euclidean norm. For a matrix X âˆˆ RnÃ—nâ€²
, ||X||F is its
Frobenius norm, and ||X||2 its operator norm (i.e. ||X||2 = supu6=0 ||Xu||/||u||). The standard scalar
product between vectors or matrices will sometimes be indicated by hx, yi or hX, Y i, respectively.
Finally, we use the standard combinatorics notation [N] = {1, 2, . . . , N} to denote the set of first N
integers.
3 Proof of Theorem 1.1 and technical results
As explained in the previous section, the crucial idea is to consider the singular value decomposition
of the trimmed matrix f
ME instead of the original matrix ME, as in Eq. (3). We shall then redefine
{Ïƒi}, {xi}, {yi}, by letting
f
ME
=
min(m,n)
X
i=1
ÏƒixiyT
i . (9)
Here ||xi|| = ||yi|| = 1, xT
i xj = yT
i yj = 0 for i 6= j and Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ 0.
We will prove Theorem 1.1 by controlling the operator norm of M âˆ’ Tr(f
ME).
Theorem 3.1. There exists C(Î±) < âˆž such that, with high probability,
1
âˆš
mn
M âˆ’ Tr(f
ME
) 2
â‰¤ C
r
Ç«
1/2
. (10)
6
This result is proved later in this section. The proof of Theorem 1.1 is a direct application the
of above theorem.
Proof. (Theorem 1.1) Since M âˆ’ Tr(f
ME) has rank at most 2r, it immediately follows that
1
âˆš
mnr
M âˆ’ Tr(f
ME
) F
â‰¤
âˆš
2r

1
âˆš
mnr
M âˆ’ Tr(f
ME
) 2

â‰¤ C
r
Ç«
1/2
,
which implies the thesis.
To prove Theorem 3.1, we will use following key technical results.
Lemma 3.2. There exists a constant C > 0 such that, with high probability
xT

Ç«
âˆš
mn
M âˆ’ f
ME

y â‰¤ C
âˆš
rÇ« , (11)
for any x âˆˆ Rm and y âˆˆ Rn such that ||x|| = ||y|| = 1.
The proof of this lemma is given in Section 4. The next lemma, which is a direct consequence of
Lemma 3.2, relates the singular values of the trimmed matrix f
ME to the singular values of M.
Lemma 3.3. There exists a constant C > 0 such that, with high probability
Ïƒq
Ç«
âˆ’ Î£q â‰¤ C
r
Ç«
1/2
, (12)
where it is understood that Î£q = 0 for q > r.
This lemma is proved later in this section. We can now prove Theorem 3.1.
Proof. (Theorem 3.1) Consider x âˆˆ Rn and y âˆˆ Rm such that ||x|| = ||y|| = 1. Then,
xT
(M âˆ’ Tr(f
ME
))y â‰¤ xT

M âˆ’
âˆš
mn
Ç«
f
ME

y + xT
âˆš
mn
Ç«
f
ME
âˆ’ Tr(f
ME
)

y
â‰¤ C
âˆš
mn
r
Ç«
1
2
+
âˆš
mn
Ïƒr+1
Ç«
â‰¤ Câ€²âˆš
mn
r
Ç«
1
2
,
where we used Lemma 3.2 for the second inequality and Lemma 3.3 for the last inequality. This
implies that 1
âˆš
mn
M âˆ’ Tr(f
ME) 2
â‰¤ Câ€² (r/Ç«)
1
2 .
We end this section with the proof of Lemma 3.3.
Proof. (Lemma 3.3) Recall the variational characterization of the singular values.
Ïƒq = min
H,dim(H)=nâˆ’q+1
max
yâˆˆH,||y||=1
||f
ME
y|| (13)
= max
H,dim(H)=q
min
yâˆˆH,||y||=1
||f
ME
y|| . (14)
Here H is understood to be a linear subspace of Rn.
7
Using Eq. (13) with H the orthogonal complement of span(v1, . . . , vqâˆ’1), we have, by Lemma 3.2,
Ïƒq â‰¤ max
yâˆˆH,||y||=1
f
ME
y
â‰¤
Ç«
âˆš
mn

max
yâˆˆH,||y||=1
My

+ max
yâˆˆH,||y||=||x||=1
xT

f
ME
âˆ’
Ç«
âˆš
mn
M

y
â‰¤ Ç«Î£q + C
âˆš
rÇ«
In order to get a lower bound, we use Eq. (14) with H = span(v1, . . . , vq). By Lemma 3.2 we
have
Ïƒq â‰¥ min
yâˆˆH,||y||=1
f
ME
y
â‰¥
Ç«
âˆš
mn

min
yâˆˆH,||y||=1
My

âˆ’ max
yâˆˆH,||y||=||x||=1
xT

f
ME
âˆ’
Ç«
âˆš
mn
M

y
â‰¥ Ç«Î£q âˆ’ C
âˆš
rÇ«
Combining the upper and lower bounds, we get the desired result.
4 Proof of Lemma 3.2
We want to show that |xT (f
ME âˆ’ Ç«
âˆš
mn
M)y| â‰¤ C
âˆš
rÇ« for each x âˆˆ Rm, y âˆˆ Rn such that ||x|| =
||y|| = 1. Following the technique of [FKS89], this will be done by first reducing ourselves to x, y
belonging to finite sets. We define
Tn =

x âˆˆ

âˆ†
âˆš
n
Z
n
: ||x|| â‰¤ 1

,
Notice that Tn âŠ† Sn â‰¡ {x âˆˆ Rn : ||x|| â‰¤ 1}. The next two remarks are proved in [FKS89, FO05],
and relate the original problem to the discretized one.
Remark 4.1. Let R âˆˆ RmÃ—n be a matrix. If |xT Ry| â‰¤ B for all x âˆˆ Tm and y âˆˆ Tn, then
|xâ€²T Ryâ€²| â‰¤ (1 âˆ’ âˆ†)âˆ’2B for all xâ€² âˆˆ Sm and yâ€² âˆˆ Sn.
Remark 4.2. |Tm| â‰¤ (10/âˆ†)m.
Hence it is enough to show that |xT (f
ME âˆ’ Ç«
âˆš
mn
M)y| â‰¤ C
âˆš
rÇ« for all x âˆˆ Tm and y âˆˆ Tn. There
are two parts to the proof of this claim. One bounds the contribution of light couples L âŠ† [m] Ã— [n],
defined as
L =

(i, j) : |xiMijyj| â‰¤
 rÇ«
mn
1/2

,
and the other bounds the contribution of its complement L, which we call heavy couples. We have
xT

f
ME
âˆ’
Ç«
âˆš
mn
M

y â‰¤
X
(i,j)âˆˆL
xi
f
ME
ij yj âˆ’
Ç«
âˆš
mn
xT
My +
X
(i,j)âˆˆL
xi
f
ME
ij yj (15)
In the next two subsections, we will prove that the first contribution is upper bounded by C1
âˆš
rÇ«
and the second by C2
âˆš
rÇ« for all x âˆˆ Tm, y âˆˆ Tn. Applying Remark 4.1 to |xT (f
ME âˆ’ Ç«
âˆš
mn
M)y|, this
proves the thesis.
8
4.1 Bounding the contribution of light couples
Let us define the subset of row and column indices which have not been trimmed as Al and Ar:
Al = {i âˆˆ [m] : deg(i) â‰¤
2Ç«
âˆš
Î±
} ,
Ar = {j âˆˆ [n] : deg(j) â‰¤ 2Ç«
âˆš
Î±} ,
where deg(Â·) denotes the degree (number of revealed entries) of a row or a column. Notice that
A = (Al, Ar) is a function of the random set E. It is easy to get a rough estimate of the sizes of Al,
Ar.
Remark 4.3. There exists C1 and C2 depending only on Î± such that, with probability larger than
1 âˆ’ 1/n3, |Al| â‰¥ m âˆ’ max{eâˆ’C1Ç«m, C2Î±}, and |Ar| â‰¥ n âˆ’ max{eâˆ’C1Ç«n, C2}.
For the proof of this claim, we refer to Appendix A. For any E âŠ† [m] Ã— [n] and A = (Al, Ar)
with Al âŠ† [m], Ar âŠ† [n], we define ME,A by setting to zero the entries of M that are not in E, those
whose row index is not in Al, and those whose column index not in Ar. Consider the event
H(E, A) =
ï£±
ï£²
ï£³
âˆƒ x, y :
X
(i,j)âˆˆL
xiME,A
ij yj âˆ’
Ç«
âˆš
mn
xT
My > C1
âˆš
rÇ«
ï£¼
ï£½
ï£¾
, (16)
where it is understood that x and y belong, respectively, to Tm and Tn. Note that f
ME = ME,A,
and hence we want to bound P{H(E, A)}. We proceed as follows
P {H(E, A)} =
X
A
P {H(E, A), A = A}
â‰¤
X
|Al|â‰¥m(1âˆ’Î´),
|Ar|â‰¥n(1âˆ’Î´)
P {H(E, A), A = A} +
1
n3
â‰¤ 2(n+m)H(Î´)
max
|Al|â‰¥m(1âˆ’Î´),
|Ar|â‰¥n(1âˆ’Î´)
P {H(E; A)} +
1
n3
, (17)
with Î´ â‰¡ max{eâˆ’C1Ç«, C2/n} and H(x) the binary entropy function.
We are now left with the task of bounding P {H(E; A)} uniformly over A where H is defined as
in Eq. (16). The key step consists in proving the following tail estimate
Lemma 4.4. Let x âˆˆ Sm, y âˆˆ Sn, Z =
P
(i,j)âˆˆL xiME,A
ij yj âˆ’ Ç«
âˆš
mn
xT My, and assume |Al| â‰¥ m(1âˆ’Î´),
|Ar| â‰¥ n(1 âˆ’ Î´) with Î´ small enough. Then
P Z > L
âˆš
rÇ«

â‰¤ exp
n
âˆ’
nÎ±1/2
2

L âˆ’ 2Âµ2
1 âˆ’ Âµ1
o
.
Proof. We begin by bounding the mean of Z as follows (for the proof of this statement we refer to
Appendix B).
Remark 4.5. |E [Z]| â‰¤ Âµ1 + Âµ2
1
 âˆš
rÇ«.
9
For A = (Al, Ar), let MA be the matrix obtained from M by setting to zero those entries whose
row index is not in Al, and those whose column index not in Ar. Define the potential contribution
of the light couples aij and independent random variables Zij as
aij =
(
xiMA
ij yj if |xiMA
ij yj| â‰¤ (rÇ«/mn)1/2
,
0 otherwise,
Zij =

ai,j w.p. Ç«/
âˆš
mn,
0 w.p. 1 âˆ’ Ç«/
âˆš
mn,
Let Z1 =
P
i,j Zij so that Z = Z1 âˆ’ Ç«
âˆš
mn
xT My. Note that
P
i,j a2
ij â‰¤
P
i,j

xiMA
ij yj
2
â‰¤ Âµ2
1r. Fix
Î» = (mn/4rÇ«)1/2 so that |Î»ai,j| â‰¤ 1/2, whence eÎ»aij âˆ’ 1 â‰¤ Î»aij + 2(Î»aij)2. It then follows that
E[eÎ»Z
] = exp
n Ç«
âˆš
mn
 X
i,j
Î»ai,j + 2
X
i,j
(Î»ai,j)2

âˆ’
Î» Ç«
âˆš
mn
xT
My
o
â‰¤ exp
n
Î»E[Z] + 2rÇ«Âµ2
1
Î»2
nÎ±1/2
o
.
The thesis follows by Chernoff bound P(Z > a) â‰¤ eâˆ’Î»aE[eÎ»Z] after simple calculus.
Note that P (âˆ’Z > L
âˆš
rÇ«) can also be bounded analogously. We can now finish the upper bound
on the light couples contribution. Consider the error event Eq. (16). Using Remark 4.2, we can
apply union bound over Tm and Tn to Eq. (17) to obtain
P{H(E, A)} â‰¤ 2

20
âˆ†
n+m
eâˆ’(C1âˆ’2Âµ2
1âˆ’Âµ1)Î±1/2n/2
+
1
n3
,
If C1 is a large enough constant, the first term is of order eâˆ’Î˜(n) (for, say, Ç« â‰¥ r) thus finishing the
proof.
4.2 Bounding the contribution of heavy couples
Let Q be an mÃ—n matrix with Qij = 1 if (i, j) âˆˆ E and i 6âˆˆ Ar, j 6âˆˆ Al (i.e. entry (i, j) is not trimmed
by our algorithm), and Qij = 0 otherwise. Due to the incoherence assumption A2, |Mij| â‰¤ Âµ1r1/2
and therefore the heavy couples satisfy |xiyj| â‰¥
p
Ç«/(Âµ2
1mn) = C
âˆš
Ç«/n. We then have
X
(i,j)âˆˆL
xi
f
ME
ij yj â‰¤ Âµ1
âˆš
r
X
(i,j)âˆˆL
Qij|xiyj|
â‰¤ Âµ1
âˆš
r
X
(i,j)âˆˆE:
|xiyj|â‰¥C
âˆš
Ç«/n
Qij|xiyj| .
Notice that Q is the adjacency matrix of a random bipartite graph with vertex sets [m] and [n]
and maximum degree bounded by 2Ç« max(Î±1/2, Î±âˆ’1/2). The following remark strengthens a result of
[FO05].
Remark 4.6. Given vectors x, y, let L
â€²
= {(i, j) âˆˆ E : |xiyj| â‰¥ C
âˆš
Ç«/n}. Then there exist a
constant Câ€² such that, w.h.p.,
P
(i,j)âˆˆL
â€² Qij|xiyj| â‰¤ Câ€²âˆš
Ç«, for all x âˆˆ Tm, y âˆˆ Tn.
10
For the readerâ€™s convenience, a proof of this fact is proposed in Appendix C. The analogous result
in [FO05] (for the adjacency matrix of a non-bipartite graph) is proved to hold only with probability
larger than 1 âˆ’ eâˆ’CÇ«. The stronger statement quoted here can be proved using concentration of
measure inequalities. The last remark implies that for all x âˆˆ Tm, y âˆˆ Tn and for large enough C
the contribution of heavy couples is, w.h.p., bounded by C2
âˆš
rÇ« for some C2 < âˆž.
5 Minimization on Grassmann manifolds and proof of Theorem 1.2
The function F(X, Y ) defined in Eq. (4) and to be minimized in the last part of the algorithm
can naturally be viewed as defined on Grassmann manifolds. Here we recall from [EAS99] a few
important facts on the geometry of Grassmann manifold and related optimization algorithms. We
then prove Theorem 1.2. Technical calculations are deferred to section Sections 6, 7, and to the
appendices.
We recall that, for the proof of Theorem 1.2, it is assumed that Î£min, Î£max are bounded away from
0 and âˆž. Constants (denoted by C, Câ€², . . . ) depend implicitly on Î£min, Î£max. Finally, throughout
this section, we use the notation X(i) âˆˆ Rr to refer to the i-th row of the matrix X âˆˆ RmÃ—n or
X âˆˆ RnÃ—r.
5.1 Geometry of the Grassmann manifold
Denote by O(d) the orthogonal group of d Ã— d matrices. The Grassmann manifold is defined as the
quotient G(n, r) â‰ƒ O(n)/O(r) Ã— O(n âˆ’ r). In other words, a point in the manifold is the equivalence
class of an n Ã— r orthogonal matrix A
[A] = {AQ : Q âˆˆ O(r)} . (18)
For consistency with the rest of the paper, we will assume the normalization AT A = n 1. To represent
a point in G(n, r), we will use an explicit representative of this form. More abstractly, G(n, r) is the
manifold of r-dimensional subspaces of Rn.
It is easy to see that F(X, Y ) depends on the matrices X, Y only through their equivalence
classes [X], [Y ]. We will therefore interpret it as a function defined on the manifold M(m, n) â‰¡
G(m, r) Ã— G(n, r):
F : M(m, n) â†’ R , (19)
([X], [Y ]) 7â†’ F(X, Y ) . (20)
In the following, a point in this manifold will be represented as a pair x = (X, Y ), with X an n Ã— r
orthogonal matrix and Y an mÃ—r orthogonal matrix. Boldface symbols will be reserved for elements
of M(m, n) or of its tangent space, and we shall use u = (U, V ) for the point corresponding to the
matrix M = UÎ£V T to be reconstructed.
Given x = (X, Y ) âˆˆ M(m, n), the tangent space at x is denoted by Tx and can be identified with
the vector space of matrix pairs w = (W, Z), W âˆˆ RmÃ—r, Z âˆˆ RnÃ—r such that WT X = ZT Y = 0.
The â€˜canonicalâ€™ Riemann metric on the Grassmann manifold corresponds to the usual scalar product
hW, Wâ€²i â‰¡ Tr(WT Wâ€²). The induced scalar product on Tx between w = (W, Z) and wâ€² = (Wâ€², Zâ€²)
is hw, wâ€²i = hW, Wâ€²i + hZ, Zâ€²i.
This metric induces a canonical notion of distance on M(m, n) which we denote by d(x1, x2)
(geodesic or arc-length distance). If x1 = (X1, Y1) and x2 = (X2, Y2) then
d(x1, x2) â‰¡
p
d(X1, X2)2 + d(Y1, Y2)2 (21)
11
where the arc-length distances d(X1, X2), d(Y1, Y2) on the Grassmann manifold can be defined ex-
plicitly as follows. Let cos Î¸ = (cos Î¸1, . . . , cos Î¸r), Î¸i âˆˆ [âˆ’Ï€/2, Ï€/2] be the singular values of XT
1 X2/n.
Then
d(X1, X2) = ||Î¸||2 . (22)
The Î¸iâ€™s are called the â€˜principal anglesâ€™ between the subspaces spanned by the columns of X1 and
X2. It is useful to introduce two equivalent notions of distance:
dc(X1, X2) =
1
âˆš
n
min
Q1,Q2âˆˆO(r)
||X1Q1 âˆ’ X2Q2||F (chordal distance), (23)
dp(X1, X2) =
1
âˆš
2n
||X1XT
1 âˆ’ X2XT
2 ||F (projection distance). (24)
Notice that dc and dp do not depend on the specific representatives X1, X2, but only on the equiv-
alence classes [X1] and [X2]. Distances on M(m, n) are defined through Pythagorean theorem, e.g.
dc(x1, x2) =
p
dc(X1, X2)2 + dc(Y1, Y2)2.
Remark 5.1. The geodesic, chordal and projection distance are equivalent, namely
2
Ï€
d(X1, X2) â‰¤
1
âˆš
2
dc(X1, X2) â‰¤ dp(X1, X2) â‰¤ dc(X1, X2) â‰¤ d(X1, X2) . (25)
For the readerâ€™s convenience, a proof of this fact is proposed in Section D.
An important remark is that geodesics with respect to the canonical Riemann metric admit an
explicit and efficiently computable form. Given u âˆˆ M(m, n), w âˆˆ Tx the corresponding geodesic
is a curve t 7â†’ x(t), with x(t) = u + wt + O(t2) which minimizes arc-length. If u = (U, V ) and
w = (W, Z) then x(t) = (X(t), Y (t)) where X(t) can be expressed in terms of the singular value
decomposition W = LÎ˜RT [EAS99]:
X(t) = UR cos(Î˜t)RT
+ L sin(Î˜t)RT
, (26)
which can be evaluated in time of order O(nr). An analogous expression holds for Y (t).
5.2 Gradient and incoherence
The gradient of F at x is the vector grad F(x) âˆˆ Tx such that, for any smooth curve t 7â†’ x(t) âˆˆ
M(m, n) with x(t) = x + w t + O(t2), one has
F(x(t)) = F(x) + hgrad F(x), wi t + O(t2
) . (27)
In order to write an explicit representation of the gradient of our cost function F, it is convenient to
introduce the projector operator
PE(M)ij =

Mij if (i, j) âˆˆ E,
0 otherwise.
(28)
The two components of the gradient are then
grad F(x)X = PE(XSY T
âˆ’ M)Y ST
âˆ’ XQX , (29)
grad F(x)Y = PE(XSY T
âˆ’ M)T
XS âˆ’ Y QY , (30)
where QX, QY âˆˆ RrÃ—r are determined by the condition grad F(x) âˆˆ Tx. This yields
QX =
1
m
XT
PE(M âˆ’ XSY T
)Y ST
, (31)
QY =
1
n
Y T
PE(M âˆ’ XSY T
)T
XS . (32)
12
5.3 Algorithm
At this point the gradient descent algorithm is fully specified. It takes as input the factors of Tr(f
ME),
to be denoted as x0 = (X0, Y0), and minimizes a regularized cost function
e
F(X, Y ) = F(X, Y ) + Ï G(X, Y ) (33)
â‰¡ F(X, Y ) + Ï
m
X
i=1
G1
||X(i)||2
2Âµ0r
!
+ Ï
n
X
j=1
G1
||Y (j)||2
2Âµ0r
!
, (34)
where X(i) denotes the i-th row of X, and Y (j) the j-th row of Y . The role of the regularization is
to force x to remain incoherent during the execution of the algorithm.
G1(z) =

0 if z â‰¤ 1,
e(zâˆ’1)2
âˆ’ 1 if z â‰¥ 1.
(35)
We will take Ï = nÇ«. Notice that G(X, Y ) is again naturally defined on the Grassmann manifold,
i.e. G(X, Y ) = G(XQ, Y Qâ€²) for any Q, Qâ€² âˆˆ O(r).
Let
K(Âµâ€²
) â‰¡
n
(X, Y ) such that ||X(i)
||2
â‰¤ Âµâ€²
r, ||Y (j)
||2
â‰¤ Âµâ€²
r
o
. (36)
We have G(X, Y ) = 0 on K(2Âµ0). Notice that u âˆˆ K(Âµ0) by the incoherence property. Without loss
of generality we can assume x0 âˆˆ K(2Âµ0), because otherwise we can rescale all lines of X0, Y0 that
violate the constraint.
Gradient descent( matrix ME, factors x0 )
1: For k = 0, 1, . . . do:
2: Compute wk = grad e
F(xk);
4: Let t 7â†’ xk(t) be the geodesic with xk(t) = xk + wkt + O(t2);
5: Minimize t 7â†’ e
F(xk(t)) for t â‰¥ 0, subject to d(xk(t), x0) â‰¤ Î³;
6: Set xk+1 = xk(tk) where tk is the minimum location;
7: End For.
In the above, Î³ must be set in such a way that d(u, x0) â‰¤ Î³. The next remark determines the
correct scale.
Remark 5.2. Let U, X âˆˆ RnÃ—r with UT U = XT X = m1, V, Y âˆˆ RmÃ—r with V T V = Y T Y = n1,
and M = UÎ£V T , c
M = XSY T for Î£ = diag(Î£1, . . . , Î£r) and S âˆˆ RrÃ—r. If Î£1, . . . , Î£r â‰¥ Î£min, then
dp(U, X) â‰¤
1
mÎ£min
||M âˆ’ c
M||F , dp(V, Y ) â‰¤
1
nÎ£min
||M âˆ’ c
M||F (37)
As a consequence of this remark and Theorem 1.1, we can assume that d(u, x0) â‰¤ C r/
âˆš
Ç«. We
shall then set Î³ = Câ€²r/
âˆš
Ç« (the value of Câ€² is set in the course of the proof).
Before passing to the proof of Theorem 1.2, it is worth discussing a few important points con-
cerning the gradient descent algorithm.
(i) The appropriate choice of Î³ might seem to pose a difficulty. In reality, this parameter is
introduced only to simplify the proof. We will see that the constraint d(xk(t), x0) â‰¤ Î³ is, with
high probability, never saturated.
13
(ii) Indeed, the line minimization instruction 5 (which might appear complex to implement) can
be replaced by a standard step selection procedure, such as the one in [Arm66].
(iii) Similarly, there is no need to know the actual value of Âµ0 in the regularization term. One can
start with Âµ0 = 1 and then repeat the optimization doubling it at each step.
(iv) The Hessian of F can be computed explicitly as well. This opens the way to quadratically
convergent minimization algorithms (e.g. the Newton method).
5.4 Proof of Theorem 1.2
The proof of Theorem 1.2 breaks down in two lemmas. The first one implies that, in a sufficiently
small neighborhood of u, the function x 7â†’ F(x) is well approximated by a parabola.
Lemma 5.3. Assume Ç« â‰¥ A max{r log n, r2} with A large enough. Then there exists constants
C1, C2, Î´ > 0 (independent of m, n, Ç« and r) such that, with high probability
C1 nÇ« d(x, u)2
+ ||S âˆ’ Î£||2
F

â‰¤ F(x) â‰¤ C2 nÇ« d(x, u)2
(38)
for all x âˆˆ M(m, n) âˆ© K(3Âµ0) such that d(x, u) â‰¤ Î´ (where S âˆˆ RrÃ—r is the matrix realizing the
minimum in Eq. (4)).
The second Lemma implies that x 7â†’ F(x) does not have any other stationary point (apart from
u) within such a neighborhood.
Lemma 5.4. Assume Ç« â‰¥ A max{r log n, r2} with A large enough. Then there exists constants
C, Î´ > 0 (independent of m, n, Ç« and r) such that, with high probability
||grad e
F(x)||2
â‰¥ C nÇ«2
d(x, u)2
(39)
for all x âˆˆ M(m, n) âˆ© K(3Âµ0) such that d(x, u) â‰¤ Î´.
We can now prove Theorem 1.2.
Proof. (Theorem 1.2) Let Î´ > 0 be such that Lemma 5.3 and Lemma 5.4 are verified, and C1, C2
be defined as in Lemma 5.3. We further assume Î´ â‰¤
p
(e1/4 âˆ’ 1)/C1. Take Ç« large enough that
d(u, x0) â‰¤ C r/
âˆš
Ç« â‰¤ min(1, (C1/2C2)1/2)Î´/10. Further, set the algorithm parameter to Î³ = Î´/4.
We make the following claims:
1. xk âˆˆ K(3Âµ0) for all k.
Indeed x0 âˆˆ K(2Âµ0) whence e
F(x0) = F(x0) â‰¤ C2nÇ« Î´2. The claim follows because e
F(xk) is
non-increasing and e
F(x) â‰¥ Ï G(X, Y ) â‰¥ nÇ«(e1/4 âˆ’ 1) for x 6âˆˆ K(3Âµ0).
2. d(xk, u) â‰¤ Î´/10 for all k.
Indeed by triangular inequality we can assume to have d(xk, u) â‰¤ Î´/2. Since d(x0, u) â‰¤
(C1/2C2)1/2Î´/10, we have e
F(x) â‰¥ F(x) â‰¥ F(x0) for all x such that d(x, u) âˆˆ [Î´/10, Î´]. Since
e
F(xk) is non-increasing and e
F(x0) = F(x0), the claim follows.
Notice that, by the last observation, the constraint d(xk(t), x0) â‰¤ Î³ is never saturated, and
therefore our procedure is just gradient discent with exact line search. Therefore [Arm66] this must
converge to the unique stationary point of e
F in K(3Âµ0) âˆ© {x : d(x, u) â‰¤ Î´/10, which, by Lemma 5.4,
is u.
14
6 Proof of Lemma 5.3
We will make use of the following Lemma.
Lemma 6.1. Assume Ç« = A log n with A large enough. Then there exists C > 0 such that with high
probability
X
(i,j)âˆˆE
xiyj â‰¤
CÇ«
n
||x||1||y||1 + C
âˆš
Ç«||x||2 ||y||2 . (40)
for all x âˆˆ Rm, y âˆˆ Rn.
Proof. Write xi = x0 + xâ€²
i where
P
i xâ€²
i = 0. Then
X
(i,j)âˆˆE
xiyj = x0
X
jâˆˆ[n]
deg(j)yj +
X
(i,j)âˆˆE
xâ€²
iyj , (41)
where we recall that deg(j) = {i âˆˆ [m] : such that (i, j) âˆˆ E}. Further |x0| = |
P
i xi/n| â‰¤ ||x||1/n.
The first term is upper bounded by
x0 max
j
deg(j)||y||1 â‰¤ max
j
deg(j)||x||1||y||1/n . (42)
For Ç« = A log n, the maximum degree is with high probability of the same order as the average one,
and therefore this term is at most CÇ«||x||1||y||1/n.
The second term is upper bounded by C
âˆš
Ç«||xâ€²||2||y||2 using Theorem 1.1 in [FO05] or, equiva-
lently, Theorem 3.3 in the case r = 1. The thesis follows because ||xâ€²||2 â‰¤ ||x||2.
Proof. (Lemma 5.3) Throughout the proof we assume m = n to simplify notations.
Let w = (W, Z) âˆˆ Tu, and t 7â†’ (X(t), Y (t)) be the geodesic such that (X(t), Y (t)) = (U, V ) +
(W, Z)t+O(t2). By setting (X, Y ) = (X(1), Y (1)), we establish a one-to-one correspondence between
the points x as in the statement and a neighborhood of the origin in Tu. If we let W = LÎ˜RT be
the singular value decomposition of W (with LT L = n1 and RT R = 1), the explicit expression for
geodesics in Eq. (26) yields
X = U + W , W = UR(cos Î˜ âˆ’ 1)RT
+ L sin Î˜RT
. (43)
An analogous expression can obviously be written for Y = V +Z. Notice that, if u, x âˆˆ K(3Âµ0), then
(W, Z) âˆˆ K(12Âµ0) and w âˆˆ K(48Âµ0/Ï€2). In the first case this follows from ||W
(i)
||2 â‰¤ 2||X(i)||2 +
2||U(i)||2. In order to prove w âˆˆ K(48Âµ0/Ï€2), we notice that
||W(i)
||2
= ||Î˜L(i)
||2
â‰¤
4
Ï€2
|| sin Î˜L(i)
||2
â‰¤
4
Ï€2
||X(i)
âˆ’ R cos Î˜RT
U(i)
||2
â‰¤
8
Ï€2

||X(i)
||2
+ ||U(i)
||2

.
The claim follows by showing a similar bound for ||Z(i)||2.
Denote by S âˆˆ RrÃ—r the matrix realizing the minimum in Eq. (4). We will start by proving the
lower bound in Eq. (38):
F(X, Y ) =
1
2
X
(i,j)âˆˆE
(U(S âˆ’ Î£)V T
+ USZ
T
+ WSV T
+ WSZ
T
)2
ij
â‰¥
1
4
A2
âˆ’ B2
15
where in we used Cauchy-Schwarz inequality to argue that (1/2)(A+B)2 â‰¥ (A2/4)âˆ’B2 and defined
A2
â‰¡
X
(i,j)âˆˆE
(U(S âˆ’ Î£)V T
+ USZ
T
+ WSV T
)2
ij ,
B2
â‰¡
X
(i,j)âˆˆE
(WSZ
T
)2
ij .
We will show that, with high probability A2 â‰¥ CnÇ«||S âˆ’Î£||2
F +CnÇ«d(x, u)2 and B2 â‰¤ (C/100)nÇ« (1+
||S âˆ’ Î£||2
F )d(x, u)2 whence the lower bound in Eq. (38) follows.
Lower bound on A. By Theorem 4.1 in [CR08], we have A2 â‰¥ (1âˆ’Î¾)E{A2} with high probability
for each Î¾ > 0. Further
E{A2
} =
Ç«
n
||U(S âˆ’ Î£)V T
+ USZ
T
+ WSV T
||2
F =
=
Ç«
n
||U(S âˆ’ Î£)V T
||2
F +
Ç«
n
||USZ
T
||2
F +
Ç«
n
||WSV T
||2
F
+
2Ç«
n
hUSZ
T
, WSV T
i +
2Ç«
n
hU(S âˆ’ Î£)V T
, WSV T
i +
2Ç«
n
hUSZ
T
, U(S âˆ’ Î£)V T
i .
The first term is equal to nÇ«||S âˆ’ Î£||2
F . The second and third terms are lower bounded by
Ç«Ïƒmin(S)2
(||Z||2
F + ||W||2
F ) â‰¥ CÏƒmin(S)2
nÇ«dc(x, u)2
â‰¥ Câ€²
Ïƒmin(S)2
nÇ«d(x, u)2
.
The absolute value of the fourth term can be written as
E4 =
Ç«
n
|hUSZ
T
, WSV T
i| â‰¤
Ç«
n
||USZ
T
||F ||WSV T
||F â‰¤
Ç«
n
Ïƒmax(S)2
||W
T
U||F ||V T
Z||F
â‰¤
Ç«
n
Ïƒmax(S)2
(||W
T
U||2
F + ||V T
Z||2
F ) .
In order proceed, consider Eq. (43). Since by tangency condition UT L = 0, we have UT W =
nR(cos Î˜ âˆ’ 1)RT whence
||UT
W||F = n|| cos Î¸ âˆ’ 1|| =
n
2
||4 sin2
(Î¸/2)|| â‰¤
n
2
||2 sin(Î¸/2)||2
(44)
(here Î¸ = (Î¸1, . . . , Î¸r) is the vector containing the diagonal elements of Î˜). A similar calculation
reveals that ||W||2
F = n||2 sin(Î¸/2)||2 thus proving ||UT W||2
F â‰¤ ||W||4
F /4 â‰¤ CnÎ´2||W||2
F . The bound
||V T Z||2
F â‰¤ CnÎ´2||Z||2
F is proved in the same way, thus yielding
E4 â‰¤ CnÇ«Ïƒmax(S)2
Î´2
d(x, u)2
.
Proceeding analogously for the other terms in the expression of E{A2}, we proved that with high
probability
A2
â‰¥ nÇ«||S âˆ’ Î£||2
F + nÇ« CÏƒmin(S)2
âˆ’ Câ€²
Î´2
Ïƒmax(S)2

d(x, u)2
(45)
â‰¥ nÇ«(1 âˆ’ Cd(x, u)2
)||S âˆ’ Î£||2
F + nÇ« CÎ£2
min âˆ’ CÎ´2
Î£2
max

d(x, u)2
, (46)
where we used the bounds Ïƒmin(S)2 â‰¥ Î£2
min/2 âˆ’ ||S âˆ’ Î£||2
F and Ïƒmax(S)2 â‰¤ 2Î£2
max + 2 ||S âˆ’ Î£||2
F .
The above inequality implies the desired claim if we take d(x, u) â‰¤ Î´ small enough.
16
Upper bound on B. By Lemma 6.1 we have
B2
â‰¤ Ïƒmax(S)2
X
a,b
X
(i,j)âˆˆE
W
2
iaZ
2
jb (47)
â‰¤
CÇ«
n
Ïƒmax(S)2
X
i,j
||W
(i)
||2
||Z
(j)
||2
+ CÏƒmax(S)2âˆš
Ç«
X
a,b
X
i
||W
(i)
||4
!1/2
ï£«
ï£­
X
j
||Z
(j)
||4
ï£¶
ï£¸
1/2
(48)
â‰¤
CÇ«
n
Ïƒmax(S)2
X
i,j
||W
(i)
||2
||Z
(j)
||2
+ Câ€²
Ïƒmax(S)2âˆš
Ç«r
X
i
||W
(i)
||2
!1/2
ï£«
ï£­
X
j
||Z
(j)
||2
ï£¶
ï£¸
1/2
.
(49)
where in the last step we used the incoherence condition. We conclude therefore that
B2
â‰¤ nÏƒmax(S)2
CÇ« Î´2
+ Câ€²âˆš
Ç«r

d(x, u)2
(50)
â‰¤ CnÇ«(Î£2
max + ||S âˆ’ Î£||2
F ) max(r/
âˆš
Ç«, Î´2
)d(x, u)2
(51)
which implies the thesis for r/
âˆš
Ç«, Î´ small enough.
In order to prove the upper bound in Eq. (38) we can set Î£ = S, thus obtaining
F(X, Y ) =
1
2
X
(i,j)âˆˆE
(UÎ£Z
T
+ WÎ£V T
+ WÎ£Z
T
)2
ij
â‰¤ b
A2
+ b
B2
,
where we defined
b
A2
â‰¡
X
(i,j)âˆˆE
(UÎ£Z
T
+ WÎ£V T
)2
ij ,
b
B2
â‰¡
X
(i,j)âˆˆE
(WÎ£Z
T
)2
ij .
Bounds for these two quantities are derived as for A2 and B2. More precisely, by Theorem 4.1 in
[CR08], we have b
A2 â‰¤ (1 âˆ’ Î¾)E{ b
A2} and
E{ b
A2
} =
Ç«
n
||UÎ£Z
T
+ WÎ£V T
||2
F =
=
2Ç«
n
||UÎ£Z
T
||2
F +
2Ç«
n
||WÎ£V T
||2
F
â‰¤ Ç«Î£2
max(||Z||2
F + ||W||2
F ) â‰¤ CnÇ«d(x, u)2
.
Further by setting S = Î£ in the derivation for B we get b
B2 â‰¤ (C/100)nÇ« d(x, u)2.
7 Proof of Lemma 5.4
Throughout this proof we assume m = n to lighten the notation.
As in the proof of Lemma 5.3, we let t 7â†’ x(t) = (X(t), Y (t)) be the geodesic starting at x(0) = u
with velocity xÌ‡(0) = w = (W, Z) âˆˆ Tu. We also define x = x(1) = (X, Y ) with X = U + W and
Y = V + Z. Let b
w = xÌ‡(1) = (c
W, b
Z) be its velocity when passing through x. An explicit expression
17
is obtained in terms of the singular value decomposition of W and Z. If we let W = LÎ˜RT , we
obtain
c
W = âˆ’URÎ˜ sin Î˜ RT
+ LÎ˜ cos Î˜ RT
. (52)
An analogous expression holds for b
Z. Since LT U = 0, we have ||c
W||2
F = n||Î˜ sin Î˜||2
F +n||Î˜ cos Î˜||2
F =
n||Î¸||2. Hence ||b
w||2 = nd(x, u)2. In order to prove the thesis, it is therefore sufficient to show
that hgrad e
F(x), b
wi â‰¥ CnÇ« d(x, u)2. In the following we will indeed show that hgrad F(x), b
wi â‰¥
CnÇ« d(x, u)2, and hgrad G(x), b
wi â‰¥ 0.
As a preliminary remark, we notice that b
w âˆˆ K((3Ï€2/2 + 96/Ï€2)Âµ0). Indeed
||c
W(i)
||2
â‰¤ 2||Î˜ sin Î˜RT
U(i)
||2
+ 2||Î˜ cos Î˜L(i)
||2
â‰¤
Ï€2
2
||U(i)
||2
+ 2||W(i)
||2
, (53)
By assumption we have ||U(i)||2 â‰¤ 3Âµ0r and we proved ||W(i)||2 â‰¤ 48Âµ0r/Ï€2 in the previous section.
7.1 Lower bound on grad F(x)
Recalling that PE is the projector defined in Eq. (28), and using the expression (29), (30), for the
gradient, we have
hgrad F(x), b
wi = hPE(XSY T
âˆ’ M), (XS b
ZT
+ c
WSY T
)i
= hPE(U(S âˆ’ Î£)V T
+ USZ
T
+ WSV T
+ WSZ
T
), (US b
ZT
+ c
WSV T
+ WS b
ZT
+ c
WSZ
T
)i
â‰¥ A âˆ’ B1 âˆ’ B2 âˆ’ B3 (54)
where we defined
A = hPE(USZ
T
+ WSV T
), (US b
ZT
+ c
WSV T
)i , (55)
B1 = |hPE(USZ
T
+ WSV T
), (WS b
ZT
+ c
WSZ
T
)i| , (56)
B2 = |hPE(U(S âˆ’ Î£)V T
+ WSZ
T
), (US b
ZT
+ c
WSV T
)i| , (57)
B3 = |hPE(U(S âˆ’ Î£)V T
+ WSZ
T
), (W S b
ZT
+ c
WSZ
T
)i| . (58)
At this point the proof becomes very similar to the one in the previous section and consists in lower
bounding A and upper bounding B1, B2, B3. One important fact that we will use is that c
W is well
approximated by W or by W, and b
Z is well approximated by Z or by Z. Before proceeding, it is
worth deriving a few estimates of this type. In particular, using Eqs. (43) and (52) we get
||c
W||2
F = ||W||2
F = n||Î¸||2
, (59)
||W||2
F = n||2 sin Î¸/2||2
, (60)
hc
W, Wi = n
r
X
a=1
Î¸a sin Î¸a , (61)
hc
W, Wi = n
r
X
a=1
Î¸2
a cos Î¸a , (62)
and therefore
||c
W âˆ’ W||2
F = n
r
X
i=a
[(2 sin Î¸a/2)2
+ Î¸2
a âˆ’ 2Î¸a sin Î¸a] (63)
â‰¤ n
r
X
i=a
(Î¸a âˆ’ 2 sin Î¸a/2)2
â‰¤
n
242
||Î¸||4
â‰¤
n
242
d(u, x)4
. (64)
18
Analogously
||c
W âˆ’ W||2
F = n
r
X
i=a
[2Î¸2
a âˆ’ 2Î¸2
a cos Î¸a] â‰¤ n ||Î¸||4
â‰¤ n d(u, x)4
(65)
The last inequality implies in particular
||UT c
W||F = ||UT
(W âˆ’ c
W)||F â‰¤ nd(u, x)2
. (66)
Similar bounds hold of course for Z, b
Z, Z (for instance we have ||V T b
Z||F â‰¤ nd(u, x)2). Finally, we
shall use repeatedly the fact that ||S âˆ’ Î£||2
F â‰¤ Cd(x, u)2, which follows from Lemma 5.3. This in
turns implies
Ïƒmax(S) â‰¤ Î£max + C d(x, u)2
, (67)
Ïƒmin(S) â‰¥ Î£min âˆ’ C d(x, u)2
. (68)
We can now bound the various terms on Eq. (54).
Lower bound on A. Using Theorem 4.1 in [CR08] we obtain, with high probability for any Î¾ > 0:
A â‰¥
Ç«
n
h(USZ
T
+ WSV T
), (US b
ZT
+ c
WSV T
)i (69)
âˆ’
Î¾ Ç«
n
||USZ
T
+ WSV T
||F ||US b
ZT
+ c
WSV T
||F â‰¥ (1 âˆ’ Î¾)A0 âˆ’ (1 + Î¾)B0 (70)
where
A0 =
Ç«
n
||USZ
T
+ WSV T
||2
F (71)
B0 =
Ç«
n
||USZ
T
+ WSV T
||F ||US(Z âˆ’ b
Z)T
+ (W âˆ’ c
W)SV T
||F . (72)
The term A0 is lower bounded analogously to E{A2} in the proof of Lemma 5.3 (see Eq. (44) and
below). Using the eigenvalue bounds Eq. (67) and (68), we obtain A0 â‰¥ CnÇ«d(x, u)2. As for the
second term we notice that
B0 â‰¤ 2Ç«Tr(SST
(Z âˆ’ b
Z)(Z âˆ’ b
Z)T
) + 2Ç«Tr(ST
S(W âˆ’ c
W)(W âˆ’ c
W)T
) (73)
â‰¤ 2Ç«Ïƒmax(S)(||Z âˆ’ b
Z||2
F + ||W âˆ’ c
W||2
F ) â‰¤ CnÇ«d(x, u)4
. (74)
Therefore for Î´ â‰¥ d(x, u) small enough A0 > 2B0, whence A0 â‰¥ CnÇ«d(x, u)2/4.
Upper bound on B1. We begin by using Cauchy-Schwarz inequality:
B1 â‰¤ ||PE(USZ
T
+ WSV T
)||F ||PE(WS b
ZT
+ c
WSZ
T
)||F , (75)
Proceeding as above we obtain (with high probability)
||PE(USZ
T
+ WSV T
)||2
F â‰¤ (1 + Î¾)
Ç«
n
||USZ
T
+ WSV T
||2
F (76)
â‰¤ (1 + Î¾)
2Ç«
n
(||USZ
T
||2
F + ||WSV T
||2
F ) (77)
â‰¤ (1 + Î¾)2Ç«

Tr(ST
SZ
T
Z) + Tr(SST
W
T
W)

(78)
â‰¤ CnÇ«d(x, u)2
. (79)
19
In order to estimate the second factor in Eq. (75), we first notice that ||PE(WS b
ZT + c
WSZ
T
)||2
F â‰¤
2||PE(WS b
ZT )||2
F + 2||PE(c
WSZ
T
)||2
F and then bound each of the two terms in the same way. Con-
sider, to be definite, the first one:
||PE(WS b
ZT
)||2
F =
X
(ij)âˆˆE
(WS b
ZT
)2
ij â‰¤ Ïƒmax(S)2
r
X
a,b=1
X
(ij)âˆˆE
W
2
ia
b
Z2
jb .
At this point we can apply Lemma the same argument as after Eq. (47). Bounding Ïƒmax(S)2 and
using the fact that both W, b
Z âˆˆ K(CÂµ0) we get
||PE(WS b
ZT
)||2
F = CnÇ« max(r/
âˆš
Ç«, Î´2
)d(x, u)2
.
Putting together Eqs. (79) and (80) we finally get
B1 = CnÇ« max(r1/2
/Ç«1/4
, Î´)d(x, u)2
,
which is smaller than A/100 for Î´, r/
âˆš
Ç« small enough.
Upper bound on B2. We have
B2 â‰¤ ||PE(US b
ZT
+ c
WSV T
)||F ||WSZ
T
||F + |hPE(US b
ZT
), U(S âˆ’ Î£)V T
i|
+|hPE(c
WSV T
), U(S âˆ’ Î£)V T
i|
â‰¡ Bâ€²
2 + Bâ€²â€²
2 + Bâ€²â€²â€²
2
The upper bound on Bâ€²
2 is obtained similarly to the the one on B1. Indeed proceeding as above we
obtain
||PE(US b
ZT
+ c
WSV T
)||2
F â‰¤ CnÇ«d(x, u)2
, (80)
||WSZ
T
||2
F â‰¤ CnÇ« max(r/
âˆš
Ç«, Î´)d(x, u)2
, (81)
whence Bâ€²
2 â‰¤ CnÇ«d(x, u)2/100 for Î´, r/
âˆš
Ç« small enough.
Consider now Bâ€²â€²
2 . By Theorem 4.1 in [CR08], we have
Bâ€²â€²
2 â‰¤
Ç«
n
|hUS b
ZT
, U(S âˆ’ Î£)V T
i| +
Î¾Ç«
n
||US b
ZT
||F ||U(S âˆ’ Î£)V T
||F (82)
â‰¤ Ïƒmax(S)Ç«||S âˆ’ Î£||F || b
ZT
V ||F + Î¾Ç«||US b
ZT
||F ||S âˆ’ Î£||F . (83)
The first term is bounded using (the analogous of) Eq. (66) and ||S âˆ’Î£||F â‰¤ d(x, u). For the second
term we use || b
Z||2
F â‰¤ Cnd(x, u)2, thus getting
Bâ€²â€²
2 â‰¤ CnÇ«d(x, u)3
+ CnÇ«Î¾d(x, u)2
â‰¤ CnÇ«(Î´ + Î¾)d(x, u)2
(84)
whence Bâ€²â€²
2 â‰¤ CnÇ«d(x, u)2/100 for Î´ small enough. The same argument applies to Bâ€²â€²â€²
2 thus proving
the desired bound.
Upper bound on B3. Finally for the last term it is sufficient to use a crude bound
B3 â‰¤ 4

||PE(WS b
ZT
)||F + ||PE(c
WSZ
T
)||F

||PE(U(S âˆ’ Î£)V T
)||F + ||PE(WSZ
T
)||F

, (85)
and all of the factors have been estimated above.
20
7.2 Lower bound on grad G(x)
By the definition of G in Eq. (34), we have
hgrad G(x), b
wi =
1
Âµ0r
m
X
i=1
Gâ€²
1
||X(i)||2
2Âµ0r
!
hX(i)
, c
W(i)
i +
1
Âµ0r
n
X
j=1
Gâ€²
1
||Y (i)||2
2Âµ0r
!
hY (i)
, b
Z(i)
i . (86)
It is therefore sufficient to show that if ||X(i)||2 > 2Âµ0r, then hX(i), c
W(i)i > 0, and if ||Y (j)||2 >
2Âµ0r, then hY (j), b
Z(j)i > 0. We will just consider the first statement, the second being completely
symmetrical.
From the explicit expressions (43) and (52) we get
X(i)
= R
n
cos Î˜ RT
U(i)
+ sin Î˜ L(i)
o
, (87)
c
W(i)
= R
n
Î˜ cos Î˜L(i)
âˆ’ Î˜ sin Î˜RT
U(i)
o
. (88)
From the first expression it follows that
|| sin Î˜ L(i)
||2
â‰¤ ||X(i)
||2
+ || cos Î˜ RT
U(i)
||2
â‰¤ 3 Âµ0r . (89)
On the other hand, by taking the difference of Eqs. (87) and (88) we have
||X(i)
âˆ’ c
W(i)
|| â‰¤ ||(sin Î˜ âˆ’ Î˜ cos Î˜)L(i)
|| + ||(cos Î˜ + Î˜ sin Î˜)RT
U(i)
|| (90)
â‰¤ max
i
(Î¸2
i )|| sin Î˜L(i)
|| + ||U(i)
|| â‰¤ Î´
p
3Âµ0r +
âˆš
Âµ0r . (91)
where we used the inequality (sin Ï‰ âˆ’ Ï‰ cos Ï‰) â‰¤ Ï‰2 sin Ï‰ valid for Ï‰ âˆˆ [0, Ï€/2]. For Î´ small enough
we have therefore ||X(i) âˆ’ c
W(i)|| â‰¤ (9/10)
âˆš
2Âµ0r. To conclude, for ||X(i)|| â‰¥ 2Âµ0r
hX(i)
, c
W(i)
i â‰¥ ||X(i)
||2
âˆ’ ||X(i)
|| ||X(i)
âˆ’ c
W(i)
|| â‰¥ ||X(i)
||(
p
2Âµ0r âˆ’ (9/10)
p
2Âµ0r) â‰¥ 0 . (92)
Acknowledgements
We thank Emmanuel CandeÌs and Benjamin Recht for stimulating discussions on the subject of this
paper. This work was partially supported by a Terman fellowship and an NSF CAREER award
(CCF-0743978).
A Proof of Remark 4.3
The proof consists in showing that |Al| > max{eâˆ’C1Ç«m, C2Î±} with probability less than 1/2n3, using
Chernoff bound. In the case of large Ç«, when Ç« > 2
âˆš
Î± log(n), we have P

|Al| > C2Î± â‰¤ 1/2n3, for
C2 > 4/Î±. In the case of small Ç«, when Ç« â‰¤ 2
âˆš
Î± log(n), P

|Al| > eâˆ’C1Ç«m â‰¤ 1/2n3, for C1 < 1/4
âˆš
Î±,
which proves the thesis.
Analogously, we can prove that P

|Ar| > max{eâˆ’C1Ç«n, C2} â‰¤ 1/2n3 , which finishes the proof
of Remark 4.3.
21
B Proof of Remark 4.5
The expectation of the contribution of light couples, when each edge is independently revealed with
probability Ç«/
âˆš
mn, is
E[Z] =
Ç«
âˆš
mn
ï£«
ï£­
X
(i,j)âˆˆL
xiMA
ij yj âˆ’ xT
My
ï£¶
ï£¸ ,
where we define MA by setting to zero the rows of M whose index is not in Al and the columns of
M whose index is not in Ar.
In order to bound
P
(i,j)âˆˆL xiMA
ij yj âˆ’ xT My, we write,
X
(i,j)âˆˆL
xiMA
ij yj âˆ’ xT
My = xT

MA
âˆ’ M

y âˆ’
X
(i,j)âˆˆL
xiMA
ij yj
â‰¤ xT

MA
âˆ’ M

y +
X
(i,j)âˆˆL
xiMA
ij yj .
The first term can be bounded by noting that |(MA âˆ’ M)ij| is positive only if i /
âˆˆ Al or j /
âˆˆ Ar
in which case |(MA âˆ’ M)ij| â‰¤ Âµ1
âˆš
r by the incoherence condition A2. Also, by Remark 4.3, there
exists Î´ â‰¤ max{eâˆ’C1Ç«, C2/n} such that |i : i /
âˆˆ Al| â‰¤ Î´m and |j : j /
âˆˆ Ar| â‰¤ Î´n. Denoting by I( Â· ) the
indicator function, we have
xT

MA
âˆ’ M

y â‰¤
X
ij
xi yj

I(i /
âˆˆ Al) + I(j /
âˆˆ Ar)

Âµ1
âˆš
r
=
ï£«
ï£­
X
i
xi I(i /
âˆˆ Al)
X
j
yj +
X
j
yj I(j /
âˆˆ Ar)
X
i
xi
ï£¶
ï£¸ Âµ1
âˆš
r
â‰¤
âˆš
Î´m
âˆš
n +
âˆš
Î´n
âˆš
m

Âµ1
âˆš
r
â‰¤
Âµ1
âˆš
mnr
âˆš
Ç«
.
for Î´ â‰¤ 1
4Ç« . We can bound the second term as follows
X
(i,j)âˆˆL
xiMA
ij yj â‰¤
X
(i,j)âˆˆL
xiMA
ij yj
2
xiMA
ij yj
â‰¤
r
mn
rÇ«
X
(i,j)âˆˆL
xiMA
ij yj
2
â‰¤
r
mn
rÇ«
X
iâˆˆ[m],jâˆˆ[n]
xiMA
ij yj
2
â‰¤
Âµ2
1
âˆš
mnr
âˆš
Ç«
,
22
where the second inequality follows from the definition of heavy couples and the last inequality is
due to incoherence condition A2.
Hence summing two contributions, we get
|E [Z]| â‰¤ Âµ1 + Âµ2
1
 âˆš
rÇ« .
C Proof of Remark 4.6
We can associate to the matrix Q a bipartite graph G = ([m], [n], E). The proof is similar to the one
in [FKS89, FO05] and is based on two properties of the graph cG:
1. The graph G has maximum degree bounded by a constant times the average degree:
deg(i) â‰¤
2Ç«
âˆš
Î±
, (93)
deg(j) â‰¤ 2Ç«
âˆš
Î± , (94)
for all i âˆˆ [m] and j âˆˆ [n].
2. Discrepancy. We will say that G (equivalently, the adjacency matrix Q) has the discrepancy
property if, for any A âŠ† [m] and B âŠ† [n], one of the following is true:
1.
e(A, B)
Âµ(A, B)
â‰¤ Î¾1 , (95)
2. e(A, B) ln
 e(A, B)
Âµ(A, B)

â‰¤ Î¾2 max{|A|, Î±|B|} ln
 âˆš
mn
max{|A|, Î±|B|}

. (96)
for two numerical constants Î¾1, Î¾2 (independent of n and Ç«). Here e(A, B) denotes the number
of edges between A and B and Âµ(A, B) = |A||B||E|/mn denotes the average number of edges
between A and B before trimming.
We will prove that the discrepancy property holds with high probability later in this section, see
Lemma C.1.
Let us partition row and column indices with respect to the value of xu and yv:
Ai = {u âˆˆ [m] :
âˆ†
âˆš
m
2iâˆ’1
â‰¤ |xu| <
âˆ†
âˆš
m
2i
} ,
Bj = {v âˆˆ [n] :
âˆ†
âˆš
n
2jâˆ’1
â‰¤ |yv| <
âˆ†
âˆš
n
2j
} ,
for i âˆˆ {1, 2, . . . , âŒˆlog (
âˆš
m/âˆ†)/ log 2âŒ‰}, and j âˆˆ {1, 2, . . . , âŒˆlog (
âˆš
n/âˆ†)/ log 2âŒ‰}, and we denote the
size of subsets Ai and Bj by ai and bj respectively. Furthermore, we define ei,j to be the number
of edges between two subsets Ai and Bj, and we let Âµi,j = aibj(Ç«/
âˆš
mn). Notice that all indices u
of non zero xu fall into one of the subsets Aiâ€™s defined above, since, by discretization, the smallest
non-zero element of x âˆˆ Tm in absolute value is âˆ†/
âˆš
m. The same applies for the entries of y âˆˆ Tn.
23
By grouping the summation into Aiâ€™s and Bjâ€™s, we get
X
(u,v):
|xuyv|â‰¥ C
âˆš
Ç«
n
Quv|xuyv| â‰¤
X
(i,j):2i+jâ‰¥ 4C
âˆš
Î±Ç«
âˆ†2
ei,j
âˆ†2i
âˆš
m
âˆ†2j
âˆš
n
= âˆ†2
X
aibj
Ç«
âˆš
mn
ei,j
Âµi,j
2i
âˆš
m
2j
âˆš
n
= âˆ†2âˆš
Ç«
X
ai
22i
m
| {z }
Î±i
bj
22j
n
| {z }
Î²j
ei,j
âˆš
Ç«
Âµi,j2i+j
| {z }
Ïƒi,j
.
Note that, by definition, we have
X
i
Î±i â‰¤ 4||x||2
/âˆ†2
, (97)
X
i
Î²i â‰¤ 4||y||2
/âˆ†2
. (98)
We are now left with task of bounding
P
Î±iÎ²jÏƒi,j, for Q that satisfies bounded degree property and
discrepancy property.
Define,
C1 â‰¡

(i, j) : 2i+j
â‰¥
4C
âˆš
Î±Ç«
âˆ†2
and (Ai, Bj) satisfies (95)

, (99)
C2 â‰¡

(i, j) : 2i+j
â‰¥
4C
âˆš
Î±Ç«
âˆ†2
and (Ai, Bj) satisfies (96)

\ C1 . (100)
We need to show that
P
(i,j)âˆˆC1âˆªC2
Î±iÎ²jÏƒi,j is bounded.
For the terms in C1 this bound is easy. Since summation is over pairs of indices (i, j) such that
2i+j â‰¥ 4C
âˆš
Î±Ç«
âˆ†2 , it follows that Ïƒi,j â‰¤ Î¾1âˆ†2/4C
âˆš
Î±. By Eqs. (97) and (98), we have
P
C1
Î±iÎ²jÏƒi,j â‰¤
(Î¾1âˆ†2/4C
âˆš
Î±)(2/âˆ†)4 = O(1).
For the terms in C2 the bound is more complicated. We assume ai â‰¤ Î±bj for simplicity and
the other case can be treated in the same manner. By change of notation the second discrepancy
condition becomes
ei,j log

ei,j
Âµi,j

â‰¤ Î¾2 max{ai, Î±bj} log
 âˆš
mn
max{ai, Î±bj}

. (101)
We start by changing variables on both sides of Eq. (101).
ei,jaibjÇ«
Âµi,j
âˆš
mn
log

ei,j
Âµi,j

â‰¤ Î¾2Î±bj log

22j
Î²j
âˆš
Î±

.
Now, multiply each side by 2i/bj
âˆš
Ç«2j to get
Ïƒi,jÎ±i log

ei,j
Âµi,j

â‰¤
Î¾22i
âˆš
Ç«2j

log(22j
) âˆ’ log(Î²j
âˆš
Î±)

. (102)
To achieve the desired bound, we partition the analysis into 5 cases:
24
1. Ïƒi,j â‰¤ 1 : By Eqs. (97) and (98), we have
P
Î±iÎ²jÏƒi,j â‰¤ (2/âˆ†)4 = O(1).
2. 2i >
âˆš
Ç«2j : By the bounded degree property in Eq. (94), we have ei,j â‰¤ ai2Ç«/
âˆš
Î±, which implies
that ei,j/Âµi,j â‰¤ 2n/bj. For a fixed i we have,
P
j Î²jÏƒi,jI(2i >
âˆš
Ç«2j) â‰¤ 2
âˆš
Ç«
P
j 2jâˆ’iI(2i >
âˆš
Ç«2j) â‰¤
4. Then,
P
Î±iÎ²jÏƒi,j â‰¤ 16/âˆ†2 = O(1).
3. log (ei,j/Âµi,j) > 1
4

log(22j) âˆ’ log(Î²j
âˆš
Î±)

: From Eq.(102), it immediately follows that Ïƒi,jÎ±i â‰¤
4Î¾22i
âˆš
Ç«2j . Because of case 2, we can assume 2i â‰¤
âˆš
Ç«2j, which implies that for a fixed j we have the
following inequality :
P
i Ïƒi,jÎ±i â‰¤ 4Î¾2
P
i
2i
âˆš
Ç«2j I(2i â‰¤
âˆš
Ç«2j) â‰¤ 8Î¾2. Then it follows by Eq. (98)
that
P
Î±iÎ²jÏƒi,j â‰¤ 32Î¾2/âˆ†2 = O(1).
4. log(22j) â‰¥ âˆ’ log(Î²j
âˆš
Î±) : Because of case 3, we can assume log (ei,j/Âµi,j) â‰¤ 1
4

log(22j) âˆ’ log(Î²j
âˆš
Î±)

,
which implies that log (ei,j/Âµi,j) â‰¤ log(2j). Further, because of case 1, we assume 1 < Ïƒi,j =
ei,j
âˆš
Ç«/Âµi,j2i+j. Combining those two inequalities, we get 2i â‰¤
âˆš
Ç«.
Since in defining C2 we excluded C1, if (i, j) âˆˆ C2 then log (ei,j/Âµi,j) â‰¥ 1. Applying Eq. (102)
we get Ïƒi,jÎ±i â‰¤ Ïƒi,jÎ±i log (ei,j/Âµi,j) â‰¤ (Î¾22iâˆ’j/
âˆš
Ç«)

log(22j) âˆ’ log(Î²j
âˆš
Î±)

â‰¤ 4Î¾22i/
âˆš
Ç«.
Combining above two results, it follows that
P
i Ïƒi,jÎ±i â‰¤ 4Î¾2
P
i
2i
âˆš
Ç«
I(2i â‰¤
âˆš
Ç«) â‰¤ 8Î¾2 . Then,
we have the desired bound :
P
Î±iÎ²jÏƒi,j â‰¤ 32Î¾2
âˆ†2 = O(1).
5. log(22j) < âˆ’ log(Î²j
âˆš
Î±) : Because of case 4, we assume log(22j) â‰¤ âˆ’ log(Î²j
âˆš
Î±). Then it
follows, since weâ€™re not in case 3, that log (ei,j/Âµi,j) â‰¤ 1
4

log(22j) âˆ’ log(Î²j
âˆš
Î±)

â‰¤ âˆ’ log(Î²j
âˆš
Î±).
Hence, ei,j/Âµi,j â‰¤ 1/Î²j
âˆš
Î±. This implies that Ïƒi,j = ei,j
âˆš
Ç«/Âµi,j2i+j â‰¤
âˆš
Ç«/Î²j
âˆš
Î±2i+j. Since the
summation is over pairs of indices (i, j) such that 2i+j â‰¥ 4C
âˆš
Î±Ç«/âˆ†2, we have
P
j Ïƒi,jÎ²j â‰¤ âˆ†2
2Î±C .
Then it follows that
P
Î±iÎ²jÏƒi,j â‰¤ 2
Î±C = O(1).
Summing up the results, we get that there exists a constant Câ€² â‰¤ 16
âˆ†4 + 4Î¾1
Câˆ†2
âˆš
Î±
+ 16
âˆ†2 + 32Î¾2
âˆ†2 + 2
Î±C ,
such that
X
(i,j):2i+jâ‰¥ 4C
âˆš
Î±Ç«
âˆ†2
Î±iÎ²jÏƒi,j â‰¤ Câ€²
.
This finishes the proof of Remark 4.6.
Lemma C.1. The adjacency matrix Q has discrepancy property with probability at least 1 âˆ’ 1/n.
Proof. The proof is a generalization of analogous result in [FKS89, FO05] which is proved to hold only
with probability larger than 1âˆ’eâˆ’CÇ«. The stronger statement quoted here is a result of the observation
that, when we trim the graph the number of edges between any two subsets does not increase. Define
Q0 to be the adjacency matrix corresponding to original random matrix ME before trimming. If
the discrepancy assumption holds for Q0, then it also holds for Q, since eQ(A, B) â‰¤ eQ0 (A, B), for
A âŠ† [m] and B âŠ† [n].
Now we need to show that the desired property is satisfied for Q0. This is proved for the case
of non-bipartite graph in Section 2.2.5 of [FO05], and analogous analysis for bipartite graph shows
that for all subsets A âŠ† [m] and B âŠ† [n], with probability at least 1âˆ’1/n, the discrepancy condition
holds with Î¾1 = max{4, 2e} and Î¾2 = max{12 + 15
Î± , 15 + 12
Î± }.
25
D Proof of remarks 5.1 and 5.2
Proof. (Remark 5.1.) Let Î¸ = (Î¸1, . . . , Î¸p), Î¸i âˆˆ [âˆ’Ï€/2, Ï€/2] be the principal angles between the
planes spanned by the columns of X1 and X2. It is known that dc(X1, X2) = ||2 sin(Î¸/2)||2 and
dp(X1, X2) = || sin Î¸||2. The thesis follows from the elementary inequalities
1
Ï€
Î± â‰¤
âˆš
2 sin(Î±/2) â‰¤ sin Î± â‰¤ 2 sin(Î±/2) (103)
valid for Î± âˆˆ [0, Ï€/2].
Proof. (Remark 5.2.) We start by observing that
dp(V, Y ) =
1
âˆš
n
min
AâˆˆRrÃ—r
||V âˆ’ Y A||F . (104)
Indeed the minimization on the right hand side can be performed explicitly (as ||V âˆ’ Y A||2
F is a
quadratic function of A) and the minimum is achieved at A = Y T V/n. The inequality follows by
simple algebraic manipulations.
Take A = ST XT UÎ£âˆ’1/n. Then
||V âˆ’ Y A||F = sup
B,||B||F â‰¤1
hB, (V âˆ’ Y A)i (105)
= sup
B,||B||F â‰¤1
hBT
,
1
n
Î£âˆ’1
UT
(UÎ£V T
âˆ’ XSY T
)i (106)
=
1
n
sup
B,||B||F â‰¤1
hUÎ£âˆ’1
BT
, (M âˆ’ c
M)i (107)
â‰¤
1
n
sup
B,||B||F â‰¤1
||UÎ£âˆ’1
BT
||F ||M âˆ’ c
M||F . (108)
On the other hand
||UÎ£âˆ’1
BT
||2
F = Tr(BÎ£âˆ’1
UT
UÎ£âˆ’1
BT
) = nTr(BT
BÎ£âˆ’2
) â‰¤ nÎ£âˆ’2
min||B||2
F ,
whereby the last inequality follows from the fact that Î£ is diagonal. Together (104) and (108), this
implies the thesis.
References
[AFK+01] Y. Azar, A. Fiat, A. Karlin, F. McSherry, and J. Saia, Spectral analysis of data, Pro-
ceedings of the thirty-third annual ACM symposium on Theory of computing (New York,
NY, USA), ACM, 2001, pp. 619â€“626.
[AM07] D. Achlioptas and F. McSherry, Fast computation of low-rank matrix approximations, J.
ACM 54 (2007), no. 2, 9.
[Arm66] L. Armijo, Minimization of functions having lipschitz continuous first partial derivatives,
Pacific J. Math. 16 (1966), no. 1, 1â€“3.
[BDJ99] M. W. Berry, Z. DrmacÌ, and E. R. Jessup, Matrices, vector spaces, and information
retrieval, SIAM Review 41 (1999), no. 2, 335â€“362.
26
[Ber92] M. W. Berry, Large scale sparse singular value computations, International Journal of
Supercomputer Applications 6 (1992), 13â€“49.
[CCS08] J-F Cai, E. J. CandeÌ€s, and Z. Shen, A singular value thresholding algorithm for matrix
completion, arXiv:0810.3286, 2008.
[CR08] E. J. CandeÌ€s and B. Recht, Exact matrix completion via convex optimization,
arxiv:0805.4471, 2008.
[CRT06] E. J. Candes, J. K. Romberg, and T. Tao, Robust uncertainty principles: exact signal
reconstruction from highly incomplete frequency information, IEEE Trans. on Inform.
Theory 52 (2006), 489â€“ 509.
[CT09] E. J. CandeÌ€s and T. Tao, The power of convex relaxation: Near-optimal matrix comple-
tion, arXiv:0903.1476, 2009.
[Don06] D. L. Donoho, Compressed Sensing, IEEE Trans. on Inform. Theory 52 (2006), 1289â€“
1306.
[EAS99] A. Edelman, T. A. Arias, and S. T. Smith, The geometry of algorithms with orthogonality
constraints, SIAM J. Matr. Anal. Appl. 20 (1999), 303â€“353.
[Faz02] M. Fazel, Matrix rank minimization with applications, Ph.D. thesis, Stanford University,
2002.
[FKS89] J. Friedman, J. Kahn, and E. SzemereÌdi, On the second eigenvalue in random regular
graphs, Proceedings of the Twenty-First Annual ACM Symposium on Theory of Com-
puting (Seattle, Washington, USA), ACM, may 1989, pp. 587â€“598.
[FKV04] A. Frieze, R. Kannan, and S. Vempala, Fast monte-carlo algorithms for finding low-rank
approximations, J. ACM 51 (2004), no. 6, 1025â€“1041.
[FO05] U. Feige and E. Ofek, Spectral techniques applied to sparse random graphs, Random Struct.
Algorithms 27 (2005), no. 2, 251â€“275.
[KMO08] R. H. Keshavan, A. Montanari, and S. Oh, Learning low rank matrices from O(n) entries,
Proc. of the Allerton Conf. on Commun., Control and Computing, September 2008.
[KOM09] R. H. Keshavan, S. Oh, and A. Montanari, Matrix completion from a few entries,
arXiv:0901.3150, January 2009.
[Net] Netflix prize.
[RFP07] B. Recht, M. Fazel, and P. Parrilo, Guaranteed minimum rank solutions of matrix equa-
tions via nuclear norm minimization, arxiv:0706.4138, 2007.
[SC09] A. Singer and M. Cucuringu, Uniqueness of low-rank matrix completion by rigidity theory,
arXiv:0902.3846, January 2009.
27
