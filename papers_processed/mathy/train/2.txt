JMLR:WorkshopandConferenceProceedingsvol23(2012)33.1–33.3425thAnnualConferenceonLearningTheoryAMethodofMomentsforMixtureModelsandHiddenMarkovModelsAnimashreeAnandkumarA.ANANDKUMAR@UCI.EDUUniversityofCalifornia,IrvineDanielANANDKUMARHSUKAKADEofstatisticalconsistencyundermildconditions.Unfortunately,themethodoftenrunsintotroublewithlargemixturesofhigh-dimensionaldistributions.Thisisbecausetheequationsdeterminingtheparametersaretypicallybasedonmomentsoforderequaltothenumberofmodelparameters,andhigh-ordermomentsareexceedinglydifficulttoestimateaccuratelyduetotheirlargevariance.Thisworkdevelopsacomputationallyefficientmethodofmomentsbasedononlylow-ordermomentsthatcanbeusedtoestimatetheparametersofabroadAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSentlearningobjective)canbethoughtofasmodernimplementationsofthemethodofmoments,andtheirexponentialdependenceonkisnotsurprisinggiventheliteratureonothermomentmethodsformixturemodels.Inparticular,anumberofmomentmethodsforbothdiscreteandcontinuousmixturemodelshavebeendevelopedusingtechniquessuchastheVandermondedecompositionsofHankelmatrices(Lindsay,1989;LindsayandBasak,1993;Boleyetal.,1997;Gravinetal.,2012).Inthesemethods,followingthespiritofPearson’soriginalsolution,themodelparame-tersarederivedfromtherootsofpolynomialswhosecoefficientsarebasedonmomentsuptotheΩ(k)-thorder.Theaccurateestimationofsuchmomentsgenerallyhascomputationalandsamplecomplexityexponentialink.Spectralapproachtoparameterestimationwithlow-ordermoments.Thepresentworkisbasedonanotableexceptiontotheabovesituation,namelyChang’sspectraldecompositiontechniquefordiscreteMarkovmodelsofevolution(Chang,1996)(seealsoMosselandRoch(2006)andHsuetal.(2009)foradaptationstootherdiscretemixturemodelssuchasdiscreteHMMs).Thisspectraltechniquedependsonlyonmomentsuptothethird-order;consequently,theresultingalgorithmshavecomputationalandsamplecomplexitythatscalesonlypolynomiallyinthenumberofmixturecomponentsk.Thesuccessofthetechniquedependsonacertainrankconditionofthetransitionmatrices;butthisconditionismuchmilderthanseparationconditionsofclusteringworks,anditremainssufficientevenwhenthedimensionoftheobservationspaceisverylarge(Hsuetal.,2009).Inthiswork,weextendChang’sspectraltechniquetodevelopageneralmethodofmomentsapproachtoparameterestimation,whichisapplicabletoalargeclassofmixturemodelsandHMMswithbothdiscreteandcontinuouscomponentdistributionsinhigh-dimensionalspaces.LikethemomentmethodsofMoitraandValiant(2010)andBelkinandSinha(2010),ouralgorithmdoesnotrequireaseparationcondition;butunlikethosepreviousmethods,thealgorithmhascomputationalandsamplecomplexitypolynomialink.Somepreviousspectralapproachesforrelatedlearningproblemsonlyusesecond-ordermo-ments,buttheseapproachescanonlyestimateasubspacecontainingtheparametervectorsandnottheparametersthemselves(McSherry,2001).Indeed,itisknownthattheparametersofevenverysimplediscretemixturemodelsarenotgenerallyidentifiablefromonlysecond-ordermo-ments(Chang,1996)1.Wenotethatmomentsbeyondthesecond-order(specifically,fourth-ordermoments)havebeenexploitedinthemethodsofFriezeetal.(1996)andNguyenandRegev(2009)fortheproblemoflearningaparallelepipedfromrandomsamples,andthatthesemethodsareveryrelatedtotechniquesusedforindependentcomponentanalysis(HyvärinenandOja,2000).Adapt-ingthesetechniquesforotherparameterestimationproblemsisanenticingpossibility.Multi-viewlearning.Thespectraltechniqueweemploydependsontheavailabilityofmultipleviews,andsuchamulti-viewassumptionhasbeenexploitedinpreviousworksonlearningANANDKUMARHSUKAKADEliterature(Dasgupta,1999;VempalaandWang,2002;ChaudhuriandRao,2008;Feldmanetal.,2005,2006);thecombinationofthisandanon-degeneracyassumptioniswhatallowsustoavoidthesamplecomplexitylowerboundofMoitraandValiant(2010)forGaussianmixturemodels.Themulti-viewassumptionalsonaturallyarisesinmanyapplications,suchasinmultimediadatawith(say)text,audio,andvideocomponents(BlaschkoandLampert,2008;Chaudhurietal.,2009);aswellasinlinguisticdata,wherethedifferentwordsinasentenceorparagraphareconsiderednoisypredictorsoftheunderlyingsemantics(Galeetal.,1992).Intheveinofthislatterexample,weconsiderestimationinasimplebag-of-wordsdocumenttopicmodelasawarm-uptoourgeneralmethod;eventhissimplermodelillustratesthepowerofpair-wiseandtriple-wise(i.e.,bigramandtrigram)statisticsthatwerenotexploitedbypreviousworksonmulti-viewlearning.1.2.OutlineSection2firstdevelopsthemethodofmomentsinthecontextofasimplediscretemixturemodelmotivatedbydocumenttopicmodeling;anexplicitalgorithmandconvergenceanalysisarealsoprovided.ThegeneralsettingisconsideredinSection3,wherethemainalgorithmanditsaccom-panyingcorrectnessandefficiencyguaranteearepresented.Applicationstolearningmulti-viewmixturesofGaussiansandHMMsarediscussedinSection4.Proofsandadditionaldiscussionareprovidedintheappendices.1.3.NotationsThestandardinnerproductbetweenvectors~uand~visdenotedbyh~u,~vi=~u>~v.Wedenotethep-normofavector~vbyk~vkp.ForamatrixA∈Rm×n,weletkAk2denoteitsspectralnormkAk2:=sup~v6=~0kA~vk2/k~vk2,kAkFdenoteitsFrobeniusnorm,σi(A)denotethei-thlargestsingularvalue,andκ(A):=σ1(A)/σmin(m,n)(A)denoteitsconditionnumber.Let∆n−1:={(p1,p2,...,pn)∈Rn:pi≥0∀i,Pni=1pi=1}denotetheprobabilitysimplexinRn,andletSn−1:={~u∈Rn:k~uk2=1}denotetheunitsphereinRn.Let~ei∈Rddenotethei-thcoordinatevectorwhosei-thentryis1andtherestarezero.Finally,forapositiveintegern,let[n]:={1,2,...,n}.2.Warm-up:bag-of-wordsdocumenttopicmodelingWefirstdescribeourmethodofmomentsinthesimplercontextofbag-of-wordsmodelsfordocu-ments.ProofsoflemmasandtheoremsinthissectionaregiveninAppendixA.2.1.SettingSupposeadocumentcorpuscanbepartitionedbytopic,witheachdocumentbeingassignedasingletopic.Further,supposethewordsinadocumentaredrawnindependentlyfromamultinomialdistributioncorrespondingtothedocument’stopic.Letkbethenumberofdistincttopicsinthecorpus,dbethenumberofdistinctwordsinthevocabulary,and`≥3bethenumberofwordsineachdocument(sothedocumentsmaybequiteshort).Thegenerativeprocessforadocumentisgivenasfollows:1.Thedocument’stopicisdrawnaccordingtothemultinomialdistributionspecifiedbytheprobabilityvector~w=(w1,w2AMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSvariablehsuchthatPr[h=j]=wj,j∈[k].2.Giventhetopich,thedocument’s`wordsaredrawnindependentlyANANDKUMARHSUKAKADE2.3.ObservableoperatorsandtheirspectralpropertiesThepair-wiseandtriple-wiseprobabilitiescanberelatedinawaythatessentiallyrevealsthecon-ditionalprobabilitymatrixM.Thisisachievedthroughamatrixcalledan“observableoperator”.Similarobservableoperatorswerepreviouslyusedtocharacterizemultiplicityautomata(Schützenberger,1961;Jaeger,2000)and,morerecently,forlearningdiscreteHMMs(viaanoperatorparameteriza-tion)(Hsuetal.,2009).Lemma2AssumeCondition1.LetU∈Rd×kandV∈Rd×kbematricessuchthatbothU>MandV>Mareinvertible.ThenU>PairsVisinvertible,andforall~η∈Rd,the“observableoperator”B(~η)∈Rk×k,givenbyB(~η):=(U>Triples(~η)V)(U>PairsV)−1,satisfiesB(~η)=(U>M)diag(M>~η)(U>M)−1.ThematrixB(~η)iscalled“observable”becauseitisonlyafunctionoftheobservablevariables’jointprobabilities(e.g.,Pr[~x1=~ei,~x2=~ej]).Inthecase~η=~exforsomex∈[d],thematrixB(~ex)issimilar(inAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSAlgorithmA1.Obtainempiricalfrequenciesofwordpairsandtriplesfromagivensampleofdocuments,andformthetables[Pairs∈Rd×dANANDKUMARHSUKAKADEDefinetheconditionalmeanvectorsas~µv,j:=E[~xv|h=j],v∈[`],j∈[k],andletMv∈Rd×kbethematrixwhosej-thAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSLemma6ConsiderthesettinganddefinitionsfromLemma5.LetΘ∈Rk×kbeaninvertiblematrix,andlet~θ>i∈Rkbeitsi-throw.Moreover,foralli∈[k],letλi,1,λi,2,...,λi,kdenotethekeigenvaluesofB1,2,3(U3~θi)intheorderspecifiedbythematrixofANANDKUMARHSUKAKADECondition3ThereexistpositivescalarsN0,C1,2,C1,3,C1,2,3,andafunctionf(N,δ)(decreasinginNandδ)suchthatforanyN≥N0andδ∈(0,1),1.PrhkP̂a,b−Pa,bk2≤Ca,b·f(N,δ)i≥1−δfor{a,b}∈{{1,2},{1,3}},2.∀~v∈Rd,PrhkP̂1,2,3(~v)−P1,2,3(~v)k2≤C1,2,3·k~vk2·f(N,δ)i≥1−δ.Moreover(fortechnicalconvenience),P̂1,3isindependentofP̂1,2,3(whichmaybeachieved,say,bysplittingasampleofsize2N).ForthediscretemodelssuchasthedocumenttopicmodelofSection2.1anddiscreteHMMs(Mos-selandRoch,2006;Hsuetal.,2009),Condition3holdswithN0=C1,2=C1,3=C1,2,3=1,andf(N,δ)=(1+pln(1/δ))/√N.Usingstandardtechniques(e.g.,Chaudhurietal.(2009);Vershynin(2012)),theconditioncanalsobeshowntoholdformixturesofvariouscontinuousdistributionssuchasmultivariateGaussians.Nowwearereadytopresentthemaintheoremofthissection.Theorem7ThereexistsaconstantC>0suchthatthefollowingholds.Assumethethree-viewmixturemodelsatisfiesCondition2andCondition3.Pickany∈(0,1)andδ∈(0,δ0).Further,assumeΘ∈Rk×kisanindependentrandomrotationmatrixdistributeduniformlyovertheStiefelmanifold{Q∈Rk×k:Q>Q=I}.IfthenumberofsamplesNsatisfiesN≥N0andf(N,δ/k)≤C·mini6=jkM3(~ei−~ej)k2·σk(P1,2)C1,2,3·k5·κ(M1)4·δln(k/δ)·,f(N,δ)≤C·min(mini6=jkM3(~ei−~ej)k2·σk(P1,2)2C1,2·kP1,2,3k2·k5·κ(M1)4·δln(k/δ),σk(P1,3)C1,3)·wherekP1,2,3k2:=max~v6=~0kP1,2,3(~v)k2,thenwithprobabilityatleast1−5δ,AlgorithmBreturnsM̂3=[µ̂3,1|µ̂3,2|···|µ̂3,k]withthefollowingguarantee:thereexistsapermutationτon[k]suchthatforeachj∈[k],kµ̂3,j−~µ3,τ(j)k2≤maxj0∈[k]k~µ3,j0k2·.4.ApplicationsInadditiontothedocumentclusteringmodelfromSection2,anumberofnaturallatentvariablemodelsfitintothismulti-viewframework.Wedescribetwosuchcasesinthissection:GaussianmixturemodelsandHMMs,bothofwhichhavebeen(atleastpartially)studiedAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSdistributionoftheobservedrandomvector~xgivenhisamultivariateGaussianwithmean~µhandcovarianceΣh.Themulti-viewANANDKUMARHSUKAKADEandtheparametersoftheresultingthree-viewmixturemodelon(h,~x1,~x2,~x3)are~w:=T~π,M1:=AMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSJ.Feldman,R.O’Donnell,andR.Servedio.PAClearningmixturesofaxis-alignedGaussianswithnoseparationassumption.InCOLT,2006.A.M.Frieze,M.Jerrum,andR.Kannan.Learninglineartransformations.InFOCS,1996.W.A.Gale,K.W.Church,andD.Yarowsky.Onesenseperdiscourse.In4thDARPASpeechandNaturalLanguageWorkshop,1992.N.Gravin,J.Lasserre,D.Pasechnik,andS.Robins.Theinversemomentproblemforconvexpolytopes.DiscreteandComputationalGeometry,2012.Toappear.H.Hotelling.Themostpredictablecriterion.JournalofEducationalPsychology,26(2):139–142,1935.D.Hsu,S.M.Kakade,andT.Zhang.AspectralalgorithmforlearninghiddenMarkovmodels.InCOLT,2009.D.Hsu,S.M.Kakade,andT.Zhang.AspectralalgorithmforlearninghiddenMarkovmodels.JournalofComputerandSystemSciences,2012.Toappear.A.HyvärinenandE.Oja.Independentcomponentanalysis:algorithmsandapplications.NeuralNetworks,13(4–5):411–430,2000.H.Jaeger.Observableoperatormodelsfordiscretestochastictimeseries.NeuralComputation,12(6),2000.A.T.Kalai,A.Moitra,andG.Valiant.EfficientlylearningmixturesoftwoGaussians.InSTOC,2010.R.Kannan,H.Salmasian,andS.Vempala.Thespectralmethodforgeneralmixturemodels.InCOLT,2005.B.G.Lindsay.Momentmatrices:applicationsinmixtures.AnnalsofStatistics,17(2):722–740,1989.B.G.Lindsay.Mixturemodels:theory,geometryandapplications.AmericanStatisticalAssocia-tion,1995.B.G.LindsayandP.Basak.Multivariatenormalmixtures:afastconsistentmethod.JournaloftheAmericanStatisticalAssociation,88(422):468–476,1993.F.McSherry.Spectralpartitioningofrandomgraphs.InFOCS,ANANDKUMARHSUKAKADEh~x1~x2···~x`h1h2···h`~x1~x2~x`(a)(b)FigureAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSsoPairs=Mdiag(~w)M>.Moreover,writing~η=(η1,η2,...,ηd),Triples(~η)i,j=dXx=1ηxPr[~x1=~ei,~x2=~ej,~x3=~ex]=dXx=1kXt=1ηx·Mi,t·Mj,t·Mx,t·wt=kXt=1Mi,t·Mj,t·wt·(M>~η)tsoTriples(~η)=Mdiag(M>~η)diag(~w)M>.A.2.ProofofLemma2Sincediag(~w)0byCondition1andU>PairsV=(U>M)diag(~w)M>VbyLemma1,itfollowsthatU>PairsVisinvertiblebytheassumptionsonUandV.Moreover,alsobyLemma1,B(~η)=(U>Triples(~η)V)(U>PairsV)−1=(U>Mdiag(M>~η)diag(~w)M>V)(U>PairsV)−1=(U>M)diag(M>~η)(U>M)−1(U>Mdiag(~w)M>V)(U>PairsV)−1=(U>M)diag(M>~η)(U>M)−1.A.3.AccuracyofmomentestimatesLemma9Fixδ∈(0,1).Let[PairsbetheempiricalaverageofNindependentcopiesof~x1⊗~x2,andlet\TriplesbetheempiricalaverageofNindependentcopiesof(~x1⊗~x2)h~η,~x3i.Then1.Pr"k[Pairs−PairskF≤1+pln(1/δ)√N#≥1−δ,and2.Pr"∀~η∈Rd,k\Triples(~η)−Triples(~η)kF≤k~ηk2(1+pln(1/δ))√N#≥1−δ.ProofThefirstclaimfollowsfromapplyingLemma24tothevectorizationsof[PairsandPairs(whereupontheFrobeniusnormistheEuclideannormofthevectorizedmatrices).ForthesecondANANDKUMARHSUKAKADENowconditiononthisevent.Forany~η=(η1,η2,...,ηd)∈Rd,k\Triples(~η)−Triples(~AMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSandκ(R)≤κ(Û>M)2≤k(1−ε21)·σk(M)2.(7)TheANANDKUMARHSUKAKADEB̂(~eformat)B̂(~egod)B̂(~ekey)B̂(~epolygon)B̂(~eteam)B̂(~etoday)sourcegodkeypolygonwingamefindwritebittimegametigerpostjesuschipsaverunbitimagechristiansystemreferteamrunfealchristencryptbookyearpitchintersectpeoplecarsourcedondayemailtimerepositorymanwatchteamrpiaprveroutinegoodtruetimesinpublicnetcomscorelotproblembibleAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSB.2.ProofofLemma5WehaveU>1P1,2U2=(U>1M1)diag(~w)(M>2U2)byLemma4,whichisinvertiblebytheassump-tionsonUvandCondition2.Moreover,alsobyLemma4,B1,2,3(~η)=(U>1P1,2,3(~η)U2)(U>1P1,2U2)−1=(U>1M1diag(M>3~η)diag(~w)M>2U2)(U>1P1,2U2)−1=(U>1M1)diag(M>3~η)(U>1M1)−1(U>1M1diag(~w)M>2U2)(U>1P1,2U2)−1=(U>1M1)diag(M>3~η)(U>1M1)−1(U>1P1,2U2)(U>1P1,2U2)−1=(U>1M1)diag(M>3~η)(U>1M1)−1.B.3.ProofofLemma6ByLemma5,(U>1M1)−1B1,2,3(U3~θi)(U>1M1)=diag(M>3U3~θi)=diag(h~θi,U>3M3~e1i,h~θi,U>3M3~e2i,...h~θi,U>3M3~eki)=diag(λi,1,λi,2,...,λi,k)foralli∈[k],andthereforeL=h~θ1,U>3M3~e1ih~θ1,U>3M3~e2i···h~θ1,U>3M3~ekih~θ2,U>3M3~e1ih~θ2,U>3M3~e2i···h~θ2,ANANDKUMARHSUKAKADEB.5.EstimatingthemixingweightsGiventheestimateofM̂3,onecanobtainanestimateof~wusingŵ:=M̂†3Ê[~x3]whereA†denotestheMoore-PenrosepseudoinverseofA(thoughothergeneralizedinversesmayworkaswell),andÊ[~x3]istheempiricalaverageof~x3.Thisestimatorisbasedonthefollowingobservation:E[~x3]=E[E[~x3|h]]=M3E[~eh]=M3~wandthereforeM†3E[~x3]=M†3M3~w=~wsinceM3hasfullcolumnrank.B.6.ProofofTheorem7TheproofissimilartothatofTheorem3,sowejustdescribetheessentialdifferences.Asbefore,mostperturbationargumentsaredeferredtoAppendixC.First,letE1betheeventinwhichkP̂1,2−P1,2k2≤C1,2·f(N,δ),kP̂1,3−P1,3k2≤C1,3·f(N,δ)andkP̂1,2,3(Û3~θi)−P1,2,3(Û3~θi)k2≤C1,2,3·f(N,δ/k)foralli∈[k].ThereforebyCondition3andaunionbound,wehavePr[E1]≥1−3δ.Second,letE2betheeventinwhichγ:=mini∈[k]minj6=j0|h~θi,Û>3M3(~ej−~ej0)i|>minj6=j0kÛ>3M3(~ej−~ej0)k2·δ√ekk2kandλmax:=maxi,j∈[k]|h~θi,Û>3M3~eji|≤maxj∈[k]kM3~ejk2√k1+p2ln(k2/δ).Sinceeach~θiisdistributeduniformlyoverSk−1,itfollowsfromLemma15andaunionboundthatPr[E2|E1]≥1−2δ.ThereforePr[E1∩E2]≥(1−3δ)(1−2δ)≥1−5δ.LetU3∈Rd×kbethematrixoftopkorthonormalleftsingularvectorsofM3.ByLemma10andtheconditionsonN,wehaveσk(Û>3U3)≥1/2,andthereforeγ≥mini6=i0kM3(~ei−~ei0)k2·δ2√ekk2kandλmaxγ≤√ek3(1+p2ln(k2/δ))δ·κ0(M3)whereκ0(M3AMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSLet~ηi:=Û3~θifori∈[k].ByLemma10,Û>1P1,2Û2isinvertible,sowemaydefineB̃1,2,3(~ηi):=(Û>1P1,2,3(~ηi)Û2)(Û>1P1,2Û2)−1.ByLemma5,B̃1,2,3(~ηi)=(Û>1M1)diag(M>3~ηi)(Û>1M1)−1.AlsodefineR:=Û>1M1diag(kÛ>1M1~e1k2,kÛ>1MANANDKUMARHSUKAKADEAppendixC.PerturbationanalysisforobservableoperatorsThefollowinglemmaestablishestheaccuracyofapproximatingthefundamentalsubspaces(i.e.,therowandcolumnspaces)ofamatrixXbycomputingthesingularvaluedecompositionAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSProofLetŜ:=Û>X̂V̂andS̃:=Û>XV̂.ByLemma10,Û>X̂V̂isinvertible,σk(S̃)≥σk(Û>U)·σk(X)·σk(V̂>V)≥(1−ε21)·σk(X)(soS̃isalsoinvertible),andkŜ−S̃k2≤ANANDKUMARHSUKAKADErowofR̂−1.Fixaparticulari∈[k].Since{~ξ1,~ξ2,...,~ξk}formsabasisforRk,wecanwriteˆξi=Pkj=1ci,j~ξjforsomecoefficientsci,1,ci,2,...,ci,k∈R.Wemayassumeci,i≥0(orelsewereplaceˆξiwith−ˆξi).Thefactthatkˆξik2=k~ξjk2=1forallj∈[k]andthetriangleinequalityimply1=kˆξik2≤ci,ik~ξik2+Pj6=i|ci,j|k~ξjk2=ci,i+Pj6=i|ci,j|,andthereforekˆξi−~ξik2≤|1−ci,i|k~ξik2+Xj6=i|ci,jk~ξjk2≤2Xj6=i|ci,j|againbythetriangleinequality.Therefore,itsufficestoshow|ci,j|≤2kR−1k2·ε3forj6=itoprovethesecondclaim.ObservethatAˆξi=A(Pki0=1ci,i0~ξi0)=Pki0=1ci,i0λi0~ξi0,andthereforekXi0=1ci,i0λi0~ξi0+(Â−A)ˆξi=Âˆξi=λ̂iˆξi=λikXi0=1ci,i0~ξi0+(λ̂i−λi)ˆξi.Multiplyingthroughtheaboveequationby~ζ>j,andusingthefactthat~ζ>j~ξi0=1{j=i0}givesci,jλj+~ζ>i(Â−A)ˆξi=λici,j+(λ̂i−λi)~ζ>jˆξi.Theaboveequationrearrangesto(λj−λi)ci,j=(λ̂i−λi)~ζ>jˆξi+~ζ>j(A−Â)ˆξiandtherefore|ci,j|≤k~ζjk2·(|λ̂i−λi|+k(Â−A)ˆξik2)|λj−λi|≤kR−1k2·(|λ̂i−λi|+kÂ−Ak2)|λj−λi|bytheCauchy-Schwarzandtriangleinequalitiesandthesub-multiplicativepropertyofthespectralnorm.Thebound|ci,j|≤2kR−1k2·ε3thenfollowsfromthefirstclaim.Thethirdclaimfollowsfromstandardcomparisonsofmatrixnorms.ThenextlemmagivesperturbationboundsforestimatingtheeigenvaluesofsimultaneouslydiagonalizablematricesA1,A2,...,Ak.TheeigenvectorsR̂aretakenfromaperturbationofthefirstmatrixA1,andarethensubsequentlyusedtoapproximatelydiagonalizetheperturbationsoftheremainingmatricesA2,...,Ak.Inpractice,onemayuseJacobi-likeprocedurestoapproximatelysolvethejointeigenvalueproblem.Lemma13LetA1,A2,...,Ak∈Rk×kbediagonalizablematricesthatarediagonalizedbythesamematrixinvertibleR∈Rk×kwithunitlengthcolumnskR~ejk2=1,suchthateachAihaskdistinctrealeigenvalues:R−1AiR=diag(λi,1,λi,2,...,λi,k).LetÂ1,Â2,...,Âk∈Rk×kbegiven.DefineA:=maxikÂi−Aik2,γA:=miniminj6=j0|λi,j−λi,j0|,λmax:=maxi,j|λi,j|,ε3:=κ(R)·AγA,andε4:=4k1.5·kR−1k22·ε3.Assumeε3<12andε4<1.Thenthereexistsapermutationτon[k]suchthatthefollowingholds.1.ThematrixÂ1haskdistinctrealeigenvaluesλ̂1,1,λ̂1,2,...,λ̂1,k∈R,and|λ̂1,j−λ1,τAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMS2.ThereexistsamatrixR̂∈Rk×kwhosej-thcolumnisarighteigenvectorcorrespondingtoλ̂1,j,scaledsokR̂~ejk2=1forallj∈[k],suchthatkR̂−Rτk2≤ε4kR−1k2,whereRτisthematrixobtainedbypermutingthecolumnsofRwithτ.3.ThematrixR̂isinvertibleanditsinversesatisfieskR̂−1−R−1τk2≤kR−1k2·ε41−ε4;4.Foralli∈{2,3,...,k}andallj∈[k],the(j,j)-thelementofR̂−1ÂiR̂,denotedbyλ̂i,j:=~e>jR̂−1ÂiR̂~ej,satisfies|λ̂i,j−λi,τ(j)|≤1+ε41−ε4·1+ε4√k·κ(R)·ε3·γA+κ(R)·11−ε4+1√k·κ(R)+1√k·ε41−ε4·ε4·λmax.Ifε4≤12,then|λ̂i,j−λi,τ(j)|≤3ε3·γA+4κ(R)·ε4·λmax.ProofThefirstandsecondclaimsfollowfromapplyingLemma12ANANDKUMARHSUKAKADEcontinuingfrom(13),|λ̂i,j−λi,τ(j)|isboundedas|λ̂i,j−λi,τ(j)|≤kR−1k2·kRk2·A+kR−1k2·A·4k·kR−1k2·ε3+kR−1k2·ε41−ε4·A·kRk2+kR−1k2·ε41−ε4·A·4k·kR−1k2·ε3+λmax·kR−1k2·ε41−ε4·kRk2+λmax·kR−1k2·4k·kR−1kAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSForthesecondclaim,applyLemma25withβ:=1+tandt:=p2ln(m/δ)toobtainPr|h~θ,A~eii|≥kA~eik2√m·(1+t)≤exp121−(1+t)2+2ln(1+t)≤exp121−(1+t)2+2t=e−t2/2=δ/m.Thereforethesecondclaimfollowsbytakingaunionboundoveralli∈[m].AppendixD.ProofsanddetailsfromSection4Inthissection,weprovideomittedproofsanddetailsfromSection4.D.1.LearningmixturesofproductdistributionsInthissection,weshowhowtouseAlgorithmBwithmixturesofproductdistributionsinRnthatsatisfyanincoherenceconditiononthemeans~µ1,~µ2,...,~µk∈Rnofkcomponentdistributions.Notethatproductdistributionsarejustaspecialcaseofthemoregeneralclassofmulti-viewdistri-butions,whicharedirectlyhandledbyAlgorithmB.Thebasicideaistorandomlypartitionthecoordinatesinto`≥3“views”,eachofroughlythesamedimension.Undertheassumptionthatthecomponentdistributionsareproductdistributions,themulti-viewassumptionissatisfied.Whatremainstobecheckedisthatthenon-degeneracycondition(Condition2)issatisfied.Theorem16(below)showsthatitsufficesthattheoriginalmatrixofcomponentmeanshaverankkandsatisfythefollowingincoherencecondition.Condition4(Incoherencecondition)Letδ∈(0,1),`∈[n],andM=[~µ1|~µ2|···|~µk]∈Rn×kbegiven;letM=USV>bethethinsingularvaluedecompositionofM,whereU∈Rn×kisamatrixoforthonormalcolumns,S=diag(σ1(M),σ2(M),...,σk(M))∈Rk×k,andV∈Rk×kisorthogonal;andletcM:=maxj∈[n]nk·kU>~ejk22.Thefollowinginequalityholds:cM≤932·bn/`ck·ln`·kδ.NotethatcMisalwaysintheinterval[1,n/k];itissmallestwhentheleftsingularvectorsinUhave±1/√nentries(asinaHadamardbasis),andlargestwhenthesingularvectorsarethecoordinateaxes.Roughlyspeaking,theincoherenceconditionrequiresthatthenon-degeneracyofamatrixMbewitnessedbymanyverticalblocksofM.Whentheconditionissatisfied,thenwithhighprobability,arandompartitioningofthecoordinatesinto`groupsinducesablockpartitioningofMinto`matricesM1,M2,...,M`(withroughlyequalnumberofrows)suchthatthek-thlargestsingularvalueofMvisnotmuchsmallerthanthatofM(foreachv∈[`]).ChaudhuriandRao(2008)showthatunderasimilarcondition(whichtheycallaspreadingcon-dition),arandompartitioningofthecoordinatesintotwoANANDKUMARHSUKAKADEthemeansofkcomponentdistributions.Theythenfollowthispreprocessingwithaprojectionbasedonthecorrelationsacrossthetwoviews(similartoCCA).However,theiroverallalgorithmrequiresaminimumseparationconditiononthemeansofthecomponentdistributions.Incontrast,AlgorithmBdoesnotrequireaminimumseparationconditionatallinthissetting.Theorem16AssumeCondition4holds.Withprobabilityatleast1−δ,auniformlychosenrandompartitioningof[n]into`disjointsets[n]=I1∪I2∪···∪I`,eachofsizeatleast|Iv|≥329·cM·k·ln`·kδ,hasthefollowingproperty:foreachv∈[`],thematrixMv∈R|Iv|×kformedbyselectingtherowsofMindexedbyIvandscalingbypn/|Iv|,satisfiesσk(Mv)≥σk(M)/2.ProofFollowsfromLemma17(below)togetherwithaunionbound.Lemma17AssumeCondition4holds.Considerarandomsubset{J1,J2,...,Jd}⊆[n]ofsizedchosenuniformlyatrandomwithoutreplacement,andletfMbetherandomd×kmatrixgivenbyfM:=rnd·~e>J1M~e>J2M...~e>JdM.Ifd≥329·cM·k·lnkδ,thenPrhσk(fM)≥σk(M)/2i≥1−δ.ProofLet{I1,I2,...,Id}⊆[n]bearandomsubsetofsizedchosenuniformlyatrandomwithreplacement,andletcMbetherandomd×kmatrixgivenbycM:=rnd·~e>I1M~e>I2M...~e>IdM.ByProposition18,foranyτ>0,Pr[σk(fM)<τ]≤Pr[σk(cM)<AMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSNotethatσk(cM)=qλmin(cM>cM).Foreachj∈[d],letXj:=n·(U>~eIj)⊗(U>~eIj),socM>cM=nddXj=1(M>~eIj)⊗(M>~eIj)=VS1ddXj=1XjSV>andλmin(cM>cM)≥λmin(S)2·λmin1ddXj=1Xj=σk(M)2·λmin1ddXj=1Xj.ObservethatE[Xj]=nXi=1Pr[Ij=i]·n·(U>~ei)⊗(U>~ei)=Iandthatλmax(Xj)≤n·maxi∈[n]{kU>~eik22}=cM·k,almostsurely.ByLemma26(aChernoffboundonextremaleigenvaluesofrandomsymmetricmatrices),Pr"λmin1ddXj=1Xj≤14#≤k·e−d(3/4)2/(2cMk)≤δ.Theclaimfollows.Proposition18(Reductiontosamplingwithreplacement)Consideranym×nmatrixA.Foranyt∈[m],leteAtbearandomt×nsubmatrixofAformedbychoosingarandomsubsetoftrowsofAuniformlyatrandomwithoutreplacement;andletbAtbearandomt×nsubmatrixofAformedbychoosingarandomsubsetoftrowsofAuniformlyatrandomwithreplacement.Fixanyt∈[m]andτ>0.ThenPr[σn(eAt)<τ]≤Pr[σn(bAt)<τ].ProofThisargumentissimilartoonegivenbyRecht(2009).WefirstprovethatPr[σn(eAt)<τ]isnon-increasingint.Foranyt0≤t,considerthefollowingcouplingbetweeneAtandeAt0:1.First,sampletrowindicesin[m]uniformlyatrandomwithoutreplacement,andselectthoserowsinAtoformeAt.2.Then,giventhesetrowindices,chooset−t0ofthemuniformlyatrandomwithoutreplace-ment,andremovethemtoformeAt0.Sinceσn(eAt0)≤σn(eAt),σn(eAt)<τ=⇒σn(eAt0)<τ,andconsequentlyPr[σn(eAt)<τ]≤Pr[σn(eAt0)<τANANDKUMARHSUKAKADENowweprovetheproposition.LetUniquet∈[t]bethenumberofdistinctrowindicesselectedtoformbAt.ThenPr[bAt<τ]=tAMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSwherecov(~x|h=j):=E[(~xv−E[~xv|h=j])⊗(~xv−E[~xv|h=j])|h=j]isassumedtobepositivedefinite.Usingstandardtechniques(e.g.,Vershynin(2012)),Condition3canbeshowntoholdundertheaboveconditionswiththefollowingparameters(forsomeuniversalconstantc>0):wmin:=minj∈[k]wjN0:=c·α3/2(d+log(1/δ))wminlogα3/2(d+log(1/δ))wminCa,b:=c·maxnkcov(~xv|h=j)k1/22,kE[~xv|h=j]k2:v∈{a,b},j∈[k]o2C1,2,3:=c·maxkcov(~xv|h=j)k1/22,kE[~xv|h=j]k2:v∈[3],j∈[k]o3f(N,δ):=rk2log(1/δ)N+sα3/2plog(N/δ)(d+log(1/δ))wminN.D.4.RecoveringthecomponentcovariancesWhileAlgorithmBrecoversjustthemeansofthemixturecomponents,weremarkthataslightvariationcanbeusedtorecoverthecovariancesaswell.NotethatE[~xv⊗~xv|h]=(Mv~eh)⊗(Mv~eh)+Σv,h=~µv,h⊗~µv,h+Σv,hforallv∈[`].Forapairofvectors~φ∈Rdand~ψ∈Rd,definethematrixQ1,2,3(~φ,~ψ)∈Rd×doffourth-ordermomentsbyQ1,2,3(~φ,~ψ):=E[(~x1⊗~x2)h~φ,~x3ih~ψ,~x3i].Proposition19UnderthesettingofLemma5,thematrixgivenbyF1,2,3(~φ,~ψ):=(U>1Q1,2,3(~φ,~ψ)U2)(U>1P1,2U2)−1satisfiesF1,2,3(~φ,~ψ)=(U>1M1)diag(h~φ,~µ3,tih~ψ,~µ3,ti+h~φ,Σ3,t~ψi:t∈[k])(U>1M1)−1andhenceisdiagonalizable(infact,bythesamematricesasB1,2,3(~η)).ProofAsintheproofofLemma4,itiseasytoshowthatQ1,2,3(~φ,~ψ)=E[E[~x1|h]⊗E[~x2|h]h~φ,E[~x3⊗~x3|h]~ψi]=M1E[~eh⊗~ehh~φ,(~µ3,h⊗~µ3,h+Σ3,h)~ψi]M>2=M1diag(h~φ,~µ3,tih~ψ,~µ3,ti+h~φ,Σ3,t~ψi:t∈[k])diag(~w)M>2.TheclaimthenfollowsfromthesameargumentsusedintheproofofLemma5.D.5.ProofofProposition8TheconditionalindependencepropertiesfollowfromtheHMMconditionalindependenceassump-tions.Tochecktheparameters,observefirstthatPr[h1=i|h2=j]=Pr[h2=j|h1=i]·Pr[h1=i]Pr[h2=j]=Tj,iπi(T~π)ANANDKUMARHSUKAKADEbyBayes’rule.ThereforeM1~ej=E[~x1|h2=j]=OE[~eh1|h2=j]=Odiag(~π)T>diag(T~π)−1~ej.Therestoftheparametersaresimilartoverify.AppendixE.GeneralresultsfrommatrixperturbationtheoryThelemmasinthissectionarestandardresultsfrommatrixperturbationtheory,takenfromStewartandSun(1990).Lemma20(Weyl’stheorem)AMETHODOFMOMENTSFORMIXTUREMODELSANDHMMSAppendixF.ProbabilityinequalitiesLemma24(Accuracyofempiricalprobabilities)Fix~µ=(µ1,µ2,...,µnANANDKUMARHSUKAKADE1.~1>Q=~1>;2.MQ−1,Qdiag(~w)M>diag(M~w)−1,andQ~whavenon-negativeentries;3.Qdiag(~w)Q>isadiagonalmatrix.Thenthemarginaldistributionover(x1,x2)isidenticaltothatinthecasewherethemodelhasparametersM̃:=MQ−1andw̃:=Q~w.Asimpleexampleford=k=2canbeobtainedfromM:=p1−p1−pp,~w:=1/21/2,Q:=p1+√1+4p(1−p)21−p1−√1+4p(1−p)2forsomep∈(0,1).Wetakep=0.25,inwhichcaseQsatisfiestheconditionsofProposition27,andM=0.250.750.750.25,~w=0.50.5,M̃=MQ−1≈0.66140.11290.33860.8871,w̃=Q~w≈0.70570.2943.Inthiscase,both(M,~w)and(M̃,w̃)giverisetothesamepair-wiseprobabilitiesMdiag(~w)M>=M̃diag(w̃)M̃>≈0.31250.18750.18750.3125.However,thetriple-wiseprobabilities,forη=(1,0),differ:for(M,~w),wehaveMdiag(M>η)diag(~w)M>≈0.21880.09380.09380.0938;whilefor(M̃,