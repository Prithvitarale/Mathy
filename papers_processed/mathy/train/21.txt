Hilbert Space Embeddings of Hidden Markov Models
Le Song lesong@cs.cmu.edu
Byron Boots beb@cs.cmu.edu
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
Sajid M. Siddiqi siddiqi@google.com
Google, Pittsburgh, PA 15213, USA
Geoffrey Gordon ggordon@cs.cmu.edu
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
Alex Smola alex@smola.org
Yahoo! Research, Santa Clara, CA 95051, USA
Abstract
Hidden Markov Models (HMMs) are impor-
tant tools for modeling sequence data. How-
ever, they are restricted to discrete latent
states, and are largely restricted to Gaussian
and discrete observations. And, learning al-
gorithms for HMMs have predominantly re-
lied on local search heuristics, with the ex-
ception of spectral methods such as those de-
scribed below. We propose a nonparamet-
ric HMM that extends traditional HMMs to
structured and non-Gaussian continuous dis-
tributions. Furthermore, we derive a local-
minimum-free kernel spectral algorithm for
learning these HMMs. We apply our method
to robot vision data, slot car inertial sensor
data and audio event classification data, and
show that in these applications, embedded
HMMs exceed the previous state-of-the-art
performance.
1. Introduction
Hidden Markov Models (HMMs) have successfully
modeled sequence data in a wide range of applications
including speech recognition, analysis of genomic se-
quences, and analysis of time series. HMMs are latent
variable models of dynamical systems: they assume a
latent state which evolves according to Markovian dy-
namics, as well as observations which depend only on
the hidden state at a particular time.
Appearing in Proceedings of the 26th
International Confer-
ence on Machine Learning, Haifa, Israel, 2010. Copyright
2010 by the author(s)/owner(s).
Despite their simplicity and wide applicability, HMMs
are limited in two major respects: first, they are usu-
ally restricted to discrete or Gaussian observations,
and second, the latent state variable is usually re-
stricted to have only moderate cardinality. For non-
Gaussian continuous observations, and for structured
observations with large cardinalities, standard infer-
ence algorithms for HMMs run into trouble: we need
huge numbers of latent states to capture such obser-
vation distributions accurately, and marginalizing out
these states during inference can be very computation-
ally intensive. Furthermore, standard HMM learning
algorithms are not able to fit the required transition
and observation distributions accurately: local search
heuristics, such as the EM algorithm, lead to bad lo-
cal optima, and standard approaches to regularization
result in under- or overfitting.
Recently, Hsu et al. (2009) proposed a spectral algo-
rithm for learning HMMs with discrete observations
and hidden states. At its core, the algorithm performs
a singular value decomposition of a matrix of joint
probabilities of past and future observations, and then
uses the result, along with additional matrices of joint
probabilities, to recover parameters which allow track-
ing or filtering. The algorithm employs an observable
representation of a HMM, and avoids explicitly recov-
ering the HMM transition and observation matrices.
This implicit representation enables the algorithm to
find a consistent estimate of the distribution of obser-
vation sequences, without resorting to local search.
Unfortunately, this spectral algorithm is only formu-
lated for HMMs with discrete observations. In con-
trast, many sources of sequential data are continous
Hilbert Space Embeddings of Hidden Markov Models
or structured; the spectral algorithm does not apply
to such data without discretization and flattening. So,
the goal of the current paper is to provide a new kernel-
based representation and kernelized spectral learning
algorithm for HMMs; this new representation and al-
gorithm will allow us to learn HMMs in any domain
where we can define a kernel. Furthermore, our algo-
rithm is free of local minima and admits finite-sample
generalization guarantees.
In particular, we will represent HMMs using a re-
cent concept called Hilbert space embedding (Smola
et al., 2007; Sriperumbudur et al., 2008). The essence
of Hilbert space embedding is to represent probabil-
ity measures (in our case, corresponding to distribu-
tions over observations and latent states in a HMM)
as points in Hilbert spaces. We can then perform in-
ference in the HMM by updating these points, en-
tirely in their Hilbert spaces, using covariance oper-
ators (Baker, 1973) and conditional embedding opera-
tors (Song et al., 2009). By making use of the Hilbert
space’s metric structure, our method works naturally
with continous and structured random variables, with-
out the need for discretization.
In addition to generalizing HMMs to arbitary do-
mains where kernels are defined, our learning algo-
rithm contributes to the theory of Hilbert space em-
beddings with hidden variables. Previously, Song et al.
(2009) derived a kernel algorithm for HMMs; however,
they only provided results for fully observable models,
where the training data includes labels for the true la-
tent states. By contrast, our algorithm only requires
access to an (unlabeled) sequence of observations.
We provide experimental results comparing embedded
HMMs learned by our spectral algorithm to several
other well-known approaches to learning models of
time series data. The results demonstrate that our
novel algorithm exceeds the previous state-of-the-art
performance, often beating the next best algorithm by
a substantial margin.
2. Preliminaries
In this paper, we follow the convention that upper-
case letters denote random variables (e.g. Xt, Ht) and
lowercase letters their instantiations (e.g. xt, ht). We
will use P to denote probability distribution in the dis-
crete cases and density in the continuous cases. For
matrices and vectors, we will use notation u = (ui)i
and C = (Cij)ij to list their entries. Following (Hsu
et al., 2009), we abbreviate a sequence (x1, . . . , xt) by
x1:t, and its reverse (xt, . . . , x1) by xt:1. When we
use a sequence as a subscript, we mean the prod-
uct of quantities indexed by the sequence elements
(e.g. Axt:1 = Axt . . . Ax1 ). We use 1m to denote an
m × 1 column of ones.
A discrete HMM defines a probability distribution over
sequences of hidden states, Ht ∈ {1, . . . , N}, and ob-
servations, Xt ∈ {1, . . . , M}. We assume N  M,
and let T ∈ RN×N
be the state transition probability
matrix with Tij = P(Ht+1 = i|Ht = j), O ∈ RM×N
be
the observation probability matrix with Oij = P(Xt =
i|Ht = j), and π ∈ RN
be the stationary state dis-
tribution with πi = P(Ht = i). The conditional inde-
pendence properties of the HMM imply that T, O and
π fully characterize the probability distribution of any
sequence of states and observations.
2.1. Observable representation for HMMs
Jaeger (2000) demonstrated that discrete HMMs can
be formulated in terms of ‘observation operators’ Axt .
Each Axt is a matrix of size N ×N with its ij-th entry
defined as P(Ht+1 = i|Ht = j)P(Xt = xt|Ht = j),
or in matrix notation, Axt
= T diag(Oxt,1, . . . , Oxt,m).
Then the probability of a sequence of observations,
x1:t, can be written as matrix operations,
P(x1:t) = 1>
N Axt
. . . Ax1
π = 1>
N Axt:1
π. (1)
Essentially, each Axt incorporates information about
one-step observation likelihoods and one-step hidden
state transitions. The sequence of matrix multi-
plications in equation (1) effectively implements the
marginalization steps for the sequence of hidden vari-
ables, Ht+1:1. Likewise, the predictive distribution for
one-step future Xt+1 given a history of observations
can be written as a sequence of matrix multiplications,
(P(Xt+1 = i|x1:t))
M
i=1 ∝ OAxt:1 π (2)
The drawback of the representations in (1) and (2) is
that they requires the exact knowledge of the transi-
tion matrix T and observation matrix O, and neither
quantity is available during training (since the latent
states are usually not observable).
A key observation concerning Equations (1) and (2)
is that if we are only interested in the final quantity
1>
N Axt:1
π and OAxt:1
π, we may not need to recover the
Axt
s exactly. Instead, it will suffice to recover them up
to some invertible transformation. More specifically,
suppose that matrix S is invertible, we can define a
set of new quantities,
b1 := Sπ, b∞ := OS−1
, Bx := SAxS−1
(3)
and equivalently compute OAxt:1
π by cancelling out
all S during matrix multiplications, resulting in
OAxt:1
π = OS−1

SAxt
S−1

. . . SAx1
S−1

(Sπ)
= b∞Bxt:1
b1 (4)
Hilbert Space Embeddings of Hidden Markov Models
The natural question is how to choose S such that b1,
b∞ and Bx can be computed based purely on observa-
tion sequences, x1:t.
Hsu et al. (2009) show that S = U>
O works, where U
is the top N left singular vectors of the joint probabil-
ity matrix (assuming stationarity of the distribution):
C2,1 := (P(Xt+1 = i, Xt = j))
M
i,j=1 . (5)
Furthermore, b1, b∞ and Bx can also be computed
from observable quantities (assuming stationarity),
u1 := (P(Xt = i))
M
i=1 , (6)
C3,x,1 := (P(Xt+2 = i, Xt+1 = x, Xt = j))
M
i,j=1 (7)
which are the marginal probability vector of sequence
singletons, and one slice of the joint probability matrix
of sequence triples (i.e. a slice indexed by x from a 3-
dimensional matrix). Hsu et al. (2009) showed
b1 = U>
u, b∞ = C2,1(U>
C2,1)†
(8)
Bx = (U>
C3,x,1)(U>
C2,1)†
. (9)
2.2. A spectral algorithm for learning HMMs
The spectral algorithm for learning HMMs proceeds by
first estimating u1, C2,1 and C3,x,1. Given a dataset
of m i.i.d. triples

(xl
1, xl
2, xl
3)
m
l=1
from a HMM (su-
perscripts index training examples), we estimate
û1 = 1
m
X
l=1:m
ϕ(xl
1) (10)
Ĉ2,1 = 1
m
X
l=1:m
ϕ(x2)ϕ(x1)>
(11)
Ĉ3,x,1 = 1
m
X
l=1:m
I[xl
2 = x]ϕ(xl
3)ϕ(xl
1)>
(12)
where the delta function (or delta kernel) is defined as
I[xl
2 = x] = 1 if xl
2 = x and 0 otherwise; and we have
used 1-of-M representation for discrete variables. In
this representation, ϕ(x = i) is a vector of length M
with all entries equal to zero except 1 at i-th position.
For instance, if x = 2, then ϕ(x) = (0, 1, 0, . . . , 0)>
.
Furthermore, we note that Ĉ3,x,1 is not a single but
a collection of matrices each indexed by an x. Effec-
tively, the delta function I[xl
2 = x] partition the obser-
vation triples according to x, and each Ĉ3,x,1 only gets
a fraction of the data for the estimation.
Next, a ‘thin’ SVD is computed for Ĉ2,1. Let its top
N left singular vectors be Û, then the observable rep-
resentation for the HMM (b̂1, b̂∞ and B̂x) can be es-
timated by replacing the population quantities with
their corresponding finite sample counterparts.
A key feature of the algorithm is that it does not ex-
plicitly estimate the transition and observation mod-
els; instead it estimates a set of observable quantities
that differ by an invertible transformation. The core
part of the algorithm is a SVD which is local mini-
mum free. Furthermore, (Hsu et al., 2009) also prove
that under suitable conditions this spectral algorithm
for HMMs efficiently estimates both the marginal and
predictive distributions.
3. Hilbert Space Embeddings of HMMs
The spectral algorithm for HMMs derived by Hsu et al.
(2009) is only formulated for discrete random vari-
ables. Based on their formulation, it is not clear how
one can apply this algorithm to general cases with
continuous and structured variables. For instance, a
difficulty lies in estimating Ĉ3,x,1. As we mentioned
earlier, to estimate each Ĉ3,x,1, we need to partition
the observation triples according to x, and each Ĉ3,x,1
only gets a fraction of the data for the estimation. For
continous observations, x can take infinite number of
possibile values, which makes the partition estimator
impractical. Alternatively, one can perform a Parzen
window density estimation for continuous variables.
However, further approximations are needed in order
to make Parzen window compatible with this spectral
algorithm (Siddiqi et al., 2009).
In the following, we will derive a new presentation and
a kernel spectral algorithm for HMMs using a recent
concept called Hilbert space embeddings of distribu-
tions (Smola et al., 2007; Sriperumbudur et al., 2008).
The essence of our method is to represent distributions
as points in Hilbert spaces, and update these points en-
tirely in the Hilbert spaces using operators (Song et al.,
2009). This new approach avoids the need for parti-
tioning the data making it applicable to any domain
where kernels can be defined.
3.1. Hilbert space embeddings
Let F be a reproducing kernel Hilbert space (RKHS)
associated with kernel k(x, x0
) := hϕ(x), ϕ(x0
)iF .
Then for all functions f ∈ F and x ∈ X we have
the reproducing property: hf, ϕ(x)iF = f(x), i.e. the
evaluation of function f at x can be written as an in-
ner product. Examples of kernels include the Gaussian
RBF kernel k(x, x0
) = exp(−s kx − x0
k
2
), however ker-
nel functions have also been defined on strings, graphs,
and other structured objects.
Let P be the set of probability distributions on X,
and X the random variable with distribution P ∈ P.
Following Smola et al. (2007), we define the mapping of
P ∈ P to RKHS F, µX := EX∼P[ϕ(X)], as the Hilbert
space embedding of P or simply mean map. For all
f ∈ F, EX∼P[f(X)] = hf, µXiF by the reproducing
property. A characteristic RKHS is one for which the
mean map is injective: that is, each distribution has a
unique embedding (Sriperumbudur et al., 2008). This
Hilbert Space Embeddings of Hidden Markov Models
property holds for many commonly used kernels (eg.
the Gaussian and Laplace kernels when X = Rd
).
As a special case of the mean map, the marginal proba-
bility vector of a discrete variable X is a Hilbert space
embedding, i.e. (P(X = i))M
i=1 = µX. Here the ker-
nel is the delta function k(x, x0
) = I[x = x0
], and the
feature map is the 1-of-M representation for discrete
variables (see section 2.2).
Given m i.i.d. observations

xl m
l=1
, an estimate of the
mean map is straightforward: µ̂X := 1
m
Pm
l=1 ϕ(xl
) =
1
m Υ1m, where Υ := (ϕ(x1
), . . . , ϕ(xm
)) is a conceptual
arrangement of feature maps into columns. Further-
more, this estimate computes an approximation within
an error of Op(m−1/2
) (Smola et al., 2007).
3.2. Covariance operators
The covariance operator is a generalization of the co-
variance matrix. Given a joint distribution P(X, Y )
over two variables X on X and Y on Y1
, the uncen-
tered covariance operator CXY is (Baker, 1973)
CXY := EXY [ϕ(X) ⊗ φ(Y )], (13)
where ⊗ denotes tensor product. Alternatively, CXY
can simply be viewed as an embedding of joint dis-
tribution P(X, Y ) using joint feature map ψ(x, y) :=
ϕ(x)⊗φ(y) (in tensor product RKHS G ⊗F). For dis-
crete variables X and Y with delta kernels on both do-
mains, the covariance operator will coincide with the
joint probability table, i.e. (P(X = i, Y = j)M
i,j=1 =
CXY (also see section 2.2).
Given m pairs of i.i.d. observations

(xl
, yl
)
m
l=1
,
we denote by Υ = ϕ(x1
), . . . , ϕ(xm
)

and Φ =
φ(y1
), . . . , φ(ym
)

. Conceptually, the covariance op-
erator CXY can then be estimated as ˆ
CXY = 1
m ΥΦ>
.
This estimate also computes an approximation within
an error of Op(m−1/2
) (Smola et al., 2007).
3.3. Conditional embedding operators
By analogy with the embedding of marginal distribu-
tions, the conditional density P(Y |x) can also be rep-
resented as an RKHS element, µY |x := EY |x[φ(Y )].
We emphasize that µY |x now traces out a family of
embeddings in G, with each element corresponding to
a particular value of x. These conditional embeddings
can be defined via a conditional embedding operator
CY |X : F 7→ G (Song et al., 2009),
µY |x = CY |Xϕ(x) := CY XC−1
XXϕ(x). (14)
For discrete variables with delta kernels, condi-
tional embedding operators correspond exactly to
1
a kernel l(y, y0
) = hφ(y), φ(y0
)iG is define on Y with
associated RKHS G.
conditional probability tables (CPT), i.e. (P(Y =
i|X = j))M
i,j=1 = CY |X, and each individual condi-
tional embedding corresponds to one column of the
CPT, i.e. (P(Y = i|X = x))M
i=1 = µY |x.
Given m i.i.d. pairs

(xl
, yl
)
m
l=1
from P(X, Y ), the
conditional embedding operator can be estimated as
ˆ
CY |X = ΦΥ>
m (ΥΥ>
m + λI)−1
= Φ(K + λmI)−1
Υ>
(15)
where we have defined the kernel matrix K := Υ>
Υ
with (i, j)th entry k(xi, xj). The regularization pa-
rameter λ is to avoid overfitting. Song et al. (2009)
also showed µ̂Y |x − µY |x G
= Op(λ1/2
+ (λm)−1/2
).
3.4. Hilbert space observable representation
We will focus on the embedding µXt+1|x1:t
for the pre-
dictive density P(Xt+1|x1:t) of a HMM. Analogue to
the discrete case, we first express µXt+1|x1:t
as a set of
Hilbert space ‘observable operators’ Ax. Specifically,
let the kernels on the observations and hidden states be
k(x, x0
) = hϕ(x), ϕ(x0
)iF and l(h, h0
) = hφ(h), φ(h0
)iG
respectively. For rich RKHSs, we define a linear oper-
ator Ax : G 7→ G such that
Axφ(ht) = P(Xt = x|ht) EHt+1|ht
[φ(Ht+1)]. (16)
Then, by applying variable elimination, we have
µXt+1|x1:t
= EHt+1|x1:t
EXt+1|Ht+1
[ϕ(Xt+1)]
= CXt+1|Ht+1
EHt+1|x1:t
[φ(Ht+1)]
= CXt+1|Ht+1
Axt
EHt|x1:t−1
[φ(Ht)]
= CXt+1|Ht+1
Yt
τ=1
Axτ

µH1 . (17)
where we used the following recursive relation
EHt+1|x1:t
[φ(Ht+1)]
= EHt|x1:t−1

P(Xt = xt|Ht) EHt+1|Ht
[φ(Ht+1)]

= Axt
EHt|x1:t−1
[φ(Ht)] . (18)
If we let T := CXt|Ht
, O := CXt+1|Ht+1
and π := µH1
,
we obtain a form µXt+1|x1:t
= OAxt:1
π analogous to
the discrete case (Equation (2)). The key difference
is that Hilbert space representations are applicable to
general domains with kernels defined.
Similar to the discrete case, the operators Ax cannot
be directly estimated from the data since the hidden
states are not provided. Therefore we derive a repre-
sentation for µXt+1|x1:t
based only on observable quan-
tities (assuming stationarity of the distribution):
µ1 := EXt
[ϕ(Xt)] = µXt
(19)
C2,1 := EXt+1Xt
[ϕ(Xt+1) ⊗ ϕ(Xt)] = CXt+1Xt
(20)
C3,x,1 := EXt+2(Xt+1=x)Xt
[ϕ(Xt+2) ⊗ ϕ(Xt)]
= P(Xt+1 = x)C3,1|2ϕ(x). (21)
Hilbert Space Embeddings of Hidden Markov Models
where we have defined C3,1|2 := CXt+2Xt|Xt+1
. First, we
examine the relation between these observable quanti-
ties and the unobserved O, T and π:
µ1 = EHt EXt|Ht
[ϕ(Xt)] = CXt|Ht
EHt [φ(Ht)]
= Oπ (22)
C2,1 = EHt

EXt+1Ht+1|Ht
[ϕ(Xt+1)] ⊗ EXt|Ht
[ϕ(Xt)]

=CXt+1|Ht+1
CHt+1|Ht
CHtHt
C>
Xt|Ht
= OT CHtHt
O>
(23)
C3,x,1 = EHt

OAxT φ(Ht) ⊗ EXt|Ht
[ϕ(Xt)]

= OAxT CHtHt
O>
(24)
In (24), we plugged in the following expansion
EXt+2Ht+2Ht+1(Xt+1=x)|Ht
[ϕ(Xt+2)]
= EHt+1|Ht

P(x|Ht+1)EHt+2|Ht+1
EXt+2|Ht+2
[ϕ(Xt+2)]

= OAxT φ(Ht) (25)
Second, analogous to the discrete case, we perform a
‘thin’ SVD of the covariance operator C2,1, and take
its top N left singular vectors U, such that the oper-
ator U>
O is invertible. Some simple algebraic manip-
ulations establish the relation between observable and
unobservable quantities
β1 := U>
µ1 = (U>
O)π (26)
β∞ := C2,1(U>
C2,1)†
= O(U>
O)−1
(27)
Bx := (U>
C3,x,1)(U>
C2,1)†
= (UO)Ax(UO)−1
. (28)
With β1, β∞ and Bxt:1
, µXt+1|x1:t
can be expressed as
the multiplication of observable quantities
µXt+1|x1:t
= β∞Bxt:1
β1 (29)
In practice, C3,x,1 (in equation (24)) is difficult to es-
timate, since it requires partitioning the training sam-
ples according to Xt+1 = x. Intead, we use C3,1|2ϕ(x)
which does not require such partitioning, and is only a
fixed multiplicative scalar P(x) away from C3,x,1. We
define B̄x := (U>
(C3,1|2ϕ(x)))(U>
C2,1)†
, and we have
µXt+1|x1:t
∝ β∞B̄xt:1
β1.
We may want to predict i steps into future, i.e. obtain
embeddings µXt+i|xt:1
instead of µXt+1|xt:1
. This can
be achieved by defining an i-step covariance operator
Ci+1,1 := EXt+iXt [ϕ(Xt+i) ⊗ ϕ(Xt)] and replacing C2,1
in β∞ (equation (27)) by Ci+1,1. We then obtain the
embedding µXt+i|xt:1
∝ βi
∞B̄xt:1
β1 where we use βi
∞
to denote Ci+1,1(U>
C2,1)†
.
3.5. Kernel spectral algorithm for HMMs
Given a sample of m i.i.d. triplets

(xl
1, xl
2, xl
3)
m
l=1
from a HMM, the kernel spectral algorithm for HMMs
proceeds by first performing a ‘thin’ SVD of the
sample covariance ˆ
C2,1. Specifically, we denote fea-
ture matrices Υ = (ϕ(x1
1), . . . , ϕ(xm
1 )) and Φ =
Algorithm 1 Kernel Spectral Algorithm for HMMs
In: m i.i.d. triples

(xl
1, xl
2, xl
3)
m
l=1
, a sequence x1:t.
Out: µ̂Xt+1|xt:1
1: Denote feature matrices Υ = (ϕ(x1
1), . . . , ϕ(xm
1 )),
Φ = (ϕ(x1
2) . . . ϕ(xm
2 )) and Ψ = (ϕ(x1
3) . . . ϕ(xm
3 )).
2: Compute kernel matrices K = Υ>
Υ, L = Φ>
Φ,
G = Φ>
Υ and F = Φ>
Ψ.
3: Compute top N generalized eigenvectors αi using
LKLαi = ωiLαi (ωi ∈ R and αi ∈ Rm
).
4: Denote A = (α1, . . . , αN ), Ω = diag(ω1, . . . , ωN )
and D = diag (α>
1 Lα1)−1/2
, . . . , (α>
N LαN )−1/2

.
5: β̂1 = 1
m D>
A>
G1m
6: β̂∞ = ΦQ where Q = KLADΩ−1
7: B̂xτ
= P(xτ )
m D>
A>
F diag (L + λI)−1
Φ>
ϕ(xτ )

Q,
for τ = 1, . . . , t.
8: µ̂Xt+1|xt:1
= β̂∞B̂xt:1
β̂1
(ϕ(x1
2), . . . , ϕ(xm
2 )), and estimate ˆ
C2,1 = 1
m ΦΥ>
.
Then the left singular vector v = Φα (α ∈ Rm
) can be
estimated as follows
ΦΥ>
ΥΦ>
v = ωv ⇔ ΦKLα = ωΦα ⇔
LKLα = ωLα, (α ∈ Rm
, ω ∈ R) (30)
where K = Υ>
Υ and L = Φ>
Φ are the kernel matri-
ces, and α is the generalized eigenvector. After nor-
malization, we have v = 1
√
α>Lα
Φα. Then the U oper-
ator in equation (26), (27) and (28) is the column con-
catenation of the N top left singular vectors, i.e. Û =
(v1, . . . , vN ). If we let A := (α1, . . . , αN ) ∈ Rm×N
be
the column concatenation of the N top αi, and D :=
diag (α>
1 Lα1)−1/2
, . . . , (α>
N LαN )−1/2

∈ RN×N
, we
can concisely express Û = ΦAD.
Next we estimate µ̂1 = 1
m Υ1m, and according to (26)
β̂1 = 1
m D>
A>
Φ>
Υ1m. Similarly, according to (27)
β̂∞ = 1
m ΦΥ>
D>
A>
Φ> 1
m ΦΥ>
†
= ΦKLADΩ−1
,
where we have defined Ω := diag (ω1, . . . , ωN ), and
used the relation LKLA = LAΩ and A>
LA = D−2
.
Last denote Ψ = ϕ(x1
3), . . . , ϕ(xm
3 )

, then ˆ
C3,1|2(·) =
Ψ diag (L + λI)−1
Φ>
(·)

KLADΩ−1
in (21).
The kernel spectral algorithm for HMMs can be sum-
marized in Algorithm 1. Note that in the algorithm,
we assume that the marginal probability P(xτ ) (τ =
1 . . . t) is provided to the algorithm. In practice, this
quantity is never explicitly estimated. Therefore, the
algorithm returns β̂∞B̄xt:1
β̂1 which is just a constant
scaling away from µXt+1|xt:1
(note B̄x := Bx/P(x)).
3.6. Sample complexity
In this section, we analyze the sample complexity of
our kernel spectral algorithm for HMMs. In particu-
Hilbert Space Embeddings of Hidden Markov Models
lar, we want to investigate how the difference between
the estimated embedding µ̂Xt+1|x1:t
and its population
counterpart scales with respect to the number m of
training samples and the length t of the sequence x1:t
in the conditioning. We use Hilbert space distances as
our error measure and obtain the following result (the
proof follows the template of Hsu et al. (2009), and it
can be found in the appendix):
Theorem 1 Assume kϕ(x)kF ≤ 1, kφ(h)kG ≤ 1,
maxx kAxk2 ≤ 1. Then µXt+1|x1:t
− µ̂Xt+1|x1:t F
=
Op(t(λ1/2
+ (λm)−1/2
)).
We expect that Theorem 1 can be further improved.
Currently it suggests that given a sequence of length t,
in order to obtain an unbiased estimator of µXt+1|x1:t
,
we need to decrease λ with a schedule of Op(m−1/2
)
and obtain an overall convergence rate of Op(tm−1/4
).
Second, the assumption, maxx kAxk2 ≤ 1, im-
poses smoothness constrants on the likelihood function
P(x|Ht) for the theorem to hold. Finally, the current
bound depends on the length t of the conditioning se-
quence. Hsu et al. (2009) provide a result that is in-
dependent of t using the KL-divergence as the error
measure. For Hilbert space embeddings, it remains an
open question as to how to estimate the KL-divergence
and obtain a bound independent of t.
3.7. Predicting future observations
We have shown how to maintain the Hilbert space
embeddings µXt+1|x1:t
for the predictive distribution
P(Xt+1|x1:t). The goal here is to determine the most
probable future observations based on µXt+1|x1:t
. We
note that in general we cannot directly obtain the
probability of the future observation based on the em-
bedding presentation of the distribution.
However, for a Gaussian RBF kernel defined over a
compact subset of a real vector space, the embedding
µXt+1|x1:t
can be viewed as a nonparametric density
estimator after proper normalization. In particular,
let f be a constant function in the RKHS such that
hf, ϕ(Xt+1)iF = 1, then the normalization constant
Z can be estimated as Ẑ = f, µ̂Xt+1|x1:t F
. Since
µ̂Xt+1|x1:t
is represented as
Pm
l=1 γiϕ(xl
3), Ẑ is simply
Pm
l=1 γi. We can then find the maximum a posteri
(MAP) future observation by
x̂t+1 = argmaxxt+1
µ̂Xt+1|x1:t
, ϕ(xt+1) F
/Ẑ (31)
Since kϕ(x)kF = 1 for a Gaussian RBF kernel, a geo-
metric interpretation of the above MAP estimate is
to find a delta distribution δxt+1
such that its em-
bedding ϕ(xt+1) is closest to µ̂Xt+1|x1:t
, i.e. x̂t+1 =
argminxt+1
kϕ(xt+1) − µ̂Xt+1|x1:t
kF . The optimization
in (31) may be a hard problem in general. In some
cases, however, it is possible to define the feature map
xi + j i +1 xi xi −1 i −k
. . . xi +1 xi x −1
xi + j xi xi −1 i −k
. . . xi xi −1
−1
C3,x,1
C2,1
Cfuture,x,past
Cfuture,past
i
... x
... x
... x
...
Figure 1. Operators Cfuture,past and Cfuture,x,past capture
the dependence of sequences of k past and j future obser-
vations instead of single past and future observations.
ϕ(x) in such a way that an efficient algorithm for solv-
ing the optimization can be obtained, e.g. Cortes et al.
(2005). In practice, we can also decode x̂t+1 by choos-
ing the best one from existing training examples.
3.8. Learning with sequences of observations
In the learning algorithm formulated above, each vari-
able Xt corresponds to a single observation xt from a
data sequence. In this case, the operator C2,1 only cap-
tures the dependence between a single past observation
and a single future observation (similarly for C3,x,1).
In system identification theory, this corresponds to
assuming 1-step observability (Van Overschee & De
Moor, 1996) which is unduly restrictive for many par-
tially observable real-world dynamical systems of in-
terest. More complex sufficient statistics of past and
future may need to be modeled, such as the block Han-
kel matrix formulations for subspace methods (Van
Overschee & De Moor, 1996), to identify linear systems
that are not 1-step observable. To overcome this lim-
itation one can consider sequences of observations in
the past and future and estimate operators Cfuture,past
and Cfuture,x,past accordingly (Figure 1). As long as
past and future sequences never overlap, these ma-
trices have rank equal to that of the dynamics model
and the theoretical properties of the learning algorithm
continue to hold (see (Siddiqi et al., 2009) for details).
4. Experimental Results
We designed 3 sets of experiments to evaluate the effec-
tiveness of learning embedded HMMs for difficult real-
world filtering and prediction tasks. In each case we
compare the learned embedded HMM to several alter-
native time series models including (I) linear dynami-
cal systems (LDS) learned by Subspace Identification
(Subspace ID) (Van Overschee & De Moor, 1996) with
stability constraints (Siddiqi et al., 2008), (II) discrete
HMMs learned by EM, and (III) the Reduced-rank
HMM (RR-HMM) learned by spectral methods (Sid-
diqi et al., 2009). In these experiments we demonstrate
that the kernel spectral learning algorithm for embed-
ded HMMs achieves the state-of-the-art performance.
Robot Vision. In this experiment, a video of 2000
frames was collected at 6 Hz from a Point Grey Bum-
blebee2 stereo camera mounted on a Botrics Obot d100
mobile robot platform circling a stationary obstacle
Hilbert Space Embeddings of Hidden Markov Models
A. Example Images
Environment
Path
B.
0 10 20 30 40 50 60 70 80 90 100
3
4
5
6
7
8
x 106
Prediction Horizon
Avg.
Prediction
Err.
RR-HMM
LDS
HMM
Mean
Last
2
Embedded
9
1
Figure 2. Robot vision data. (A) Sample images from the
robot’s camera. The figure below depicts the hallway envi-
ronment with a central obstacle (black) and the path that
the robot took through the environment (the red counter-
clockwise ellipse). (B) Squared error for prediction with
different estimated models and baselines.
(under imperfect human control) (Figure 2(A)) and
1500 frames were used as training data for each model.
Each frame from the training data was reduced to 100
dimensions via SVD on single observations. The goal
of this experiment was to learn a model of the noisy
video, and, after filtering, to predict future image ob-
servations.
We trained a 50-dimensional2
embedded HMM us-
ing Algorithm 1 with sequences of 20 consecutive ob-
servations (Section 3.8). Gaussian RBF kernels are
used and the bandwidth parameter is set with the
median of squared distance between training points
(median trick). The regularization parameter λ is set
of 10−4
. For comparison, a 50-dimensional RR-HMM
with Parzen windows is also learned with sequences of
20 observations (Siddiqi et al., 2009); a 50-dimensional
LDS is learned using Subspace ID with Hankel matri-
ces of 20 time steps; and finally a 50-state discrete
HMM and axis-aligned Gaussian observation models
is learned using EM algorithm run until convergence.
For each model, we performed filtering3
for different
extents t1 = 100, 101, . . . , 250, then predicted an im-
age which was a further t2 steps in the future, for
t2 = 1, 2..., 100. The squared error of this prediction
in pixel space was recorded, and averaged over all the
different filtering extents t1 to obtain means which are
plotted in Figure 2(B). As baselines, we also plot the
error obtained by using the mean of filtered data as a
predictor (Mean), and the error obtained by using the
last filtered observation (Last).
Any of the more complex algorithms perform better
than the baselines (though as expected, the ‘Last’ pre-
dictor is a good one-step predictor), indicating that
this is a nontrivial prediction problem. The embedded
HMM learned by the kernel spectral algorithm yields
significantly lower prediction error compared to each of
the alternatives (including the recently published RR-
2
Set N = 50 in Algorithm 1.
3
Update models online with incoming observations.
A. B.
0 10 20 30 40 50 60 70 80 90 100
3
4
5
6
7
8
x 106
Prediction Horizon
Avg.
Prediction
Err.
2
1
IMU
Slot
Car
0
Racetrack
RR-HMM
LDS
HMM
Mean
Last
Embedded
Figure 3. Slot car inertial measurement data. (A) The slot
car platform and the IMU (top) and the racetrack (bot-
tom). (B) Squared error for prediction with different esti-
mated models and baselines.
HMM) consistently for the duration of the prediction
horizon (100 timesteps, i.e. 16 seconds).
Slot Car Inertial Measurement. In a second ex-
periment, the setup consisted of a track and a minia-
ture car (1:32 scale model) guided by a slot cut into
the track. Figure 3(A) shows the car and the attached
IMU (an Intel Inertiadot) in the upper panel, and
the 14m track which contains elevation changes and
banked curves. At each time step we extracted the es-
timated 3-D acceleration of the car and the estimated
difference between the 3-D orientation of the car from
the previous time step at a rate of 10Hz. We collected
3000 successive measurements of this data while the
slot car circled the track controlled by a constant pol-
icy. The goal was to learn a model of the noisy IMU
data, and, after filtering, to predict future readings.
We trained a 20-dimensional embedded HMM using
Algorithm 1 with sequences of 150 consecutive obser-
vations (Section 3.8). The bandwidth parameter of
the Gaussian RBF kernels is set with ‘median trick’.
The regularization parameter λ is 10−4
. For compari-
son, a 20-dimensional RR-HMM with Parzen windows
is learned also with sequences of 150 observations; a
20-dimensional LDS is learned using Subspace ID with
Hankel matrices of 150 time steps; and finally, a 20-
state discrete HMM (with 400 level of discretization
for observations) is learned using EM algorithm.
For each model, we performed filtering for different
extents t1 = 100, 101, . . . , 250, then predicted an im-
age which was a further t2 steps in the future, for
t2 = 1, 2..., 100. The squared error of this prediction
in the IMU’s measurement space was recorded, and
averaged over all the different filtering extents t1 to
obtain means which are plotted in Figure 3(B). Again
the embedded HMM yields lower prediction error com-
pared to each of the alternatives consistently for the
duration of the prediction horizon.
Audio Event Classification. Our final experiment
concerns an audio classification task. The data, re-
cently presented in Ramos et al. (2010), consisted of
sequences of 13-dimensional Mel-Frequency Cepstral
Hilbert Space Embeddings of Hidden Markov Models
Coefficients (MFCC) obtained from short clips of raw
audio data recorded using a portable sensor device.
Six classes of labeled audio clips were present in the
data, one being Human speech. For this experiment
we grouped the latter five classes into a single class
of Non-human sounds to formulate a binary Human
vs. Non-human classification task. Since the original
data had a disproportionately large amount of Human
Speech samples, this grouping resulted in a more bal-
anced dataset with 40 minutes 11 seconds of Human
and 28 minutes 43 seconds of Non-human audio data.
To reduce noise and training time we averaged the data
every 100 timesteps (equivalent to 1 second).
For each of the two classes, we trained embedded
HMMs with 10, 20, . . . , 50 latent dimensions using
spectral learning and Gaussian RBF kernels with
bandwidth set with the ‘median trick’. The regular-
ization parameter λ is 10−1
. For comparison, regular
HMMs with axis-aligned Gaussian observation models,
LDSs and RR-HMMs were trained using multi-restart
EM (to avoid local minima), stable Subspace ID and
the spectral algorithm of (Siddiqi et al., 2009) respec-
tively, also with 10, . . . , 50 latent dimensions.
For RR-HMMs, regular HMMs and LDSs, the class-
conditional data sequence likelihood is the scoring
function for classification. For embedded HMMs, the
scoring function for a test sequence x1:t is the log of
the product of the compatibility scores for each obser-
vation, i.e.
Pt
τ=1 log ϕ(xτ ), µ̂Xτ |x1:τ−1 F

.
For each model size, we performed 50 random 2:1
partitions of data from each class and used the re-
sulting datasets for training and testing respectively.
The mean accuracy and 95% confidence intervals over
these 50 randomizations are reported in Figure 4. The
graph indicates that embedded HMMs have higher ac-
curacy and lower variance than other standard alter-
natives at every model size. Though other learning
algorithms for HMMs and LDSs exist, our experiment
shows this to be a non-trivial sequence classification
problem where embedded HMMs significantly outper-
form commonly used sequential models trained using
typical learning and model selection methods.
5. Conclusion
We proposed a Hilbert space embedding of HMMs
that extends traditional HMMs to structured and non-
Gaussian continuous observation distributions. The
essence of this new approach is to represent distribu-
tions as elements in Hilbert spaces, and update these
elements entirely in the Hilbert spaces using opera-
tors. This allows us to derive a local-minimum-free
Latent Space Dimensionality
Accuracy
(%)
HMM
LDS
RR−HMM
Embedded
10 20 30 40 50
60
70
80
90
85
75
65
Figure 4. Accuracies and 95% confidence intervals for Hu-
man vs. Non-human audio event classification, comparing
embedded HMMs to other common sequential models at
different latent state space sizes.
kernel spectral algorithm for learning the embedded
HMMs, which exceeds previous state-of-the-art in real
world challenging problems. We believe that this new
way of combining kernel methods and graphical mod-
els can potentially solve many other difficult problems
in graphical models and advance kernel methods to
more structured territory.
Acknowledgement
LS is supported by a Ray and Stephenie Lane fellow-
ship. SMS was supported by the NSF under grant number
0000164, by the USAF under grant number FA8650-05-
C-7264, by the USDA under grant number 4400161514,
and by a project with MobileFusion/TTC. BB was sup-
ported by the NSF under grant number EEEC-0540865.
BB and GJG were supported by ONR MURI grant num-
ber N00014-09-1-1052.
References
Baker, C. (1973). Joint measures and cross-covariance op-
erators. Trans. A.M.S., 186, 273–289.
Cortes, C., Mohri, M., & Weston, J. (2005). A general re-
gression techinque for learning transductions. In ICML.
Hsu, D., Kakade, S., & Zhang, T. (2009). A spectral algo-
rithm for learning hidden markov models. In COLT.
Jaeger, H. (2000). Observable operator models for discrete
stochastic time series. Neural Computation, 12(6), 1371–
1398.
Ramos, J., Siddiqi, S., Dubrawski, A., Gordon, G., &
Sharma, A. (2010). Automatic state discovery for un-
structured audio scene classification. In ICASSP.
Siddiqi, S., Boots, B., & Gordon, G. (2008). A constraint
generation approach to learning stable linear dynamical
systems. In NIPS.
Siddiqi, S., Boots, B., & Gordon, G. (2009). Reduced-rank
hidden markov models. http://arxiv.org/abs/0910.0902.
Smola, A., Gretton, A., Song, L., & Schölkopf, B. (2007).
A Hilbert space embedding for distributions. In ALT.
Song, L., Huang, J., Smola, A., & Fukumizu, K. (2009).
Hilbert space embeddings of conditional distributions.
In ICML.
Sriperumbudur, B., Gretton, A., Fukumizu, K., Lanckriet,
G., & Schölkopf, B. (2008). Injective Hilbert space em-
beddings of probability measures. In COLT.
Van Overschee, P., & De Moor, B. (1996). Subspace iden-
tification for linear systems: Theory, implementation,
applications. Kluwer.
