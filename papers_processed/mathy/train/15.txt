DirichletProcessMixturesofGeneralizedMallowsModelsMarinaMeilăDepartmentofStatisticsUniversityofWashingtonSeattle,WA98195-4322mmp@stat.washington.eduHarrChenComputerScienceandArtificialIntelligenceLaboratoryMassachusettsInstituteofTechnologyCambridge,MA02139-4307harr@csail.mit.eduAbstractWepresentaDirichletprocessmixturemodeloverdiscreteincompleterankingsandstudytwoGibbssamplinginferencetechniquesforestimatingposteriorclusterings.Thefirstap-proachusesaslicesamplingsubcomponentforestimatingclusterparameters.Thesec-ondapproachmarginalizesoutseveralclusterparametersbytakingadvantageofapprox-imationsVjcanbesetindependentlyinspecifyingaπ,whichisnottrueofπ(l)values.TheseVj’sarecalledthecodeofπ.Thiscodecanbedefinedw.r.t.toanyreferenceper-mutationσbywhereNjisthenumberofdataelementsoflengthatleastj.Thepriorspresentedherearedefineduptoanormalizationconstant.Ingeneral,thereisnoclosed-formexpressionforthisconstant.Insummary,theGMisanexponentialfamilymodelwithsimplesufficientstatistics.Becausethecentralpermutationisanexplicitparameter,thismodelisbothmoreinterpretableandtractablethanother(ex-ponentialfamily)modelsoverpermutations.WeuseitasabuildingblockfortheDirichletprocessmixturemodel,whichwebrieflyreviewbelow.3DIRICHLETPROCESSMIXTUREMODELSADirichletprocessmixture[DPM;Antoniak,1974]isagenerativeclusteringmodel.Generatingdataπ1:NfromaDPMofGMsinvolvesthesesteps:G∼DP(α,P0(σ,~θ|ν,r)),σi,~θi∼G,πi∼GM(π|σi,~θi).First,adiscretedistributionGoverGMdistributionsissampledfromtheDirichletprocessprior.Thispriortakesasaparameteradistributionoverσand~θ,inourcasetheconjugatepriorP0.Next,aspecificGMdis-tributionwithparametersσi,~θiisdrawnfromG.DatapointπiisfinallysampledfromthisGMdistribution.Ifwesampledatasequentiallyfromthismodel,thenthe(N+1)thsamplewillbedistributedaccordingtoσN+1,~θN+1∼1N+αXi≤Nδσi,~θi+αN+αP0ν,r.(9)Hence,anyfinitesamplewillbeafinitemixtureofGMs,allowingtheDPMtorepresentrankingdatathatismultimodal,withpermutationsclusteredaroundseveralcenters.AnothercharacterizationoftheDPMisthateachdatapointπiisassociatedwithaclusterlabelci∈1,...,C,andeachclustercwithasetofGMparametersσcand~θc.Unlikeafinitemixture,thenumberofclustersintheDPMisitselfarandomvariable.Itwillgrowwiththesizeofthedatainawaycontrolledbytheconcen-trationparameterα.ThismakesDPMmodelsidealforscenarioswherethenumberofmixturecomponentsisnotwell-definedinadvance.DPMshavefoundex-tensivepracticalapplicationsinareassuchastopicmodeling[Tehetal.,2006],naturallanguageprocess-ing[Liangetal.,2007],vision[Sudderthetal.,2005],andcomputationalbiology[Rasmussenetal.,2009].BayesianinferenceintheDPMmodelistypicallycon-ductedviaMCMC[Neal,2000]orvariationalmeth-AlgorithmSlice-GibbsInputParametersν,α,t,r1:t,T,TGibbs,TSlices,Dataπ1:Noflengthst1:NOutputSamplesc1:N,σc,~θcInitializec1:N,σc,~θcrandomlyRepeatTtimes1.ResampleclusterassignmentsForallpointsπisampleciaccordingtoP[ci=c]∼(N−i,cN+α−1GM(πi|σc,~θc)ifN−i,c6=0αN+α−1(n−ti)!n!ifN−i,c=0IfN−i,c=0forthesampledcluster,sampleanewσc|πiandρc|πiaccordingtoStep2below2.ResampleclustercentersForallclustersc,repeatTGibbstimes(a)SampleσcbySample-σ-Stagewise(b)Sample~θcbySample-θ-SliceFigure1:Slice-GibbsalgorithmforestimatingaDPMofGMs.ods[BleiandJordan,2006].Wefocusonthefor-merapproach,wherethegoalistoproducesam-plesdrawnfromtheappropriateposteriordistribu-tion.Inparticular,ifweareinterestedinparame-terestimation,ourobjectiveistodrawsamplesfromP(c1:N,σ1:C,~θ1:C|α,ν,r,π1:N),whereciistheclus-terassignmentofdatapointπi,andeachclusterchasGMparameters(σc,~θc).Whilepreviouswork[Neal,2000]hasmadeitstraight-forwardtowritetheexpressionofthisposterior(seethefollowingsections),ourmainchallengeisinmak-inginferencepractical.DesigningsuchmethodsandmakingthemefficientfornontrivialmodelsizesnandsamplesizesNisthemaincontributionofthispaper.4THESLICE-GIBBSSAMPLERWefirstpresentanaı̈veGibbssamplerforestimatingaDPMofGMs,followingtheapproachofNeal[2000].OurmaingoalistobuildaGibbsMarkovchainoverclusterassignmentsc1:NwhosestationaryAlgorithmSample-σ-StagewiseInputParameters~θ,sufficientstatisticsR1:t,priorparameterν,optionalpriorparametersR01:tOutputSampleσ1.CalculatematrixR=Ptj=1θj(Rj+νR0j),orR=Ptj=1θjRjifpriorforσisuninformative2.Forj=1:nandwhileR6=0(a)Calculatecolumnsumsρ1:nofR(b)Sampleσ−1(j)=iw.p.∝e−ρi(c)Setrowandcolumnσ−1(j)ofRtozero3.FillinremainingranksofσuniformlyatrandomwithitemsnotyetselectedFigure2:Sample-σ-Stagewisealgorithmforexactlysamplingσfromtheconjugateposteriorgiven~θ.areknown,thisyieldsthefollowingresamplingupdatefortheclusterassignmentofdatapointπi:P(ci=c|c−i,σ,~θ)(11)∝N−i,cN+α−1GM(πi|σc,~θc)ifN−i,c6=0,αN+α−1RGM(πi|σ,~θ)P0(σ,~θ|ν,r)dσd~θifN−i,c=0.Here,N−i,cisthenumberofelementsinclusterc,excludingdatapointi.InmanyapplicationsoftheDPMitispossibletointegrateoverclusterparametersandexplicitlysampleonlyclusterassignments(knownascollapsedsampling).InthecaseoftheGM,despiteouruseofaconjugatepriorthemarginalizationoverσand~θisanalyticallyintractable,inpartbecauseoftheunknownnormalizationterm.ThusforourfirstsamplerweresorttobuildingaMarkovchainoverthestatespace(c1:N,σC,~θC),whereeachvariableisex-plicitlyresampledconditionedontheothervariables.ThealgorithmispresentedinFigure1,whileitsstepsarediscussedindetailbelow.Tosampleci|σ,~θasin(11)weneedtocalculatetheprobabilitiesontherighthandside.Thisisstraight-forwardforN−i,c>0,using(2).ForN−i,c=0weusethefollowingLemma(seeAppendixforproofs).Lemma1Themarginalprobabilityofasingleobser-vationisP(πi|ν,r)=(n−ti)!n!.Nextweneedσc|~θc,πi∈c.LetRj=Rj(πi∈c)bethesufficientstatisticsofclusterc,andSj(σc)=Lσc(Rj).ThesestatisticsareinputtothealgorithmdescribedbyLemma2.AlgorithmSample-θ-SliceInputParametersν,t,r1:t,TSlices,statisticsS1:t(σ)OutputSamplesθ1:tInitializeθ1:taccordingtoprevioussampleForj=1:t,repeatTSlicestimes1.Sampleu∼Uniform(0,P̃(θj)),whereP̃(θj)=e−(νrj+Sj(σ))θj−(ν+Nj)lnψn−j(θj)(10)2.Determineslice[a,b]usingstep-outprocedure3.Repeatedlysampleθj∼Uniform(a,b)untilu<P̃(θj),shrinking[a,b]withrejectedsamplesFigure3:Sample-θ-Slicealgorithmforslicesam-pling~θgivenσ.Lemma2P(σ|~θ,ν,r,π1:N)canbesampledexactlybyAlgorithmSample-σ-Stagewise(Figure2).Samplingfrom~θc|σc,πi∈cismorechallenging.Themainobstacletostraightforwardsamplingistheun-knownnormalizationfactorofthisdistribution.How-ever,theposteriorofeachθ1:tisindependentanduni-modal.1Thissuggeststhatslicesampling[Neal,2003]isaviablewayofdrawingvaluesfor~θc.Lemma3P(~θ|σ,ν,r,πi∈c)canbesampledusingAl-gorithmSample-θ-Slice(Figure3).ThestructureofAlgorithmSample-θ-SlicefollowsdirectlyfromNeal[2000].ThefullSlice-Gibbssam-pler,sonamedforitsinclusionofaslicesampler,ispresentedinFigure1.Italternatesbetweenresam-plingclusterassignmentsciofdatapointsandclusterparametersσcand~θc.BecausetheclusterparametersthemselvesformaGibbschain,wetakeTGibbsstepstoensureconvergence;furthermore,theslicesamplertakesTSlicesstepsforeachθjduetoitsserialcorrela-tion.InourexperimentswefindthatTGibbs=10andTSlices=3aretypicallysufficientvalues.5THEBETA-GIBBSSAMPLERTheprevioussectionhasdemonstratedthedifficultyofsamplingAlgorithmSample-σ-N1InputTop-trankingπ,priorparametersrj,νOutputSampleσ1.Forj=1:t(a)SampleVj=kw.p.∝Beta(νrj+k,ν+2)fork=0:n−j(b)Placeπ(j)atthe(Vj+1)thpreviouslyunas-signedpositionofσ2.FilltheremainingranksofσuniformlyatrandomwithitemsnotinπFigure4:Sample-σ-N1algorithmforapproximatelysamplingσfromtheconjugateposteriorwhenN=1.andmarginalizationswillbedoneinclosedform.ThekeyinsightisthattheinfinitegeneralizedMallowsmodel[MeilăandBao,2008]canbeusedtoapproxi-matesomeofthesamplingdistributions.Thefirstresultarisesfromthefactthatasn→∞thenormalizationconstantψjapproachesthevalueψ∞(θ)=11−eθ.Thisformofthenormalizationcon-stantpermitsseveralcomputationsinclosedform.Lemma4[MeilăandBao,2008]Ifthenumberofitemsnisinfiniteandcountablethen:P(θj|σ,ν,r,π1:N)=Beta(e−θj;νrj+Sj(σ),ν+Nj+1),(12)P(σ|ν,r,π1:N)∝Qtj=1Beta(νrj+Sj(σ),ν+Nj+1).(13)Intheabove,(12)usestheBetadistribution,and(13)usestheBetafunction;NjisthenumberofrankingsoflengthatleastjandSj(σ)isagainLσ(Rj(π1:N)).Forthefinitecase,wedefineananaloguetotheBetafunctionthatarisesinthemarginalizationof~θ:B̃eta(a,b,n)≡Z∞0e−θa1−e−(n+1)θ1−e−θ−b+1dθ.(14)Usingthisrepresentationitcanbeeasilyverifiedthatforfiniten,P(σ|ν,r,π1:N)∝Qtj=1B̃eta(νrj+Sj(σ),ν+Nj+1,n−j).(15)Notethatasn→∞,B̃eta(a,b,n)→Beta(a,b),whichwillformthecoreofourapproximation.WecannowshowthefollowingLemmas(seeAppendixforproofs).AlgorithmBeta-GibbsInputParametersν,α,t,r1:t,T,TGibbs,TSlices,Dataπ1:Noflengthst1:NOutputSamplesc1:N,σc,~θcInitializec1:N,σc,~θcrandomlyRepeatTtimes1.ResampleclusterassignmentsForallpointsπisampleciaccordingtoP[ci=c]∼N−i,cN+α−1Qtj=1Beta(sj(πi|σc)+νrj+Sj(σc),ν+Nc,j+2)Beta(νrj+Sj(σc),ν+Nc,j+1)ifN−i,c6=0αN+α−1(n−ti)!n!ifN−i,c=0IfN−i,c=0forthesampledcluster,sampleanewσc|πibySample-σ-N12.ResampleclustercentersForallclustersc,ifNc>1repeatTGibbstimes(a)Sampleσc|~θcbySample-σ-Stagewise(b)Sample~θc|σcusing(12)fromLemma4IfNc=1sampleσcbySample-σ-N1Figure5:Beta-GibbsalgorithmforestimatingaDPMofGMs.Lemma5nXs=0B̃eta(s+a,N+1,n)=B̃eta(a,N,n).(16)Thisresultalsoholdsasn→∞.Lemma6Marginalizingover~θforasingleπyields:P(π|σ,ν,r,πi∈c)=(17)tYj=1B̃eta(sj(π|σ)+νrj+Sj(σ),ν+Nj+2,n−j)B̃eta(νrj+Sj(σ),ν+Nj+1,n−j).Lemma7P(σ|ν,r,π)(i.e.,whenN=1)canbesampledapproximatelybyAlgorithmSample-σ-N1(Figure4).Lemmas6and7,togetherwithLemmas1and2,allowusto(approximately)marginalizeoutthecontinuous~θparametersformuchofthesampling.Thisalgorithm,calledBeta-Gibbsbecauseof05010015020025010−410−2100iterationsVIdistance05010015020025010−410−2100iterationsVIdistance05010015020025010−410−2100iterationsVIdistance05010015020025010−410−2100iterationsVIdistanceBeta−GibbsSlice−Gibbs0000Dataset1Dataset3Dataset2Dataset4Figure6:PerformanceofSlice-GibbsandBeta-Gibbsonfourartificialdatasets,averagedovertenreplicates.EachplotdisplaysVIdistancetothetruedatalabeling.Lowerisbetter.N−i,c>0,andP(σc|π)whenNc=1.Fortheresamplingofσcfornon-singletonclusters,weresorttoaninnerGibbssampler,settingTGibbsto10aswithSlice-Gibbs.6EMPIRICALCOMPARISONOFSLICE-GIBBSANDBETA-GIBBSThepurposeofintroducingtheBeta-Gibbssamplerwas(1)tomaketheresamplingoftheparametersmoreefficient,and(2)moreimportantly,toreducevarianceandaccelerateconvergencetothestationarydistribu-tion,whichisatypicaleffectofmarginalizingovercer-tainparameters.Wenowverifyhowwellwesucceededbyrunningexperimentsonfourartificialdatasetsun-dervaryingconditions.Foreachexperiment,wegen-erate500pointsfromeachof10clustersforatotalofN=5000samples.Eachcluster’spointsaregeneratedfromaGMwithtrueσ∗and~θ∗givenbelow.Toensurethatthedatasetisnottooeasilyseparable,eachσ∗isdrawnfromtheconjugateposteriorofσ,conditionedon100permutationsdrawnrandomlyfromaGMwith~θ=0.7.Datasetnt~θ∗12010θ∗i=122019θ∗i=132010θ∗i=1.5−(i−1)×0.142019θ∗i=1.5−(i−1)×0.05WemeasuretheVariationofInformation(VI)dis-tance[Meilă,2007]betweenthesampledandtrueclus-teringsateachiteration.WeaverageovertenrunsfortheoreticallyprincipledistheKernelDensityEstima-tor(KDE)ofLebanonandMao[2008],inwhichthekernelistheGMmodelwithθj=θ,andwherethedataispartialrankingsofagiventype.OneofRankCluster1(8.1%)Cluster2(6.2%)Cluster3(6.0%)Cluster4(5.8%)Cluster5(5.7%)1Business(Dublin)CompAppl(Dublin)Business(Limerick)Engineering(Dublin)CompAppl(Dublin)2Commerce(Dublin)CompSys(Limerick)Commerce(Galway)Engineering(Galway)Engineering1234567891000.511.522.53rankθjAllClustersSmallClustersLargeClustersFigure11:Averageofθjweightedbyclustersizeasafunctionofrankjforthecollegeapplicationdata,replicatedacrossfourruns.Thedecreasingtrendsup-portstheintuitionthattopcoursepreferencesarebet-terseparatedthanless-desiredchoices.advantageofusingtheGMformodelingclusterscom-paredtothePlackett-Lucemodel.Wecomputeanaverageofeachθj,weightedbyclustersize,acrossthefourruns.Wealsoperformthisanalysisforonlylargeandonlysmallclusters,thresholdingatasizeof5%ofthedatapoints(splittingthedatapointsroughlyequallyintolargeandsmall).Figure11presentstheseaverages.Theclearlydecreasingoveralltrendrein-forcestheintuitionthattop-rankedchoicestendtobemorecoherentanddistinctivethanlaterentriesinthetop-10ranking.Furthermore,wefindthatsmallclus-terstendtodivergelessattopranksthanlargeclus-ters,butthistrendreversesaroundthefourthrank.Aqualitativeexaminationofthedatasuggeststhatthismaybebecausesmallclusterstendtocorrespondtomorespecializedinterestswithfewerrelevantcourses(e.g.,coursesatonespecificsmallerschool),leavingfewerchoicesforthetopranksbutallowingforgreaterdivergencelateron.49DISCUSSIONWeintroducednonparametricBayesianDPMsonrankeddatadomainswithtop-trankingsofvariablelengths.Ourinferencealgorithmsareabletorunonsubstantialdatasetsizesandlargen’s.Weleveragedacombinationofstatisticalandcompu-tationalinsightsindevelopingourtechniques.Statisti-4Infact,theaveragerankinglengthtfordatapointsinsmallclustersisshorterthanforlargeclusters:6.15comparedto6.63.cally,therichcombinatorialstructureoftheparameterspaceallowedThelastequalityisobtainedfromLemma5.Hence,Z1(π)=(n−t)!/n!.ProofofLemma2FromMeilăandBao[2008],foranygivenθ,P(σ|θ,π1:N,ν,r)∝e−Ptj=1[θj(Lσ(Rj(π1:N))+νrj)+(N+ν)lnψn−j(θj)]∝e−Ptj=1θjLσ(Rj(π1:n))=e−Lσ(R).WeuseakeyobservationofMeilăetal.[2007],whichisthatforadistributionoverpermutationsliketheoneabove,thefirstrank