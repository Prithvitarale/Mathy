HowtoEscapeSaddlePointsEfficientlyChiJin1RongGe2PraneethNetrapalli3ShamM.Kakade4MichaelI.Jordan1AbstractThispapershowsthataperturbedformofgradi-entdescentconvergestoasecond-orderstation-arypointinanumberiterationswhichdependsonlypoly-logarithmicallyondimension(i.e.,itisalmost“dimension-free”).Theconvergencerateofthisprocedurematchesthewell-knowncon-vergencerateofgradientdescenttofirst-orderstationarypoints,uptologfactors.Whenallsad-dlepointsarenon-degenerate,allsecond-orderstationarypointsarelocalminima,andourresultthusshowsthatperturbedgradientdescentcanescapesaddlepointsalmostforfree.Ourresultscanbedirectlyappliedtomanymachinelearningapplications,includingdeeplearning.Asapar-ticularconcreteexampleofsuchanapplication,weshowthatourresultscanbeuseddirectlytoestablishsharpglobalconvergenceratesforma-trixfactorization.Ourresultsrelyonanovelcharacterizationofthegeometryaroundsaddlepoints,whichmaybeofindependentinteresttothenon-convexoptimizationcommunity.1.HowtoEscapeSaddlePointsEfficientlyAlgorithm1PerturbedGradientDescent(Meta-algorithm)fort=0,1,...doifperturbationconditionholdsthenxt←xt+ξt,ξtuniformly∼B0(r)xt+1←xt−η∇f(xt)noise.However,thisresultdoesnotboundthenumberofstepsneededtoreachalocalminimum.Previousworkexplainswhygradientdescentavoidssad-dlepointsinthenonconvexsetting,butnotwhyitiseffi-cient—allofthemhaveruntimeguaranteeswithhighpoly-nomialdependencyindimensiond.Forinstance,thenum-berofiterationsrequiredinGeetal.(2015)isatleastΩ(d4),whichisprohibitiveinhighdimensionalsettingsuchasdeeplearning(typicallywithmillionsofparame-ters).Therefore,wewonderwhethergradientdescenttypeofalgorithmsarefundamentallyslowinescapingsaddlepoints,oritisthelackofourtheoreticalunderstandingwhilegradientdescentisindeedefficient.Thismotivatesthefollowingquestion:Cangradientdescentescapesad-dlepointsandconvergetolocalminimainanumberofiterationsthatis(almost)dimension-free?Inordertoanswerthisquestionformally,thispaperinves-tigatesthecomplexityoffinding-second-orderstationarypoints.Forρ-HessianLipschitzfunctions(seeDefinition5),thesepointsaredefinedas(Nesterov&Polyak,2006):k∇f(x)k≤,andλmin(∇2f(x))≥−√ρ.Undertheassumptionthatallsaddlepointsarestrict(i.e.,foranysaddlepointxs,λmin(∇2f(xs))<0),allsecond-orderstationarypoints(=0)arelocalminima.Therefore,convergencetosecond-orderstationarypointsisequivalenttoconvergencetolocalminima.Thispaperstudiesasimplevariantofgradientdescent(withphasicperturbations,seeAlgorithm1).For`-smoothfunctionsthatarealsoHessianLipschitz,weshowthatper-turbedgradientdescentwillconvergetoan-second-orderstationarypointinÕ(`(f(x0)−f?)/2),whereÕ(·)hidespolylogfactors.Thisguaranteeisalmostdimensionfree(uptopolylog(d)factors),answeringtheabovehighlightedHowtoEscapeSaddlePointsEfficientlyTable1.Oraclemodelsanditerationcomplexityforconvergencetosecond-orderstationarypointAlgorithmIterationsOracleGeetal.(2015)O(poly(d/))GradientLevy(2016)O(d3poly(1/))GradientThisWorkO(log4(d)/2)GradientAgarwaletal.(2016)O(log(d)/7/4)Hessian-vectorCarmonetal.(2016)O(log(d)/7/4)Hessian-vectorCarmon&Duchi(2016)O(log(d)/2)Hessian-vectorNesterov&Polyak(2006)O(1/1.5)HessianCurtisetal.(2014)O(1/1.5)Hessiangivestrongguarantees,theanalysesareheavilytailoredtospecificproblems,anditisunclearhowtogeneralizethemtoawiderclassofnon-convexfunctions.Forgeneralnon-convexoptimization,thereareafewpre-viousresultsonfindingsecond-orderstationarypoints.TheseresultscanbedividedintothefollowingHowtoEscapeSaddlePointsEfficiently2.2.GradientDescentThetheoryofgradientdescentoftentakesitspointofde-parturetobethestudyofconvexoptimization.Definition1.Adifferentiablefunctionf(·)is`-smooth(or`-gradientLipschitz)if:∀x1,x2,k∇f(x1)−∇f(x2)k≤`kx1−x2k.Definition2.Atwice-differentiablefunctionf(·)isα-stronglyconvexif∀x,λmin(∇2f(x))≥αSuchsmoothnessHowtoEscapeSaddlePointsEfficientlyAlgorithm2PerturbedGradientDescent:PGD(x0,`,ρ,,c,δ,∆f)χ←3max{log(d`∆fc2δ),4},η←c`,r←√cχ2·`gthres←√cχ2·,fthres←cχ3·q3ρ,tthres←χc2·`√ρtnoise←−tthres−1fort=0,1,...doifk∇f(xt)k≤gthresandt−tnoise>tthresthenx̃t←xt,tnoise←txt←x̃t+ξt,ξtuniformly∼B0(r)ift−tnoise=tthresandf(xt)−f(x̃tnoise)>−fthresthenreturnx̃tnoisext+1←xt−η∇f(xt)Thealgorithmthatweanalyzeisaperturbedformofgra-dientdescent(seeAlgorithm2).Thealgorithmisbasedongradientdescentwithstepsizeη.Whenthenormofthecurrentgradientissmall(≤gthres)(whichindicatesthatthecurrentiteratex̃tisHowtoEscapeSaddlePointsEfficientlyThatis,foranyx,atleastoneoffollowingholds:•k∇f(x)k≥θ.•λmin(∇2f(x))≤−γ.•xisζ-closetoX?—thesetoflocalminima.Intuitively,thestrictsaddleassumptionstatesthattheRdspacecanbedividedintothreeregions:1)aregionwherethegradientislarge;2)aregionwheretheHessianhasasignificantnegativeeigenvalue(aroundsaddlepoint);and3)theHowtoEscapeSaddlePointsEfficiently4.Example—MatrixFactorizationAsasimpleexampletoillustratehowtoapplyourgen-eraltheoremstospecificnon-convexoptimizationprob-lems,weconsiderasymmetriclow-rankmatrixfactoriza-tionproblem,basedonthefollowingobjectivefunction:minU∈Rd×rf(U)=12kUU>−M?k2F,(2)whereM?∈Rd×d.Forsimplicity,weassumerank(M?)=r,anddenoteσ?1:=σ1(M?),σ?r:=σr(M?).Clearly,inthiscasetheglobalminimumoffunc-tionvalueiszero,whichisachievedatV?=TD1/2whereTDT>istheSVDofthesymmetricrealmatrixM?.ThefollowingtwolemmasshowthattheobjectivefunctioninEq.(2)satisfiesthegeometricassumptionsA1,A2,andA3.b.Moreover,alllocalminimaareglobalminima.Lemma6.ForanyΓ≥σ?1,thefunctionf(U)definedinEq.(2)is8Γ-smoothand12Γ1/2-HessianLipschitz,insidetheregion{U|kUk2<Γ}.Lemma7.Forfunctionf(U)definedinEq.(2),alllo-calminimaareglobalminima.ThesetofglobalminimaisX?={V?R|RR>=HowtoEscapeSaddlePointsEfficientlyFigure1.Pertubationballin3Dand“thinpancake”stuckregionwFigure2.“Narrowband”stuckregionin2Dundergradientflow5.2.EscapingfromSaddlePointsQuicklyTheproofofLemma9isstraightforwardandfollowsfromtraditionalanalysis.ThekeytechnicalcontributionofthispaperistheproofofLemma10,whichgivesanewcharac-terizationofthegeometryaroundsaddlepoints.Considerapointx̃thatsatisfiesthethepreconditionsofLemma10(k∇f(x̃)k≤gthresandλmin(∇2f(x̃))≤−√ρ).Afteraddingtheperturbation(x0=x̃+ξ),wecanviewx0ascomingfromauniformdistributionoverBx̃(r),whichwecalltheperturbationball.WecandividethisperturbationballBx̃(r)intotwodisjointregions:(1)anes-capingregionXescapewhichconsistsofallthepointsx∈Bx̃(r)whosefunctionvaluedecreasesbyatleastfthresaf-tertthressteps;(2)astuckregionXstuck=Bx̃(r)−Xescape.OurgeneralproofstrategyistoshowthatXstuckconsistsofaverysmallproportionofthevolumeofperturbationball.Afteraddingaperturbationtox̃,pointx0hasaverysmallchanceoffallinginXstuck,andhencewillescapefromthesaddlepointefficiently.LetusconsiderthenatureofXstuck.Forsimplicity,letusimaginethatx̃isanexactsaddlepointwhoseHessianhasonlyonenegativeeigenvalue,andd−1positiveeigenval-ues.Letusdenotetheminimumeigenvaluedirectionase1.Inthiscase,iftheHessianremainsconstant(andwehaveaquadraticfunction),thestuckregionXstuckconsistsofpointsxsuchthatx−x̃hasasmalle1component.Thisisastraightbandintwodimensionsandaflatdiskinhighdimensions.However,whentheHessianisnotconstant,theshapeofthestuckregionisdistorted.Intwodimen-sions,itformsa“narrowband”asplottedinFigure2ontopofthegradientflow.Inthreedimensions,itformsa“thinpancake”asshowninFigure1.Themajorchallengehereistoboundthevolumeofthishigh-dimensionalnon-flat“pancake”shapedregionXstuck.Acrudeapproximationofthis“pancake”byHowtoEscapeSaddlePointsEfficientlyReferencesAgarwal,Naman,Allen-Zhu,Zeyuan,Bullins,Brian,Hazan,Elad,andMa,Tengyu.Findingapproximatelo-calminimafornonconvexoptimizationinlineartime.arXivpreprintarXiv:1611.01146,2016.Bhojanapalli,Srinadh,Neyshabur,Behnam,andSrebro,Nathan.Globaloptimalityoflocalsearchforlowrankmatrixrecovery.arXivpreprintarXiv:1605.07221,2016.Bubeck,Sébastienetal.Convexoptimization:Algorithmsandcomplexity.FoundationsandTrendsRinMachineLearning,8(3-4):231–357,2015.Candes,EmmanuelJ,Li,Xiaodong,andSoltanolkotabi,Mahdi.Phaseretrievalviawirtingerflow:Theoryandalgorithms.IEEETransactionsonInformationTheory,61(4):1985–2007,2015.Carmon,YairandDuchi,JohnC.Gradientdescenteffi-cientlyfindsthecubic-regularizednon-convexnewtonstep.arXivpreprintarXiv:1612.00547,2016.Carmon,Yair,Duchi,JohnC,Hinder,Oliver,andSidford,Aaron.Acceleratedmethodsfornon-convexoptimiza-tion.arXivpreprintarXiv:1611.00756,2016.Choromanska,Anna,Henaff,Mikael,Mathieu,Michael,Arous,GérardBen,andLeCun,Yann.Thelosssurfaceofmultilayernetworks.arXiv:1412.0233,2014.Curtis,FrankE,Robinson,DanielP,andSamadi,Moham-madreza.Atrustregionalgorithmwithaworst-caseiter-ationcomplexityof\mathcal{O}(\epsilonˆ{-3/2})fornonconvexoptimization.MathematicalProgramming,pp.1–32,2014.Dauphin,YannN,Pascanu,Razvan,Gulcehre,Caglar,Cho,Kyunghyun,Ganguli,Surya,andBengio,Yoshua.Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convexoptimization.InAdvancesinNeuralInformationProcessingSystems,pp.2933–2941,2014.Ge,Rong,Huang,Furong,Jin,Chi,andYuan,Yang.Es-capingfromsaddlepoints—onlinestochasticgradientfortensordecomposition.InCOLT,2015.Ge,Rong,Lee,JasonD,andMa,Tengyu.Matrixcom-pletionhasnospuriouslocalminimum.InAdvancesinNeuralInformationProcessingSystems,pp.2973–2981,2016.Harman,RadoslavandLacko,Vladimı́r.Ondecomposi-tionalalgorithmsforuniformsamplingfromn-spheresandn-balls.JournalofMultivariateAnalysis,101(10):2297–2304,2010.Jain,Prateek,Jin,Chi,Kakade,ShamM,andNetrapalli,Praneeth.Computingmatrixsquarerootvianonconvexlocalsearch.arXivpreprintarXiv:1507.05854,2015.Kawaguchi,Kenji.Deeplearningwithoutpoorlocalmin-ima.InAdvancesInNeuralInformationProcessingSys-tems,pp.586–594,2016.Lee,JasonD,Simchowitz,Max,Jordan,MichaelI,andRecht,Benjamin.Gradientdescentonlyconvergestominimizers.InConferenceonLearningTheory,pp.1246–1257,2016.Levy,