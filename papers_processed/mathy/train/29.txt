HowtoEscapeSaddlePointsEfficientlyChiJin1RongGe2PraneethNetrapalli3ShamM.Kakade4MichaelI.Jordan1AbstractThispapershowsthataperturbedformofgradi-entdescentconvergestoasecond-orderstation-arypointinanumberiterationswhichdependsonlypoly-logarithmicallyondimension(i.e.,itisalmostâ€œdimension-freeâ€).Theconvergencerateofthisprocedurematchesthewell-knowncon-vergencerateofgradientdescenttofirst-orderstationarypoints,uptologfactors.Whenallsad-dlepointsarenon-degenerate,allsecond-orderstationarypointsarelocalminima,andourresultthusshowsthatperturbedgradientdescentcanescapesaddlepointsalmostforfree.Ourresultscanbedirectlyappliedtomanymachinelearningapplications,includingdeeplearning.Asapar-ticularconcreteexampleofsuchanapplication,weshowthatourresultscanbeuseddirectlytoestablishsharpglobalconvergenceratesforma-trixfactorization.Ourresultsrelyonanovelcharacterizationofthegeometryaroundsaddlepoints,whichmaybeofindependentinteresttothenon-convexoptimizationcommunity.1.HowtoEscapeSaddlePointsEfficientlyAlgorithm1PerturbedGradientDescent(Meta-algorithm)fort=0,1,...doifperturbationconditionholdsthenxtâ†xt+Î¾t,Î¾tuniformlyâˆ¼B0(r)xt+1â†xtâˆ’Î·âˆ‡f(xt)noise.However,thisresultdoesnotboundthenumberofstepsneededtoreachalocalminimum.Previousworkexplainswhygradientdescentavoidssad-dlepointsinthenonconvexsetting,butnotwhyitiseffi-cientâ€”allofthemhaveruntimeguaranteeswithhighpoly-nomialdependencyindimensiond.Forinstance,thenum-berofiterationsrequiredinGeetal.(2015)isatleastâ„¦(d4),whichisprohibitiveinhighdimensionalsettingsuchasdeeplearning(typicallywithmillionsofparame-ters).Therefore,wewonderwhethergradientdescenttypeofalgorithmsarefundamentallyslowinescapingsaddlepoints,oritisthelackofourtheoreticalunderstandingwhilegradientdescentisindeedefficient.Thismotivatesthefollowingquestion:Cangradientdescentescapesad-dlepointsandconvergetolocalminimainanumberofiterationsthatis(almost)dimension-free?Inordertoanswerthisquestionformally,thispaperinves-tigatesthecomplexityoffinding-second-orderstationarypoints.ForÏ-HessianLipschitzfunctions(seeDefinition5),thesepointsaredefinedas(Nesterov&Polyak,2006):kâˆ‡f(x)kâ‰¤,andÎ»min(âˆ‡2f(x))â‰¥âˆ’âˆšÏ.Undertheassumptionthatallsaddlepointsarestrict(i.e.,foranysaddlepointxs,Î»min(âˆ‡2f(xs))<0),allsecond-orderstationarypoints(=0)arelocalminima.Therefore,convergencetosecond-orderstationarypointsisequivalenttoconvergencetolocalminima.Thispaperstudiesasimplevariantofgradientdescent(withphasicperturbations,seeAlgorithm1).For`-smoothfunctionsthatarealsoHessianLipschitz,weshowthatper-turbedgradientdescentwillconvergetoan-second-orderstationarypointinOÌƒ(`(f(x0)âˆ’f?)/2),whereOÌƒ(Â·)hidespolylogfactors.Thisguaranteeisalmostdimensionfree(uptopolylog(d)factors),answeringtheabovehighlightedHowtoEscapeSaddlePointsEfficientlyTable1.Oraclemodelsanditerationcomplexityforconvergencetosecond-orderstationarypointAlgorithmIterationsOracleGeetal.(2015)O(poly(d/))GradientLevy(2016)O(d3poly(1/))GradientThisWorkO(log4(d)/2)GradientAgarwaletal.(2016)O(log(d)/7/4)Hessian-vectorCarmonetal.(2016)O(log(d)/7/4)Hessian-vectorCarmon&Duchi(2016)O(log(d)/2)Hessian-vectorNesterov&Polyak(2006)O(1/1.5)HessianCurtisetal.(2014)O(1/1.5)Hessiangivestrongguarantees,theanalysesareheavilytailoredtospecificproblems,anditisunclearhowtogeneralizethemtoawiderclassofnon-convexfunctions.Forgeneralnon-convexoptimization,thereareafewpre-viousresultsonfindingsecond-orderstationarypoints.TheseresultscanbedividedintothefollowingHowtoEscapeSaddlePointsEfficiently2.2.GradientDescentThetheoryofgradientdescentoftentakesitspointofde-parturetobethestudyofconvexoptimization.Definition1.Adifferentiablefunctionf(Â·)is`-smooth(or`-gradientLipschitz)if:âˆ€x1,x2,kâˆ‡f(x1)âˆ’âˆ‡f(x2)kâ‰¤`kx1âˆ’x2k.Definition2.Atwice-differentiablefunctionf(Â·)isÎ±-stronglyconvexifâˆ€x,Î»min(âˆ‡2f(x))â‰¥Î±SuchsmoothnessHowtoEscapeSaddlePointsEfficientlyAlgorithm2PerturbedGradientDescent:PGD(x0,`,Ï,,c,Î´,âˆ†f)Ï‡â†3max{log(d`âˆ†fc2Î´),4},Î·â†c`,râ†âˆšcÏ‡2Â·`gthresâ†âˆšcÏ‡2Â·,fthresâ†cÏ‡3Â·q3Ï,tthresâ†Ï‡c2Â·`âˆšÏtnoiseâ†âˆ’tthresâˆ’1fort=0,1,...doifkâˆ‡f(xt)kâ‰¤gthresandtâˆ’tnoise>tthresthenxÌƒtâ†xt,tnoiseâ†txtâ†xÌƒt+Î¾t,Î¾tuniformlyâˆ¼B0(r)iftâˆ’tnoise=tthresandf(xt)âˆ’f(xÌƒtnoise)>âˆ’fthresthenreturnxÌƒtnoisext+1â†xtâˆ’Î·âˆ‡f(xt)Thealgorithmthatweanalyzeisaperturbedformofgra-dientdescent(seeAlgorithm2).ThealgorithmisbasedongradientdescentwithstepsizeÎ·.Whenthenormofthecurrentgradientissmall(â‰¤gthres)(whichindicatesthatthecurrentiteratexÌƒtisHowtoEscapeSaddlePointsEfficientlyThatis,foranyx,atleastoneoffollowingholds:â€¢kâˆ‡f(x)kâ‰¥Î¸.â€¢Î»min(âˆ‡2f(x))â‰¤âˆ’Î³.â€¢xisÎ¶-closetoX?â€”thesetoflocalminima.Intuitively,thestrictsaddleassumptionstatesthattheRdspacecanbedividedintothreeregions:1)aregionwherethegradientislarge;2)aregionwheretheHessianhasasignificantnegativeeigenvalue(aroundsaddlepoint);and3)theHowtoEscapeSaddlePointsEfficiently4.Exampleâ€”MatrixFactorizationAsasimpleexampletoillustratehowtoapplyourgen-eraltheoremstospecificnon-convexoptimizationprob-lems,weconsiderasymmetriclow-rankmatrixfactoriza-tionproblem,basedonthefollowingobjectivefunction:minUâˆˆRdÃ—rf(U)=12kUU>âˆ’M?k2F,(2)whereM?âˆˆRdÃ—d.Forsimplicity,weassumerank(M?)=r,anddenoteÏƒ?1:=Ïƒ1(M?),Ïƒ?r:=Ïƒr(M?).Clearly,inthiscasetheglobalminimumoffunc-tionvalueiszero,whichisachievedatV?=TD1/2whereTDT>istheSVDofthesymmetricrealmatrixM?.ThefollowingtwolemmasshowthattheobjectivefunctioninEq.(2)satisfiesthegeometricassumptionsA1,A2,andA3.b.Moreover,alllocalminimaareglobalminima.Lemma6.ForanyÎ“â‰¥Ïƒ?1,thefunctionf(U)definedinEq.(2)is8Î“-smoothand12Î“1/2-HessianLipschitz,insidetheregion{U|kUk2<Î“}.Lemma7.Forfunctionf(U)definedinEq.(2),alllo-calminimaareglobalminima.ThesetofglobalminimaisX?={V?R|RR>=HowtoEscapeSaddlePointsEfficientlyFigure1.Pertubationballin3Dandâ€œthinpancakeâ€stuckregionwFigure2.â€œNarrowbandâ€stuckregionin2Dundergradientflow5.2.EscapingfromSaddlePointsQuicklyTheproofofLemma9isstraightforwardandfollowsfromtraditionalanalysis.ThekeytechnicalcontributionofthispaperistheproofofLemma10,whichgivesanewcharac-terizationofthegeometryaroundsaddlepoints.ConsiderapointxÌƒthatsatisfiesthethepreconditionsofLemma10(kâˆ‡f(xÌƒ)kâ‰¤gthresandÎ»min(âˆ‡2f(xÌƒ))â‰¤âˆ’âˆšÏ).Afteraddingtheperturbation(x0=xÌƒ+Î¾),wecanviewx0ascomingfromauniformdistributionoverBxÌƒ(r),whichwecalltheperturbationball.WecandividethisperturbationballBxÌƒ(r)intotwodisjointregions:(1)anes-capingregionXescapewhichconsistsofallthepointsxâˆˆBxÌƒ(r)whosefunctionvaluedecreasesbyatleastfthresaf-tertthressteps;(2)astuckregionXstuck=BxÌƒ(r)âˆ’Xescape.OurgeneralproofstrategyistoshowthatXstuckconsistsofaverysmallproportionofthevolumeofperturbationball.AfteraddingaperturbationtoxÌƒ,pointx0hasaverysmallchanceoffallinginXstuck,andhencewillescapefromthesaddlepointefficiently.LetusconsiderthenatureofXstuck.Forsimplicity,letusimaginethatxÌƒisanexactsaddlepointwhoseHessianhasonlyonenegativeeigenvalue,anddâˆ’1positiveeigenval-ues.Letusdenotetheminimumeigenvaluedirectionase1.Inthiscase,iftheHessianremainsconstant(andwehaveaquadraticfunction),thestuckregionXstuckconsistsofpointsxsuchthatxâˆ’xÌƒhasasmalle1component.Thisisastraightbandintwodimensionsandaflatdiskinhighdimensions.However,whentheHessianisnotconstant,theshapeofthestuckregionisdistorted.Intwodimen-sions,itformsaâ€œnarrowbandâ€asplottedinFigure2ontopofthegradientflow.Inthreedimensions,itformsaâ€œthinpancakeâ€asshowninFigure1.Themajorchallengehereistoboundthevolumeofthishigh-dimensionalnon-flatâ€œpancakeâ€shapedregionXstuck.Acrudeapproximationofthisâ€œpancakeâ€byHowtoEscapeSaddlePointsEfficientlyReferencesAgarwal,Naman,Allen-Zhu,Zeyuan,Bullins,Brian,Hazan,Elad,andMa,Tengyu.Findingapproximatelo-calminimafornonconvexoptimizationinlineartime.arXivpreprintarXiv:1611.01146,2016.Bhojanapalli,Srinadh,Neyshabur,Behnam,andSrebro,Nathan.Globaloptimalityoflocalsearchforlowrankmatrixrecovery.arXivpreprintarXiv:1605.07221,2016.Bubeck,SeÌbastienetal.Convexoptimization:Algorithmsandcomplexity.FoundationsandTrendsRinMachineLearning,8(3-4):231â€“357,2015.Candes,EmmanuelJ,Li,Xiaodong,andSoltanolkotabi,Mahdi.Phaseretrievalviawirtingerflow:Theoryandalgorithms.IEEETransactionsonInformationTheory,61(4):1985â€“2007,2015.Carmon,YairandDuchi,JohnC.Gradientdescenteffi-cientlyfindsthecubic-regularizednon-convexnewtonstep.arXivpreprintarXiv:1612.00547,2016.Carmon,Yair,Duchi,JohnC,Hinder,Oliver,andSidford,Aaron.Acceleratedmethodsfornon-convexoptimiza-tion.arXivpreprintarXiv:1611.00756,2016.Choromanska,Anna,Henaff,Mikael,Mathieu,Michael,Arous,GeÌrardBen,andLeCun,Yann.Thelosssurfaceofmultilayernetworks.arXiv:1412.0233,2014.Curtis,FrankE,Robinson,DanielP,andSamadi,Moham-madreza.Atrustregionalgorithmwithaworst-caseiter-ationcomplexityof\mathcal{O}(\epsilonË†{-3/2})fornonconvexoptimization.MathematicalProgramming,pp.1â€“32,2014.Dauphin,YannN,Pascanu,Razvan,Gulcehre,Caglar,Cho,Kyunghyun,Ganguli,Surya,andBengio,Yoshua.Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convexoptimization.InAdvancesinNeuralInformationProcessingSystems,pp.2933â€“2941,2014.Ge,Rong,Huang,Furong,Jin,Chi,andYuan,Yang.Es-capingfromsaddlepointsâ€”onlinestochasticgradientfortensordecomposition.InCOLT,2015.Ge,Rong,Lee,JasonD,andMa,Tengyu.Matrixcom-pletionhasnospuriouslocalminimum.InAdvancesinNeuralInformationProcessingSystems,pp.2973â€“2981,2016.Harman,RadoslavandLacko,VladimÄ±Ìr.Ondecomposi-tionalalgorithmsforuniformsamplingfromn-spheresandn-balls.JournalofMultivariateAnalysis,101(10):2297â€“2304,2010.Jain,Prateek,Jin,Chi,Kakade,ShamM,andNetrapalli,Praneeth.Computingmatrixsquarerootvianonconvexlocalsearch.arXivpreprintarXiv:1507.05854,2015.Kawaguchi,Kenji.Deeplearningwithoutpoorlocalmin-ima.InAdvancesInNeuralInformationProcessingSys-tems,pp.586â€“594,2016.Lee,JasonD,Simchowitz,Max,Jordan,MichaelI,andRecht,Benjamin.Gradientdescentonlyconvergestominimizers.InConferenceonLearningTheory,pp.1246â€“1257,2016.Levy,