         Graph Clustering: Block-models and model free
                                                  results
                                            Anonymous Author(s)
                                                   Affiliation
                                                    Address
                                                     email
                                                  Abstract
 1           Clustering graphs under the Stochastic Block Model (SBM) and extensions are
 2           well studied. Guarantees of correctness exist under the assumption that the data
 3           is sampled from a model. In this paper, we propose a framework, in which we
 4           obtain “correctness” guarantees without assuming the data comes from a model.
 5           The guarantees we obtain depend instead on the statistics of the data that can be
 6           checked. We also show that this framework ties in with the existing model-based
 7           framework, and that we can exploit results in model-based recovery, as well as
 8           strengthen the results existing in that area of research.
 9 1    Introduction: a framework for clustering with guarantees without model
10      assumptions
11 In the last few years, model-based clustering in networks has witnessed spectacular progress. At
12 the central of intact are the so-called block-models, the Stochastic Block Model (SBM), Degree-
13 Corrected SBM (DC-SBM) and Preference Frame Model (PFM). The understanding of these models
14 has been advanced, especially in understanding the conditions when recovery of the true clustering is
15 possible with small or no error. The algorithms for recovery with guarantees has also been improved.
16 However, the impact of the above results is limited by the assumption that the observed data come
17 from the model.
18 This paper proposes a framework to provide theoretical guarantees for the results of model based
19 clustering algorithms, without making any assumption about the data generating process. To de-
20 scribe the idea, we need some notation. Assume that a graph G on n nodes is observed. A model-
21 based algorithm clusters G, and outputs clustering C and parameters M(G, C).
22 The framework is as follows: if M(G, C) fits the data G well, then we shall prove that any other
23 clustering C 0 of G that also fits G well will be a small perturbation of C. If this holds, then C with
24 model parameters M(G, C) can be said to capture the data structure in a meaningful way.
25 We exemplify our approach by obtaining model-free guarantees for the SBM and PFM models.
26 Moreover, we show that model-free and model-based results are intimately connected.
27 2    Background: graphs, clusterings and block models
28 Graphs, degrees, Laplacian, and clustering Let G be a graph on n nodes, described by its ad-
                                       Pn
29
   jacency matrix Â. Define dˆi = j=1 Âij the degree of node i, and D̂ = diag{dˆi } the diagonal
30 matrix of the node degrees. The (normalized) Laplacian of G is defined as1 L̂ = D̂−1/2 ÂD̂−1/2 . In
      1
        Rigorously speaking, the normalized graph Laplacian is I − L̂ [10].
   Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.

31 extension, we define the degree matrix D and the Laplacian L associated to any matrix A ∈ Rn×n ,
32 with Aij = Aji ≥ 0, in a similar way.
33 Let C be a partitioning (clustering) of the nodes of G into K clusters. We use the shorthand notation
34 i ∈ k for “node i belongs to cluster k”. We will represent C by its n × K indicator matrix Z, defined
35 by
                        Zik = 1 if i ∈ k, 0 otherwise, for i = 1, . . . n, k = 1, . . . K.               (1)
36 Note that Z T Z = diag{nk } with nk counting the number of nodes in cluster k, and Z T ÂZ =
37 [nkl ]Kk,l=1 with nkl counting the edges in G between clusters k and l. Moreover, for two indicator
38 matrices Z, Z 0 for clusterings C, C 0 , (Z T Z 0 )kk0 counts the number of points in the intersection of
   cluster k of C with cluster k 0 of C 0 , and (Z T D̂Z 0 )kk0 computes i∈k∩k0 dˆi the volume of the same
                                                                         P
39
40 intersection.
41 “Block models” for random graphs (SBM, DC-SBM, PFM) This family of models contains
42 Stochastic Block Models (SBM) [19, 1], Degree-Corrected SBM (DC-SBM) [18] and Prefer-
43 ence Frame Models (PFM) [21]. Under each of these model families, a graph G with adja-
44 cency matrix Â over n nodes is generated by sampling its edges independently following the law
45 Âij ∼ Bernoulli(Aij ), for all i > j. The symmetric matrix A = [Aij ] describing the graph is the
46 edge probability matrix. The three model families differ in the constraints they put on an acceptable
47 A. Let C ∗ be a clustering. The entries of A are defined w.r.t C ∗ as follows (and we say that A is
48 compatible with C ∗ ).
49    SBM: Aij = Bkl whenever i ∈ k, j ∈ l, with B = [Bkl ] ∈ RK×K symmetric and non-negative.
50DC-SBM: Aij = wi wj Bkl whenever i ∈ k, j ∈ l, with B as above and w1 , . . . wn non-negative
51            weights associated with the graph nodes.
52    PFM: A satisfies D = diag(A1), D−1 AZ = ZR where 1 denotes the vector of all ones, Z is
53            the indicator matrix of C ∗ , and R is a stochastic matrix (R1 = 1, Rkl ≥ 0), the details are
54            in [21]
55 While perhaps not immediately obvious, the SBM is a subclass of the DC-SBM, and the latter a
56 subclass of the PFM. Another common feature of block-models, that will be significant throughout
57 this work is that for all three, Spectral Clustering algorithms [16] have been proved to work well
58 estimating C ∗ .
59 3     Main theorem: blueprint and results for PFM, SBM
60 Let M be a model class, such as SBM, DC-SBM, PFM, and denote M(G, C) ∈ M to be a model
61 that is compatible with C and is fitted in some way to graph G (we do not assume in general that this
62 fit is optimal).
63 Theorem 1 (Generic Theorem) We say that clustering C fits G well w.r.t M iff M(G, C) is “close
64 to” G. If C fits G well w.r.t M, then (subject to other technical conditions) any other clustering C 0
65 which also fits G well is close to C, i.e. dist(C, C 0 ) is small.
66 In what follows, we will instantiate this Generic Theorem, and the concepts therein; in
67 particular the following will be formally defined. (1) Model construction, i.e an algorithm
68 to fit a model in M to (G, C).                This is necessary since we want our results to be
69 computable in practice.         (2) A goodness of fit measure between M(C, G) and the data G.
70 (3) A distance between clusterings. We adopt the widely used Misclassification Error (or Hamming)
71 distance defined below.
72 The Misclassification Error (ME) distance between two clusterings C, C 0 over the same set of n
73 points is
                                                             1        X
                                    dist(C, C 0 ) = 1 − max                  1,                          (2)
                                                             n π∈SK
                                                                    i∈k∩π(k)
74 where π ranges over all permutations of K elements SK , and π(k) indexes a cluster in C 0 . If the
75 points are weighted by their degrees, a natural measure on the node set, the Weighted ME (wME)
                                                           2

76 distance is
                                                             1             X
                                distdˆ(C, C 0 ) = 1 − Pn           max           dˆi .                     (3)
                                                                ˆi π∈SK
                                                                d
                                                            i=1         i∈k∩π(k)
   In the above, i∈k∩k0 dˆi represents the total weight of the set of points assigned to cluster k by C
                   P
77
78 and to cluster k 0 by C 0 . Note that in the indicator matrix representation of clusterings, this is the
79 k, k 0 element of the matrix Z T D̂Z 0 ∈ RK×K . While dist is more popular, we believe distdˆ is more
80 natural, especially when node degrees are dissimilar, as dˆ can be seen as a natural measure on the
81 set of nodes, and distdˆ is equivalent to the earth-mover’s distance.
82 3.1    Main result for PFM
83 Constructing a model Given a graph G and clustering C of its nodes, we wish to construct a PFM
84 compatible with C, so that its Laplacian L satisfies that ||L̂ − L|| is small.
85 Let the spectral decomposition of L̂ be
                                                             
                                        Λ̂      0         Ŷ T
                    L̂ = [Ŷ Ŷlow ]                        T
                                                                                                T
                                                                   = Ŷ Λ̂Ŷ T + Ŷlow Λ̂low Ŷlow         (4)
                                        0 Λ̂low          Ŷlow
86 where Ŷ ∈ Rn×K , Ŷlow ∈ Rn×(n−K) and Λ̂, Λ̂low diagonal matrices of dimension K, respectively
87 n − K. To ensure that the matrices Ŷ , Ŷlow are uniquely defined we assume throughout the paper
88 that L̂’s K-th eigengap, i.e, |λK | − |λK+1 |, is non-zero.
89 Assumption 1 The eigenvalues of L̂ satisfy λ̂1 = 1 ≥ |λ̂2 | ≥ . . . ≥ |λ̂K | > |λ̂K+1 | ≥ . . . |λ̂n |.
90 Denote the subspace spanned by the columns of M , for any M matrix, by R(M ), and || || the
91 Euclidean or spectral norm.
               PFM Estimation Algorithm
               Input Graph G with Â, D̂, L̂, Ŷ , Λ̂, clustering C with indicator matrix Z.
               Output (A, L) = P F M (G, C)
            1. Construct an orthogonal matrix derived from Z.
                         YZ = D̂1/2 ZC −1/2 , with C = Z TD̂Z the column normalization of Z.                (5)
               Note Ckk = i∈k dˆi the volume of cluster k.
                             P
            2. Project YZ on Ŷ and perform Singular Value Decomposition.
                                                   F = YZT Ŷ = U ΣV T                                      (6)
            3. Change basis in R(YZ ) to align with Ŷ .
                           Y = YZ U V T . Complete Y to an orthonormal basis [Y B] of Rn .                  (7)
            4. Construct Laplacian L and edge probability matrix A.
                                L = Y Λ̂Y T + (BB T )L̂(BB T ),             A = D̂1/2 LD̂1/2 .              (8)
92
93 Proposition 2 Let G, Â, D̂, L̂, Ŷ , Λ̂ and Z be defined as above, and (A, L) = P F M (G, C). Then,
94         1. D̂ and L, or A define a PFM with degrees dˆ1:n .
95         2. The columns of Y are eigenvectors of L with eigenvalues λ̂1:K .
96         3. D̂1/2 1 is an eigenvector of both L and L̂ with eigenvalue λ̂1 = 1.
                                                           3

 97 The proof is relegated to the Supplement, as are all the omitted proofs.
 98 P F M (G, C) is an estimator for the PFM parameters given the clustering. It is evidently not the
 99 Maximum Likelihood estimator, but we can show that it is consistent in the following sense.
100 Proposition 3 (Informal) Assume that G is sampled from a PFM with parameters D∗ , L∗ and com-
101 patible with C ∗ , and let L = P F M (G, C ∗ ). Then, under standard recovery conditions for PFM (e.g
102 [21]) ||L∗ − L|| = o(1) w.r.t. n.
103 Assumption 2 (Goodness of fit for PFM) ||L̂ − L|| ≤ ε.
104 P F M (G, C) instantiates M(G, C), and Assumption 2 instantiates the goodness of fit measure. It
105 remains to prove an instance of Generic Theorem 1 for these choices.
106 Theorem 4 (Main Result (PFM)) Let G be a graph with dˆ1:n , D̂, L̂, λ̂1:n as defined, and L̂ sat-
107 isfy Assumption 1. Let C, C 0 be two clusterings with K clusters, and L, L0 their correspond-
                                                                                                 2
108 ing Laplacians, defined as in (8), and satisfy Assumption 2. Set δ = (|λ̂ 4(K−1)ε    |−|λ̂     |)2
                                                                                                       and δ0 =
                                                                                       K       K+1
109 mink Ckk / maxk Ckk with C defined as in (5), where k indexes the clusters of C. Then, whenever
110 δ ≤ δ0 ,
                                                              maxk Ckk
                                            distdˆ(C, C 0 ) ≤ P           δ,                                 (9)
                                                                  k Ckk
111 with distdˆ being the weighted ME distance (3).
112 In the remainder of this section we outline the proof steps, while the partial results of Proposition 5,
113 6, 7 are proved in the Supplement. First, we apply the perturbation bound called the Sinus Theorem
114 of Davis and Kahan, in the form presented in Chapter V of [20].
115 Proposition 5 Let Ŷ , λ̂1:n , Y be defined as usual. If Assumptions 1 and 2 hold, then
                                                                        ε
                                || diag(sin θ1:K (Ŷ , Y ))|| ≤                   = ε0                      (10)
                                                                |λ̂K | − |λ̂K+1 |
116 where θ1:K are the canonical (or principal) angles between R(Ŷ ) and R(Y ) (see e.g [8]).
117 The next step concerns the closeness of Y, Ŷ in Frobenius norm. Since Proposition 5 bounds the
118 sinuses of the canonical angles, we exploit the fact that the cosines of the same angles are the singular
119 values of F = Y T Ŷ of (6).
120 Proposition 6 Let M = Y Y T , M̂ = Ŷ Ŷ T and F, ε0 as above. Assumptions 1 and 2 imply that
121        1. ||F ||2F = trace M M̂ T ≥ K − (K − 1)ε02 .
122        2. ||M − M̂ ||2F ≤ 2(K − 1)ε02 .
123 Now we show that all clusterings which satisfy Proposition 6 must be close to each other in the
124 weighted ME distance. For this, we first need an intermediate result. Assume we have two clus-
125 terings C, C 0 , with K clusters, for which we construct YZ , Y, L, M , respectively YZ0 , Y 0 , L0 , M 0 as
126 above. Then, the subspaces spanned by Y and Y 0 will be close.
127 Proposition 7 Let L̂ satisfy Assumption 1 and let C, C 0 represent two clusterings for which L, L0
128 satisfy Assumption 2. Then, ||YZT YZ0 ||2F ≥ K − 4(K − 1)ε02 = K − δ
129 The main result now follows from Proposition 7 and Theorem 9 of [14], as shown in the Supplement.
130 This proof approach is different from the existing perturbation bounds for clustering, which all use
131 counting arguments. The result of [14] is a local equivalence, which bounds the error we need in
132 terms of δ defined above (“local” meaning the result only holds for small δ).
                                                            4

133 3.2   Main Theorem for SBM
134 In this section, we offer an instantiation of Generic Theorem 1 for the case of the SBM. As before,
135 we start with a model estimator, which in this case is the Maximum Likelihood estimator.
               SBM Estimation Algorithm
               Input Graph with Â, clustering C with indicator matrix Z.
               Output A = SBM (G, C)
            1. Construct an orthogonal matrix derived from Z: YZ = ZC −1/2 with C = Z T Z.
            2. Estimate the edge probabilities: B = C −1 Z T ÂZC −1 .
136         3. Construct A from B by A = ZBZ T .
137 Proposition 8 Let B̃ = C 1/2 BC 1/2 and denote the eigenvalues of B̃, ordered by decreasing mag-
138 nitude, by λ1:K . Let the spectral decomposition of B̃ be B̃ = U ΛU T , with U an orthogonal matrix
139 and Λ = diag(λ1:K ). Then
140       1. A is a SBM.
141       2. λ1:K are the K principal eigenvalues of A. The remaining eigenvalues of A are zero.
142       3. A = Y ΛY T where Y = YZ U .
143 Assumption 3 (Eigengap) B is non-singular (or, equivalently, |λK | > 0.
144 Assumption 4 (Goodness of fit for SBM) ||Â − A|| ≤ ε.
145 With the model (SBM), estimator, and goodness of fit defined, we are ready for the main result.
146 Theorem 9 (Main Result (SBM)) Let G be a graph with incidence matrix Â, and λ̂A          K the K-th
147 singular value of Â. Let C, C 0 be two clusterings with K clusters, satisfying Assumptions 3 and 4.
                    2
148 Set δ = |4Kε
               λ̂A |2
                      and δ0 = mink nk / maxk nk , where k indexes the clusters of C. Then, whenever
                 K
149 δ ≤ δ0 , dist(C, C 0 ) ≤ δ maxk nk /n, where dist represents the ME distance (2).
150 Note that the eigengap of Â, Λ̂A   K is not bounded above, and neither is ε. Since the SBM is less
151 flexible than the PFM, we expect that for the same data G, Theorem 9 will be more restrictive than
152 Theorem 4.
153 4    The results in perspective
154 4.1   Cluster validation
155 Theorems like 4, 9 can provide model free guarantees for clustering. We exemplify this procedure in
156 the experimental Section 6, using standard spectral clustering as described in e.g [19, 18, 16]. What
157 is essential is that all the quantities such as ε and δ are computable from the data.
158 Moreover, if Y is available, then the bound in Theorem 4 can be improved.
159 Proposition
      p            10 Theorem 4 holds when δ is replaced by δY = K− < M̂ , M >F +(K − 1)(ε0 )2 +
160 2 2(K − 1)ε0 ||M̂ − M ||F , with ε0 = ε/(|λ̂K | − |λ̂K+1 |) and M, M̂ defined in Proposition 6.
161 4.2   Using existing model-based recovery theorems to prove model-free guarantees
162 We exemplify this by using (the proof of) Theorem 3 of [21] to prove the following.
163 Theorem 11 (Alternative result based on [21] for PFM) Under the same conditions as in Theo-
                                                            Kε2
164 rem 4, distdˆ(C, C 0 ) ≤ δWM , with δWM = 128 (|λ̂ |−|    λ̂     |)2
                                                                         .
                                                         K       K+1
                                                          5

165 It follows, too, that with the techniques in this paper, the error bound in [21] can be improved by a
166 factor of 128.
167 Similarly, if we use the results of [19] we obtain alternative model-free guarantee for the SBM.
168 Assumption 5 (Alternative goodness of fit for SBM) ||L̂2 −L2 ||F ≤ ε, where L̂, L are the Lapla-
169 cians of Â and A = SBM (G, C) respectively.
170 Theorem 12 (Alternative result based on [19] for SBM) Under the same conditions as in Theo-
                                                                                                        2
171 rem 9, except for replacing Assumption 4 with 5, dist(C, C 0 ) ≤ δRCY with δRCY = |λ̂ε |4 16 max              n
                                                                                                                    k nk
                                                                                                                         .
                                                                                                       K
172 A problem with this result is that Assumption 5 is much stronger than 4 (being in Frobenius norm).
173 The more recent results of [18] (with unspecified constants) in conjunction with our original As-
174 sumptions 3, 4, and the assumption that all clusters have equal sizes, give a bound of O(Kε2 /λ̂2K )
175 for the SBM; hence our model-free Theorem 9 matches this more restrictive model-based theorem.
176 4.3    Sanity checks and Extensions
177 It can be easily verified that if indeed G is sampled from a SBM, or PFM, then for large enough n,
178 and large enough model eigengap, Assumptions 1 and 2 (or 3 and 4) will hold.
179 Some immediate extensions and variations of Theorems 4, 9 are possible. For example, one could
180 replace the spectral norm by the Frobenius norm in Assumptions 2 and 4, which would simplify
181 some of the proofs. However, using the Frobenius norm would be a much stronger assumption [19]
182 Theorem 4 holds not just for simple graphs, but in the more general case when Â is a weighted graph
183 (i.e. a similarity matrix). The theorems can be extended to cover the case when C 0 is a clustering
184 that is α-worse than C, i.e when ||L0 − L̂|| ≥ ||L − L̂||(1 − α).
185 4.4    Clusterability and resilience
186 Our Theorems also imply the stability of a clustering to perturbations of the graph G. Indeed, let L̂0
187 be the Laplacian of G 0 , a perturbation of G. If ||L̂0 − L̂|| ≤ ε, then ||L̂0 − L|| ≤ 2ε, and (1) G 0 is
188 well fitted by a PFM whenever G is, and (2) C is δ stable w.r.t G 0 , hence C is what some authors [9]
189 call resilient.
190 A graph G is clusterable when G can be fitted well by some clustering C ∗ . Much work [4, 7] has
191 been devoted to showing that clusterability implies that finding a C close to C ∗ is computationally
192 efficient. Such results can be obtained in our framework, by exploiting existing recovery theorems
193 such as [19, 18, 21], which give recovery guarantees for Spectral Clustering, under the assumption of
194 sampling from the model. For this, we can simply replace the model assumption with the assumption
195 that there is a C ∗ for which L (or A) satisfies Assumptions 1 and 2 (or 3 and 4).
196 5     Related work
197 To our knowledge, there is no work of the type of Theorem 1 in the literature on SBM, DC-SBM,
198 PFM. The closest work is by [6] which guarantees approximate recovery assuming G is close to a
199 DC-SBM.
200 Spectral clustering is also used for loss-based clustering in (weighted) graphs and some stability
201 results exist in this context. Even though they measure clustering quality by different criteria, so that
202 the ε values are not comparable, we review them here. The recent paper of [17], Theorem 1.2 states
203 that if the K-way Cheeger constant of G is ρ(k) ≤ (1 − λ̂K+1 )/(cK 3 ) then the clustering error2
204 distdˆ(C, C opt ) ≤ C/c = δP SZ . In the current proof, the constant C = 2 × 105 ; moreover, ρ(K)
205 cannot be computed tractably. In [15], the bound δM SX depends on εM SX , the Normalized Cut
206 scaled by the eigengap. Since both bounds refer to the result of spectral clustering, we can compare
207 the relationship between δM SX and εM SX ; for [15], this is δM SX = 2εM SX [1 − εM SX /(K − 1)],
        2
          The results is stronger, bounding the perturbation of each cluster individually by δP SZ , but it also includes
    a factor larger than 1, bounding the error of K-means algorithm.
                                                             6

208 which is about K − 1 times larger than δ when  = M SX . In [5], dist(C, C 0 ) is defined in terms of
209 ||YZT − YZ0 ||2F , and the loss is (closely related) to ||Â − SBM (G, C)||2F . The bound does not take
210 into account the eigengap, that is, the stability of the subspace Ŷ itself.
211 Bootstrap for validating a clustering C was studied in [11] (see also references therein for earlier
212 work). In [3] the idea is to introduce a statistics, and large deviation bounds for it, conditioned on
213 sampling from a SBM (with covariates) and on a given C.
214 6    Experimental evaluation
215 Experiment Setup Given G, we obtain a clustering C0 by spectral clustering [16]. Then we
216 calculate clustering C by perturbing C0 with gradually increasing noise. For each C, we construct
217 PFM (C, G)and SBM(C, G) model, and further compute , δ and δ0 . If δ ≤ δ0 , C is guaranteed to be
218 stable by the theorems. In the remainder of this section, we describe the data generating process for
219 the simulated datasets and the results we obtained.
220
221 PFM Datasets We generate from PFM model with K = 5, n = 10000, λ1:K =
222 (1, 0.875, 0.75, 0.625, 0.5). eigengap = 0.48, n1:K = (2000, 2000, 2000, 2000, 2000). The
223 stochastic matrix R and its stationary distribution ρ are shown below. We sample an adjacency
224 matrix Â from A (shown below).
                                                           A                             Â
                                        
      ρ=    25   .12   .17   .18    .28
             .79   .02   .06   .03    .10
                                          
           .03    .71   .23   .00    .02  
      R =  .09    .16   .69   .00    .06
                                          
                                           
           .04    .00   .00   .80    .16  
             .10   .01   .03   .11    .76
225
226 Perturbed PFM Datasets A is obtained from the previous model by perturbing its principal
227 subspace (details in Supplement). Then we sample Â from A.
228
229 Lancichinetti-Fortunato-Radicchi (LFR) simulated matrix [12] The LFR benchmark graphs
230 are widely used for community detection algorithms, due to heterogeneity in the distribution
231 of node degree and community size. A LFR matrix is simulated with n = 10000, K = 4,
232 nk = (2467, 2416, 2427, 2690) and µ = 0.2, where µ is the mixing parameter indicating the
233 fraction of edges shared between a node and the other nodes from outside its community.
234
235 Political Blogs Dataset A directed network A        ~ of hyperlinks between weblogs on US politics,
236 compiled from online directories by Adamic and Glance [2], where each blog is assigned a political
237 leaning, liberal or conservative, based on its blog content. The network A contains 1490 blogs.
238 After erasing the disconnected nodes, n = 983. We study Â = (A                ~ 3 , which is a smoothed
                                                                              ~ T A)
239                             ~ T ~
    undirected graph. For A A we find no guarantees.
240
241 The first two data sets are expected to fit the PFM well, but not the SBM, while the LFR data is
242 expected to be a good fit for a SBM. Since all bounds can be computed on weighted graphs as well,
243 we have run the experiments also on the edge probability matrices A used to generate the PFM and
244 perturbed PFM graphs.
245 The results of these experiments are summarized in Figure 1. For all of the experiments, the cluster-
246 ing C is ensured to be stable by Theorem 4 as the unweighted error grows to a breaking point, then
247 the assumptions of the theorem fail. In particular, the C0 is always stable in the PFM framework.
                                                          7

248 Comparing δ from Theorem 9 to that from Theorem 4, we find that Theorem 9 (guarantees for SBM)
249 is much harder to satisfy. All δ values from Theorem 9 are above 1, and not shown.3 In particular,
250 for the SBM model class, the C cannot be proved stable even for the LFR data.
251 Note that part of the reason why with the PFM model very little difference from the clustering C0 can
252 be tolerated for a clustering to be stable is that the large eigengap makes P F M (G, C) differ from
253 P F M (G, C0 ) even for very small perturbations. By comparing the bounds for Â with the bounds
254 for the “weighted graphs” A, we can evaluate that the sampling noise on δ is approximately equal
255 to that of the clustering perturbation. Of course, the sampling noise varies with n, decreasing for
256 larger graphs. Moreover, from Political Blogs data, we see that “smoothing” a graph, by e.g. taking
257 powers of its adjacency matrix, has a stability inducing effect.
    Figure 1:      Quantities , δ, δ0 from Thm 4 plotted vs dist(C, C0 ) for various datasets: Â denotes a simple graph, while A denotes a
    weighted graph (i.e. a non-negative matrix). For the Political Blogs: Truth means C0 is true clustering of [2], spectral means C0 is obtained
    from spectral clustering. For SBM, δ is always greater than δ0 .
258 7     Discussion
259 This paper makes several contributions. At a high level, it poses the problem of model free validation
260 in the area of community detection in networks. The stability paradigm is not entirely new, but
261 using it explicitly with model-based clustering (instead of cost-based) is. So is “turning around” the
262 model-based recovery theorems to be used in a model-free framework.
263 All quantities in our theorems are computable from the data and the clustering C, i.e do not con-
264 tain undetermined constants, and do not depend on parameters that are not available. As with
265 distribution-free results in general, making fewer assumptions allows for less confidence in the con-
266 clusions, and the results are not always informative. Sometimes this should be so, e.g when the
267 data does not fit the model well. But it is also possible that the fit is good, but not good enough
268 to satisfy the conditions of the theorems as they are currently formulated. This happens with the
269 SBM bounds, and we believe tighter bounds are possible for this model. It would be particularly
270 interesting to study the non-spectral, sharp thresholds of [1] from the point of view of model-free
271 recovery. A complementary problem is to obtain negative guarantees (i.e that C is not unique up to
272 perturbations).
273 At the technical level, we obtain several different and model-specific stability results, that bound the
274 perturbation of a clustering by the perturbation of a model. They can be used both in model-free
275 and in existing or future model-based recovery guarantees, as we have shown in Section 3 and in the
276 experiments. The proof techniques that lead to these results are actually simpler, more direct, and
277 more elementary than the ones found in previous papers.
        3
          We also computed δRCY but the bounds were not informative.
                                                                          8

278 References
279  [1] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models:
280      fundamental limits and efficient recovery algorithms. arXiv preprint arXiv:1503.00609, 2015.
281  [2] Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election:
282      divided they blog. In Proceedings of the 3rd international workshop on Link discovery, pages
283      36–43. ACM, 2005.
284  [3] Edoardo M. Airoldi, David S. Choi, and Patrick J. Wolfe. Confidence sets for network struc-
285      ture. Technical Report arXiv:1105.6245, 2011.
286  [4] Pranjal Awasthi. Clustering under stability assumptions. In Encyclopedia of Algorithms, pages
287      331–335. 2016.
288  [5] Francis Bach and Michael I. Jordan. Learning spectral clustering with applications to speech
289      separation. Journal of Machine Learning Research, 7:1963–2001, 2006.
290  [6] Maria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, and Shang-Hua
291      Teng. Finding endogenously formed communities. In Proceedings of the Twenty-Fourth An-
292      nual ACM-SIAM Symposium on Discrete Algorithms, pages 767–783. SIAM, 2013.
293  [7] Shai Ben-David. Computational feasibility of clustering under clusterability assumptions.
294      CoRR, abs/1501.00437, 2015.
295  [8] Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.
296  [9] Yonatan Bilu and Nathan Linial. Are stable instances easy? CoRR, abs/0906.3162, 2009.
297 [10] Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997.
298 [11] Brian Karrer, Elizaveta Levina, and M. E. J. Newman. Robustness of community structure in
299      networks. Phys. Rev. E, 77:046119, Apr 2008.
300 [12] Andrea Lancichinetti, Santo Fortunato, and Filippo Radicchi. Benchmark graphs for testing
301      community detection algorithms. Physical review E, 78(4):046110, 2008.
302 [13] Marina Meila and Jianbo Shi. Learning segmentation by random walks. In Neural Information
303      Processing Systems, 2001.
304 [14] Marina Meilă. Local equivalence of distances between clusterings – a geometric perspective.
305      Machine Learning, 86(3):369–389, 2012.
306 [15] Marina Meilă, Susan Shortreed, and Liang Xu. Regularized spectral learning. In Robert Cow-
307      ell and Zoubin Ghahramani, editors, Proceedings of the Artificial Intelligence and Statistics
308      Workshop(AISTATS 05), 2005.
309 [16] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm.
310      In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information
311      Processing Systems 14, Cambridge, MA, 2002. MIT Press.
312 [17] Richard Peng, He Sun, and Luca Zanetti. Partitioning well-clustered graphs with k-means and
313      heat kernel. In Proceedings of the Annual Conference on Learning Theory (COLT), pages
314      1423–1455, 2015.
315 [18] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic
316      blockmodel. In Advances in Neural Information Processing Systems, pages 3120–3128, 2013.
317 [19] Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional
318      stochastic blockmodel. The Annals of Statistics, pages 1878–1915, 2011.
319 [20] Gilbert W Stewart and Ji-guang Sun. Matrix perturbation theory, volume 175. Academic press
320      New York, 1990.
321 [21] Yali Wan and Marina Meila. A class of network models recoverable by spectral clustering.
322      In Daniel Lee and Masashi Sugiyama, editors, Advances in Neural Information Processing
323      Systems (NIPS), page (to appear), 2015.
                                                      9

324 8     Supplementary Material for Graph Clustering: Block-models and model
325       free results
326 Proof of Proposition 2
327         1. Proof by verification.
328         2. LY = Y Λ̂Y T Y + (BB T )L̂(BB T )Y = Y Λ̂. Since B is the orthogonal complement of
329             Y , it follows that it is a stable subspace as well.
330         3. This is a well known result; see for example [20].
331 The celebrated Sinus Theorem is reproduced here for completeness.
332 Theorem 13 (Sinus Theorem of Davis-Kahan, from [20], Theorem V.3.6) Let L̂ be a Hermi-
333 tian matrix with spectral resolution given by (4), Y be any n × K matrix with orthonormal
334 columns, and M any symmetric K × K matrix with eigenvalues µ1:K . Let R = L̂Y − Y M
335 and ∆ = minλ∈λ̂K+1:n ,µ∈µ1:K |λ − µ| > 0. Then, for any unitarily invariant norm || ||,
                                       ||R||
336 || diag(sin θ1:K (Ŷ , Y ))|| ≤      ∆ ,  where θ1:K are the canonical angles between R(Ŷ ) and R(Y ).
    Proof of Proposition 5 This is a corollary of Theorem 3.6 in [20]. If eigenvalues are sorted by their
    absolute values, then λ̂K+1:n ∈ [−|λ̂K+1 |, |λ̂K+1 |] and µ1:K ∈ R \ (−|λ̂K+1 | − ∆, |λ̂K+1 | + ∆). If
    we set M = Λ̂, so that λ̂1:K ∈ R \ (−|λ̂K+1 | − ∆, |λ̂K+1 | + ∆). Now we view Y as a perturbation
    of Ŷ , hence
                            R   = L̂Y − Y Λ̂ = L̂Y − LY + (LY − Y Λ̂) = (L̂ − L)Y                           (11)
                        ||R|| = ||(L̂ − L)Y || ≤ ||L̂ − L||||Y || ≤ ε.                                      (12)
337 From Theorem 13 the result follows.                                                                        2
    Proof of Proposition 6 For 1:
           ||F ||2F   =     trace F F T = trace U ΣV T V ΣU T = trace U T U ΣV T V Σ = trace Σ2
                                X K                    XK                         XK
                      =     1+      cos2 θk = 1 +         (1 − sin2 θk ) = K −         sin2 θk since θ1 = 0 (13)
                                k=2                    k=2                        k=2
                      ≥     K − (K − 1)ε02                                                                  (14)
338 For 2: Denote trace M̂ T M =< M̂ , M >F . Then ||M − M̂ ||2F = ||M ||2F + ||M̂ ||2F − 2 <
339 M̂ , M >F ≤ K + K − 2(K − (K − 1)ε02 ) = 2(K − 1)ε02 .                                                     2
    Proof of Proposition 7 We have that | < M − M̂ , M 0 − M̂ >F | ≤ ||M − M̂ ||F ||M 0 − M̂ ||F .
    From Proposition 6 the r.h.s is no larger than 2(K − 1)ε02 .
    − < M − M̂ , M 0 − M̂ >F              ≤    ||M − M̂ ||F ||M 0 − M̂ ||F ≤ 2(K − 1)ε02                     (15)
                                 0                                        0                              02
                    − < M, M >F           +    < M̂ , M >F + < M̂ , M       >F −||M̂ ||2F ≤ 2(K − 1)ε        (16)
                                 0                                        0                       02
                       < M, M >F          ≥    < M̂ , M >F + < M̂ , M >F −K − 2(K − 1)ε                      (17)
                                          ≥    2K − 2(K − 1)ε02 − K − 2(K − 1)ε02 = K − 4(K − 1)ε(18)         02
                                                                                                                 2
340 Now, note that trace M M 0 = trace Y Y T Y 0 (Y 0 )T = trace((Y 0 )T Y ))(Y T Y 0 ) = ||Y T Y 0 ||2F .
341 Moreover, by (7), YZ and Y differ by a unitary transformation. Since || ||F is unitarily invariant,
342 the result follows.
343 Proof of Theorem 4 We apply Theorem 9 of [14] with AX = Z, AX 0 = Z 0 , and ÃX = Y , ÃX 0 =
                                                        Pn
    Y 0 . It follows that pXYkk0 = i∈k∩k0 dˆi / i=1 dˆi . Hence, the point weights are proportional to
                                          P
344
345 dˆ1:n . Also, evidently, pmin /pmax = δ0 , and the result follows.
346 Note that we use the fact that both PFM’s have degrees equal to dˆ1:n to obtain this proof.                2
                                                                                                     0
347 Proposition 14 Assumptions 3 and 4, imply || diag(sin θ1:K (Ŷ , Y ))|| ≤ ε/|λ̂A        K | = ε , where λ̂K
                                                                                                               A
348 is the K-th eigenvalue of Â.
                                                            10

    Proof of Proposition 14 We consider Â a perturbation of A, its eigenvectors Ŷ as the perturbed
    eigenvectors of A and M = Λ̂. Then, R = AŶ − Ŷ Λ̂
                                 ||R|| = ||AŶ − Ŷ Λ̂||                                              (19)
                                          = ||(AŶ − ÂŶ ) + (ÂŶ − Ŷ Λ̂)||                        (20)
                                          ≤    ||(A − Â)Ŷ ||                                        (21)
                                          ≤    ||A − Â||||Ŷ || ≤ ε.                                 (22)
349 The separation between Λ̂ and the residual spectrum of A is |λ̂K |. From the main Davis-Kahan
350 theorem 13 the result follows.                                                                       2
351 Proof of Proposition 8 The proofs of 1 and 2 are straightforward. To show 3, note that A =
352 ZC −1 Z T ÂZC −1 Z T = YZ C 1/2 BC 1/2 YZT = YZ U ΛU T YZT = Y ΛY T . The definition of B
353 above shows that this is the Maximum Likelihood estimator of B given the clustering C.
                                               #edges from cluster k to cluster l
                               ⇔     Bkl =                                                            (23)
                                                                nk nl
354 Proof of Theorem 9 We now follow the steps outlined in section 3 with ε0 from Proposition 14 to
355 obtain our main stability result.
356 Proof of Proposition 10 In the Proof of Proposition 7, we replace the bounds corresponding to
357 < M̂ , M >F , ||M̂ − M ||F by the actual values computed from M, M̂ . We obtain
                                                                      p
               < M, M 0 >F ≥< M̂ , M >F −(K − 1)(ε0 )2 − 2 2(K − 1)ε0 ||M̂ − M ||F .                  (24)
358 Proof of Proposition 3
359 From the Proof of this theorem, we have that ||L∗ − L̂|| = o(1), ||(D∗ )1/2 − D̂1/2 || = o(1),
360 ||λ∗ − Λ̂|| = o(1), and ||Ŷ − Y ∗ || = o(1). Let Z be the indicator matrix of C ∗ . The principal
361 eigenvectors of L∗ are Y ∗ = (D∗ )1/2 Z(C ∗ )−1/2 . It follows then that ||Z T D̂Z − Z T D∗ Z|| =
362 o(1), and since C = Z T D̂Z, YZ = D̂1/2 ZC −1/2 we have that ||YZ − Y ∗ || = o(1), ||F ∗ −
363 F || = o(1) where F ∗ = Y T Y ∗ . Moreover, since ||Ŷ − Y ∗ || = o(1), ||F − I|| = o(1) Hence
364 ||U V T − I|| = o(1). Since the choice of B depends only on R(YZ ), it follows immediately that
365 ||BB T L̂B T B − B ∗ (B ∗ )T L∗ (B ∗ )T B ∗ || = o(1). Now, L = YZ U V T Λ̂V U T YZT + BB T L̂B T B,
366 and L∗ = Y ∗ Λ∗ (Y ∗ )T + B ∗ (B ∗ )T L∗ (B ∗ )T B ∗ , which completes the proof.                    2
367 perturbation of the PFM model To obtain a noisy PFM model A, we calculate the first K piecewise
368 constant [15] eigenvectors V of the transition matrix P = D−1 A, from which we obtain V ∗ by
369 perturbing each entry in V with a noise  ∼ unif (0, 10−4 ). The perturbed similarity matrix A is
370 then obtained as A = D1/2 (D1/2 V ∗ Λ̂V ∗T D1/2 + Ŷlow Λ̂low Ŷlow T
                                                                          )D1/2 . An adjacency matrix Â is
371 generated from A. In figure 2, we show the perturbed graphs A and Â.
                                    A                                        Â
      Figure 2: Left: the visualization of the perturbed A. Right: the visualization of the perturbed Â
                                                        11

