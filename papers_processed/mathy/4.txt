          Dirichlet Process Mixtures of Generalized Mallows Models
                           Marina Meilă                                         Harr Chen
                      Department of Statistics         Computer Science and Artificial Intelligence Laboratory
                      University of Washington                 Massachusetts Institute of Technology
                       Seattle, WA 98195-4322                         Cambridge, MA 02139-4307
                     mmp@stat.washington.edu                                harr@csail.mit.edu
                        Abstract                           properties [Lebanon and Mao, 2008, Meilă et al., 2007].
                                                           However, as an exponential family model, the GM is
      We present a Dirichlet process mixture model         unimodal and thus suitable only for a limited range of
      over discrete incomplete rankings and study          applications when applied in isolation. By incorporat-
      two Gibbs sampling inference techniques for          ing it in a model hierarchy like the DPM, we can enjoy
      estimating posterior clusterings. The first ap-      the benefits of a fully generative multimodal model of
      proach uses a slice sampling subcomponent            ranked data.
      for estimating cluster parameters. The sec-          To estimate the posterior of a DPM of GMs, we present
      ond approach marginalizes out several cluster        two Gibbs sampling approaches. In the first, we explic-
      parameters by taking advantage of approx-            itly draw all model parameters, relying on slice sam-
      imations to the conditional posteriors. We           pling for one of the posterior distributions. Our second
      empirically demonstrate (1) the effectiveness        approach marginalizes out several parameters by using
      of this approximation for improving conver-          approximations to the conditional posteriors, acceler-
      gence, (2) the benefits of the Dirichlet pro-        ating convergence at the cost of introducing error in
      cess model over alternative clustering tech-         the stationary distribution.
      niques for ranked data, and (3) the applica-
      bility of the approach to exploring large real-      We conduct three sets of experiments exploring the
      world ranking datasets.                              properties of our approach. First, we compare the
                                                           two sampling approaches and find that the approxi-
                                                           mation is beneficial for improving convergence. Sec-
1     MOTIVATION                                           ond, we study the performance of the DPM of GMs in
                                                           relation to previous clustering techniques for ranked
Dirichlet process mixtures (DPM) are among the most        data, showing improvements in held-out test likeli-
successful ways of modeling multimodal distributions       hood. Third, we conduct a qualitative analysis of a
in a nonparametric Bayesian framework. They pro-           large set of college admissions rankings, drawing con-
vide an elegant tradeoff between parameter sharing         clusions that build upon observations made in previous
and parameter variability between clusters, are ex-        work.
tremely versatile due to the flexibility in choosing base
distributions, and enjoy all the other advantages of a     2    GENERALIZED MALLOWS
fully generative probabilistic model. However, the fea-
                                                                MODELS
ture that makes the DPM model so useful – the fact
that it represents a fully nonparametric posterior –
                                                           This section provides background on the general-
also poses its greatest challenge, in that the posterior
                                                           ized Mallows model, following Fligner and Verducci
is not computable in closed form. Therefore inference
                                                           [1986]. Let π denote a permutation over the set
in a DPM must be performed using approximate tech-
                                                           {1, 2, 3, . . . , n}, where π(l) is the rank of element l
niques such as Monte Carlo sampling.
                                                           in π and π −1 (j) is the element at rank j. One
This paper introduces the DPM for the generalized          can uniquely determine any π by the n − 1 inte-
Mallows (GM) model, a family of distributions over         P V1 (π), V2 (π), . . . , Vn−1 (π) defined by Vj (π) =
                                                           gers
rankings. The GM has registered increasing popular-           l>j 1[l≺π j] . In words, Vj is the number of elements in
ity in recent years, partly because of a growing interest  {j + 1, . . . , n} that are ranked before j by π. It follows
in ranked data and partly for its elegant computational    that Vj takes values in {0, . . . , n − j}. Each element

Vj can be set independently in specifying a π, which is            GMs model for top-t rankings has sufficient statistics.
not true of π(l) values. These Vj ’s are called the code           In contrast, neither of these statements hold for Vj
of π.                                                              codes and GMV over top-t rankings.
This code can be defined w.r.t. to any reference per-              In the rest of this paper, we will be considering data
mutation σ by Vj (π | σ) ≡ Vj (πσ −1 ). For any π and              that consists of both full rankings and top-t rankings of
σ we define s1 , . . . , sn−1 to be a reciprocal form of the       varying lengths (a full ranking is simply a top-t ranking
code, by exchanging the places of σ and π:                         with t = n − 1). Thus our focus is on the GMs model,
                                            X                      and GM should be understood to refer to GMs .
              sj (π | σ) = Vj (σ | π) =           1[l≺σ j] .   (1)
                                           l≻π j                   2.2    SUFFICIENT STATISTICS AND
                                                                          CONJUGATE PRIOR
Equivalently, sj is equal to one less than the rank of
π −1 (j) in σ \ π −1 (1 : j − 1).                                  For a given permutation π we define matrix Rj (π) as
Based on this code representation, Fligner and Ver-                                Rj,ii′ (π) = 1[π−1 (j)=i and i′ 6≺π i] ,               (5)
ducci [1986] introduced the following family of expo-
nential models called the generalized Mallows (GM)                 and for a dataset π1:N of lengths t1:N we define
models:                                                            Rj (π1:N ) as
                                    P n−1                                                                  N
                                           θj sj (π|σ)
                                 e−   j=1
                                                                                                         X
                  GMθ,σs                                                                Rj (π1:N ) =            Rj (πk ).                 (6)
                       ~ (π)  =                        .       (2)
                                        ψ(~θ)                                                            k=1
                                                                   In words, each Rj corresponds to a rank j, and element
The GMs distribution is parametrized by the cen-
                                                                   Rj,ii′ counts how many times i was present at rank j,
tral permutation σ and concentration parameters ~θ ≡
                                                                   minus how many of those times i′ preceded i; Rj,ii = 0
θ1:n−1 ≥ 0; ψ(~      θ) is a normalization constant that does
                                                                   for all i, j. If the data consists of top-t rankings of
not depend on σ:
                                                                   different lengths, Rj (π1:N ) will depend only on those
                  n−1               n−1                            rankings of length at least j, and Rj (π1:N ) = 0 for
                                         1 − e−(n−j+1)θj
   ψ(~θ) =
                  Y                 Y
                      ψn−j (θj ) =                           . (3) j > max(t1:N ). For datasets of varying lengths, we
                  j=1               j=1
                                              1 − e−θj             will refer to max(t1:N ) as simply t.
The GMs model factors into a product of independent                For any top-t ranking π and complete ranking σ, we
univariate exponential models, one for each sj :                   have sj (π | σ) = Lσ (Rj (π)), where Lσ (A) denotes the
                                                                   sum of the elements in the lower triangle of matrix A,
                                          e−θj k                   after its rows and columns are permuted by σ [Meilă
                   P [sj (π|σ) = k] =                .         (4) and Bao, 2008].
                                        ψn−j (θj )
                                                                   Matrices R1:t (π1:N ) are the sufficient statistics of the
For θ1:n−1 = 0, GMs is the uniform distribution. For
                                                                   GM for both the central permutation σ and the param-
θ1:n−1 > 0, the GMs distribution has a unique maxi-
                                                                   eters θ~ [Meilă and Bao, 2008]. The existence of finite
mum at V1:n−1 = 0, i.e., at π = σ. Thus the GMs is
                                                                   sufficient statistics implies that the GM will have a
centered around σ with exponential decay controlled
     ~                                                             conjugate prior, whose parameters are an equivalent
by θ.
                                                                   sample size ν > 0, and a set of equivalent sufficient
One can replace sj (π | σ) with Vj (π | σ) in (2), ob-                                            0
                                                                   statistics of the form R1:t       . This prior is fully described
taining a GMV with similar form to GMs . These two                 by Meilă and Bao [2008].
models are equivalent only when all θj are equal.
                                                                   In many contexts, including our present clustering
                                                                   task, one desires to be uninformative w.r.t. to the cen-
2.1       TOP-t RANKINGS
                                                                   tral permutation while expressing knowledge about the
A permutation π is a top-t ranking when one only                   parameters ~θ. In this case, the prior has the form
observes the top t ranks (π −1 (1), . . . , π −1 (t)) rather               P 0 (σ, θ~ ; ν, r) ∝ e−ν
                                                                                                          P
                                                                                                             j [θj rj +ln ψn−j (θj )] ,   (7)
than the entire permutation. In a top-t ranking, the
codes s1:t are fixed while the remaining st+1:n−1 are              with r = [r1 r2 . . . rt ], rj > 0 being a vector of posi-
undetermined and can take any value in their respec-               tive parameters. This prior was used by Fligner and
tive ranges. For the GMs model the marginals w.r.t.                Verducci [1988]. The corresponding posterior is:
s1 , . . . , st for some t < n represent the probability of           P (σ, θ~ | ν, r, π1:N )
a top-t ranking (π −1 (1), . . . π −1 (t)) [Fligner and Ver-                  P
                                                                       ∝ e−      j [(νrj +Lσ (Rj (π1:N )))θj +(ν+Nj ) ln ψn−j (θj )]    , (8)
ducci, 1986]. Meilă and Bao [2008] showed that the

where Nj is the number of data elements of length                Algorithm Slice-Gibbs
at least j. The priors presented here are defined up
to a normalization constant. In general, there is no             Input Parameters ν, α, t, r1:t , T, TGibbs , TSlices, Data
closed-form expression for this constant.                            π1:N of lengths t1:N
In summary, the GM is an exponential family model                Output Samples c1:N , σc , ~θc
with simple sufficient statistics. Because the central
                                                                 Initialize c1:N , σc , θ~c randomly
permutation is an explicit parameter, this model is
both more interpretable and tractable than other (ex-            Repeat T times
ponential family) models over permutations. We use
it as a building block for the Dirichlet process mixture          1. Resample cluster assignments
model, which we briefly review below.                                For all points πi sample ci according to
                                                                         P [ci = c] ∼
3     DIRICHLET PROCESS                                                     (
                                                                                N−i,c                  ~
      MIXTURE MODELS                                                          N +α−1 GM (πi | σc , θc )              6 0
                                                                                                            if N−i,c =
                                                                                   α    (n−ti )!
                                                                              N +α−1       n!               if N−i,c = 0
A Dirichlet process mixture [DPM; Antoniak, 1974] is
a generative clustering model. Generating data π1:N                  If N−i,c = 0 for the sampled cluster, sample a new
from a DPM of GMs involves these steps:                              σc | πi and ρc | πi according to Step 2 below
                  G ∼   DP (α, P 0 (σ, θ~ | ν, r)),               2. Resample cluster centers
                  ~                                                  For all clusters c, repeat TGibbs times
             σi , θi ∼  G,
                 πi  ∼  GM (π | σi , θ~i ).                           (a) Sample σc by Sample-σ-Stagewise
                                                                      (b) Sample θ~c by Sample-θ-Slice
First, a discrete distribution G over GM distributions
is sampled from the Dirichlet process prior. This prior        Figure 1: Slice-Gibbs algorithm for estimating a
takes as a parameter a distribution over σ and θ,     ~ in our
                                                               DPM of GMs.
case the conjugate prior P 0 . Next, a specific GM dis-
tribution with parameters σi , ~   θi is drawn from G. Data
point πi is finally sampled from this GM distribution.         ods [Blei and Jordan, 2006]. We focus on the for-
                                                               mer approach, where the goal is to produce sam-
If we sample data sequentially from this model, then
                                                               ples drawn from the appropriate posterior distribu-
the (N + 1)th sample will be distributed according to
                                                               tion. In particular, if we are interested in parame-
                       1                        α              ter estimation, our objective is to draw samples from
   σN +1 , θ~N +1 ∼
                               X
                                   δσi ,θ~i +       P 0 . (9)
                     N +α                     N + α ν,r        P (c1:N , σ1:C , ~θ1:C | α, ν, r, π1:N ), where ci is the clus-
                               i≤N
                                                               ter assignment of data point πi , and each cluster c has
Hence, any finite sample will be a finite mixture              GM parameters (σc , θ~c ).
of GMs, allowing the DPM to represent ranking
                                                               While previous work [Neal, 2000] has made it straight-
data that is multimodal, with permutations clustered
                                                               forward to write the expression of this posterior (see
around several centers. Another characterization of
                                                               the following sections), our main challenge is in mak-
the DPM is that each data point πi is associated with
                                                               ing inference practical. Designing such methods and
a cluster label ci ∈ 1, . . . , C, and each cluster c with a
                                                               making them efficient for nontrivial model sizes n and
set of GM parameters σc and θ~c .
                                                               sample sizes N is the main contribution of this paper.
Unlike a finite mixture, the number of clusters in the
DPM is itself a random variable. It will grow with
                                                               4    THE SLICE-GIBBS SAMPLER
the size of the data in a way controlled by the concen-
tration parameter α. This makes DPM models ideal
                                                               We first present a naı̈ve Gibbs sampler for estimating
for scenarios where the number of mixture components
                                                               a DPM of GMs, following the approach of Neal [2000].
is not well-defined in advance. DPMs have found ex-
                                                               Our main goal is to build a Gibbs Markov chain over
tensive practical applications in areas such as topic
                                                               cluster assignments c1:N whose stationary distribution
modeling [Teh et al., 2006], natural language process-
                                                               is the desired model posterior. Taking advantage of
ing [Liang et al., 2007], vision [Sudderth et al., 2005],
                                                               exchangeability, we can sample each point πi ’s cluster
and computational biology [Rasmussen et al., 2009].
                                                               assignment ci as if it were the last point to be gen-
Bayesian inference in the DPM model is typically con-          erated, i.e., conditioned on the assignments of other
ducted via MCMC [Neal, 2000] or variational meth-              data points. Assuming the cluster parameters (σc , ~θc )

  Algorithm Sample-σ-Stagewise                                         Algorithm Sample-θ-Slice
  Input Parameters θ,       ~ sufficient statistics R1:t , prior       Input Parameters ν, t, r1:t , TSlices, statistics S1:t (σ)
                                                            0
       parameter ν, optional prior parameters R1:t
                                                                       Output Samples θ1:t
  Output Sample σ
                                    Pt                                 Initialize θ1:t according to previous sample
   1. Calculate matrix R = j=1 θj (Rj + νRj0 ), or R =
       Pt                                                              For j = 1 : t, repeat TSlices times
         j=1 θj Rj if prior for σ is uninformative
                                                                        1. Sample u ∼ Uniform(0, P̃ (θj )), where
   2. For j = 1 : n and while R 6= 0
       (a) Calculate column sums ρ1:n of R                                      P̃ (θj ) = e−(νrj +Sj (σ))θj −(ν+Nj ) ln ψn−j (θj ) (10)
       (b) Sample σ −1 (j) = i w.p. ∝ e−ρi
       (c) Set row and column σ −1 (j) of R to zero                     2. Determine slice [a, b] using step-out procedure
   3. Fill in remaining ranks of σ uniformly at random                  3. Repeatedly sample θj ∼ Uniform(a, b) until u <
       with items not yet selected                                          P̃ (θj ), shrinking [a, b] with rejected samples
Figure 2: Sample-σ-Stagewise algorithm for exactly                   Figure 3: Sample-θ-Slice algorithm for slice sam-
sampling σ from the conjugate posterior given θ.            ~        pling ~θ given σ.
are known, this yields the following resampling update               Lemma 2 P (σ | θ,       ~ ν, r, π1:N ) can be sampled exactly
for the cluster assignment of data point πi :                        by Algorithm Sample-σ-Stagewise (Figure 2).
                      ~
 P (ci = c | c−i , σ, θ)                                        (11) Sampling from θ~c | σc , πi∈c is more challenging. The
        N−i,c
                                     ~                               main obstacle to straightforward sampling is the un-
                   GM (πi | σc , θc )
        N +α−1                                                      known normalization factor of this distribution. How-
       
       
               if N−i,c 6= 0,
    ∝        α
                    R
                                        ~ 0 (σ, θ~ | ν, r)dσd~θ      ever, the posterior of each θ1:t is independent and uni-
       
         N +α−1      GM (πi | σ, θ)P                                modal.1 This suggests that slice sampling [Neal, 2003]
       
               if N−i,c = 0.
                                                                     is a viable way of drawing values for θ~c .
Here, N−i,c is the number of elements in cluster c,
                                                                     Lemma 3 P (~θ | σ, ν, r, πi∈c ) can be sampled using Al-
excluding data point i. In many applications of the
                                                                     gorithm Sample-θ-Slice (Figure 3).
DPM it is possible to integrate over cluster parameters
and explicitly sample only cluster assignments (known
                                                                     The structure of Algorithm Sample-θ-Slice follows
as collapsed sampling). In the case of the GM, despite
                                                                     directly from Neal [2000]. The full Slice-Gibbs sam-
our use of a conjugate prior the marginalization over
                                                                     pler, so named for its inclusion of a slice sampler, is
σ and ~θ is analytically intractable, in part because of
                                                                     presented in Figure 1. It alternates between resam-
the unknown normalization term. Thus for our first
                                                                     pling cluster assignments ci of data points and cluster
sampler we resort to building a Markov chain over the
                                                                     parameters σc and ~θc . Because the cluster parameters
state space (c1:N , σC , ~   θC ), where each variable is ex-
                                                                     themselves form a Gibbs chain, we take TGibbs steps
plicitly resampled conditioned on the other variables.
                                                                     to ensure convergence; furthermore, the slice sampler
The algorithm is presented in Figure 1, while its steps
                                                                     takes TSlices steps for each θj due to its serial correla-
are discussed in detail below.
                                                                     tion. In our experiments we find that TGibbs = 10 and
To sample ci | σ, θ~ as in (11) we need to calculate the             TSlices = 3 are typically sufficient values.
probabilities on the right hand side. This is straight-
forward for N−i,c > 0, using (2). For N−i,c = 0 we use
                                                                     5      THE BETA-GIBBS SAMPLER
the following Lemma (see Appendix for proofs).
Lemma 1 The marginal probability of a single obser-                  The previous section has demonstrated the difficulty
                                                                     of sampling from the conjugate posterior of a GM,
vation is P (πi | ν, r) = (n−t   n!
                                    i )!
                                         .
                                                                     and how it can be overcome by using slice sampling
                                                                     inside the Gibbs sampling step. We now present an
Next we need σc | ~     θc , πi∈c . Let Rj = Rj (πi∈c ) be the
                                                                     alternative approach in which several sampling steps
sufficient statistics of cluster c, and Sj (σc ) = Lσc (Rj ).
These statistics are input to the algorithm described                    1
                                                                           Only θ1:t needs to be sampled, as θt+1:n−1 does not
by Lemma 2.                                                          affect the rest of the sampling procedure.

  Algorithm Sample-σ-N1                                       Algorithm Beta-Gibbs
  Input Top-t ranking π, prior parameters rj , ν              Input Parameters ν, α, t, r1:t , T, TGibbs , TSlices, Data
                                                                   π1:N of lengths t1:N
  Output Sample σ
                                                              Output Samples c1:N , σc , ~θc
   1. For j = 1 : t
                                                              Initialize c1:N , σc , θ~c randomly
       (a) Sample Vj = k w.p. ∝ Beta(νrj + k, ν + 2)
            for k = 0 : n − j                                 Repeat T times
       (b) Place π(j) at the (Vj + 1)th previously unas-
            signed position of σ                               1. Resample cluster assignments
                                                                   For all points πi sample ci according to
   2. Fill the remaining ranks of σ uniformly at random
      with items not in π                                          P [ci = c] ∼
                                                                     
                                                                        N−i,c Qt        Beta(sj (πi |σc )+νrj +Sj (σc ),ν+Nc,j +2)
                                                                     N +α−1 j=1
                                                                                            Beta(νrj +Sj (σc ),ν+Nc,j +1)
Figure 4: Sample-σ-N1 algorithm for approximately
                                                                            if N−i,c 6= 0
sampling σ from the conjugate posterior when N = 1.                   α (n−ti )! if N
                                                                     
                                                                       N +α−1     n!           −i,c = 0
and marginalizations will be done in closed form. The              If N−i,c = 0 for the sampled cluster, sample a new
key insight is that the infinite generalized Mallows               σc |πi by Sample-σ-N1
model [Meilă and Bao, 2008] can be used to approxi-
                                                               2. Resample cluster centers
mate some of the sampling distributions.
                                                                   For all clusters c, if Nc > 1 repeat TGibbs times
The first result arises from the fact that as n → ∞
                                                                    (a) Sample σc | θ~c by Sample-σ-Stagewise
the normalization constant ψj approaches the value
ψ∞ (θ) = 1−e  1
                θ . This form of the normalization con-
                                                                    (b) Sample ~θc | σc using (12) from Lemma 4
stant permits several computations in closed form.
                                                                   If Nc = 1 sample σc by Sample-σ-N1
Lemma 4 [Meilă and Bao, 2008] If the number of
                                                            Figure 5: Beta-Gibbs algorithm for estimating a
items n is infinite and countable then:
                                                            DPM of GMs.
       P (θj | σ, ν, r, π1:N ) =
          Beta(e−θj ; νrj + Sj (σ), ν + Nj + 1),       (12) Lemma 5
       P (σ | ν, r, π1:N ) ∝                                    X n
          Qt                                                         B̃eta(s + a, N + 1, n) = B̃eta(a, N, n).               (16)
             j=1 Beta(νrj + Sj (σ), ν + Nj + 1).       (13)
                                                                s=0
In the above, (12) uses the Beta distribution, and (13)     This result also holds as n → ∞.
uses the Beta function; Nj is the number of rankings
of length at least j and Sj (σ) is again Lσ (Rj (π1:N )).   Lemma 6 Marginalizing over θ~ for a single π yields:
For the finite case, we define an analogue to the Beta      P (π | σ, ν, r, πi∈c ) =                                            (17)
function that arises in the marginalization of θ:  ~
                                                                t
                                                               Y    B̃eta(sj (π | σ) + νrj + Sj (σ), ν + Nj + 2, n − j)
                          ∞                     −b+1                                                                             .
                                   1 − e−(n+1)θ                            B̃eta(νrj + Sj (σ), ν + Nj + 1, n − j)
                      Z          
   B̃eta(a, b, n) ≡         e−θa                      dθ.      j=1
                        0            1 − e−θ
                                                       (14) Lemma 7 P (σ | ν, r, π) (i.e., when N = 1) can
Using this representation it can be easily verified that    be sampled approximately by Algorithm Sample-σ-
for finite n,                                               N1 (Figure 4).
P (σ | ν, r, π1:N )                                         Lemmas 6 and 7, together with Lemmas 1 and 2, allow
                                                            us to (approximately) marginalize out the continuous ~θ
      Qt
   ∝ j=1 B̃eta(νrj + Sj (σ), ν + Nj + 1, n − j). (15)
                                                            parameters for much of the sampling. This algorithm,
Note that as n → ∞, B̃eta(a, b, n) → Beta(a, b),            called Beta-Gibbs because of the extensive use of the
which will form the core of our approximation. We           Beta function, is given in Figure 5. The algorithm
can now show the following Lemmas (see Appendix             approximates B̃eta(a, b, n) with the easily computable
for proofs).                                                Beta(a, b) for sampling P (~θc | σc ), P (ci | σc ) when

                                                                                                              n = 10                                      n = 20
                   0
                  10            Dataset 1                             0
                                                                     10            Dataset 2                                                                              −3
    VI distance                                        VI distance
                                                                                                                                                                   x 10
                                                                                                                 0.05                                          5
                   −2                                                 −2
                                                                                               Beta−Gibbs
                  10                                                 10                        Slice−Gibbs                                                     0
                                                                                                                   0
                   −4                                                 −4
                  10                                                 10                                                                                       −5
                   0                                                  0                                       −0.05
                       0   50   100 150 200 250                           0   50   100 150 200 250                                                        −10
                                iterations                                         iterations
                                                                                                                 −0.1
                                                                                                                                                          −15
                   0                                                  0
                  10            Dataset 3                            10            Dataset 4
    VI distance                                        VI distance
                                                                                                              −0.15                                       −20
                   −2                                                 −2
                                                                                                                   2    4   6          8             10      2                 4        6    8   10
                  10                                                 10
                   −4                                                 −4
                                                                                                                                n = 50
                  10                                                 10                                                                     −4
                                                                                                                                     x 10
                   0                                                  0                                                          2
                       0   50   100 150 200 250                           0   50   100 150 200 250
                                iterations                                         iterations
                                                                                                                                 0
                                                                                                                                −2
Figure 6: Performance of Slice-Gibbs and Beta-                                                                                  −4
                                                                                                                                                                                         4
                                                                                                                                                                                         8
Gibbs on four artificial datasets, averaged over ten                                                                            −6                                                      16
                                                                                                                                                                                        32
replicates. Each plot displays VI distance to the true                                                                           2               4        6           8            10
data labeling. Lower is better.
                                                                                                             Figure 7: Relative error (1 − Beta(a, b)/B̃eta(a, b, n))
                                                                                                             as a function of a, the equivalent number of inversions,
N−i,c > 0, and P (σc | π) when Nc = 1. For the
                                                                                                             for various values of b, the equivalent sample size plus
resampling of σc for non-singleton clusters, we resort
                                                                                                             two, for n = 10, 20, 50.
to an inner Gibbs sampler, setting TGibbs to 10 as with
Slice-Gibbs.
                                                                                                             we note that each iteration of Slice-Gibbs is typically
6                 EMPIRICAL COMPARISON OF                                                                    slower than Beta-Gibbs, due to the additional itera-
                  SLICE-GIBBS AND BETA-GIBBS                                                                 tions of slice sampling. Interestingly, the comparison
                                                                                                             does not change when complete rankings are observed
The purpose of introducing the Beta-Gibbs sampler                                                            (datasets 2 and 4), where the Beta approximation of
was (1) to make the resampling of the parameters more                                                        B̃eta is poorest. These results support the use of the
efficient, and (2) more importantly, to reduce variance                                                      Beta-Gibbs approximation for estimating a DPM of
and accelerate convergence to the stationary distribu-                                                       GMs in the general case.
tion, which is a typical effect of marginalizing over cer-                                                   We also assessed the quality of our Beta(a, b) approx-
tain parameters. We now verify how well we succeeded                                                         imations w.r.t. the correct values B̃eta(a, b, n) in Fig-
by running experiments on four artificial datasets un-                                                       ure 7. The approximation will be more accurate for
der varying conditions. For each experiment, we gen-                                                         larger n, so we consider the values n = 10, 20, 50 and
erate 500 points from each of 10 clusters for a total of                                                     ν = 1. The relative error is largest for small b and
N = 5000 samples. Each cluster’s points are generated                                                        large a. This would occur in very small clusters, and
from a GM with true σ ∗ and θ~∗ given below. To ensure                                                       worsens when consensus worsens, when the parameter
that the dataset is not too easily separable, each σ ∗ is                                                    prior is more diffuse (i.e., with large rj ), and when j is
drawn from the conjugate posterior of σ, conditioned                                                         higher. The effect is to overestimate the θj ’s for higher
on 100 permutations drawn randomly from a GM with                                                            ranks and the probability of assigning points to small
~θ = 0.7.                                                                                                    clusters.
                  Dataset          n        t    ~
                                                 θ∗
                        1          20       10   θi∗   =1                                                    7      COMPARISON TO RELATED
                        2          20       19   θi∗   =1
                        3          20       10   θi∗   = 1.5 − (i − 1) × 0.1
                                                                                                                    WORK
                        4          20       19   θi∗   = 1.5 − (i − 1) × 0.05
                                                                                                             Modeling of multimodal ranking data has been at-
We measure the Variation of Information (VI) dis-                                                            tempted in a variety of paradigms. For instance, top-
tance [Meilă, 2007] between the sampled and true clus-                                                      t ranked data representing votes in Irish elections has
terings at each iteration. We average over ten runs for                                                      been modeled as mixtures of Plackett-Luce and Benter
each dataset, initializing randomly with 20 clusters.                                                        models by Gormley and Murphy [2008a] and Gormley
Priors α, ν, and r1:t are all set to one.                                                                    and Murphy [2008b], respectively. Busse et al. [2007]
                                                                                                             developed an EM algorithm for estimating finite mix-
Figure 6 shows the results of this experiment. In ev-
                                                                                                             tures of GM models for top-t rankings.
ery case, Beta-Gibbs converges to the true clustering
much more rapidly than Slice-Gibbs. Furthermore                                                              Of the nonparametric methods, the most flexible and

theoretically principled is the Kernel Density Estima-                          −2
tor (KDE) of Lebanon and Mao [2008], in which the
kernel is the GM model with θj = θ, and where the                               −3
data is partial rankings of a given type. One of the
main algorithmic contributions of Lebanon and Mao is                            −4
                                                               log−likelihood
the tractable evaluation of the kernel, which includes                                                                      true
summation over entire cosets of super-exponential car-                                                                      DPMM
                                                                                −5
dinalities. Meilă and Bao [2008] introduced the expo-                                                                      EBMS
nential blurring mean-shift (EBMS) clustering algo-                                                                         KDE
                                                                                −6
rithm, which also uses the GM model with all-equal
θj ’s as the kernel. The algorithm features a heuris-
tic method for estimating the kernel width θ and does                           −7
not need a stopping rule, so it requires no outside pa-
rameters. Finally, Guiver and Snelson [2009] present                            −8
                                                                                     50       150    400   1100   2980   8100    22030
an example of elegant Bayesian estimation where in-                                                         N
tractable inference in the Plackett-Luce model is ap-                            1.5
proximated efficiently.                                                                                                  DPMM train
The most relevant comparisons to this work are with                                                                      EBMS train
the nonparametric approaches of EBMS [Meilă and                                     1                                   DPMM test
Bao, 2008] and KDE [Lebanon and Mao, 2008]. The                                                                          EBMS test
                                                               VI distance
former is not a generative model, and has various para-                                                                  true test
metric limitations: the kernel width θ is represented by                         0.5
a single parameter and the output rankings are trun-
cated at some user-set standard length. The latter
model is generative, and applies to any type of par-                                 0
tial rankings, which is beyond the scope of our current
model. On the other hand, KDE has only one parame-
ter for kernel width θ, just like EBMS, and this param-                         −0.5
eter must be manually set. This is a limitation since                                    50    150   400   1100   2980    8100   22030
                                                                                                            N
in many instances of real-world data higher ranks are
more concentrated around the mean than lower ranks.
                                                               Figure 8: Performance of DPM, EBMS, and KDE on a
                                                               mixture of K = 3 Mallows models, with n = 12, t = 5,
7.1   EXPERIMENTS
                                                               and training sample sizes N = 100, . . . , 10000, aver-
We now conduct experimental comparisons of DPM,                aged over 10 replicates. The test set size is 3000. Top:
EBMS, and KDE on both artificial and real data. For            test set log-likelihood, higher is better; bottom: VI dis-
the former, the data is generated from a fixed mixture         tance to true data labeling, lower is better. EBMS was
model with K = 3 mixture components, all having the            too slow for the larger N ’s.
same single parameter θ1:t = 1.2 We set n = 12, t = 5,
and varied N from 100 to 10000. We fit each of the
                                                               The results are shown in Figure 8. One sees that
three models to this data and calculate log-likelihood
on a held-out test set. For DPM and EBMS, we also              DPM, even though it has more parameters than neces-
calculate the quality of the obtained clustering using         sary to explain the data, clearly performs better than
                                                               EBMS and KDE in terms of likelihood, being almost
VI distance [Meilă, 2007]. To give an idea of the best
achievable performance, we use the same criteria to            equal to the true model. The σ and θ estimates are also
evaluate the true model. For KDE the kernel width              centered on the true values (not shown). On VI dis-
                                                               tance, the heuristic EBMS performs surprisingly well
is set to the true θ. These conditions are the most
favorable for the competing alternatives to the DPM.3          on the training data, occasionally surpassing DPM,
                                                               but produces poor clusters on the test data. The VI
    2                                                          of the true model shows that this is a case of well-
      An experiment with K = 30 clusters produces similar
results, though then EBMS does not scale well to large n.      separated mixtures, but not a trivial one.
    3
      The chosen kernel width is not provably optimal for      We     run     a    similar    comparison      on    the
KDE, as the optimal kernel width varies with the sample
size. However, we have tested the KDE model under a            Jester dataset [Goldberg et al., 2001], which
wide range of sample sizes, and it is very likely that θ = 1   consists of joke preferences for a large group of people.
is near optimal for at least one of these.                     Of the 100 available jokes we restrict the data to the

 Rank                   Cluster 1 (8.1%)           Cluster 2 (6.2%)          Cluster 3 (6.0%)                Cluster 4 (5.8%)            Cluster 5 (5.7%)
    1                   Business (Dublin)          Comp Appl (Dublin)        Business (Limerick)             Engineering (Dublin)        Comp Appl (Dublin)
    2                   Commerce (Dublin)          Comp Sys (Limerick)       Commerce (Galway)               Engineering (Galway)        Engineering (Dublin)
    3                   Business (Dublin)          Software Dev (Limerick)   Commerce (Cork)                 Civil Eng (Galway)          Engineering (Dublin)
    4                   Marketing (Dublin)         Comp Appl (Cork)          Business (Waterford)            Engineering (Dublin)        Comp Sci (Dublin)
    5                   Marketing (Dublin)         Appl Comp (Waterford)     Humanities (Galway)             Elec Eng (Limerick)         Comp Sci (Dublin)
    6                   Humanities (Dublin)        Comp Network (Carlow)     Humanities (Cork)               Mech Eng (Limerick)         Science (Dublin)
    7                   Bus & Econ (Dublin)        Software Dev (Cork)       Admin (Limerick)                Engineering (Dublin)        Engineering (Dublin)
    8                   Bus & Legal (Dublin)       Software (Athlone)        Business (Dublin)               Elec Eng (Galway)           Comp Sci (Dublin)
    9                   Acct & Fin (Dublin)        Comp Sci (Maynooth)       Business (Dublin)               Info Tech (Limerick)        Computing (Dublin)
   10                   Acct & HR (Dublin)         Info Tech (Limerick)      Comp Sys (Limerick)             Civil Eng (Cork)            Comp Sci (Maynooth)
                        Business, Dublin           Comp Sci, ex-Dublin       Business, ex-Dublin             Engineering                 Comp Sci, Dublin
Table 1: Top courses of the five largest clusters found in a representative run of the DPM over college admissions
data. Proportions of the data assigned to each cluster is shown next to the cluster number. We list course names
and school locations, and summarize the theme of each cluster.
                        −8                                                                           4
                                                                                                    10
                        −9
                                                                                                     3
                       −10                                                                          10
      log−likelihood
                       −11
                                                                                             size
                                                                                                     2
                                                                 DPMM                               10
                       −12                                       KDE0.03
                                                                 KDE0.1
                       −13                                       KDE0.4                              1
                                                                                                    10
                                                                 KDE1
                       −14
                                                                                                     0
                       −15                                                                          10
                                                                                                         0           20             40          60
                       −16                                                                                                cluster
                          50    150     400        1100   2980    8100
                                               N
                                                                                    Figure 10: Cluster size distribution for a representative
Figure 9: Test set log-likelihood of DPM and KDE on                                 run of the college applications data, N = 53757. Out
the Jester data, with n = 70, t = 5 and training                                    of 177 clusters, 114 were singletons, 35 had at least 54
sample sizes N = 100, 1000, 3000, averaged over 10                                  points (0.1%), and 27 had at least 538 points (1%).
replicates. Higher is better. The test set size is 3000.
Further reducing the kernel width for KDE leads to
results almost identical to the width 0.03 case.                                    ing, running four samplers to 500 iterations each. The
                                                                                    four runs yield between 23 and 27 substantial clusters
                                                                                    (those with at least 1% of the data points), with similar
n = 70 most frequently rated, and to the top t = 5                                  central permutations recurring across runs. Figure 10
rankings. As Figure 9 shows, we again observe that                                  illustrates the sizes of the induced clusters.
DPM outperforms KDE over all kernel widths tried.
DPM finds between 4 and 9 clusters in 10 trials, with                               Table 1 displays the top-10 courses from the central
θj in the range (0.03, 0.06) for all j’s and N ’s.                                  permutations of the largest clusters of a representa-
                                                                                    tive run. The results show clear thematic consistency
                                                                                    in the top ranked courses by vocation and/or loca-
8   ANALYSIS OF COLLEGE                                                             tion, concordant in particular with Gormley and Mur-
    COURSE RANKINGS                                                                 phy’s observation of the “frequent distinction between
                                                                                    sets of applicants who apply for degrees of a simi-
We also conduct an analysis of Irish third-level col-                               lar discipline but are deemed separate on the basis
lege applications, where prospective students rank up                               of whether or not the institutions to which they ap-
to ten preferred academic courses across a number of                                ply are in Dublin.” Notably, their analysis revealed
schools [Gormley and Murphy, 2006]. In combination                                  distinct clusters of computer science preferences, one
with examination scores, this data is used by the Cen-                              for Dublin-based schools and one with regional varia-
tral Applications Office (http://www.cao.ie/) to de-                                tion. We additionally find a clear separation between
termine placements into third-level degree programs.                                Dublin-based business programs and outside business
                                                                                    programs, a phenomenon that was observed by Gorm-
The dataset consists of N = 53757 students in the year
                                                                                    ley and Murphy but not explicitly identified by their
2000 selecting from n = 533 courses and ranking up
                                                                                    clustering.
to t = 10 of them. To facilitate a comparison with
previous work [Gormley and Murphy, 2006], we set α                                  We can also interpret the posterior samples of ~θ to
and ν to 100 so as to induce a finer-grained cluster-                               gain insight into data separation by rank, which is an

                                                             cally, the rich combinatorial structure of the parameter
        3
                                   All Clusters              space allowed us to perform explicit marginalizations
                                   Small Clusters            and normalizations in special cases. Computationally,
     2.5
                                   Large Clusters            we exploited the special structure of the Rj sufficient
                                                             statistics and of the Lσ operator, thereby eliminating
        2
                                                             n from the most intensive computations. While the
θj   1.5                                                     faster Beta-Gibbs algorithm uses approximate pos-
                                                             teriors, we have verified empirically the quality of that
        1                                                    approximation and the advantages it yields to conver-
                                                             gence.
     0.5                                                     Our algorithm works with informative priors as well,
                                                             with only a minor modification (replacing Sample-σ-
        0                                                    N1). One avenue of future work is to explore other
            1   2   3   4   5 6    7   8   9 10
                            rank                             sampling schemes than the one described by Neal
                                                             [2000], such as split-merge algorithms [Jain and Neal,
                                                             2007].
Figure 11: Average of θj weighted by cluster size as
a function of rank j for the college application data,       Acknowledgments
replicated across four runs. The decreasing trend sup-
                                                             Harr Chen is supported by a National Science Foun-
ports the intuition that top course preferences are bet-
                                                             dation Graduate Fellowship.
ter separated than less-desired choices.
                                                             Appendix
advantage of using the GM for modeling clusters com-
                                                             Proof of Lemma 1 The marginal of a single π of
pared to the Plackett-Luce model. We compute an
                                                             length t is
average of each θj , weighted by cluster size, across the
four runs. We also perform this analysis for only large                  XZ ∞
                                                                Z1 (π) =                            ~ ν, r)d~θ.
                                                                                            ~ 0 (σ, θ;
                                                                              GM s (πi ; σ, θ)P
and only small clusters, thresholding at a size of 5%                              0
                                                                             σ
of the data points (splitting the data points roughly                                                                   (18)
equally into large and small). Figure 11 presents these      By Lemma 6, the integral is equal to
averages. The clearly decreasing overall trend rein-
forces the intuition that top-ranked choices tend to be              t
                                                                 1 Y B̃eta(νrj + sj (π | σ), ν + 2, n − j)
more coherent and distinctive than later entries in the                                                    .            (19)
                                                                 n! j=1    B̃eta(νrj , ν + 1, n − j)
top-10 ranking. Furthermore, we find that small clus-
ters tend to diverge less at top ranks than large clus-
                                                             Note that sj (π | σ) = Vj (σ | π), where Vj (σ | π)
ters, but this trend reverses around the fourth rank. A
                                                             should be read as “the rank in σ of item j of π” and
qualitative examination of the data suggests that this
                                                             is therefore well defined for j = 1 : t.
may be because small clusters tend to correspond to
more specialized interests with fewer relevant courses       Any configuration of Vj ’s uniquely determines a subset
(e.g., courses at one specific smaller school), leaving      of the positions in σ, and the Vj ’s can take any value in
fewer choices for the top ranks but allowing for greater     their admissible range when σ ranges over all infinite
divergence later on.4                                        permutations. Thus, sj (π | σ) ranges from 0 to n −
                                                             j, and consequently the summation over σ commutes
9       DISCUSSION                                           with the product over j. For every configuration of
                                                             s1:t , there will be (n − t)! different permutations with
                                                             that configuration. It follows that:
We introduced nonparametric Bayesian DPMs on
ranked data domains with top-t rankings of variable             t
                                                               XY
lengths. Our inference algorithms are able to run on                    B̃eta(νrj + sj (π | σ), ν + 2, n − j)
substantial dataset sizes and large n’s.                        σ j=1
                                                                                 t n−j
We leveraged a combination of statistical and compu-                             Y X
                                                                 =    (n − t)!               B̃eta(νrj + sj , ν + 2, n − j)
tational insights in developing our techniques. Statisti-
                                                                                 j=1 sj =0
    4
    In fact, the average ranking length t for data points                        t
                                                                                 Y
in small clusters is shorter than for large clusters: 6.15       =    (n − t)!         B̃eta(νrj , ν + 1, n − j).
compared to 6.63.                                                                j=1

The last equality is obtained from Lemma 5. Hence,                D. M. Blei and M. I. Jordan. Variational inference for
Z1 (π) = (n − t)!/n!.                                                Dirichlet process mixtures. Bayes Anal, 1(1):121–144,
                                                                     2006.
Proof of Lemma 2 From Meilă and Bao [2008], for                  L. M. Busse, P. Orbanz, and J. Bühmann. Cluster analysis
any given θ,                                                         of heterogeneous rank data. In Proceedings of ICML,
                                                                     2007.
   P (σ | θ, π1:N , ν, r)
               Pt                                                 M. A. Fligner and J. S. Verducci. Distance based ranking
      ∝ e− j=1 [θj (Lσ (Rj (π1:N ))+νrj )+(N +ν) ln ψn−j (θj )]      models. J Roy Stat Soc B Met, 48(3):359–369, 1986.
               Pt
      ∝ e− j=1 θj Lσ (Rj (π1:n )) = e−Lσ (R) .                    M. A. Fligner and J. S. Verducci. Multistage ranking mod-
                                                                     els. J Am Stat Assoc, 83(403):892–901, 1988.
We use a key observation of Meilă et al. [2007], which           K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigen-
is that for a distribution over permutations like the one            taste: A constant time collaborative filtering algorithm.
above, the first rank of σ is distributed proportionally             Inform Retrieval, 4(2):133–151, 2001.
to the column sums of R, the second rank is distributed           I. C. Gormley and T. B. Murphy. Analysis of Irish third-
proportionally to the column sums of R after deleting                level college applications data. J Roy Stat Soc A Sta,
                                                                     169(2):361–379, 2006.
row and column σ −1 (1), etc. Hence, the ranks of σ
can be sampled sequentially by                                    I. C. Gormley and T. B. Murphy. Exploring voting blocs
                                                                     within the Irish electorate: a mixture modeling ap-
                                                                     proach. J Am Stat Assoc, 103(483):1014–1027, 2008a.
                                      P
          P (σ −1 (1) = k) ∝       e−   i Rik
                                              ,              (20)
                                                                  I. C. Gormley and T. B. Murphy. A mixture of experts
                        ...           P                              model for rank data with applications in election studies.
               −1
          P (σ (j) = k) ∝          e−   i6∈σ−1 (1:j−1) Rik
                                                           . (21)    Ann Appl Stat, 2(4):1452–1477, 2008b.
                                                                  J. Guiver and E. Snelson. Bayesian estimation for Plackett-
Proof of Lemma 3 This follows from Neal [2003].                      Luce ranking models. In Proceedings of ICML, 2009.
                                                                  S. Jain and R. M. Neal. Splitting and merging compo-
Proof of Lemma 5                                                     nents of a nonconjugate Dirichlet process mixture model.
  X n                                                                Bayes Anal, 2(3):445–472, 2007.
       B̃eta(s + a, b, n)                                         G. Lebanon and Y. Mao. Non-parametric modeling of par-
  s=0                                                                tially ranked data. J Mach Learn Res, 9:2401–2429,
          ∞X  n                               −b+1                  2008.
                              1 − e−(n+1)θj
      Z                     
   =             e−θ(s+a)                              dθ         P. Liang, S. Petrov, M. I. Jordan, and D. Klein. The in-
        0    s=0
                                1 − e−θj                             finite PCFG using hierarchical Dirichlet processes. In
      Z ∞                                              −b+1         Proceedings of EMNLP, 2007.
                   1 − e−(n+1)θj 1 − e−(n+1)θj
                                  
   =         e−θa                                            dθ   M. Meilă. Comparing clusterings—an information based
        0            1 − e−θj          1 − e−θj                      distance. J Multivariate Anal, 98:873–895, 2007.
   = B̃eta(a, b − 1, n).                                          M. Meilă and L. Bao. Estimation and clustering with infi-
                                                                     nite rankings. In Proceedings of UAI, 2008.
Proof of Lemma 6 This follows by direct calculus.                 M. Meilă, K. Phadnis, A. Patterson, and J. Bilmes. Con-
                                                                     sensus ranking under the exponential model. In Proceed-
Proof of Lemma 7 The crucial observation here is                     ings of UAI, 2007.
the same as in Lemma 1: since N = 1, Lσ (Rj (π)) =                R. M. Neal. Markov chain sampling methods for Dirichlet
sj (π | σ) = Vj (σ | π) by (1). As a consequence, the                process mixture models. J Comput Graph Stat, 9(2):
posterior of σ is a product of multinomials, one for                 249–265, 2000.
each j = 1 : t:                                                   R. M. Neal. Slice sampling. Ann Stat, 31(3):705–767, 2003.
                                                                  C. E. Rasmussen, B. J. de la Cruz, Z. Ghahramani, and
      P [Vj = v] ∝ B̃eta(νrj + v, ν + 2, n − j).             (22)    D. L. Wild. Modeling and visualizing uncertainty in
                                                                     gene expression clusters using Dirichlet process mix-
We approximate B̃eta(a, b, n) by Beta(a, b). After Vj                tures. IEEE/ACM T Comput Bi, 6(4):615–628, 2009.
is sampled, to construct σ one places π −1 (j) in the
                                                                  E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S.
Vjth available position in σ. (See Meilă et al. [2007] for          Willsky. Describing visual scenes using transformed
the detailed proof of this procedure.) The remaining                 dirichlet processes. In Advances in NIPS, 2005.
n − t positions are filled uniformly at random from the           Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
items not in π.                                                      Hierarchical Dirichlet processes. J Am Stat Assoc, 101
                                                                     (476):1566–1581, 2006.
References
C. E. Antoniak. Mixtures of Dirichlet processes with appli-
   cations to Bayesian nonparametric problems. Ann Stat,
   2(6):1152–1174, 1974.

