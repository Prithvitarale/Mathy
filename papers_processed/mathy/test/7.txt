Fair Algorithms for Infinite and Contextual Bandits
Matthew Joseph âˆ—1
Michael Kearns â€ 1
Jamie Morgenstern â€¡1
Seth Neel Â§ 2
Aaron Roth Â¶ 1
1
Computer and Information Science, University of Pennsylvania
2
Statistics Department, The Wharton School, University of Pennsylvania
Abstract
We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced
in Joseph et al. [2016], we carry out a more refined analysis of a more general problem, achieving better
performance guarantees with fewer modelling assumptions on the number and structure of available
choices as well as the number selected. We also analyze the previously-unstudied question of fairness in
infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds
demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic
fairness in an online linear setting that is substantially more powerful, general, and realistic than the
current state of the art.
1 Introduction
The problem of repeatedly making choices and learning from choice feedback arises in a variety of settings,
including granting loans, serving ads, and hiring. Encoding these problems in a bandit setting enables one
to take advantage of a rich body of existing bandit algorithms. UCB-style algorithms, for example, are
guaranteed to yield no-regret policies for these problems.
Joseph et al. [2016], however, raises the concern that these no-regret policies may be unfair: in some
rounds, they will choose options with lower expected rewards over options with higher expected rewards,
for example choosing less qualified job applicants over more qualified ones. Consider a UCB-like algorithm
aiming to hire all qualified applicants in every round. As time goes on, any no-regret algorithm must behave
unfairly for a vanishing fraction of rounds, but the total number of mistreated people â€“ in hiring, people who
saw a less qualified job applicant hired in a round in which they themselves were not hired â€“ can be large
(see Figure 1).
Figure 1: Cumulative mistreatments for
UCB. See Section 7.7 in supplement for
details and additional experimental evalu-
ation of the structure of mistreatment.
Joseph et al. [2016] then design no-regret algorithms which
minimize mistreatment and are fair in the following sense: their
algorithms (with high probability) never at any round place higher
selection probability on a less qualified applicant than on a more
qualified applicant. However, their analysis assumes that there are
k well-defined groups, each with its own mapping from features
to expected rewards; at each round exactly one individual from
each group arrives; and exactly one individual is chosen in each
round. In the hiring setting, this equates to assuming that a
company receives one job applicant from each group and must
âˆ—majos@cis.upenn.edu
â€ mkearns@cis.upenn.edu
â€¡jamiemor@cis.upenn.edu
Â§sethneel@wharton.upenn.edu
Â¶aaroth@cis.upenn.edu
1
arXiv:1610.09559v4
[cs.LG]
29
Jun
2017
hire exactly one (rather than m or all qualified applicants) introducing an unrealistic element of competition
and unfairness both between applicants and between groups.
The aforementioned assumptions are unrealistic in many practical settings; our work shows they are
also unnecessary. Meritocratic fairness can be defined without reference to groups, and algorithms can
satisfy the strictest form of meritocratic fairness without any knowledge of group membership. Even without
this knowledge, we design algorithms which will be fair with respect to any possible group structure over
individuals. In Section 2, we present this general definition of fairness. The definition further allows for the
number of individuals arriving in any round to vary, and is sufficiently flexible to apply to settings where
algorithms can select m âˆˆ [k] individuals in each round. By virtue of the definition making no reference to
groups, the model makes no assumptions about how many individuals arriving at time t belong to any group.
A company can then consider a large pool of applicants, not necessarily stratified by race or gender, with an
arbitrary number of candidates from any one of these populations, and hire one or m or even every qualified
applicant.
We then present a framework for designing meritocratically fair online linear contextual bandit algorithms.
In Section 3, we show how to design fair algorithms when at most some finite number k of individuals
arrives in any round (the linear contextual bandits problem [Abe et al., 2003, Auer, 2002]), as well as when
m individuals may be chosen in each round (the â€œmultiple play" introduced and studied absent fairness
in Anantharam et al. [1987]). We therefore study a much more general model than [Joseph et al., 2016] and,
in Section 3, substantially improve upon their black-box regret guarantees for linear bandit problems using a
technical analysis specific to the linear setting.
However, these regret bounds still scale (polynomially) with k, the maximum number of individuals seen
in any given round. This may be undesirable for large k, thus motivating the investigation of fair algorithms
for the infinite bandit setting (the online linear optimization with bandit feedback problem Flaxman et al.
[2005]).1
In Section 4 we provide such an algorithm via an adaptation of our general confidence interval-based
framework that takes advantage of the fact that optimal solutions to linear programs must be extreme points
of the feasible region. We then prove, subject to certain assumptions, a regret upper bound that depends on
âˆ†gap, an instance-dependent parameter based on the distance between the best and second-best extreme
points in a given choice set.
In Section 5 we show that this instance dependence is almost tight by exhibiting an infinite choice set
satisfying our assumptions for which any fair algorithm must incur regret dependent polynomially on âˆ†gap,
separating this setting from the online linear optimization setting absent a fairness constraint. Finally, we
justify our assumptions on the choice set by in Section 6 exhibiting a choice set that both violates our
assumptions and admits no fair algorithm with nontrivial regret guarantees. A condensed presentation of our
methods and results appears in Figure 2.
Finally, we note that our algorithms share an overarching logic for reasoning about fairness. These
algorithms all satisfy fairness by certifying optimality, never giving preferential treatment to x over y unless
the algorithm is certain that x has higher reward than y. The algorithms accomplish this by computing
confidence intervals around the estimated rewards for individuals. If two individuals have overlapping
confidence intervals, we say they are linked; if x can be reached from y using a sequence of linked individuals,
we say they are chained.
1.1 Related Work and Discussion of Our Fairness Definition
Fairness in machine learning has seen substantial recent growth as a subject of study, and many different
definitions of fairness exist. We provide a brief overview here; see e.g. Berk et al. [2017] and Corbett-Davies
et al. [2017] for detailed descriptions and comparisons of these definitions.
Many extant fairness notions are predicated on the existence of groups, and aim to guarantee that certain
groups are not unequally favored or mistreated. In this vein, Hardt et al. [2016] introduced the notion
of equality of opportunity, which requires that a classifierâ€™s predicted outcome should be independent of a
1We note that both the finite and infinite settings have infinite numbers of potential candidates: the difference arises in how
many choices an algorithm has in a given round.
2
# selected
each round
# options
each round
Technique Notes Regret
Exactly
j â‰¤ k
â‰¤ k
Play all of chains in
descending order,
randomizing over
last chain as necessary
to pick exactly j
Requires
randomness
OÌƒ

dkj
âˆš
T

Unconstrained â‰¤ k Select all in every
chain with highest
UCB > 0
Deterministic OÌƒ

dk2
âˆš
T

Exactly 1
âˆž
bounded
convex set
âˆ†gap > 0
Play uniquely best point
or UAR from entire set
Requires
randomness
OÌƒ c Â· log(T)/âˆ†2
gap

â„¦Ìƒ(1/âˆ†gap)
â„¦(T) for âˆ†gap = 0
Figure 2: A description of various settings in which our framework provides fair algorithms. In all cases,
fairness can be imposed only across pairs for any partitioning of the input space; the bounds here assume they
bind across all pairs, and are therefore worst-case upper bounds. See Section 4 for a complete explanation of
the distribution-dependent constant c in the regret bound for the infinite case.
protected attribute (such as race) conditioned on the true outcome, and they and Woodworth et al. [2017]
have studied the feasibility and possible relaxations thereof. Similarly, Zafar et al. [2017] analyzed an
equivalent concurrent notion of (un)fairness they call disparate mistreatment. Separately, Kleinberg et al.
[2017] and Chouldechova [2017] showed that different notions of group fairness may (and sometimes must)
conflict with one another.
This paper, like Joseph et al. [2016], departs from the work above in a number of ways. We attempt to
capture a particular notion of individual and weakly meritocratic fairness that holds throughout the learning
process. This was inspired by Dwork et al. [2012], who suggest fair treatment equates to treating â€œsimilarâ€
people similarly, where similarity is defined with respect to an assumed pre-specified task-specific metric.
Taking the fairness formulation of Joseph et al. [2016] as our starting point, our definition of fairness does not
promise to correct for past inequities or inaccurate or biased data. Instead, it assumes the existence of an
accurate mapping from features to true quality for the task at hand2
and promises fairness while learning
and using this mapping in the following sense: any individual who is currently more qualified (for a job, loan,
or college acceptance) than another individual will always have at least as good a chance of selection as the
less qualified individual.
The one-sided nature of this guarantee, as well as its formulation in terms of quality, leads to the name
weakly meritocratic fairness. Weakly meritocratic fairness may then be interpreted as a minimal guarantee of
fairness: an algorithm satisfying our fairness definition cannot favor a worse option but is not required to
favor a better option. In this sense our fairness requirement encodes a necessary variant of fairness rather
than a completely sufficient one. This makes our upper bounds (Sections 3 and 4) relatively weaker and our
lower bounds (Sections 5 and 6) relatively stronger.
We additionally note that our fairness guarantees require fairness at every step of the learning process.
We view this as an important point, especially for algorithms whose learning processes may be long (or even
continuous). Furthermore, while it may seem reasonable to relax this requirement to allow a small fraction
of unfair steps, it is unclear how to do so without enabling discrimination against a correspondingly small
population.
Finally, while our fairness definition draws from Joseph et al. [2016], we work in what we believe to be a
2 Friedler et al. [2016] provide evidence that providing fairness from bias-corrupted data is quite difficult.
3
significantly more general and realistic setting. In the finite case we allow for a variable number of individuals
in each round from a variable number of groups and also allow selection of a variable number of individuals
in each round, thus dropping several assumptions from Joseph et al. [2016]. We also analyze the previously
unstudied topic of fairness with infinitely many choices.
2 Model
Fix some Î² âˆˆ [âˆ’1, 1]d
, the underlying linear coefficients of our learning problem, and T the number of rounds.
For each t âˆˆ [T], let Ct âŠ† D = [âˆ’1, 1]d
denote the set of available choices in round t. We will consider both
the â€œfiniteâ€ action case, where |Ct| â‰¤ k, and the infinite action case. An algorithm A, facing choices Ct, picks
a subset Pt âŠ† Ct, and for each xt âˆˆ Pt, A observes reward yt âˆˆ [âˆ’1, 1] such that E [yt] = hÎ², xti, and the
distribution of the noise Î·t = yt âˆ’ hÎ², xti is sub-Gaussian (see Section 7.1 for a definition of sub-Gaussian).
Refer to all observations in round t as Yt âˆˆ [âˆ’1, 1]|Pt|
where Yt,i = yt,i for each xt,i âˆˆ Pt. Finally, let
Xt = [X1; . . . ; Xt], Yt = [Y1; . . . ; Yt] refer to the design and observation matrices at round t.
We are interested in settings where an algorithm may face size constraints on Pt. We consider three cases:
the standard linear bandits problem (|Pt| = 1), the multiple choice linear bandits problem (|Pt| = m), and
the heretofore unstudied (to the best of the authorsâ€™ knowledge) case in which the size of Pt is unconstrained.
For short, we refer to these as 1-bandit, m-bandit, and k-bandit.
Regret The notion of regret we will consider is that of pseudo-regret. Facing a sequence of choice sets
C1, . . . , CT , suppose A chooses sets P1, . . . , PT .3
Then, the expected reward of A on this sequence is
Rew(A) = E
hP
tâˆˆ[T ]
P
xtâˆˆPt
yt
i
.
Refer to the sequence of feasible choices4
which maximizes expected reward as Pâˆ—,1 âŠ† C1, . . . , Pâˆ—,T âŠ† CT ,
defined with full knowledge of Î².
Then, the pseudo-regret of A on any sequence is defined as
Rew(Pâˆ—,1, . . . , Pâˆ—,T ) âˆ’ Rew(A) = R(T).
The pseudo-regret of A refers to the maximum pseudo-regret A incurs on any sequence of choice sets
and any Î² âˆˆ [âˆ’1, 1]d
. If R(T) = o(T), then A is said to be no-regret. If, for any input parameter Î´ > 0,
R(T) upper-bounds the expectation of the rewards of the sequence chosen by A with probability 1 âˆ’ Î´, then
we call this a high-probability regret bound for A.
Fairness Consider an algorithm A, which chooses a sequence of probability distributions Ï€1, Ï€2, . . . , Ï€T over
feasible sets to pick, Ï€t âˆˆ âˆ†(2Ct
). Note that distribution Ï€t depends upon C1, . . . , Ct, the choices P1, . . . , Ptâˆ’1,
and Y1, . . . , Ytâˆ’1.
We now give a formal definition of fairness of an algorithm for the 1-bandit, m-bandit, and k-bandit
problems. We adapt our fairness definition from Joseph et al. [2016], generalizing from discrete distributions
over finite action sets to mixture distributions over possibly infinite action sets. We slightly abuse notation
and refer to the probability density and mass functions of an element x âˆˆ Ct: this refers to the marginal
distribution of x being chosen (namely, the probability that x belongs to the set picked according to the
distribution Ï€t).
Definition 1 (Weakly Meritocratic Fairness). We say that an algorithm A is weakly meritocratic if, for any
input Î´ âˆˆ (0, 1] and any Î², with probability at least 1 âˆ’ Î´, at every round t, for every x, x0
âˆˆ Ct such that
hÎ², xi â‰¥ hÎ², x0
i:
3If these are randomized choices, the randomness of A is incorporated into the expected value calculations.
4We assume these have the appropriate size for each problem we consider: singletons in the 1-bandit problem, size at most m
in the m-bandit problem, and arbitrarily large in the k-bandit problem.
4
â€¢ If Ï€t is a discrete distribution: For gt(x) = Ï€t(x) (the probability mass function)
gt(x) â‰¥ gt(x0
).
â€¢ If Ï€t is a continuous distribution: For gt(x) = ft(x) (the probability density function)
gt(x) â‰¥ gt(x0
).
â€¢ If Ï€t can be written as a mixture distribution:
P
i Î±iÏ€ti,
P
i Î±i = 1, such that each constituent
distribution Ï€ti âˆˆ âˆ†(2Ct
) is either discrete or continuous and satisfies one of the above two conditions.
For brevity, as consider only this fairness notion in this paper, we will refer to weakly meritocratic fairness as
â€œfairnessâ€. We say A is round-fair at time t if Ï€t satisfies the above conditions.
This definition can be easily generalized over any partition G of D, by requiring this weak monotonicity
hold only for pairs x, x0
belonging to different elements of the partition G, G0
. The special case above of the
singleton partition is the most stringent choice of partition. We focus our analysis on the singleton partition
as a minimal worst-case framework, but this model easily relaxes to apply only across groups, as well as
to only requiring â€œone-sidedâ€ monotonicity, where monotonicity is required only for pairs where the more
qualified member belongs to group G rather than G0
.
Remark 1. In the k-bandit setting, Definition 1 can be simplified to require, with probability 1 âˆ’ Î´ over its
observations, an algorithm never select a less-qualified individual over more-qualified one in any round, and
can be satisfied by deterministic algorithms.
3 Finite Action Spaces: Fair Ridge Regression
In this section, we introduce a family of fair algorithms for linear 1-bandit, m-bandit, and the (unconstrained)
k-bandit problems. Here, an algorithm sees a slate of at most k distinct individuals each round and selects
some subset of them for reward and observation. This allows us to encode settings where an algorithm
repeatedly observes a new pool of k individuals, each represented by a vector of d features, then decides
to give some of those individuals loans based upon those vectors, observes the quality of the individuals to
whom they gave loans, and updates the selection rule for loan allocation. The regret of these algorithms will
scale polynomially in k and d as the algorithm gets tighter estimates of Î².
All of the algorithms are based upon the following template. They maintain an estimate Î²Ì‚t of Î² from
observations, along with confidence intervals around the estimate. They use Î²Ì‚t to estimate the rewards for the
individuals on day t and the confidence interval around Î²Ì‚t to create a confidence interval around each of these
estimated rewards. Any two individuals whose intervals overlap on day t will picked with the same probability
by the algorithm. Call any two individuals whose intervals overlap on day t linked, and any two individuals
belonging to the transitive closure of the linked relation chained. Since any two linked individuals will chosen
with the same probability, any two chained individuals will also be chosen with the same probability.
An algorithm constrained to pick exactly m âˆˆ [k] individuals each round will pick them in the following
way. Order the chains by their highest upper confidence bound. In that order, select all individuals from
each chain (with probability 1 while that results in taking fewer than m individuals. When the algorithm
arrives at the first chain for which it does not have capacity to accept every individual in the chain, it selects
to fill its capacity uniformly at random from that chainâ€™s individuals. If the algorithm can pick any number
of individuals, it will pick all individuals chained to any individual with positive upper confidence bound.
We now present the regret guarantees for fair 1-bandit, m-bandit, and k-bandit using this framework.
Theorem 1. Suppose, for all t, Î·t is 1-sub-Gaussian, Ct âŠ† [âˆ’1, 1]d
, and ||xt||2 â‰¤ 1 for all xt âˆˆ Ct, and
||Î²|| â‰¤ 1. Then, RidgeFair1, RidgeFairm, and RidgeFairâ‰¤k are fair algorithms for the 1-bandit, m-bandit,
and k-bandit problems, respectively. With probability 1 âˆ’ Î´, for j âˆˆ {1, m, k}, the regret of RidgeFairj is
R(T) = O

dkj
âˆš
T log

T
Î´

= OÌƒ(dkj
âˆš
T).
5
We pause to compare our bound for 1-bandit to that found in Joseph et al. [2016]. Their work supposes
that each of k groups has an independent d-dimensional linear function governing its reward and provides a
fair algorithm regret upper bound of OÌƒ

T
4
5 k
6
5 d
3
5 , k3

. To directly encode this setting in ours, one would
need to use a single dk-dimensional linear function, yielding a regret bound of OÌƒ

dk2
âˆš
T

. This is an
improvement on their upper bound for all values of T for which the bounds are non-trivial (recalling that the
bound from Joseph et al. [2016] becomes nontrivial for T > d3
k6
, while the bound here becomes nontrivial
for T > d2
k4
). We also briefly observe that RidgeFairâ‰¤k satisfies an additional â€œfairnessâ€ property: with
high probability, it always selects every available individual with positive expected reward.
Each of these algorithms will use `2-regularized least-squares regressor to estimate Î². Given a design matrix
X, response vector Y, and regularization parameter Î³ â‰¥ 1 this is of the form Î²Ì‚ = (XT
X + Î³I)âˆ’1
XT
Y. Valid
confidence intervals (that contain Î² with high probability) are nontrivial to derive for this estimator (which
might be biased); to construct them, we rely on martingale matrix concentration results [Abbasi-Yadkori
et al., 2011].
We now sketch how the proof of Theorem 1 proceeds, deferring a full proof (of this and all other results
in this paper) and pseudocode to the supplementary materials. We first establish that, with probability
1 âˆ’ Î´, for all rounds t, for all xt,i âˆˆ Ct, that yt,i âˆˆ [`t,i, ut,i] (i.e. that the confidence intervals being used are
valid). Using this fact, we establish that the algorithm is fair. The algorithm plays any two actions which are
linked with equal probability in each round, and any action with a confidence interval above another actionâ€™s
confidence interval with weakly higher probability. Thus, if the payoffs for the actions lie anywhere within
their confidence intervals, RidgeFair is fair, which holds as the confidence intervals are valid.
Proving a bound on the regret of RidgeFair requires some non-standard analysis, primarily because the
widths of the confidence intervals used by the algorithm do not shrink uniformly. The sum of the widths of
the intervals of our selected (and therefore observed) actions grows sublinearly in t. UCB variants, by virtue
of playing an action a with highest upper confidence bound, have regret in round t bounded by aâ€™s confidence
interval width. RidgeFair, conversely, suffers regret equal to the sum of the confidence widths of the chained
set, while only receiving feedback for the action it actually takes. We overcome this obstacle by relating the
sum of the confidence interval widths of the linked set to the sum of the widths of the selected actions.
4 Fair algorithms for convex action sets
In this section we analyze linear bandits with infinite choice sets in the familiar 1-bandit setting.5
We provide
a fair algorithm with an instance-dependent sublinear regret bound for infinite choice sets â€“ specifically
convex bodies â€“ below. In Section 5 we match this with lower bounds showing that instance dependence is an
unavoidable cost for fair algorithms in an infinite setting.
A naive adaptation of RidgeFair to an infinite setting requires maintenance of infinitely many confidence
intervals and is therefore impractical. We instead assume that our choice sets are convex bodies and exploit
the resulting geometry: since our underlying function is linear, it is maximized at an extremal point. This
simplifies the problem, since we need only reason about the relative quality of extremal points. The relevant
quantity is âˆ†gap, a notion adapted from Dani et al. [2008] that denotes the difference in reward between the
best and second-best extremal points in the choice set. When âˆ†gap is large it is easier to confidently identify
the optimal choice and select it deterministically without violating fairness. When âˆ†gap is small, it is more
difficult to determine which of the top two points is best â€“ and since deterministically selecting the wrong
one violates fairness for any points infinitesimally close to the true best point, we are forced to play randomly
from the entire choice set.
Our resulting fair algorithm, FairGap, proceeds as follows: in each round it uses its current estimate of
Î² to construct confidence intervals around the two choices with highest estimated reward and selects the
higher one if these intervals do not overlap; otherwise, it selects uniformly at random from the entire convex
5Note that no-regret guarantees are in general impossible for infinite choice sets in m-bandit and k-bandit settings, since the
continuity of the infinite choice sets we consider makes selecting multiple choices while satisfying fairness impossible without
choosing uniformly at random from the entire set.
6
body. We prove fairness and bound regret by analyzing the rate at which random exploration shrinks our
confidence intervals and relating it to the frequency of exploitation, a function of âˆ†gap. We begin by formally
defining âˆ†gap below.
Definition 2 (Gap, adapted from Dani et al. [2008]). Given sequence of action sets C = (C1, . . . , CT ), define
â„¦t to be the set of extremal points of Ct, i.e. the points in Ct that cannot be expressed as a proper convex
combination of other points in Ct, and let xâˆ—
t = maxxâˆˆCt hÎ², xi. The gap of Ct is
âˆ†gap = min
1â‰¤tâ‰¤T

inf
xtâˆˆâ„¦t,xt6=xâˆ—
t
hÎ², xâˆ—
t âˆ’ xti

.
âˆ†gap is a lower bound on difference in payoff between the optimal action and any other extremal action
in any Ct. When âˆ†gap > 0, this implies the existence of a unique optimal action in each Ct. Our algorithm
(implicitly) and our analysis (explicitly) exploits this quantity: a larger gap enables us to confidently identify
the optimal action more quickly.
We now present the regret and fairness guarantees for FairGap.
Theorem 2. Given sequence of action sets C = (C1, . . . , CT ) where each Ct has nonzero Lebesgue measure
and is contained in a ball of radius r and feedback with R-sub-Gaussian noise, FairGap is fair and achieves
Regret (T) = O

r6
R2
ln(2T/Î´)
Îº2Î»2âˆ†2
gap

where Îº = 1 âˆ’ r
q
2 ln(2dT
Î´ )
T Î» and Î» = min1â‰¤tâ‰¤T

Î»min(Extâˆ¼UARCt [xt
T
xt])

A full proof of FairGapâ€™s fairness and regret bound, as well as pseudocode, appears in the supplement.
We sketch the proof here: our proof of fairness proceeds by bounding the influence of noise on the confidence
intervals we construct (via matrix Chernoff bounds) and proving that, with high probability, FairGap
constructs correct confidence intervals. This requires reasoning about the spectrum of the covariance matrix
of each choice set, which is governed by Î», a quantity which, informally, measures how quickly we learn from
uniformly random actions. 6
. With correct confidence intervals in hand, fairness follows almost immediately,
and to bound regret we analyze the rate at which these confidence intervals shrink.
The analysis above implies identical regret and fairness guarantees when each Ct is finite. For comparison,
the results of Section 3 guarantee Regret (T) = O(dk
âˆš
T). This result, in comparison, enjoys a regret
independent of k which may prove especially useful for cases involving large k.
Finally, our analysis so far has elided any computational efficiency issues arising from sampling randomly
from C. We note that it is possible to circumvent this issue by relaxing our definition of fairness to approximate
fairness and obtain similar regret bounds for an efficient implementation. We achieve this using results from
the broad literature on sampling and estimating volume in convex bodies, as well as recent work on finding
â€œ2nd bestâ€ extremal solutions to linear programs. Full details appear in Section 7.4 of the Supplement.
5 Instance-dependent Lower Bound for Fair Algorithms
We now present a lower bound instance for which any fair algorithm must suffer gap-dependent regret. More
formally, we show that when each choice set is a square, i.e. Ct = [0, 1]2
for all t, for any fair algorithm
Regret (T) = â„¦Ìƒ(1/âˆ†gap) with probability at least 1 âˆ’ Î´. This also implies the weaker result that no fair
algorithm enjoys an instance-independent sub-linear regret bound o(T) holding uniformly over all Î². We
therefore obtain a clear separation between fair learning and the unconstrained case Dani et al. [2008], and
show that an instance-dependent upper bound like the one in Section 4 is unavoidable. Our arguments
establish fundamental constraints on fair learning with large choice sets and quantify through the âˆ†gap
6Î» can be computed directly for finite Ct or approximated by any positive lower bound for infinite Ct and substituted directly
into our results.
7
parameter how choice set geometry can affect the performance of fair algorithms. The lower bound employs a
Bayesian argument resembling that in Joseph et al. [2016] but with a novel â€œchainingâ€ argument suited to
infinite action sets. We present the result for d = 2 for simplicity; the proof technique holds in any dimension
d â‰¥ 2.
Theorem 3. For all t let Ct = [âˆ’1, 1]d
, Î² âˆˆ [âˆ’1, 1]d
, and yt = hxt, Î²i + Î·t, where Î·t âˆ¼ U[âˆ’1, 1]. Let A be
any fair algorithm. Then for every gap âˆ†gap, there is a distribution over instances with gap â„¦(âˆ†gap) such
that any fair algorithm has regret Regret (T) = â„¦Ìƒ(1/âˆ†gap) with probability 1 âˆ’ Î´.
We a sketch of the central ideas in the proof, relegating a full proof to the Supplement. We start with the
fact that any fair algorithm A is required to be fair for any value Î² of the linear parameter. Thus if we draw
Î² âˆ¼ Ï„, A must be round-fair for all t â‰¥ 1 with probability at least 1 âˆ’ Î´, where now the probability includes
the random draw Î² âˆ¼ Ï„. Then Bayesâ€™ rule implies that the procedure that draws Î² âˆ¼ Ï„ and then plays
according to A is identical to the procedure which at each step t re-draws Î² from its posterior distribution
given the past Ï„|ht
.
Next, given the prior Ï„, Aâ€™s round fairness at step t requires that (with high probability) if A plays action
x with higher probability than action y, we must have
PÎ²âˆ¼Ï„|ht
[hÎ², xi > hÎ², yi] >
3
4
. (1)
This enables us to reason about the fairness and regret of the algorithm via a specific analysis of the
posterior distribution Ï„|ht
. We formalize this argument in Lemmas 7 and 8. This Bayesian trick, first applied
in Joseph et al. [2016], is a general technique useful for proving fairness lower bounds.
We then show that for a choice of prior specific to our choice set C, that two things hold: (i) whenever
Ï„|ht = Ï„, Equation 1 forces A to play uniformly from C, and (ii) with high probability Ï„ = Ï„|ht until t > â„¦Ìƒ(1/),
where  is a parameter of the prior that acts as a proxy for âˆ†gap. Playing an action uniformly from C incurs
â„¦(1) regret per round, so these two facts combine to show that with high probability Regret (T) = â„¦Ìƒ(1/).
Finally we consider Regret (T) conditional on the event that âˆ†gap(Î²) > Î´ Â· , which by our construction
of Ï„ happens with probability 1 âˆ’ Î´. Let Ï„gap be the conditional distribution of Î² given that âˆ†gap(Î²) > Î´ Â· .
Then
PÎ²âˆ¼Ï„

Regret (T) â‰¥ â„¦

1


â‰¤ PÎ²âˆ¼Ï„gap

Regret (T) â‰¥ â„¦

1


(1 âˆ’ Î´) + Î´
which implies
PÎ²âˆ¼Ï„gap

Regret (T) â‰¥ â„¦

1


â‰¥
1 âˆ’ 2Î´
1 âˆ’ Î´
.
Note that 1âˆ’2Î´
1âˆ’Î´ â†’ 1 as Î´ â†’ 0, and so this is a high-probability bound. Since for every Î² in the support of
Ï„gap, we have that âˆ†gap(Î²) â‰¥ Î´ Â· , weâ€™ve exhibited a distribution Ï„gap such that when Î² âˆ¼ Ï„gap, with high
probability, Regret (T) = â„¦Ìƒ(1/) = â„¦Ìƒ(1/âˆ†gap), as desired.
The proof uses the fact that when Ï„ = Ï„|ht
, Equation 1 forces A to play uniformly at random. This
happens by transitivity: if Equation 1 forces A to play x equiprobably with y and y equiprobably with z,
then x must be played equiprobably with z. The fact that any two actions in C can be connected via such a
(finite) transitive chain is illustrated in Figure 7.5 and formalized in Lemma 10.
Remark 2. We note that this impossibility result only holds for d â‰¥ 2. When d = 1, the choice set reduces
to [âˆ’1, 1], and similarly Î² âˆˆ [âˆ’1, 1]. Thus, the optimal action is sign(Î²)). It takes O(1/Î²2
) observations to
determine the sign of Î², so a simple fair algorithm may play randomly from [âˆ’1, 1] until it has determined
sign(Î²), and then play sign(Î²) for every following round. Because the maximum per-round regret of any action
is O(Î²), and because the maximum cumulative regret obtained by the algorithm is with high probability
O(Î² Â· 1/Î²2
) = O(1/Î²), the regret of this simple algorithm over T rounds is O(min(Î² Â· T, 1/Î²2
)). Taking
the worst case over Î², we see that this quantity is bounded uniformly by O(
âˆš
T), a sublinear parameter
independent regret bound.
8
6 Zero Gap: Impossibility Result
Section 4 presents an algorithm for which the sublinear regret bound has dependence 1/âˆ†2
gap on the instance
gap. Section 5 exhibits an choice set C with a â„¦Ìƒ(1/âˆ†gap) dependence on the gap parameter. We now exhibit
a choice set C for which âˆ†gap = 0 for every Î², and for which no fair algorithm can obtain non-trivial regret
for any value of Î². This precludes even instance-dependent fair regret bounds on this action space, in sharp
contrast with the unconstrained bandit setting.
Theorem 4. For all t let Ct = S1
, the unit circle, and Î·t âˆ¼ Unif(âˆ’1, 1). Then for any fair algorithm
A, âˆ€Î² âˆˆ S1
, âˆ€T â‰¥ 1, we have
EÎ²[Regret (T)] = â„¦(T).
S1
makes fair learning difficult for the following reasons: since S1
has no extremal points, there is no
finite set of points which for any Î² contains the uniquely optimal action, and for any point in S1
, and any
finite set of observations, there is another point in S1
for which the algorithm cannot confidently determine
relative reward. Since this property holds for every point, the fairness constraint transitively requires that
the algorithm play every point uniformly at random, at every round.
9
References
Yasin Abbasi-Yadkori, DÃ¡vid PÃ¡l, and Csaba SzepesvÃ¡ri. Improved algorithms for linear stochastic bandits. In Advances in
Neural Information Processing Systems, pages 2312â€“2320, 2011.
Naoki Abe, Alan W Biermann, and Philip M Long. Reinforcement learning with immediate rewards and linear hypotheses.
Algorithmica, 37(4):263â€“293, 2003.
Venkatachalam Anantharam, Pravin Varaiya, and Jean Walrand. Asymptotically efficient allocation rules for the multiarmed
bandit problem with multiple plays â€“ part i: I.i.d. rewards. IEEE Transactions on Automatic Control, AC-32(Nov):968â€“976,
1987.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):
397â€“422, 2002.
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments:
The state of the art. arXiv preprint arXiv:1703.09207, 2017.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. arXiv
preprint arXiv:1703.00056, 2017.
Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of
fairness. arXiv preprint arXiv:1701.08230, 2017.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In COLT, pages
355â€“366, 2008.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings
of ITCS 2012, pages 214â€“226. ACM, 2012.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting:
gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,
pages 385â€“394. Society for Industrial and Applied Mathematics, 2005.
Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im)possibility of fairness. In arXiv, volume
abs/1609.07236, 2016. URL http://arxiv.org/abs/1609.07236.
Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona,
Spain, volume abs/1610.02413, 2016. URL http://arxiv.org/abs/1610.02413.
Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits.
In Advances in Neural Information Processing Systems, pages 325â€“333, 2016.
J. Kleinberg, S. Mullainathan, and M. Raghavan. Inherent trade-offs in the fair determination of risk scores. In ITCS, Jan 2017.
Erik M Lindgren, Alexandros G Dimakis, and Adam Klivans. Facet guessing for finding the m-best integral solutions of a linear
program. In NIPS Workshop on Optimization for Machine Learning, 2016.
LÃ¡szlÃ³ LovÃ¡sz and Santosh Vempala. Hit-and-run from a corner. SIAM Journal on Computing, 35(4):985â€“1005, 2006.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends R in Machine Learning, 8
(1-2):1â€“230, 2015.
Santosh Vempala. Geometric random walks: a survey. Combinatorial and computational geometry, 52(573-612):2, 2005.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-discriminatory predictors. arXiv
preprint arXiv:1702.06081, 2017.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment
and disparate impact: Learning classification without disparate mistreatment. In Proceedings of World Wide Web Conference,
2017.
10
7 Appendix
7.1 Sub-Gaussian Definition
Sub-Gaussian random variables have moment generating functions bounded by the Gaussian moment
generating function, and hence can be controlled via Chernoff bounds.
Definition 3. A random variable X with Âµ = E [X] is R > 0 sub-Gaussian if, for all t âˆˆ R, E

et(Xâˆ’Âµ)

â‰¤
eRt2
/2
.
7.2 Proofs from Section 3
We start with full pseudocode for RidgeFairm.
Proof of Theorem 1. We first claim that confidence intervals are valid: that with probability 1 âˆ’ Î´, for all
t âˆˆ [T] and all xt,i âˆˆ Ct, yt,i âˆˆ [`t,i, ut,i]. Assuming this claim, we prove that RidgeFairm is fair. With
probability 1 âˆ’ Î´, for all rounds t and all individuals xt,i, yt,i âˆˆ [yÌ‚t,i âˆ’ wt,i, yÌ‚t,i + wt,i]. So, for any pair of
individuals xt,i, xt,j âˆˆ Ct, if yt,i > yt,j, then yÌ‚t,i + wt,i â‰¥ yÌ‚t,j âˆ’ wt,j. So, if j belongs to some chain from
which arms are selected, either i belongs to a higher chain or the same chain as j. Every individual belonging
to a higher chain is played with weakly higher probability to any individual belonging to a lower chain, and
every two individuals belonging to the same chain are played with equal probability, so i is played with
weakly higher probability than j. Thus, at all rounds and for all pairs of individuals, the fairness constraint
is satisfied by this distribution over Pt, and so RidgeFairm is fair.
We now prove the confidence intervals are valid: that with probability 1âˆ’Î´, for all t âˆˆ [T] and all xt,i âˆˆ Ct,
yt,i âˆˆ [`t,i, ut,i]. We adopt the notation in Abbasi-Yadkori et al. [2011]: let VÌ„t = Xt
T
Xt + Î³I, where Xt is
the design matrix at time t. Let Î²Ì‚t = (VÌ„t)âˆ’1
Xt
T
Yt be the regularized least squares estimator at time t.
Consider a feature vector xt,i at time t. For a d-dimensional vector z and a d Ã— d positive definite
matrix A, let hz1, z2iA denote zt
1Az2. Let Î·t be the noise sequence prior to round t. Then, we have
Î²Ì‚t = (VÌ„t)âˆ’1
Xt
T
(XtÎ² + Î·t). Then some matrix algebra in the proof of Theorem 2 of Abbasi-Yadkori et al.
[2011] shows
xt,i Â· (Î²Ì‚t âˆ’ Î²) = xt,i
T
(VÌ„t)âˆ’1
Xt
T
Î·t âˆ’ Î³xt,i
T
(VÌ„t)âˆ’1
Î²,
which using the above notation gives
xt,i Â· (Î²Ì‚t âˆ’ Î²) = hxt,i, Xt
T
Î·ti(VÌ„t)âˆ’1 âˆ’ Î³hxt,i, Î²i(VÌ„t)âˆ’1
Applying Cauchy-Schwarz,
|xt,i Â·

Î²Ì‚t âˆ’ Î²

| â‰¤ ||xt,i||(VÌ„t)âˆ’1 (||Xt
T
Î·t||(VÌ„t)âˆ’1 +
âˆš
Î³)
which follows from the fact that ||Î²||(VÌ„t)âˆ’1 â‰¤ 1
âˆš
Î³ (a basic corollary of the Rayleigh quotient, and the fact
that by assumption ||Î²|| â‰¤ 1.
We now present a result derived from [Abbasi-Yadkori et al., 2011] that will help us upper bound this
quantity. The upper bound on ||XT
Î·|| at the bottom of page 13 of Abbasi-Yadkori et al. [2011] and the upper
bound on log(det(VÌ„t)) at the top of page 15, combined with our assumption that our noise is 1-sub-Gaussian,
implies that
11
||Xt
T
Î·t||(VÌ„t)âˆ’1 â‰¤
p
d log (1 + t/dÎ³) âˆ’ 2 log Î´
=
r
d log (1 + t/Î³) + 2 log
1
Î´
â‰¤
r
2d log (1 + t/Î³) + 2d log
1
Î´
=
s
2d log

1
Î´
(1 + t/dÎ³)

â‰¤
s
2d log

1
Î´
(1 + t/Î³)

=
s
2d log

1 + t/Î³
Î´

.
Using this result and combining the inequalities we get that over all rounds t â‰¥ 0 with probability 1 âˆ’ Î´
|xt,i Â·

Î²Ì‚t âˆ’ Î²

| â‰¤ ||xt,i||(VÌ„t)âˆ’1
s
2d log

1 + t/Î³
Î´

+
âˆš
Î³
!
(2)
and therefore the claim that the confidence intervals are valid holds.
Regret bound for RidgeFair1 We now proceed with upper-bounding the regret of RidgeFair1. With
probability 1 âˆ’ Î´, the confidence intervals are valid. We will condition on that event for the analysis of the
regret of the algorithm, since this regret bound will hold with high probability (namely, with probability
1 âˆ’ Î´).
We start with a bound that will be useful for analyzing the width of our confidence intervals. The top of
page 15 of of [Abbasi-Yadkori et al., 2011] notes that log det(VÌ„t) â‰¤ d log(Î³ + t/d), and we combine this with
the fact that
PT
t=1 ||xt,i||2
VÌ„t
â‰¤ 2 log det(VÌ„ T
) âˆ’ 2 log det(V ) (proven as part of Lemma 11 in [Abbasi-Yadkori
et al., 2011]) to get that
T
X
t=1
||xt,i||2
(VÌ„t)âˆ’1 â‰¤ 2d log

1 +
T
dÎ³

. (3)
We now have all the tools needed to analyze the algorithmâ€™s regret. First note that the choice of the
algorithm is a singleton, i.e. that Pt = {iÌ‚t}, for some iÌ‚t âˆˆ St, the active chained set. Further, since the
confidence intervals are valid, iâˆ—,t âˆˆ St for the best action iâˆ—,t âˆˆ Ct. By the definition of St, the instantaneous
regret rt,i for any i âˆˆ St is at most rt,i â‰¤
P
jâˆˆSt
wt,j (as any i âˆˆ St is chained to some other arm in St). So,
we have that
12
R(T) â‰¤
X
t
rt,iÌ‚t
â‰¤
X
t
X
jâˆˆSt
2wt,j Conditioning on w.p. 1 âˆ’ Î´ valid confidence intervals
= 2
X
t
|St| Â· E
h
wt,iÌ‚t
i
When uniformly selecting iÌ‚t âˆˆ St; note this holds w.p. 1 conditioned on valid CI
â‰¤ 2k Â·
X
t
E
h
wt,iÌ‚t
i
= 2k Â· E
"
X
t
wt,iÌ‚t
#
By linearity of expectation
= 2k Â· E
"
X
t
||xt,iÌ‚t
||(VÌ„t)âˆ’1
s
2d log

1 + t/Î³
Î´

+
âˆš
Î³
!#
By definition of wt,i
= 2k Â· E
"
X
t
||xt,iÌ‚t
||(VÌ„t)âˆ’1
s
2d log

1 + t/Î³
Î´
!
+
X
t
||xt,iÌ‚t
||(VÌ„t)âˆ’1 (
âˆš
Î³)
#
â‰¤ 2kE
ï£®
ï£°
s
X
t
||xt,iÌ‚t
||2
(VÌ„t)âˆ’1 Â·
ï£«
ï£­
s
X
t
2d log

1 + t/Î³
Î´

+
s
X
t
Î³
ï£¶
ï£¸
ï£¹
ï£» By Cauchy-Schwartz
â‰¤ 2k
s
2d log

1 +
T
dÎ³

Â·
ï£«
ï£­
s
X
t
2d log

1 + t/Î³
Î´

+
s
X
t
Î³
ï£¶
ï£¸ By Equation 3
â‰¤ 2k
s
2d log

1 +
T
dÎ³

Â·
s
2dT log

1 + T/Î³
Î´

+
p
TÎ³
!
or R(T) = O

dk
âˆš
T log T
Î´

= OÌƒ(dk
âˆš
T) for Î³ = 1, as desired.
Regret bound for RidgeFairm This regret bound relies on a similar analysis to RidgeFair1, with the
following changes. The algorithm now selects m individuals, not all from the top chain, but instead from
several chains. For each of the top m choices x âˆˆ Pâˆ—,t, we relate reward of that choice to the reward of one of
our choices in the following way. For each d, consider the dth top chain. We claim that if the dth top chain
contains nd of the top m choices, our algorithm selects nd individuals from the dth top chain. We prove this
claim by induction. First, however, we notice that every individual in the dth top chain has strictly higher
reward than every individual in any lower chain. For the first top chain, Pt contains either the entire chain
or m from the top chain. As every individuals in the top chain has strictly higher reward than any other
individuals, in the former case, every individual in the first top chain belongs to Pâˆ—,t ; in the latter case, Pâˆ—,t
is entirely contained in the top chain. Thus, Pt and Pâˆ—,t contain either all individuals in the top chain or
exactly m of them. Then, assuming the claim for the first d âˆ’ 1 top chains, both Pt and Pâˆ—,t have the same
â€œcapacityâ€ for individuals in the dth top chain (and therefore either take all of the dth top chain or fewer but
the same number from it). This proves the claim.
Then, we relate the reward of an i âˆˆ Pt with some action in Pâˆ—,t belonging to same chain. Following the
previous claim, we can form a matching between Pâˆ—,t and Pt for which all matches belong to the same chains.
Then, the analysis of RidgeFair1 bounds the difference between the reward of any individual in the dth top
chain to any other individual in the dth top chain. Summing up over all m choices, the total regret for all of
Pt is at most m times the loss suffered in 1-bandit.
13
Regret bound for RidgeFairâ‰¤k The regret bound for this case reduces to lower-bounding the amount
of reward incurred by playing arms with negative reward. Any individual selected by RidgeFairâ‰¤k is within
the sum of the widths of the confidence intervals in its chain, one of which has UCB which is positive. So,
the reward of any action chosen is at least âˆ’
P
iâˆˆSd
t
wt,ti for Sd
t the dth top chain, or at most the sum of all
k interval widths. Thus, summing up over all individuals selected, one gets regret which is at most k times
worse than that for RidgeFair1.
7.3 Proofs from Section 4
We begin with the full pseudocode for FairGap.
We start our proof of Theorem 4 with a lemma bounding the contribution of noise to our confidence
intervals.
Lemma 1. Let Î·1, . . . , Î·T be T i.i.d draws of R-sub-Gaussian noise. Then
P
" T
X
i=1
Î·i â‰¥ R
p
2T ln(2T/Î´)
#
â‰¤ Î´/2T.
Proof of Lemma 1. A Hoeffding bound, in the general case for unbounded variables, implies that
P
" T
X
i=1
Î·i â‰¥ c
#
â‰¤ 2 exp(âˆ’c2
/2R2
)
so taking c = R
p
2T ln (2T/Î´) yields the desired result.
Next, since the regret bound we will prove depends on Î» = min1â‰¤tâ‰¤T

Î»min(Extâˆ¼UARCt [xt
T
xt])

, the
minimum smallest eigenvalue of the expected outer product of a vector xt drawn uniformly at random from
each Ct we will need Î» > 0 in order for this bound to make sense. We prove this in another lemma.
Lemma 2. Given sequence of action sets C = (C1, . . . , CT ) where each Ct has nonzero Lebesgue measure
and is contained in a ball of radius r, Î» = min1â‰¤tâ‰¤T

Î»min(Extâˆ¼UARCt [xt
T
xt])

> 0.
Proof of Lemma 2. It suffices to prove that Î»min(Extâˆ¼UARCt[xT
t xt]) > 0 for each 1 â‰¤ t â‰¤ T. xT
x is positive
semidefinite, so it is immediate that Î» â‰¥ 0. Assume Î» = 0. Then there exists nonzero z âˆˆ Rd
such that
zExâˆ¼UARC[xT
x]zT
= 0, so by linearity of expectation
Exâˆ¼UARC[||xzT
||2
] = 0.
However, ||xzT
||2
is a non-negative random-variable with expectation 0 and must therefore be 0 with
probability 1. It follows that x âˆˆ zâŠ¥
, so
Pxâˆ¼UARC

x âˆˆ zâŠ¥

= 1,
zâŠ¥
is a dâˆ’1 dimensional subspace of Rd
, and thus has measure 0. We can decompose C = (Câˆ©zâŠ¥
)
S
(Câˆ© zâŠ¥
c
),
and since Pxâˆ¼UARC

x âˆˆ zâŠ¥

= 1, this forces Pxâˆ¼UARC

x âˆˆ (C âˆ© zâŠ¥c
)

= 0. By definition of the uniform
distribution
Pxâˆ¼UARC
h
x âˆˆ (C âˆ© zâŠ¥c
)
i
=
Âµ(C âˆ© zâŠ¥c
)
Âµ(D)
=â‡’ Âµ(C âˆ© zâŠ¥c
) = 0.
But Âµ(D) = Âµ(C âˆ© zâŠ¥
) + Âµ(C âˆ© zâŠ¥c
) = Âµ(C âˆ© zâŠ¥
) + 0 â‰¤ Âµ(zâŠ¥
) = 0, where the second to last line follows
since C âˆ© zâŠ¥
âŠ‚ zâŠ¥
. This contradicts our assumption that Âµ(C) > 0, so Î» > 0.
14
Finally, since FairGap relies on constructed confidence intervals to guide its choice of actions, its
correctness (both in terms of its regret guarantee and its fairness) relies on the correctness of those confidence
intervals, stated in the following lemma. Its proof relies on a natural argument using matrix Chernoff bounds
to bound the contribution of noise to FairGapâ€™s estimation of Î²Ì‚t and, consequently, the accuracy of its
confidence intervals.
Lemma 3. Given sequence of action sets C=(C1, . . . , CT ) where each Ct has nonzero Lebesgue measure and
is contained in a ball of radius r, with probability at least 1 âˆ’ Î´, in every round t every confidence interval
[hÎ²Ì‚t, xi âˆ’ wt, hÎ²Ì‚t, xi + wt] constructed by FairGap contains its true mean hÎ², xi.
Proof of Lemma 3. Note first that FairGap has two kinds of rounds: in round t, it either plays uniformly at
random from
Ct or deterministically plays Ë†
xt
âˆ—
, its estimate of the optimal extremal point in
Ct. In any round t with uniform random play FairGap immediately cannot violate fairness, as Ï€t(x) = 1/Âµ(
Ct) for all x âˆˆ Ct. As a result, to prove fairness it suffices to show that for any t-step execution of FairGap,
PC1,...,Ct [deterministically play xÌ‚âˆ—
i 6= xâˆ—
i in any round i] â‰¤ Î´
xâˆ—
i is the true optimal point in Ci, and t > 4dr4
/Î´Î»2
(since for smaller i FairGap just plays uniformly at
random).
In round t + 1 after observing x1 âˆ¼UAR C1, . . . , xt âˆ¼UAR
Ct, for every x âˆˆ â„¦ we have
|hx, Î²Ì‚ âˆ’ Î²i| = |hx, (XT
X)âˆ’1
XT
(XÎ² + Î·) âˆ’ Î²i|
= |xT
Î² + xT
(XT
X)âˆ’1
XT
Î· âˆ’ xT
Î²|
= |xT
(XT
X)âˆ’1
XT
Î·|
where X âˆˆ RtÃ—d
is the design matrix of x1, . . . , xi and Î· âˆˆ Rt
is its noise vector. We can then decompose
XT
Î· by round as
|xT
(XT
X)âˆ’1
XT
Î·| = xT
(XT
X)âˆ’1
t
X
i=1
xiÎ·i
=
t
X
i=1
xT
(XT
X)âˆ’1
xiÎ·i
â‰¤
t
X
i=1

||xT
(XT
X)âˆ’1
xi|| Â· |Î·i|

â‰¤
t
X
i=1
q
xxT Â· xixT
i Â· Î»max((XT
X)âˆ’1
) Â· |Î·i|
â‰¤ r2
Â· Î»max((XT
X)âˆ’1
) Â·
t
X
i=1
Î·i
=
r2
Î»min(XT X)
Â·
t
X
i=1
Î·i
where the second inequality follows from the fact that
||(XT
X)âˆ’1
|| =
q
Î»max([XT X]âˆ’1[(XT X)âˆ’1]T ) =
q
Î»max([(XT X)âˆ’1]2) = Î»max((XT
X)âˆ’1
),
the third inequality follows from the assumed bound on each Ci, and the final equality follows from
Î»max(Aâˆ’1
) = 1
Î»min(A) . To upper bound this quantity, we now lower bound Î»min(XT
X).
15
To do so, we first note that for any 1 â‰¤ i â‰¤ t and any x âˆˆ Ci we have Î»max(xT
x) â‰¤ r2
by the Gershgorin
circle theorem, which states that a square matrix has maximum eigenvalue bounded by its largest absolute
row or column sum. Next, by linearity of expectation
Î»min(Ex1âˆ¼UARC1,...,xtâˆ¼UARCt[XT
X]) = Î»min
t
X
i=1
Exiâˆ¼UARCi [xT
i xi]
!
â‰¥ tÎ»
for Î» = min1â‰¤iâ‰¤t

Î»min(Exiâˆ¼UARCi [xT
i xi])

.Taking this together with a matrix Chernoff bound (see e.g. Tropp
et al. [2015]) yields
P

Î»min(XT
X) â‰¤ ÎºtÎ»

â‰¤ de(âˆ’(1âˆ’Îº)2
tÎ»/2r2
)
for any Îº âˆˆ [0, 1). Setting
Îº = 1 âˆ’
s
2r2 ln 2dt
Î´

tÎ»
this implies
P

Î»min(XT
X) â‰¤ ÎºtÎ»

<
Î´
2t
where Îº âˆˆ [0, 1) since t > 2r2
ln(2dt/Î´)/Î». Combining this with Lemma 1 and a union bound, we get that
with probability â‰¥ 1 âˆ’ Î´/t
r2
Î»min(XT X)
Â·
t
X
i=1
Î·i â‰¤
r2
ÎºtÎ»
Â· R
p
2t ln(2t/Î´)
=
r2
R
p
2 ln(2t/Î´)
ÎºÎ»
âˆš
t
.
Taking a union bound over t rounds, it follows that with probability at least 1 âˆ’ Î´ through t rounds every
constructed confidence interval around hÎ²Ì‚, xi contains hÎ², xi. Since FairGap only plays xÌ‚âˆ—
deterministically
when the confidence intervals around xÌ‚âˆ—
and other extremal points do not overlap, this means that with
probability at least 1 âˆ’ Î´ FairGap correctly identifies xâˆ—
. FairGap is therefore fair.
Taken together, these lemmas let us prove Theorem 4.
Proof. Proof of Theorem 4 We begin by proving fairness. By Lemma 3, with probability at least 1 âˆ’ Î´ every
confidence interval constructed by FairGap contains its true mean. Conditioning on this correctness of
confidence intervals, since FairGap only chooses an action x1 non-uniformly when U1 âˆ©U2 = âˆ…, it follows that
any action chosen non-uniformly by FairGap is optimal. Thus, with probability at least 1âˆ’Î´ FairGap never
chooses a suboptimal action x with higher mixture density Ï€t(x) than a superior action x0
, and FairGap is
fair.
While FairGap plays at random from C (for some number of rounds at least 4dr4
/Î´Î»2
), it incurs at most
2r regret per round. The algorithm incurs 0 regret once the confidence intervals around the top two extremal
points no longer intersect. A sufficient condition is therefore
r2
Â· R Â·
p
2 ln(2T/Î´)
ÎºÎ»
âˆš
T
<
âˆ†gap
2
which we rearrange into
8r4
R2
ln(2T/Î´)
Îº2Î»2âˆ†2
gap
< T.
After this many rounds, with probability â‰¥ 1 âˆ’ Î´, FairGap identifies the optimal arm in every Ct and incurs
no further regret.
16
Thus, the regret in total is at most
L
X
t=1
2r2
+ Î´T â‰¤
16r6
R2
ln(2T/Î´)
Îº2Î»2âˆ†2
gap
+ Î´T
where L = 8r4
R2
ln(2t/Î´)
Îº2Î»2âˆ†2
gap
and Î´ â‰¤ 1/(T1+c
) then implies the claim.
7.4 Efficient Approximate Version of Section 4
In this section we describe an efficient implementation of FairGap using approximate fairness.
Recall that FairGap requires some method of sampling uniformly at random from a given convex body
Ct, a problem that has attracted extensive attention over the past few decades (see Vempala [2005] for a
survey of results). For our purposes, the primary contribution of this literature is that one cannot do better
than approximately uniform random sampling from a convex set Ct under polynomial time constraints.
Since our current definition of fairness fails without a perfectly uniform distribution over actions, efficiency
necessitates a relaxation of our definition to approximate fairness for infinite action spaces. Intuitively,
approximate fairness will require that an algorithm (with high probability) always uses a distribution that is
at least â€œalmost" fair.
Definition 4 (-Approximate Fairness). Given sequence of action sets C = (C1, . . . , CT ), we say that
algorithm A is -approximately fair if, for any inputs Î´ âˆˆ (0, 1],  > 0 and for all Î², with probability at least
1 âˆ’ Î´ at every round t there exists a fair distribution Ï€t
f
such that
||Ï€t âˆ’ Ï€t
f
|| < 
where Ï€t is Aâ€™s choice distribution over Ct in round t and k Â· k denotes total variation distance.
We call this -approximate fairness to highlight that a single  is input to the algorithm A in question,
but will often shorthand this as approximate fairness.
Below we provide an approximately fair algorithm that, subject to additional assumptions on choice set
structure, obtains similar regret guarantees as FairGap efficiently. We modify FairGap as follows: first, we
replace each call to a random sample with a hit-and-run random walk scheme LovÃ¡sz and Vempala [2006] to
efficiently sample approximately uniformly at random.
We use the following lemma from LovÃ¡sz and Vempala [2006] to upper-bound the mixing time hit-and-run
requires to approach a near-uniform distribution in its walk over Ct.
Lemma 4. [Corollary 1.2 in LovÃ¡sz and Vempala [2006]] Let S be a convex set that contains a ball of radius
r0
and is contained in a ball of radius r. Then, starting from a point x âˆˆ S at a distance Î± from the boundary,
after
c > 1011
d3
 r
r0
2
ln
 r
Î±

steps of a hit-and-run random walk the random walk induces a probability distribution P over points in S
such that P is -close to uniform in total variation distance.
Next, we show that, with an additional assumption on the structure of Ct, FairGapâ€™s subroutine TopTwo
can be implemented efficiently via the following known result [Lindgren et al., 2016].
Lemma 5 ( Lindgren et al. [2016]). Let Ct be defined by m intersecting half-planes. Then there exists an
algorithm running in time polynomial in m and d which computes the two vertices which maximize Î²Ì‚t over Ct.
This algorithm enables us to compute TopTwo(Ct, Î²Ì‚t) efficiently.
The following lemma guarantees that the distributions over histories generated by FairGap and Approx-
FairGap are â€œclose" during exploration.
17
Lemma 6. Let C = (C1, . . . , CT ) be a sequence of action sets where each action set, in addition to satisfying
the assumptions of Theorem 4, is an intersection of polynomially many halfspaces and contains a ball of
radius r0
. Then through t rounds of exploration
||PÏ€1,...,Ï€tâˆ¼FairGap âˆ’ PÏ€1,...,Ï€tâˆ¼ApproxFairGap(/t)||T V < 
where each P represents distributions over possible exploration histories generated by FairGap and
ApproxFairGap(/t) respectively.
Proof of Lemma 6. By construction, during exploration each Ï€i output by ApproxFairGap() has a distri-
bution within /t of a uniform distribution in total variation distance. Since these samples are independent,
each distribution over Ï€1, . . . , Ï€t forms a product distribution, and the additivity of total variation distance
over product distributions implies the claim.
Combining the results above lets us prove that ApproxFairGap is fair, efficient, and obtains a similar
regret bound as FairGap.
Theorem 5. Consider an action set C that, in addition to satisfying the assumptions of Theorem 4, is
an intersection of polynomially many halfspaces and contains a ball of radius r0
. Then through T steps
given inputs Î´0
= Î´/2 and 0
= min(/T, Î´/2T2
), ApproxFairGap(0
) is efficient, -approximately fair, and
achieves regret
Regret (T) = O

r6
R2
ln(4T/Î´)
Îº2Î»2âˆ†2
gap

where Îº = 1 âˆ’ r
q
2 ln(2dT
Î´ )
T Î» .
Proof. Proof of Theorem 5 In each round t, FairGap performs (at most) three computation-intensive
operations. First, it computes a least squares estimator Î²Ì‚t, which may be maintained online and updated in
poly(d) time. Next, it calls subroutine TopTwo (Ct, Î²Ì‚t) to compute (x1, x2) in poly(d, m) time via Lemma 5.
Finally, it may choose an action (approximately) uniformly at random from Ct, which also takes polynomial
time via Lemma 4. It follows that each round t of FairGap takes polynomial time, so FairGap is efficient.
To prove that ApproxFairGap is approximately fair, as in the exact case we analyze ApproxFairGapâ€™s
split between exploration and exploitation. In exploration rounds, by Lemma 4 we know that each random
sample is /T-close to a true uniform distribution and therefore satisfies -approximate fairness immediately.
We now bound the probability of violating fairness during exploitation. This can only happen if in some
round t ApproxFairGap misidentifies the optimal extremal point xâˆ—
t to exploit and instead deterministically
plays xÌ‚âˆ—
t 6= xâˆ—
t . Since ApproxFairGap only uses exploration rounds to construct its design matrix, the
identified xÌ‚âˆ—
t is a deterministic function of the k â‰¤ t âˆ’ 1 exploration rounds h1, . . . , hk seen before round
t. Lemma 6 implies that FairGap and ApproxFairGap have distributions over h1, . . . , ht within  of
each other. We then combine two facts. First, here FairGap has at most Î´/2 probability of constructing
incorrect confidence intervals assuming perfect uniform random sampling. Second, ApproxFairGap has
probability at most  â‰¤ Î´/2T2
of identifying a xÌ‚âˆ—
t different from that of FairGap by the above argument. A
union bound then implies that ApproxFairGap has probability at most Î´/2 of identifying a different xÌ‚âˆ—
t
than FairGap. Combining the probability of FairGap failing and ApproxFairGap failing to approximate
FairGap, we get that ApproxFairGap has probability at most Î´/2 + Î´/2 = Î´ of misidentifying xâˆ—
. Thus
ApproxFairGap is -approximately fair.
To analyze ApproxFairGapâ€™s regret, note that in the case where ApproxFairGap correctly identifies xâˆ—
t ,
ApproxFairGapâ€™s use of Î´/2 rather than Î´ adds a factor of 2 inside the log in the original regret statement
of FairGap. Next, ApproxFairGap incorrectly identifies xâˆ—
t with probability at most Î´ by the logic above,
so taking Î´ â‰¤ 1/T1+c
as in the proof of Theorem 4 implies the claim.
18
7.5 Proofs from Section 5
Proof of Theorem 3. Let A be a fair algorithm. For any input Î´, A is round-fair for all t â‰¥ 1 with probability
1 âˆ’ Î´. Since this holds for any Î² with probability at least 1 âˆ’ Î´, then it necessarily holds with probability
at least 1 âˆ’ Î´ over any prior Ï„ on Î² with support contained in the unit rectangle. Our first lemma gives an
alternative way to view the framework which draws Î² âˆ¼ Ï„ and then plays according to A.
Let xt denote the action chosen by A at time step t, and let yt denote the observed reward. Let the
joint distribution of ((x1, y1), . . . (xt, yt), Î²) be denoted by Wt. Lemma 7 is similar in content to Lemma 4 in
Joseph et al. [2016]; its proof follows from Bayesâ€™ Rule.
Lemma 7. Let Î²0
at time t be drawn from Ï„|ht, its posterior distribution given the observed sequence of
choices and rewards ht = ((x1, y1), . . . (xtâˆ’1, ytâˆ’1)) âˆˆ (C Ã— R)tâˆ’1
. Then let W0
t be the joint distribution of
(ht, (xt, yt), Î²0
). Wt and W0
t are identical distributions.
Lemma 7 states that whether the instance draws Î² âˆ¼ Ï„ once and then plays according to A, or re-draws
Î² from its posterior at each time-step, the joint distribution on instances and observations is unchanged at
each step. We can thus assume without loss of generality that, given a prior Ï„, at each time step t we redraw
Î² âˆ¼ Ï„|ht . Taking this posterior viewpoint, we have the following lemma.
Lemma 8. Given a fixed prior Ï„, let A be fair and let Î² âˆ¼ Ï„. Let Ï€t be the distribution on actions of A at time
t, and let ft be the pdf of Ï€t. Then with probability at least 1 âˆ’ 4Î´, at each time t, if PÏ„|ht
[hÎ², yi > hÎ², xi] > 1
4 ,
then ft(y) â‰¥ ft(x).
This means that with probability at least 1 âˆ’ 4Î´, whenever the posterior distribution at time t tells us
that point y has a higher reward than point x with probability at least 1
4 over the posterior distribution of Î²,
we must play y with at least the same probability as x.
We will use this lemma, in combination with results about a specific posterior, to constrain the possible
actions any fair algorithm can take.
We now introduce the specific prior Ï„. Let Î² have prior distribution Ï„ âˆ¼ {1} Ã— U[âˆ’, âˆ’]. We first
analyze the posterior distribution of Î². We then show that with probability at least 1 âˆ’ 4Î´, until the posterior
distribution differs from the prior, Lemma 8 forces A to play uniformly from Ct.
Suppose that we have observed (x1, y1) . . . (xtâˆ’1, ytâˆ’1). Since the prior in the second coordinate is U[âˆ’, ],
and the noise Î·i is also uniform, the posterior in the second coordinate is uniform over all Î²2 consistent with
the observed data in the following sense: since the noise Î·t0 is bounded, each pair (xt0 , yt0 ) gives a bound on
Î²2. Combining yt0 = xt0,1 + Î²2xt0,2 + Î·t0 and Î·t0 âˆˆ [âˆ’1, 1] we get
Î²2 âˆˆ [lt0 , ut0 ] =

min

yt0 âˆ’ xt0,1 âˆ’ 1
xt0,2
,
yt0 âˆ’ xt0,1 + 1
xt0,2

, max

yt0 âˆ’ xt0,1 âˆ’ 1
xt0,2
,
yt0 âˆ’ xt0,1 + 1
xt0,2

. (4)
Since by the prior we know Î²2 âˆˆ [âˆ’, ], we say that Î²2 is consistent with ht if Î²2 âˆˆ [âˆ’, ] and
Î²2 âˆˆ [supt0 lt0 , inft0 ut0 ]. This is the content of the following lemma.
Lemma 9. Let yt0 = hÎ², xt0,i + Î·t0 , Î·t0 âˆ¼ U[âˆ’1, 1], and Î² âˆ¼ {1} Ã— U[âˆ’, ]. Then Ï„(Î²2|ht) is uniform on the
set of Î²2 consistent with ht.
We now define and analyze S, the number of rounds required before the posterior distribution of Î²2
becomes non-uniform. Each (xt0 , yt0 ) gives the constraint on Î² given in Equation 4. This only changes the
posterior from the prior if lt0 > âˆ’ or ut0 < . Assume first that xt0,2 > 0 (by symmetry, a similar argument
holds for xt0,2 < 0). Then ut0 =
yt0 âˆ’ xt0,1 + 1
xt0,2
and we can calculate
P

yt0 + 1 âˆ’ xt0,1
xt0,2
< 

= P

Î·t0 + xt0,2Î²2 + 1
xt0,2
< 

= P [Î·t0 + 1 < xt0,2( âˆ’ Î²2)]
â‰¤ P [Î·t0 + 1 < 2] = 
19
where the last equality follows from the fact that Î·t + 1 âˆ¼ U[0, 2]. The probability that the lower bound is
greater than âˆ’ is similarly
P

yt0 âˆ’ 1 âˆ’ xt0,1
xt0,2
> âˆ’

= P [Î·t0 > 1 + xt0,2(âˆ’ âˆ’ Î²2)] â‰¤ .
Thus the probability that any pair (xt0 , yt0 ) alters the posterior distribution of Î²2 from U[âˆ’, ] is at most
2. 7
It follows that P(S â‰¥ t0
) â‰¥ (1 âˆ’ 2)t0
, and that the posterior coincides with the prior Ï„ for â„¦(1/) steps
in expectation.
Now assume that after t âˆ’ 1 steps the posterior distribution is equal to Ï„: we will argue that any
non-uniform distribution violates round fairness in round t with probability at least 3
4 . Call two points
a, b âˆˆ Ct = [âˆ’1, 1]2
vertically equivalent if a1 = b1, i.e. they agree in their first coordinate. Consider some
pair of points a = (x1, x2), b = (x1, x3) âˆˆ Ct which are vertically equivalent with x2 > x3. Suppose A
plays a with higher probability than b. If Î²2 < 0, then hÎ², ai < hÎ², bi, and P [Î²2 < 0] = 1/2 > 1
4 . Thus A
violates round-fairness in round t with probability more than 1
4 . Similarly, if Î²2 > 0, then hÎ², ai > hÎ², bi, and
P [Î²2 > 0] = 1/2 > 1
4 , so if A plays b with higher probability than a then A again violates round-fairness in
round t with probability strictly larger than 1
4 . Thus, any two vertically equivalent points must be played
with equal probability.
Next, consider any point b âˆˆ Ct of the form x1 âˆ’ Î±, x2 + 2Î±


for some Î± âˆˆ R. Call any two points
of this form, for fixed (x1, x2) and variable Î± âˆˆ R, diagonally equivalent. Let a = (x1, x2). If Î²2 > /2,
then hÎ², bi â‰¥ x1 âˆ’ Î± + x2Î²2 + Î± = x1 + x2Î²2 = hÎ², ai. Since Î²2 > /2 with probability 1
4 , point b must be
played with probability at least that of a to satisfy round-fairness in round t with probability greater than 3
4 .
Symmetrically, when Î²2 < âˆ’
2 , which happens with probability 1
4 , a must have at least as much probability of
being played as b. Thus, any two diagonally equivalent points must also be played with equal probability.
Given a point x âˆˆ Ct, let Hx denote the transitive closure under vertical and diagonal equivalence of the
point x. Since points that are equivalent must be played with equal probability, by the transitive property
all points in Hx must be played with equal probability by A. We now show that when x is a corner of Ct,
Hx = Ct.
Lemma 10. Let x = (1, âˆ’1). Then Hx = Ct.
Lemma 10 shows that if A is fair at a given round t with probability at least 3
4 over the posterior, and
the posterior is Ï„, then A must play uniformly at random from Ct.
Thus, we have shown for any fair A:
1. With probability at least 1 âˆ’ 4Î´, A must be fair with probability at least 3
4 at all t â‰¥ 1 (Lemma 8)
2. If S is the number of rounds until Ï„ 6= Ï„|ht
, P(S â‰¥ t) â‰¥ (1 âˆ’ 2)t
3. When Ï„ = Ï„|ht
(i.e. S â‰¥ t), and A is fair with probability > 3
4 over Ï„|ht
, then A must play uniformly
at random from Ct
Let  < min(1/2, 1/ log(2/Î´)) and let the event that A is fair with probability at least 3
4 over the posterior
at all t â‰¥ 1 be denoted by F. Recalling that S denotes the number of rounds required before the posterior
distribution of Î²2 becomes non-uniform, let the event that S â‰¥ log(1âˆ’Î´)
log(1âˆ’2) be denoted by E. Then
P [E] â‰¥ (1 âˆ’ 2)
log(1âˆ’Î´)
log(1âˆ’2) = 1 âˆ’ Î´,
so
P [E âˆ© F] â‰¥ P [E] + P [F] âˆ’ 1 â‰¥ 1 âˆ’ 5Î´.
7Note that this bound holds regardless of the particular choice of xt0 , which is why the probabilities above are over the draw
of the rewards yt0 , conditional on the chosen xt0 .
20
We now condition on F âˆ© E to show that with high probability Regret (T) = â„¦Ìƒ(1
 ):
P

Regret (T) â‰¥ â„¦Ìƒ

1


â‰¥ P

Regret (T) â‰¥ â„¦Ìƒ

1


| E âˆ© F

P [E âˆ© F]
â‰¥ P

Regret (T) â‰¥ â„¦Ìƒ

1


| E âˆ© F

(1 âˆ’ 5Î´)
(5)
where the first inequality follows from Bayesâ€™ rule. However, weâ€™ve shown that whenever E âˆ© F occurs,
for at least log(1âˆ’Î´)
log(1âˆ’2) â‰¥ log(1/[1âˆ’Î´])
2 (via log(x) â‰¤ x âˆ’ 1 for x > 0) rounds A plays uniformly at random from
Ct. Let rA(t) be the regret accrued at round t by uniformly at random play, E[rA(t)] = ||Î²||1 = â„¦(1) = c.
Then 0 â‰¤ rA(t) â‰¤ 2(1 + ), and the rA(t) are independent since A is playing uniformly at random at each t.
By Hoeffdingâ€™s inequality for bounded random variables,
P
" T
X
t=1
rA(t) â‰¤ T Â· c âˆ’
p
2T log(2/Î´)(1 + )
#
â‰¤ Î´
which means
P
" T
X
t=1
rA(t) â‰¥ T Â· c âˆ’
p
2T log(2/Î´)(1 + )
#
â‰¥ 1 âˆ’ Î´ (6)
and when taking T = 1
 we get
P
" T
X
t=1
rA(t) â‰¥
1

Â· c âˆ’
r
2

log(2/Î´)(1 + )
#
â‰¥ 1 âˆ’ Î´
or suppressing constants and lower order terms and using the fact that  < 1/ log(2/Î´), P
hPT
t=1 rA(t) â‰¥ â„¦Ìƒ(1
 )
i
â‰¥
1âˆ’Î´. This gives us that P
h
Regret (T) â‰¥ â„¦Ìƒ(1
 ) | E âˆ© F
i
â‰¥ 1âˆ’Î´
1âˆ’5Î´ . Hence by Equation 5, P
h
Regret (T) â‰¥ â„¦Ìƒ(1/)
i
â‰¥
1 âˆ’ Î´, as desired.
We now provide the proofs of the lemmas used above.
Proof of Lemma 8. By the definition of fairness, and Lemma 7, we have that
PÎ²tâˆ¼Ï„|ht ,htâˆ¼A [âˆƒt0
â‰¥ 1: A is round-unfair at time t0
] â‰¤ Î´.
Denote this probability by X. By the above E[X] â‰¤ Î´, and hence by Markovâ€™s inequality, P

X â‰¥ 1
4

â‰¤ 4Î´.
But then weâ€™ve shown that, with probability at least 1 âˆ’ 4Î´, for all t â‰¥ 1 A is fair with probability at least 3
4
over Î² âˆ¼ Ï„|ht. Now if âˆƒx, y such that PÏ„|ht
(hy0
, Î²i > hx0
, Î²i) > 1
4 but ft(x) > ft(y), then the probability that
A is unfair at time t is at least PÏ„|ht
(hy0
, Î²i > hx0
, Î²i) > 1
4 . This proves the claim.
Proof of Lemma 9. The fact that the posterior distribution of Î²2 is uniform on the set of consistent Î²2 is
immediate via Bayes rule: Ï„(Î²2|ht) = p(ht|Î²2)Ï„(Î²2), where p(ht|Î²2)Ï„(Î²2) âˆ 1 if Î²2 is consistent with ht, and
is 0 otherwise.
Proof of Lemma 10. Choose an arbitrary point y âˆˆ Ct with coordinates (z1, z2). We want to show y âˆˆ Hx.
Since any two points in Ct with the same x coordinate are vertically equivalent, it suffices to show that there
is a point with x-coordinate z1 âˆˆ Hx.
Fix 0 < Î± â‰¤ min(1, 2) and suppose 1âˆ’z1 = k Â·Î±, where k âˆˆ N. Note we can guarantee k âˆˆ N by choosing
an appropriate Î±. We now proceed by induction on k.
21
If k = 1, then by diagonal equivalence x is equivalent to x0
= (1 âˆ’ Î±, âˆ’1 + 2Î±/) = (z1, 1 + 2Î±/).
But by vertical equivalence, y âˆˆ Hx0 , and so y âˆˆ Hx, by transitivity. For the inductive step, construct
x0
= (z1 + Î±, z2 âˆ’ 2Î±/). Then 1 âˆ’ x0
1 = 1 âˆ’ z1 âˆ’ Î± = (k âˆ’ 1)Î±. Hence by induction x0
âˆˆ Hx. But since x0
is
diagonally equivalent to y = (z1, z2), then y âˆˆ Hx as desired. Since y was arbitrarily chosen, Hx = Ct. See
Figure 5 for a visualization of these equivalences.
7.6 Proofs from Section 6
Proof of Theorem 4. Let EÎ² be the event that given a fixed value of Î², A plays uniformly at random from
Ct for all t â‰¥ 1. If we can show that for any A and all Î², it is the case that P(EÎ²) = â„¦(1), this implies the
claim, since for any Î², T
E [Regret (T)] â‰¥ E [Regret (T) | EÎ²] P [EÎ²] = â„¦(T) Â· â„¦(1) = â„¦(T),
as desired.
By symmetry of S1
, P [EÎ²] = P [EÎ²0 ] for all Î², Î²0
âˆˆ S1
. So henceforth we can drop the subscript Î², and
use E to represent the event that A plays uniformly at random for all t â‰¥ 1. We now exhibit a prior Ï„ such
that for any A, P [E] = â„¦(1).
Lemmas 7 and 8 both apply; thus, we let Î² âˆ¼ Ï„, where Ï„ is the uniform distribution on S1
, U(S1
), and
we assume that at each time t, Î² is re-drawn from its posterior distribution Ï„|ht , as before. Let F again
be the event that A is round-fair with probability at least 3
4 at each round t, with respect to the posterior
distribution Ï„|ht. We again analyze the posterior distribution Ï„|ht
, showing that for any history ht, Ï„|ht
forces
A to play uniformly at random at t, conditioned on F.
As in Section 5 the posterior distribution of Î²|ht is uniform on the set of Î² âˆˆ S1
that are consistent with
the observed data. By consistent we again mean in the sense of Lemma 9; the proof is nearly identical and
relies on boundedness of the noise Î·t, so we do not repeat it here. Denote by Gt âŠ‚ S1
the set of consistent Î² at
time t. We will use Lemma 11 to reason about the topology of Gt. We use the relative topology throughout.
Lemma 11. For any t â‰¥ 1 and any history ht, Gt is a nonempty connected open subset of S1
.
Gt is an open, non-empty, connected subset of S1
; since weâ€™re working in the relative topology, it must
be exactly an open interval along the boundary of S1
, as illustrated in Figure 6. Let Gt have length , and
correspondingly Ï„|ht = U(Gt).
Condition on the occurrence of F: that A must be fair in round t with probability at least 3
4 , with respect
to Ï„|ht
. We claim that this in fact forces A to play uniformly from S1
at all time steps t, in an argument
similar to Lemma 10.
We say that two points x, y âˆˆ S1
are equivalent at time t if PÎ²âˆ¼Ï„|ht
[hÎ², xi > hÎ², yi] âˆˆ [1
4 , 3
4 ]. Let Sx,t be
the transitive closure of the set of y âˆˆ S1
that are equivalent to x at time t.
Lemma 12. Let Ï„|ht
âˆ¼ U(Gt). Then there exists x âˆˆ S1
such that Sx,t = S1
.
Proof of Lemma 12. By definition, if PÏ„|ht
[hÎ², xi < hÎ², yi] âˆˆ [1
4 , 3
4 ], then y âˆˆ Sx,t. Every point on S1
can be
represented as (cos Î¸, sin Î¸), so let Î¸x denote the angle corresponding to x, and let x be the point in Gt such
that PÏ„|ht
[Î² < Î¸x] = 1
4 .
Now let St,âˆ’ = {z âˆˆ Gt : Î¸z â‰¥ Î¸x} and let St,+ = {z âˆˆ Gt : Î¸z â‰¤ Î¸x}. If Î² âˆˆ St,+, then for all
z âˆˆ St,âˆ’, Î² Â· z â‰¤ Î² Â· x. By construction, PÏ„|ht
[Î² âˆˆ St,+] = 1
4 , and hence St,âˆ’ âŠ‚ Gx,t. But defining x1 as
PÏ„|ht
[Î² > Î¸x1
] = 1
4 , S0
t,+ as the set {z âˆˆ Gt : Î¸z > Î¸x1
}, and S0
t,âˆ’ as {z âˆˆ Gt : Î¸z < Î¸x1
}, the same reasoning
shows that S0
t,âˆ’ âŠ‚ Sx1,t. Since St,âˆ’ âˆª S0
t,âˆ’ = Gt, this forces Gt âŠ‚ Sx1,t = Sx,t.
We now show Sx,t contains the rest of the boundary of S1
, not just Gt. Let G1
+ denote the arc of length
1
4  adjoining S0
t,+ as in Figure 6, and define G1
âˆ’ accordingly. Now note that we must have G1
+ âˆˆ Gx,t, since
if Î² > Î¸x1 then for all z âˆˆ G1
+, Î² Â· z > Î² Â· x, and PÏ„|ht
[Î² > Î¸x1 ] = 1
4 . Similarly, G1
âˆ’ has to be added to
Sx1,t = Sx,t as well. But then letting the segment G1
âˆ’ âˆª G1
+ âˆª Gt be denoted by G0
t, we can repeat the
argument: we set x0
, x0
1 to be their initial locations x1, x translated 1
4  to the right and left respectively, and
define G2
+, G2
âˆ’ analogously, as in the Figure 6.
22
Now we have that G2
+ âˆˆ Sx0,t, since if Î² âˆˆ S0
t,+ then for all z âˆˆ G2
+, Î² Â·z > Î² Â·x0
, and hence z âˆˆ Sx0,t = Sx,t.
The same logic shows that G2
+ âŠ‚ Sx0
1,t = Sx,t.
Since we can keep recursively chaining segments of fixed length 
4 to Sx,t, and S1
is of fixed length, a
simple induction argument forces Sx,t = S1
, as desired.
So Lemma 8 in combination with the above lemma forces the following: when A is constrained to be fair
with probability at least 3
4 with respect to the posterior distribution of Î², for all times t â‰¥ 1 and all histories
ht, A must play uniformly at random from S1
. But then P(E) â‰¥ P(E|F)P(F) = P(F) â‰¥ 1 âˆ’ 4Î´ = â„¦(1), by
Lemma 8.
Proof of Lemma 11. Ct 6= âˆ… is immediate since, for the true value Î², Î² âˆˆ Ct for all t. For Î² âˆˆ S1
to be
consistent with the data, i.e. in Ct, means that max1â‰¤iâ‰¤t |yi âˆ’ hÎ², xii| < 1 and Î² âˆˆ S1
.
We can rephrase this as follows: if fi(Î²) = |y âˆ’ hÎ², xii|, and Ri = {Î² âˆˆ fâˆ’1
i (âˆ’âˆž, 1)}, then if we let
C0
t =
Tt
i=1 Ri, Ct = C0
t âˆ© S1
. Now we remark that each Ri is the intersection of the two open half spaces
{Î² : hÎ², xii < 1 + yi} and {Î² : hÎ², xii > yi âˆ’ 1}. Thus C0
t is the intersection of finitely many open half spaces,
and is thus an open, connected set (in fact, it is a convex polytope). Since Ct = S1
âˆ© C0
t, by definition Ct is
open and connected in the relative topology on S1
.
7.7 Experiments
Figure 7.7 depicts experiments conducted in the k-bandit setting. We employ a simple variant of UCB that
maintains generic normal confidence intervals around its ongoing estimate of Î² and uses these to construct
confidence intervals for the estimated rewards of the contexts is uses; it then selects all choices with a positive
upper confidence bound. We plot cumulative mistreatments through T = 10, 000 rounds, which tracks the
cumulative number of individuals who have seen an individual with lower expected quality chosen in a round
during which they were not chosen. The plot therefore shows that through 10,000 rounds our version of UCB
creates nearly 400 such mistreated people.
Our experiments use d = 2 and Î² âˆ¼ U[âˆ’1, 1]2
for each iteration. In each round we generate k = 10
contexts xi, also from U[âˆ’1, 1]2
, and generate noisy rewards Î² Â· xi + Î·t,i where Î·t,i âˆ¼ N(0, 1) is standard
normal noise. The results presented are averaged over 100 iterations. For completeness, we present Figure 7.7,
which plots cumulative mistreatments for both UCB and FairUCB and empirically validates our theoretical
fairness guarantee.
Our second experiment investigates the structure of mistreatment in UCB. We use d = 2, Î² = [1, 0], k = 10
for each iteration. At each round t with probability p âˆˆ [.8, .95] we draw a context (x, x), where x âˆ¼ U[âˆ’1, 1]
and with probability 1 âˆ’ p draw a context from U[âˆ’1, 1]2
. These two types of contexts naturally encode
two populations: in population 1, the two features are perfectly correlated and in population 2 they are
independent. However, Î² = [1, 0] crucially means that the second feature does not affect reward. Our
experiments aim to study how this correlation affects mistreatment rates in the different populations.
For each population we plot the fraction of mistreatment individuals from each population for T = 1, . . . 25,
averaging over 1000 iterations. Figure 7.7 shows that for p âˆˆ [.8, .95] unfairness accrues at substantially
different rates to the two populations. Somewhat counter-intuitively, members of the majority group are
significantly more likely to be mistreated than members of the minority group, a natural consequence of
UCB-style algorithms favoring minority contexts whose confidence intervals have more uncertainty. While
mistreating a majority population may be less obviously unfair than mistreating a minority population, it
is still undesirable. In particular, there may be natural practical settings where the group that has faced
historical discrimination is the majority population in sample (e.g. criminal sentencing) and so discriminating
against the majority is more obviously unfair.
23
1: procedure RidgeFairm(Î´, T, k, Î³ â‰¥ 1, ExactBool)
2: for t â‰¥ 1, 1 â‰¤ i â‰¤ k do
3: Let Xt, Yt = design matrix, observed payoffs before round t
4: Let Ct be the choice set in round t
5: Let VÌ„t = Xt
T
Xt + Î³I
6: Let Î²Ì‚t = (VÌ„t)âˆ’1
Xt
T
Yt . regularized least squares estimator
7: Let yÌ‚t,i = hÎ²Ì‚t, xt,ii for each xt,i âˆˆ Ct
8: Let wt,i = ||xt,i||(VÌ„t)âˆ’1 (
q
2d log(1+t/Î³
Î´ ) +
âˆš
Î³)
9: Let [`t,i, ut,i] = [yÌ‚t,i âˆ’ wt,i, yÌ‚t,i + wt,i] . Conf. int. for yÌ‚t,i
10: if ExactBool then
11: Pick (m, {(xt,i, [`t,i, ut,i])})
12: else pickâ‰¤ (m, {(xt,i, [`t,i, ut,i])})
13: Update design matrices Xt+1 = Xt :: Xt, Yt+1 = Yt :: Yt.
14: procedure Pick(m, (xt,1, [`t,1, ut,1]), . . . , (xt,k, [`t,k, ut,k]))
15: Let M = Ct
16: Let Pt = âˆ…
17: while |Pt| < m do
18: Let xt,iÌ‚ = argmaxxt,iâˆˆM ut,i . Highest UCB not yet selected
19: Let St be the set of actions in Ct chained to xt,iÌ‚ . Highest chain not yet selected
20: if |St| â‰¤ m âˆ’ |Pt| then
21: Pt = Pt âˆª St . Take the chain with probability 1
22: M = M \ St
23: else
24: Let Qt be m âˆ’ |Pt| actions chosen UAR from St
25: Let Pt = Pt âˆª Qt . fill remaining capacity UAR from the chain
26: Play Pt
27: procedure pickâ‰¤(m, (xt,1, [`t,1, ut,1]), . . . , (xt,k, [`t,k, ut,k]))
28: Let Pt = {all actions chained to any xt,i âˆˆ Ct with ut,i > 0 }
29: Let M = Ct
30: Let Pt = âˆ…
31: while |Pt| < m and ut,xt,iÌ‚
> 0 for xt,iÌ‚ = argmaxxt,iâˆˆM ut,i do
32: Let St be the set of actions in Ct chained to xt,iÌ‚ . Highest chain not yet selected
33: if |St| â‰¤ m âˆ’ |Pt| then
34: Pt = Pt âˆª St . Take the chain with probability 1
35: M = M \ St
36: else
37: Let Qt be m âˆ’ |Pt| actions chosen UAR from St
38: Let Pt = Pt âˆª Qt . fill remaining capacity UAR from the chain
39: Play Pt
Figure 3: RidgeFairm, a fair no-regret algorithm for picking â‰¤ m actions whose payoffs are linear.
24
1: procedure FairGap(Î´, C,Î»)
2: for t â‰¥ 1 do
3: if 2r ln(2dt/Î´)/Î» â‰¥ t then
4: Play xÌ‚t âˆ¼UAR Ct
5: Update design matrices Xt+1, Yt+1
6: else
7: Let Î´ = min(Î´, 1/t1+c
)
8: Let Î²Ì‚t = (Xt
T
Xt)âˆ’1
Xt
T
Yt . Least squares estimator
9: Let Îº = 1 âˆ’ r
p
2 ln(2dt/Î´)/tÎ»
10: Let wt =
r2
Â·RÂ·2
âˆš
ln(2tÎ´)
kÎ»
âˆš
t
. Confidence interval width
11: Let (x1, x2) = TopTwo(Ct, Î²Ì‚t) . Find two ext. pts. maximizing hx, Î²Ì‚ti
12: Let U1 = [hÎ²Ì‚t, x1i âˆ’ wt, hÎ²Ì‚t, x1i + wt]
13: Let U2 = [hÎ²Ì‚t, x2i âˆ’ wt, hÎ²Ì‚t, x2i + wt]
14: if U1 âˆ© U2 = âˆ… then
15: Let FoundMax = {x}
16: Play xÌ‚t = x . Play xÌ‚t once confidence intervals separate
17: else
18: Play xÌ‚t âˆ¼UAR Ct
19: Update design matrices Xt+1, Yt+1
Figure 4: FairGap, a fair no-regret algorithm for infinite, changing action sets.
x
y
(1 âˆ’ Î±, âˆ’1 + 2Î±

)
(1, âˆ’1)
(z1, z2)
k Â· Î±
Figure 5: A path connecting (1, âˆ’1) to an arbitrary point (z1, z2): red segments are vertically equivalent,
blue segments are diagonally equivalent.
25
Figure 6: A must play UAR from D = S1
. |Gt| = ; |S0
t,âˆ’| = |St,âˆ’| = 3
4 ; |S0
t,+| = |St,+| = |G1,+| = |G1,âˆ’| =
|G2,+| = |G2,âˆ’| = 
26
Figure 7: Cumulative mistreatments for UCB and FairUCB.
Figure 8: Probability of mistreatment for subpopulations under UCB.
27
