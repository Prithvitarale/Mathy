Measuring the Robustness of Graph Properties
Yali Wan
Amazon.com
Palo Alto, CA 94301
yalwan@amazon.com
Marina Meila
University of Washington
Seattle, WA 98195
mmp@stat.washington.edu
Abstract
In this paper, we propose a perturbation framework to measure the robustness of
graph properties. Although there are already perturbation methods proposed to
tackle this problem, they are limited by the fact that the strength of the perturbation
cannot be well controlled. We firstly provide a perturbation framework on graphs
by introducing weights on the nodes, of which the magnitude of perturbation can
be easily controlled through the variance of the weights. Meanwhile, the topology
of the graphs are also preserved to avoid uncontrollable strength in the perturbation.
We then extend the measure of robustness in the robust statistics literature to the
graph properties.
1 Introduction
In data science it is natural to consider an observed graph, or network, G as a realization of a
random process. Consequently, the properties of G (such as diameter, conductance, clustering)
and the inferences drawn from them are incomplete without some measure of variability or confidence.
We propose a new methodology for evaluating the robustness of graph properties by measuring the
effect of small perturbations of the graph on the respective property. This methodology is based on
concepts from robust statistics [4] and exploits the technique introduced in [8] to augment a graph
Laplacian with user-defined weights. Let G = (V, A) be a graph with n = |V| nodes and adjacency
matrix A; A can be symmetric or asymmetric (corresponding to a directed network), and we assume
Aij âˆˆ {0, 1}, Aii = 0 (simple graph) or Aij â‰¥ 0 (weighted graph). Our methodology applies to
both scenarios, and makes no assumptions on how the graph was generated.
Our methodology consists of four key components:
1. Perturb the nodes of the graph by assigning them multiplicative weights, with wi the weight
of node i, i = 1, 2, . . . n. If wi = 1 for all i, we have the original graph G.
2. Express the desired graph property f(G) as a smooth function of the weights.
3. Construct measures of robustness inspired by the robust statistics literature, such as influence
function (IF), and breakdown point (BP).
4. Evaluate these measures on the current graph G.
We exemplify our approach by examining the robustness of weighted cut (WCut), number of weakly
Connected Components (wCCâ€™s), eigengap and clustering. In this paper, we make use of the
following notations. We define wi, i = 1, 2, Â· Â· Â· , n, the weight associated with node i. Since we are
interested in both directed and undirected graphs, we define di =
Pn
j=1 Aij as the out-degree of node
i. We use the definition in [8] and define L = I âˆ’ 1
2 Dâˆ’1/2
(A + AT
)Dâˆ’1/2
as the Laplacian matrix
associated with A. This definition is consistent with the usual definition of L = I âˆ’ Dâˆ’1/2
ADâˆ’1/2
arXiv:1901.09661v1
[cs.SI]
3
Dec
2018
when the graph is symmetric. We define A, d, D, L as the properties of the observed graph. The
above quantities will be marked with a symbolËœwhen they are perturbed. This is consistent with the
notation in matrix perturbation literature. To demonstrate, AÌƒ represents the adjacency matrix of the
perturbed graph.
In the rest of this paper, we proceed as follows. In Section 2, we describe our method of
bootstrapping and perturbing the networks. In Section 3, we talk about the graph properties of
interest. In Section 4, we discuss measures for evaluating robustness of graph properties. In
Section 5, we discuss breakdown points. In Section 6, we discuss related work. In Section 7,
we use both synthetic and real datasets to analyze our methods. We conclude our findings in Section 8.
2 Perturbing the network
Existing methods of perturbing networks can be found in recent works [5, 3, 2, 1]. They mostly
involve removing or duplicating edges or nodes randomly and independently in the graph. [3]
randomly removes edges from the graph. [5] maintains the the number of vertices and edges of the
original graph, and perturbs the graph by moving the edges to the other locations. On the other
hand, [2] and [1] bootstrap the network by subsampling the nodes. Although their approach is
straightforward, the perturbation cannot be well controlled. Firstly, the topology of the graphs can
change dramatically for sparse graphs. Randomly removing or adding the nodes and edges makes the
strength of perturbation hard to control. For example, assume we have two densely clusters C1 and C2
connected by a single edge e. The effect of removing e versus removing an edge within C1 or C2 is
very different, since the former will change the graph structure drastically by making it disconnected.
Moreover, removing or adding edges (nodes) are discrete moves, therefore the perturbation cannot be
arbitrarily small.
In order to have a fine control on the amount of perturbation and make it as smooth as possible, in
our approach, we preserve the graph topology by keeping all the nodes and edges, and we perturb
the graph by assigning random weights to the nodes and then distribute these weights to edges.
Specifically, we assign weight wi to node i, where wi is generated i.i.d from a distribution with
E(wi) = 1, standard deviation Ïƒw, and support on (0, âˆž). The perturbation can be controlled
smoothly with Ïƒw. Since wi > 0, no nodes or edges is removed or added, and the topology of the
graph is preserved.
We propose two ways of constructing AÌƒ from weighted nodes.
â€¢ Asymmetric perturbation: perturb outgoing edges of node i, so that AÌƒij = wiAij (whereas
AÌƒji = wjAji). The out-degree becomes Ëœ
di = diwi. The perturbed Laplacian is,
LÌƒij = 1 âˆ’
AÌƒij + AÌƒji
2
q
Ëœ
di
Ëœ
dj
= 1 âˆ’
wiAij + wjAji
2
p
wiwjdidj
(1)
â€¢ Symmetric perturbation: place wi on both outgoing edges and incoming edges of node i.
AÌƒij = (wi + wj âˆ’ 1)Aij. Since this method leads to complicated LÌƒ in form, we discuss
perturbing one node at a time. If node t is perturbed, the Laplacian becomes,
LÌƒij =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
1 âˆ’
Atj wt+Ajtwt
2
âˆš
dtwt(
P
m6=t Ajm+Ajtwt)
i = t, j 6= t
1 âˆ’
Aij +Aji
2
âˆš
(
P
m6=t Ajm+Ajtwt)(
P
m6=t Aim+Aitwt)
i, j 6= t
0 i, j = t
(2)
Both of the above methods ensure E(AÌƒij) = Aij. Notice that in the current methods, it can be
easily shown that bias is introduced in Aij, in which case one cannot separate the perturbation from
structural change and the change of values in adjacency matrix.
2
Although both perturbation methods seem to be reasonable, they have graph-specific advantages.
The asymmetric method provides a very simple way for calculating a graph property like Ëœ
d after
perturbation. On the other hand, it is not interesting for the perturbation of weakly Connected
Components (wCCâ€™s), as we shall see in Section 4. On the other hand, the symmetric method
maintains a symmetric perturbation of adjacency matrix. If Aij = Aji, then AÌƒij = AÌƒji after
perturbation. However, LÌƒ is complicated in form. The symmetric perturbation is not very interpretable
for some graph properties (WCut), but it is useful for evaluating the robustness in eigengap and wCCâ€™s.
In the above methods, we put weights on nodes and then distribute the weights to edges. The
reason for doing this instead of directly perturbing the edges is that it is more natural to maintain
the association between nodes and edges. The edges connected to the same node should be
dependent rather than independent. Perturbing the weights on edges [5] independently neglects these
relationships, which has been found both unrealistic and disrupts the topology of sparse network.
Moreover, in many applications, nodes are more meaningful than edges. Firstly, a node often carries
its own attributes. Secondly, a node is described by the multiple nodes it is connected to. On the
other hand, it is more difficult to glean information from an edge. For example, in a Facebook
network, a node is a person with complex information including age, birth place, school, the friends
he connected to, etc. While an edge is formed when two people befriend each other, and we
cannot even learn how well these two people know each other and it tells us little on other information.
2.1 The bias in LÌƒ
The graph properties we shall study are closely related to L. It would be nice if the changes in graph
properties are only from the noise introduced in graph structure and keeping the entires in LÌƒ to be the
same with that in L in expectation. Since having bias in L introduces another factor for the change in
graph property, of which the strength of perturbation is hard to control. Therefore in principle, we
would want E(LÌƒij) = Lij. In the following paragraphs we prove that this is impossible under all the
current perturbation methods. For the current methods, where people move the edges and subsample
the nodes, the bias in LÌƒ is apparent. We show in Proposition 1 that both of our perturbation methods
introduce bias too.
Proposition 1 Assume wi, i = 1 : n are generated i.i.d from a distribution with E(wi) = 1, Ïƒw 6= 0,
wi > 0. For asymmetric perturbation, E(LÌƒij) < Lij.
If we further assume there exists a triangle < t, p, q > in the graph. For symmetric perturbation,
with node t perturbed, E(LÌƒij) < Lij.
Proof:
In the asymmetric perturbation, LÌƒ is shown in Equation 1. Assume Lij 6= 0. In order to have
E(LÌƒij) = Lij, we need
E(1 âˆ’
wiAij + wjAji
2
p
wiwjdidj
) = 1 âˆ’
Aij + Aji
2
p
didj
(3)
Aij(1 âˆ’ E(
r
wi
wj
)) + Aji(1 âˆ’ E(
r
wj
wi
)) = 0 (4)
Since wi and wj are i.i.d, we then have,
E(
r
wi
wj
) = E(
r
wj
wi
) = E(
âˆš
wi)E(
1
âˆš
wj
) = E(
âˆš
wi)E(
1
âˆš
wi
) = 1 (5)
Assume x =
âˆš
wi. Equivalently we have,
E(
1
x
) =
1
E(x)
(6)
Assume f(x) = 1
x . Since f(x) is strictly convex and Jensenâ€™s inequality yields, E(f(x)) â‰¥ f(E(x)),
that is, E(1
x ) â‰¥ 1
E(x) . where equality holds only when Ïƒ(x) = 0. Therefore E(
q
wi
wj
) > 1. We have
3
reached a contradiction. E(LÌƒ) < L.
In the symmetric perturbation, LÌƒ after perturbing node t is shown in Equation 2. In order have
E(LÌƒij) = Lij, we need,
E(1 âˆ’
Atjwt + Ajtwt
2
q
dtwt(
P
m6=t Ajm + Ajtwt)
) = 1 âˆ’
Atj + Ajt
2
p
dtdj
when i = t, j 6= t (7)
E(1 âˆ’
Aij + Aji
2
q
(
P
m6=t Ajm + Ajtwt)(
P
m6=t Aim + Aitwt)
) = 1 âˆ’
Aij + Aji
2
p
didj
when i, j 6= t (8)
That is,
E(
âˆš
wt)E(
1
p
aj + (1 âˆ’ aj)wj
) = 1, when i = t, j 6= t, (9)
E(
1
p
ai + (1 âˆ’ ai)wi
)E(
1
p
aj + (1 âˆ’ aj)wj
) = 1, when i, j 6= t, (10)
where ai =
P
m6=t Ajm
dj
and 0 â‰¤ ai â‰¤ 1.
Since there is a triangle < t, p, q > in the graph. We can easily derive from Equation 9 and 10 that
E(
âˆš
wt) = 1, and E( 1
âˆš
ai+(1âˆ’ai)wi
) = 1, âˆ€i.
E( 1
âˆš
ai+(1âˆ’ai)wi
) is a continous and differentiable function of ai. When ai = 1,
E( 1
âˆš
ai+(1âˆ’ai)wi
) = 1. When ai = 0, E( 1
âˆš
ai+(1âˆ’ai)wi
) = E( 1
âˆš
wi
) > 1
E(
âˆš
wi) = 1. Since
âˆ‚
âˆ‚ai
E( 1
âˆš
ai+(1âˆ’ai)wi
) < 0, E( 1
âˆš
ai+(1âˆ’ai)wi
) > 1 for 0 < ai < 1. We have a contradiction.
Therefore in conclusion, E(LÌƒ) < L.
We have proved that in theory there will be bias introduced in LÌƒ, it would be a good idea to examine
how much bias will actually be introduced in practice. Under asymmetric perturbation, one can
easily derive from the proof in Proposition 1 that E(
âˆš
w)E( 1
âˆš
w
) =
LÌƒij
Lij
. Therefore, the more
E(
âˆš
w)E( 1
âˆš
w
) deviates from 1, the more bias introduced in LÌƒij. Since there are very limited
number of conventional distributions that allow support on (0, âˆž) and mean equals 1, we propose a
class of mixture distributions Mixture(a, b, Ïƒ2
âˆ’, Ïƒ2
+, Tâˆ’, T+, p), where ap + b(1 âˆ’ p) = 1, Tâˆ’ is a
distribution with E(x) = a, Ïƒ(x) = Ïƒ2
âˆ’, with support on (0, 1), T+ is a distribution with E(x) = b,
Ïƒ(x) = Ïƒ2
+, with support on (1, âˆž). Assume w âˆ¼ Mixture(a, b, Ïƒ2
âˆ’, Ïƒ2
+, Tâˆ’, T+, p), then
p(w âˆ¼ Tâˆ’) = p, p(w âˆ¼ T+) = 1 âˆ’ p. We can easily show that E(w) = 1, Ïƒ2
(w) = pâˆ’Ïƒ2
âˆ’ + p+Ïƒ2
+.
The mixture distribution subsumes all the distributions concentrated on 1, with support on (0, âˆž).
In the following experiment, we generate wi from the distributions below accordingly.
1. Node resampling: wi is obtained from resampling the nodes with replacement to form a
sample of size N. Assume node i appears m times in the sample, wi = mn
N . Ïƒw =
q
nâˆ’1
N .
We can easily see as N goes up, Ïƒw decreases. We can then control Ïƒw by varying the size
of N.
2. Binary distribution: wi follows binary distribution on {a, b}, with P(wi = a) = p, P(wi =
b) = 1 âˆ’ p. b = 1âˆ’ap
1âˆ’p . E(wi) = 1, Ïƒ2
w = p(a âˆ’ 1)2
+ (1 âˆ’ p)(b âˆ’ 1)2
.
3. Gamma distribution: wi âˆ¼ Gamma(a, 1
a ). E(wi) = 1, Ïƒ2
w = 1
a .
4. Mixture-Gamma-Uniform distrbution: a = 0.5, Tâˆ’ = uniform(0, 1), T+ =
Gamma(bâˆ’1
Ïƒ+
, Ïƒ+) + 1.
5. Mixture-Lognormal-Uniform distribution: a = 0.5, Tâˆ’ = uniform(0, 1), T+ =
lognormal(b âˆ’ 1, Ïƒ+).
4
The results are shown in Figure 1. We observe that the bias is sensitive to the choice of weight
distribution. In specific, the bias introduced by Gamma distribution is growing exponentially
with Ïƒw, thus not a good option. Node resampling and Binary distribution generate smallest
bias. However node resampling can only allow the perturbation strength Ïƒw to be as much
as 1, otherwise the topology of the graph is changed. Binary distribution only allows two
choices of weight, which is very limited. The mixture distributions behave similarly in terms of
introducing bias. Although the bias is nontrivial, the fact that it is upper bounded and that wi can be
generated in a continuous manner make them good candidates for performing perturbations on graphs.
Figure 1: E(
âˆš
w)E( 1
âˆš
w
) under asymmetric perturbation. Each boxplot represents 100 repetitions.
We also evaluate the bias introduced in LÌƒ in the current methods including [5] and [2]. Since they
move the edges or delete nodes, the bias depends on the graph. We generate a graph from DC-SBM
model [10] with n = 800, wi âˆ¼ 0.5 + uniform(0.5, 1), and evaluate
E(LÌƒij )
E(Lij ) . As a reminder,
E(LÌƒij )
E(Lij ) = E(
âˆš
w)E( 1
âˆš
w
). In Figure 2, we show that both methods introduce large bias compare to
perturbation strength proportion of nodes sampled
Figure 2: Left: the bias from the method in [5]. Î± indicates the strength of perturbation. Right:
the bias from the method in [2]. Î² indicates the proprotion of nodes that are subsampled. Larger Î²
indicates larger perturbation strength.
5
our perturbation methods.
2.2 Partial perturbation and full perturbation
In this section we discussed perturbing the graph by adding i.i.d weights to the nodes. Although we
make i.i.d assumption on wi, it can be relaxed. In this paper, we consider two approaches to utilize
the weight perturbation. First, in order to evaluate the sensitivity of graph properties, we perturb all
the nodes i.i.d with different perturbation strength Ïƒw, and then investigate how the graph property
changes with respect to it. In specific, we fix E(w) = 1 and vary Ïƒw for all the nodes. Alternatively, in
order to probe the source of sensitivity, we can perturb a subset of nodes. For these nodes, we perturb
their weights i.i.d by fixing Ïƒw and vary E(w), from which we can discover the source of the ro-
bustness in graph properties. The use of these two perturbation approaches will be shown in Section 7.
3 Expressing graph properties with weights
The success of our approachs depend on our ability to express properties of interest as functions
f(G, w), which are continuous and differentiable w.r.t. the node weights w1:n. In this project we
focus on graph properties that depend on graph Laplacians. Many important graph properties depend
on Laplacians. For instance, the mixing time of the graph depends on the second smallest eigenvalue
Î»2(L). Diffusion distances [9] between nodes can also be approximated by the principal eigenvectors
and values of L. The number of connected components C of G is equal to the multiplicity of 0 in the
spectrum of L, etc. There are four graph properties that we are particularly interested in, which will
be used as examples in the following sections: the weighted cut of the graph (WCut), the number of
weakly connected components, the eigengap and clustering.
3.1 Weighted Cut(WCut)
Weighted cut is defined as a graph property associated with clustering in the general graph setting
[8], which can be used for both directed and undirected graphs. It is similar in motivation to the
normalized cut for undirected graph. Both aim in finding a cut of low weight in the graph while
balancing the sizes of the clusters. The multiway version of the normalized cut MNCut of [8] is a
special case of WCut.
Formally, WCut with respect to clustering C with K clusters is defined as
WCut(G, w, C) =
K
X
k=1
1
DÌƒk
X
iâˆˆCk
( Ëœ
di âˆ’
X
jâˆˆCk
AÌƒij) (11)
where DÌƒk =
P
iâˆˆCk
Ëœ
di. Further define DÌƒkk =
P
iâˆˆCk
P
jâˆˆCk
AÌƒij, we can then equivalently write
Wcut as,
WCut(G, w, C) =
K
X
k=1
(1 âˆ’
DÌƒkk
DÌƒk
) (12)
Small WCut suggests sparse connections between clusters, thus better quality of clustering.
3.2 Number of weakly Connected Components (wCCâ€™s) and eigengap
It is already know that the number of connected components (CCâ€™s) is not robust, since randomly
adding a node or removing some edges can easily change the number of CCâ€™s. Instead, we study the
number of weakly Connected Components (wCCâ€™s), where â€œweaklyâ€ means sparse connections
between CCâ€™s.
6
We propose a pair of functions fu(G, w, K) = Î»K+1(L(GÌƒ)) âˆ’
PK
k=1 Î»i(L(GÌƒ)),
fl(G, w, K) =
PK
k=1 Î»k(L(GÌƒ)) to describe the number of wCCâ€™s. The reason we use fl is
that, if there are K number of CCâ€™s, then fl = 0. We would then expect fl to be close to 0 for K
number of wCCâ€™s. We choose fu because we expect a significant gap between fl and Î»K+1(L(GÌƒ))
for a stable number of wCCâ€™s. When fu is away from 0 (respectively fl near 0) there are exactly K
â€œweakly connectedâ€ components in GÌƒ. If this holds for large perturbations, then K can be considered
robust.
The Kth eigengap is defined as fe(GÌƒ, w, K) = Î»K+1(L(GÌƒ)) âˆ’ Î»K(L(GÌƒ)). It indicates K principal
subspace when fe is large compared to the other eigengaps.
Notice that function f defined for number of wCCâ€™s and eigengap are only meaningful for undirected
graphs. Since the eigenvalues is not very interpretable for the directed graphs.
4 Influence functions
In order to evaluate the graph properties, we construct the target properties as differentiable functions
f(G, w) and see how much f(G, w) changes with respect to the size of perturbation of w. In this
section, we present tools for quantifying robustnes including Influence Function (IF) and Breakdown
Points. We firstly talk about Influence function (IF), which was invented by Hampel in [4]. The
importance of IF lies in its interpretation: it gives a picture of the infinitesimal behavior of the
asymptotic value. While numerous studies have been done on methods for analysis of data sampled
from a known distribution i.i.d, there has not been much work on using these tools to evaluate the
robustness of graph properties. Here we define for a perturbed graph GÌƒ,
IFt =
âˆ‚f(G, w)
âˆ‚wt w1:n=1
, (13)
which measures the local influence of wt on f.
4.1 Influence function of WCut
Proposition 2 Assume a graph G with C, di defined as usual. Assume node t âˆˆ Ck0
. dik =
P
jâˆˆCk
Aij, dki =
P
jâˆˆCk
Aji. Dk0kÂ¬
0
=
P
iâˆˆCk0
,j /
âˆˆCk0
Aij. Then using asymmetric perturbation,
the influence function for node t is
IFW Cut
t (G, C) =
dtDkk âˆ’ dtkDk
D2
k
. (14)
Using symmetric perturbation.
IFW Cut
t (G, C) =
n
X
k=1
Dkkdkt
D2
k
âˆ’
Dk0 dk0t + Dk0kÂ¬
0
Dtk0 âˆ’ Dk0k0
P
j /
âˆˆCk0
Atj
D2
k0
(15)
Proof:
We firstly perturb the graph using the asymmetric method. Assume t âˆˆ Ck, we have
âˆ‚WCut(G, w)
âˆ‚wt
=
âˆ‚
âˆ‚wt
1
P
jâˆˆCk
djwj
X
jâˆˆCk
wj(dj âˆ’ djk) (16)
=
dt
P
jâˆˆCk
wjdjk âˆ’ dtk
P
jâˆˆCk
djwj
(
P
jâˆˆCk
djwj)2
. (17)
The influence function is then derived as
âˆ‚WCut(G, w)
âˆ‚wt
|w1:n=1 =
dt
P
jâˆˆCk
djk âˆ’ dtk
P
jâˆˆCk
dj
(
P
jâˆˆCk
dj)2
=
dtDkk âˆ’ dtkDk
D2
k
(18)
(19)
7
In the symmetric perturbation, since the perturbation can be explained as perturbing one node at a
time, one can easily show that the influence function is same for perturbing one node or multiple
nodes. For simplicity, we assume perturbing node t with weight wt. WCut can be written as
WCut(G, w) =
X
k,t/
âˆˆCk
(1 âˆ’
Dkk
P
iâˆˆCk
[(
P
j6=t Aij) + Aitwt]
)+
(1 âˆ’
P
iâˆˆCk0
Aitwt +
P
jâˆˆCk0
Atjwt +
P
i,j6=t,i,jâˆˆCk0
Aij
dtwt +
P
iâˆˆCk0
,i6=t[(
P
j6=t Aij) + Aitwt]
)
(20)
âˆ‚WCut(G, w)
âˆ‚wt
|w1:n=1 =
X
k,t/
âˆˆCk
Dkk
P
iâˆˆCk
Ait
D2
k
âˆ’
(
P
iâˆˆCk0
Ait +
P
jâˆˆCk0
Atj)Dk0
âˆ’ Dk0k0
(dt +
P
iâˆˆCk0
Ait)
D2
k0
(21)
=
n
X
k=1
Dkk
P
iâˆˆCk
Ait
D2
k
âˆ’
Dk0
P
iâˆˆCk0
Ait + Dk0kÂ¬
0
P
jâˆˆCk0
Atj âˆ’ Dk0k0
P
j /
âˆˆCk0
Atj
D2
k0
(22)
=
n
X
k=1
Dkkdkt
D2
k
âˆ’
Dk0
dk0t + Dk0kÂ¬
0
dtk0
âˆ’ Dk0k0
P
j /
âˆˆCk0
Atj
D2
k0
(23)

In the asymmetric perturbation, IF has an intuitive interpretation when a point has no influence, i.e,
IFt = 0,
dtk
di
=
Dkk
Dk
=
meaniâˆˆCk
dik
meaniâˆˆCk
di
(24)
It means that, node t has 0 influence in WCut when the proportion of edges that goes to CK equals
the cluster level ratio of averages. If IFt > 0, node t tends to make WCut larger when more weight
is put upon t, the quality of clustering decreases since the clustering becomes less well separated.
Node t is therefore considered unstable to the clustering, or not well clustered. If IFt < 0, node t is
well clustered since WCut will decrease if t is weighted more. Hence IF measures the robustness of
clustering in node level. It is worth noticing that the influence of a node depends only on the cluster
that the node belongs to, and is independent of the rest of the clusters. Moreover, the influences of
the nodes within a cluster always cancel each other. In a well separated clustering, we would expect
the influences of the nodes the be concentrated around 1. When the clusters are completed separated,
that is, there is no edge between clusters, it can be easily shown that all the node influence equals 0.
X
iâˆˆCk
âˆ‚WCut(G, w)
âˆ‚wi
|w1:n=1 =
P
iâˆˆCk
di
P
jâˆˆCk
djk âˆ’
P
iâˆˆCk
dik
P
jâˆˆCk
dj
(
P
jâˆˆCk
dj)2
= 0, (25)
For the symmetric perturbation, the meaning for the above results is not very interpretable, since its
form is very complicated and IFt = 0 does not provide us with any clear interpretation in its balance
state.
4.2 Influence function of number of wCCâ€™s and eigengap
Here we consider the property of wCCâ€™s and eigengap described by fu, fl and fe, which are proposed
in Section 3.2. Since they both depend on Î», we firstly study âˆ‚Î»k
âˆ‚wt
for a single Î»k in Proposition 5..
The influence functions of interest can be easily derived from there. This is because the influence
functions can be written as,
IFfu
t =
âˆ‚Î»K+1
âˆ‚wt
âˆ’
K
X
i=1
âˆ‚Î»i
âˆ‚wt
(26)
8
IFfl
t =
K
X
i=1
âˆ‚Î»i
âˆ‚wt
(27)
IFfe
t =
âˆ‚Î»K+1
âˆ‚wt
âˆ’
âˆ‚Î»K
âˆ‚wt
(28)
Unfortunately, asymmetric perturbation is not very interesting for undirected graphs, since the
properties remain untouched despite the perturbation, as will be shown in Proposition 3. This is
because the eigengap of L is equal to the eigengap of the transition matrix P = Dâˆ’1
A, and P stays
unchanged after the asymmetric perturbation. This motivates the use of symmetric perturbation, the
results of which are shown in Proposition 5.
Proposition 3 Using asymmetric perturbation, assume A symmetric and Î»k of multiplicity 1.
âˆ‚Î»k
âˆ‚wt
|w1:n=1 = 0. vi is the i-th element of the k-th eigenvector of L.
Proof:
âˆ‚Î»Ìƒk
âˆ‚wt
=
X
ij
âˆ‚Î»Ìƒk
âˆ‚LÌƒij
âˆ‚LÌƒij
âˆ‚wt
=
X
ij
vivj
âˆ‚LÌƒij
âˆ‚wt
(29)
Since LÌƒij = 1 âˆ’
Aij wi+Ajiwj
2
âˆš
wiwj didj
We then obtain,
âˆ‚LÌƒij
âˆ‚wi
=
âˆ’2Aij
p
wiwjdidj + Aij
p
wiwjdidj + Aji
q
w3
j didj/wi
4wiwjdidj
(30)
For an undirected graph,
âˆ‚LÌƒij
âˆ‚wi
|w1:n=1 = 0 always. 
Proposition 4 [7] âˆ‚Î»k
âˆ‚Lij =
P
i,j6=t vivj, where vi is the i-th element of the k-th eigenvector of L.
Proposition 5 Define A, d, Î»k, L, LÌƒ, w as usual. Assume Î»k is of multiplicity 1. Define transition
matrix P = Dâˆ’1
A, vi is the i-th element of the k-th eigenvector of L.
In symmetric perturbation,
âˆ‚Î»Ìƒk
âˆ‚wt
|w1:n=1 = (1 âˆ’ Î»k)(
X
i
v2
i Pit âˆ’ v2
t ) (31)
Proof: After performing the symmetric perturbation, we obtain LÌƒ from Equation 2. We then calculate
the influence function
âˆ‚LÌƒij
âˆ‚wt
|w1:n=1,
âˆ‚LÌƒij
âˆ‚wt
|w1:n=1 =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
âˆ’
Atj +Ajt
4
âˆš
dtdj
(1 âˆ’
Ajt
dj
) i = t, j 6= t
Aij +Aji
4 Ã—
Ajtdi+Aitdj
(didj )3/2 i, j 6= t
0 i, j = t
(32)
Then we can derive âˆ‚Î»k
âˆ‚wt
|w1:n=1 using Proposition 4,
âˆ‚Î»Ìƒk
âˆ‚wt
|w1:n=1 =
X
ij
âˆ‚Î»Ìƒk
âˆ‚LÌƒij
âˆ‚LÌƒij
âˆ‚wt
(33)
=
X
i,j6=t
vivj
Aij + Aji
2
Ã—
Aitdj + Ajtdi
2(didj)3/2
âˆ’ vt
n
X
j=1
vj Ã—
Atj + Ajt
2
(
1
p
djdt
âˆ’
Ajt
dj
p
djdt
)
(34)
= (1 âˆ’ Î»k)(
X
i
v2
i Pit âˆ’ v2
t ), (35)

9
4.3 Clustering
Clustering, defined as a partition of nodes, cannot be written as a smooth function of weights, since
the clustering can only be changed discretely. Therefore we cannot use IF to measure the robustness
of clustering. However, a clustering can be evaluated by breakdown points (BP), which will be
discussed in the next section.
5 Breakdown points (BP)
While IF measures local infinitesimal influences, the breakdown point (BP) measures the global
reliability of the graph properties. It is developed by [4] and widely used in robust statistics literature.
It informs the range of perturbations that can be tolerated before the â€œstructural information in the
dataâ€ is lost. Similar with Influence function, the use of BP is limited by the assumption that the
data is generated i.i.d from known distributions, thus not applicable to the graph properties.
We are the first to extend the definition of BP to the graph properties. For a graph property f(G), a
BP is defined as,
Ïƒâˆ—
w := max{Ïƒw; |f(GÌƒ) âˆ’ f(G)| â‰¤  with probability 1 âˆ’ Î±} (36)
where  and Î± are defined by users. Through finding BP, we define meaningful and com-
putable thresholds where information about a specific graph property is lost. For instance, for
f(G, w) = Î»2(L(Gw)), a BP can be defined when the eigengap between Î»2 and the next largest
eigenvalue vanishes.
Another advantage of BP is that it allows robustness measure for non-differentiable graph property
functions, such as clustering. It does not require the graph properties to be written as smooth functions
of weights. It is a descriptive measure that allows global measure of robustness.
6 Related Work
In the past literature, there has been some work in perturbing the social networks. In [5], they
restrict their perturbed networks to maintain the same number of vertices and edges as the original
unperturbed network, and the perturbation is meant for the position of the edges only. The amount
of perturbation is controlled by the number of edges being moved. In [2] and [1], they focus on
subsampling the nodes of the networks. [2] propose uniform subsampling bootstrap scheme, in
which they iteratively select a subset of vertices without replacement and consider the graph induced
by the subset of vertices. They also consider a subgraph subsampling bootstrap, where they use a
enumeration scheme to find all possible subgraphs with a fixed vertice size, and they selecte the
subgraph with a fixed probability p.
We have also seen work in detecting dense communities and largest connected component in [11].
They formalize tests for the existence of a dense random subgraph based on a variant of scan statistics.
Although they offer sharpe detection bounds, their theorems only make a judgement on the existence
of the subgraph instead of finding out where the subgraph is. Moreover, one has to go through all
the subgraphs in order to calculate the test statistics, which in application, could be computationally
intensive.
7 Experiments
In this section, we perform experiments to test the robustness of clustering, WCut, number of wCCâ€™s
and eigengap. We employ both synthetic datasets and a Facebook dataset. Because symmetric
perturbation is less interpretable, we only apply asymmetric perturbation when studying connected
components and eigengap, and apply asymmetric perturbation for the remainder of the study.
10
7.1 Datasets
Synthetic datasets The synthetic datasets are generated from the DC-SBM model with the
number of clusters K = 5, and the distribution of the cluster sizes nk
n = (0.1, 0.2, 0.3, 0.2, 0.2).
DC-SBM is defined as a class of network model that Aij = wiwjBkl whenever i âˆˆ k, j âˆˆ l, with
B = [Bkl] âˆˆ RKÃ—K
symmetric and non-negative and w1, . . . , wn non-negative weights associated
with the graph nodes. . The clustering is guaranteed to be recovered with small errors by spectral
clustering algorithm if the graph is generated from DC-SBM [10, 12]. The weights of DC-SBM,
wi
DCâˆ’SBM âˆ¼ 0.5 + 0.5 Ã— Uniform(0, 1) if not otherwise specified. The graphs are generated with
different n and spectrum in the following experiments. The visualization of the graphs and the model
parametrization are shown in Figure 3.
Figure 3: The synthetic datasets. Top left: n = 800, Î»1:K = (0, 0.2, 0.4, 0.6, 0.8), wi
DCâˆ’SBM âˆ¼
0.5+0.5Ã—Uniform(0, 1). Top right: n and Î»1:K the same with top left, wi
DCâˆ’SBM âˆ¼ 0.4+0.6Ã—
Uniform(0, 1). Down left: n = 2000, Î»1:K = (0, 0.1, 0.11, 0.12, 0.13). Down right: n = 800,
Î»1:K = (0, 0.1, 0.2, 0.3, 0.4)
Facebook dataset The Facebook dataset [6] is an undirected connected graph which consists of
10 anonymized ego networks. It has 4039 nodes and 88234 edges. The data was collected from
survey participants using a Facebook application called Social Circles. Each cluster is consists of the
members within an ego network. The visualization of the Facebook dataset is shown in Figure 4.
In our experiments, we only examines undirected graphs. Since the definition of L is uni-
versal for all graphs, the robustness of WCut and clustering can be measured in the same
way. The graph properties we defined for the number of wCCâ€™s and eigengap are only meaning-
ful for undirected graphs, and we do not know yet how to measure their robustness for directed graphs.
We say a dataset or a graph is hard if there are many edges between clusters, and graph properties
calculated from them are expected to be sensitive. A harder graph usually has smaller n, larger
eigengap, denser connections between clusters and sparser connections within clusters. From the
11
Figure 4: Facebook dataset
theory of the recovery of clustering [13, 12, 14], it can be indicated that the graph properties in the
harder graphs will be less robustness.
Notice that in the real datasets, e.g. Facebook, one may argue that the perturbation may not be
meaningful since the edges are given continuous weights after the perturbation, while the edges
can only take the values 0 and 1, e.g, two people are either friends or not. It is true that in reality,
the change of this kind of social networks is restricted to adding or deleting nodes or edges. Our
perturbation methods do allow for deletion of nodes and edges when the weight are generated node
resampling mechanism. They can be viewed as deletions from the graph. We can also utilize node
resampling to generate discrete weights for the nodes and edges to make it more meaningful. One
potentail concern is that we do have the constraint that no new edges or nodes can be added from our
perturbation framework.
7.2 Robustness of clustering
We design this experiment to answer two questions. Firstly, For fixed Ïƒw, does BP depend on
weight distribution? Secondly, Is BP informative? We perform the experiment using the following
steps. Firstly, we generate w from the four weight distributions: node resampling, binary, gamma
and mixture distributions. with E(w) = 1 and Ïƒw, as described in Section 2. We assign various
magnitude of Ïƒw to capture the amount of perturbation the clusterings can tolerate.
Secondly, for each perturbed graph GÌƒ, we perform spectral clustering algorithm and obtain Ëœ
C. We
then compute misclassification errors dist(Ctrue, Ëœ
C), and dist(Cspc, Ëœ
C), where Ctrue is the true
clustering of the underlying model, and Cspc is the clustering obtained from the unperturbed graph
through spectral clustering. The misclassification error is define in Section ??. If there is a value of
Ïƒw where Ëœ
C becomes very unstable, that the misclassification error starts to have high variance, we
will call it the breakdown point.
We employ two undirected synthetic datasets from DC-SBM. They both have n = 800, and
Î»1:K = (0, 0.2, 0.4, 0.6, 0.8). The difference is that the first dataset is generated with wi
DCâˆ’SBM âˆ¼
0.5 + 0.5 Ã— Uniform(0, 1), and the second one is generated with wi
DCâˆ’SBM âˆ¼ 0.4 + 0.6 Ã—
Uniform(0, 1). The results are shown in Figure 5.
For all the cases, we observe a significant break in the variance of misclassification errors, which we
define as BP. For example, the BP for binary distribution in the easy dataset is around 0.8, which
becomes 0.6 in the hard dataset. Notice that with all weight distributions, BPâ€™s in the harder dataset
12
Figure 5: Left: easy dataset with wi
DCâˆ’SBM âˆ¼ 0.5 + 0.5 Ã— Uniform(0, 1). Right: hard dataset
with wi
DCâˆ’SBM âˆ¼ 0.4 + 0.6 Ã— Uniform(0, 1). 1st row: Node resampling. 2And row: Binary . 3rd
row: gamma distribution. 4th row: mixture-uniform gamma distirbution. Ëœ
C is the weighted version of
clustering obtained from spectral clustering algorithm. Each boxplot is consist of 100 repetitions.
are always smaller comparing to that in the eaiser dataset. This confirms that BP is informative,
since it predicts the sensitivity of graph properties in harder (less robust) dataset. For the same dataset,
across different weight distributions, the misclassification errors break at different Ïƒw, which suggests
that BP varies with different weight distributions.
7.3 Robustness of WCut
In the experiments here we want to verify that IF of WCut is informative, and then probe the source
of robustness of WCut at the node level.
For the synthetic dataset with n = 800, w âˆ¼ 0.5 + 0.5 Ã— Uniform(0, 1), We firstly obtain its
clustering C from spectral clustering algorithm. We then add noise to C by randomly picking 200
nodes and reassigning them to other clusters randomly, in which way we obtain Ëœ
C. The WCut of
13
Figure 6: The histograms of IFW Cut
for synthetic datasets. Left: C. Right: Ëœ
C.
Ëœ
C is expected to be less robust comparing to that of C, since there are already more noises in Ëœ
C.
After computing IFW Cut
for both C and Ëœ
C, we plot the histograms in Figure 6. We observe that
in the histogram for C, IFW Cut
is more concentrated on 0 with fewer large influence nodes, thus
indicating a robust clustering. On the other hand, the histogram for Ëœ
C indicates the clustering being
sensitive. These findings correspond with our expectations and show that IFW Cut
is informative.
The other experiment we do is to probe where the sensitivity comes from through a partial
perturbation. We generate synthetic dataset with n = 800, and Î»1:K = (0, 0.2, 0.4, 0.6, 0.8). We
also perform the experiment on the Facebook dataset. We select the nodes with IFW Cut
i (Ctrue) > 0.
These nodes are considered not well clustered because increasing the weights on them is expected
to increase WCut, thus worse quality of clustering. For these nodes, we generate w from
Gamma(0.1/Âµ, 10Âµ2
), where E(w) = Âµ, V ar(w) = 0.1. The rest of the nodes have w = 1. We
then assign the weights to the edges through asymmetric perturbation. We also examine the perturbed
clustering from spectral clustering algorithm.
The results are shown in Figure 7, we observe that with both datasets, WCut increases as the weights
on the nodes with bad influence increase, which indicates that the nodes with IFW Cut
causes the
quality of clustering to drop, and could potentially be not well clustered. In the synthetic dataset,
dist( Ëœ
C, Ctrue) increases as E(w) increases, and decreases slightly but steadily as E(w) decreases.
This is because spectral clustering is equivalent to optimizing WCut [8], when less weight is imposed
on nodes with bad influence for IFW Cut
i (Ctrue), WCut gets smaller and Ëœ
C becomes closer to Ctrue.
In the Facebook dataset, dist( Ëœ
C, Ctrue) decreases as E(w) increases. This is because the underlying
clustering Ctrue in Facebook does not correspond to the clustering that minimizing WCut. Therefore
minimizing WCut does not lead to Ëœ
C being close to Ctrue.
7.4 Robustness of wCCâ€™s and eigengap
We firstly probe the sensitivity of wCCâ€™s and eigengap. We calculate IFfu
i , IFfl
i and IFfe
for
the nodes and select the nodes with bad influence. We call the nodes with IFfu
i < 0 or IFfl
i > 0
or IFfe
< 0 nodes with bad influence, since these nodes make the distinction between wCCâ€™s or
eigengap less significant. We do a partial perturbation on the graphs by perturbing the weights of
these nodes with wi âˆ¼ Uniform(a, a + 0.25), a âˆˆ [0.5, 0.6, Â· Â· Â· , 1.5].
We also examine their robustness by making full perturbation on the entire graph. We generate
wi âˆ¼ Mixture(0.5, b, 0, 0.1, Tâˆ’, Gamma, p) for all the nodes, where Tâˆ’ is a distribution centered
around 0.51 with probability 1, p ranges from 0.1 to 0.9. We choose Tâˆ’ to maintain the weights on
the edges to be positive. Since the strength of perturbation is mostly coming from T+ and the choice
of the base binary distribution, we do not lose generosity. In this way E(w) = 1 and Ïƒ(w) = Ïƒw
varies.
The synthetic dataset for testing the robustness of wCCâ€™s is generated with n = 2000, and
Î»1:K = (0, 0.1, 0.11, 0.12, 0.13). In the synthetic dataset, because of the nature of the model, the
14
Figure 7: Left column: the change of WCut with respect to E(w). Right column: the change of
classification error with respect of E(w). First row: synthetic datset. Second row: Facebook datset.
Each boxplot is consist of 100 repetitions.
largest fl in the graph appears when K = 5. In the Facebook dataset, through calculation ,we find
that the largest fl appears when K = 7. Therefore, we assume initially there are 5 wCCâ€™s in the
synthetic dataset and 7 connected components in Facebook dataset. The results are shown in Figure
8.
The synthetic dataset for testing the robustness of eigengap is generated with n = 800 and Î»1:K =
(0, 0.1, 0.2, 0.3, 0.4). We call fe(i) = Î»i+1 âˆ’ Î»i the ith eigengap. Through calculation, we find that
the largest eigengap is the 5th in synthetic dataset, and 7th in the Facebook dataset.
We observe that, fl increases while fu and fe decrease as E(w) increases in both datasets, which
indicates that, as the bad influence increases, there is greater connectivity between the connected
components and the eigengap becomes less significant. BP is defined as the Ïƒw where fu, fl and fe
is dominated with another K. We observe the BPâ€™s for both the number of wCCâ€™s and eigengap.
In the Facebook dataset, they seem to be robust to perturbation. Note that the Facebook dataset is
consist of 10 ego-networks while our experiments indicate that there are only 7 robustness wCCâ€™s
inside, meaning there are 3 more connected users who are grouped to one wCC.
8 Discussion
This paper makes several contributions. Firstly, it provides an innovative way to perturb the network
by assigning the weights on the nodes and edges. The strength of perturbation can be well controlled
and can be arbitrarily small. The topology of the graph is also preserved after the perturbation.
Secondly, it extends the definitions of influence function and breakdown point to the graph properties.
15
Figure 8: Topleft: IF(Î»2, w). Left column: synthetic datset. Right column: Facebook dataset. First
row: IF(fl). Second row: IF(fu). Third row: breakdown point of fu. Each boxplot is consist of
100 repetitions.
Although these measures are widely used in the robust statistics literature, using them on graph
properties is the first time. Last but not the least, we are also able to probe the source of robustness by
quantifying the influence of nodes on the robustness of graph properties, which provides a deeper
insight into the problem.
Our perturbation framework also have its limitations. For example, no new edges or nodes can be
added to the graphs through our perturbation methods, and this may not be natural evaluating the
graph properties in some social networks. Moreover, the perturbation methods is not suitable for
evaluating some graph properties, e.g. properties related to graph distances. These could be the area
for future explorations.
16
Figure 9: Left column: synthetic dataset. Right column: Facebook dataset. First row: IF(fe).
Second row: breakdown point of fe. Each boxplot is consist of 100 repetitions.
References
[1] Waqar Ali, Anatol E Wegner, Robert E Gaunt, Charlotte M Deane, and Gesine Reinert. Com-
parison of large networks with sub-sampling strategies. Scientific reports, 6:28955, 2016.
[2] Sharmodeep Bhattacharyya, Peter J Bickel, et al. Subsampling bootstrap of count features of
networks. The Annals of Statistics, 43(6):2384â€“2411, 2015.
[3] David Gfeller, Jean-CÃ©dric Chappelier, and Paolo De Los Rios. Finding instabilities in the
community structure of complex networks. Physical Review E, 72(5):056135, 2005.
[4] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust
statistics: the approach based on influence functions, volume 114. John Wiley & Sons, 2011.
[5] B. Karrer, E. Levina, and M.E.J. Newman. Robustness of community structure in networks.
Physical Revieww E, 77(4)::046119, 2008.
[6] Jure Leskovec and Julian J Mcauley. Learning to discover social circles in ego networks. In
Advances in neural information processing systems, pages 539â€“547, 2012.
[7] Jan R Magnus and Heinz Neudecker. Matrix differential calculus with applications in statistics
and econometrics. Wiley series in probability and mathematical statistics, 1988.
[8] Marina MeilaÌ† and William Pentney. Clustering by weighted cuts in directed graphs. In Chid
Apte, David Skillicorn, and Vipin Kumar, editors, Proceedings of the SIAM Data Mining
Conference, SDM. SIAM, 2007.
[9] Boaz Nadler, Stephane Lafon, Ronald Coifman, and Ioannis Kevrekidis. Diffusion maps,
spectral clustering and eigenfunctions of fokker-planck operators. In Y. Weiss, B. SchÃ¶lkopf,
17
and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 955â€“962,
Cambridge, MA, 2006. MIT Press.
[10] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic
blockmodel. In Advances in Neural Information Processing Systems, pages 3120â€“3128, 2013.
[11] Nicolas Verzelen, Ery Arias-Castro, et al. Community detection in sparse random networks.
The Annals of Applied Probability, 25(6):3465â€“3510, 2015.
[12] Yali Wan and Marina Meila. Benchmarking recovery theorems for the DC-SBM. In Interna-
tional Symposium on Artificial Intelligence and Mathematics (ISAIM), 2015.
[13] Yali Wan and Marina Meila. A class of network models recoverable by spectral clustering.
In Daniel Lee and Masashi Sugiyama, editors, Advances in Neural Information Processing
Systems (NIPS), 2015.
[14] Yali Wan and Marina MeilaÌ†. Graph clustering: block-models and model-free results. In
Advances in Neural Information Processing Systems (NIPS), 2016.
18
