arXiv:1506.03684v1
[cs.GT]
11
Jun
2015
The Pseudo-Dimension of Near-Optimal Auctions
JAMIE MORGENSTERN∗
TIM ROUGHGARDEN †
June 12, 2015
Abstract
This paper develops a general approach, rooted in statistical learning theory, to learning an approx-
imately revenue-maximizing auction from data. We introduce t-level auctions to interpolate between
simple auctions, such as welfare maximization with reserve prices, and optimal auctions, thereby bal-
ancing the competing demands of expressivity and simplicity. We prove that such auctions have small
representation error, in the sense that for every product distribution F over bidders’ valuations, there
exists a t-level auction with small t and expected revenue close to optimal. We show that the set of
t-level auctions has modest pseudo-dimension (for polynomial t) and therefore leads to small learning
error. One consequence of our results is that, in arbitrary single-parameter settings, one can learn a
mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples.
1 Introduction
In the traditional economic approach to identifying a revenue-maximizing auction, one first posits a prior
distribution over all unknown information, and then solves for the auction that maximizes expected revenue
with respect to this distribution. The first obstacle to making this approach operational is the difficulty of
formulating an appropriate prior. The second obstacle is that, even if an appropriate prior distribution is
available, the corresponding optimal auction can be far too complex and unintuitive for practical use. This
motivates the goal of identifying auctions that are “simple” and yet nearly-optimal in terms of expected
revenue.
In this paper, we apply tools from learning theory to address both of these challenges. In our model,
we assume that bidders’ valuations (i.e., “willingness to pay”) are drawn from an unknown distribution F.
A learning algorithm is given i.i.d. samples from F. For example, these could represent the outcomes of
comparable transactions that were observed in the past. The learning algorithm suggests an auction to use
for future bidders, and its performance is measured by comparing the expected revenue of its output auction
to that earned by the optimal auction for the distribution F.
The possible outputs of the learning algorithm correspond to some set C of auctions. We view C as
a design parameter that can be selected by a seller, along with the learning algorithm. A central goal of
this work is to identify classes C that balance representation error (the amount of revenue sacrificed by
restricting to auctions in C) with learning error (the generalization error incurred by learning over C from
samples). That is, we seek a set C that is rich enough to contain an auction that closely approximates an
optimal auction (whatever F might be), yet simple enough that the best auction in C can be learned from
a small amount of data. Learning theory offers tools both for rigorously defining the “simplicity” of a set
C of auctions, through well-known complexity measures such as the pseudo-dimension, and for quantifying
the amount of data necessary to identify the approximately best auction from C. Our goal of learning a
near-optimal auction also requires understanding the representation error of different classes C; this task is
problem-specific, and we develop the necessary arguments in this paper.
∗Carnegie Mellon University, jamiemmt@cs.cmu.edu, supported in part by Simons Graduate Fellowship in Theoretical Com-
puter Science and NSF grants CCF-1415460. Part of work done while a visiting student at Stanford University.
†Stanford Universitytim@cs.stanford.edu, Roughgarden was supported in part by NSF Award CCF-1215965 and an ONR
PECASE Award.
1
1.1 Our Contributions
The primary contributions of this paper are the following. First, we show that well-known concepts from
statistical learning theory can be directly applied to reason about learning from data an approximately
revenue-maximizing auction. Precisely, for a set C of auctions and an arbitrary unknown distribution F
over valuations in [1, H], O(H2
ǫ2 dC log H
ǫ ) samples from F are enough to learn (up to a 1 − ǫ factor) the
best auction in C, where dC denotes the pseudo-dimension of the set C (defined in Section 2). Second, we
introduce the class of t-level auctions, to interpolate smoothly between simple auctions, such as welfare
maximization subject to individualized reserve prices (when t = 1), and the complex auctions that can arise
as optimal auctions (as t → ∞). Third, we prove that in quite general auction settings with n bidders,
the pseudo-dimension of the set of t-level auctions is O(nt log nt). Fourth, we quantify the number t of
levels required for the set of t-level auctions to have low representation error, with respect to the optimal
auctions that arise from arbitrary product distributions F. For example, for single-item auctions and several
generalizations thereof, if t = Ω(H
ǫ ), then for every product distribution F there exists a t-level auction with
expected revenue at least 1 − ǫ times that of the optimal auction for F.
In the above sense, the “t” in t-level auctions is a tunable “sweet spot”, allowing a designer to balance
the competing demands of expressivity (to achieve near-optimality) and simplicity (to achieve learnability).
For example, given a fixed amount of past data, our results indicate how much auction complexity (in the
form of the number of levels t) one can employ without risking overfitting the auction to the data.
Alternatively, given a target approximation factor 1 − ǫ, our results give sufficient conditions on t and
consequently on the number of samples needed to achieve this approximation factor. The resulting sample
complexity upper bound has polynomial dependence on H, ǫ−1
, and the number n of bidders. Known
results [1, 8] imply that any method of learning a (1−ǫ)-approximate auction from samples must have sample
complexity with polynomial dependence on all three of these parameters, even for single-item auctions.
1.2 Related Work
The present work shares much of its spirit and high-level goals with Balcan et al. [4], who proposed applying
statistical learning theory to the design of near-optimal auctions. The first-order difference between the
two works is that our work assumes bidders’ valuations are drawn from an unknown distribution, while
Balcan et al. [4] study the more demanding “prior-free” setting. Since no auction can achieve near-optimal
revenue ex-post, Balcan et al. [4] define their revenue benchmark with respect to a set G of auctions on each
input v as the maximum revenue obtained by any auction of G on v. The idea of learning from samples
enters the work of Balcan et al. [4] through the internal randomness of their partitioning of bidders, rather
than through an exogenous distribution over inputs (as in this work). Both our work and theirs requires
polynomial dependence on H, 1
ǫ : ours in terms of a necessary number of samples, and theirs in terms of
a necessary number of bidders; as well as a measure of the complexity of the class G (in our case, the
pseudo-dimension, and in theirs, an analagous measure). The primary improvement of our work over of the
results in Balcan et al. [4] is that our results apply for single item-auctions, matroid feasibility, and arbitrary
single-parameter settings (see Section 2 for definitions); while their results apply only to single-parameter
settings of unlimited supply.1
We also view as a feature the fact that our sample complexity upper bounds
can be deduced directly from well-known results in learning theory — we can focus instead on the non-trivial
and problem-specific work of bounding the pseudo-dimension and representation error of well-chosen auction
classes.
Elkind [12] also considers a similar model to ours, but only for the special case of single-item auctions.
While her proposed auction format is similar to ours, our results cover the far more general case of arbitrary
single-parameter settings and and non-finite support distributions; our sample complexity bounds are also
better even in the case of a single-item auction (linear rather than quadratic dependence on the number of
bidders). On the other hand, the learning algorithm in [12] (for single-item auctions) is computationally
efficient, while ours is not.
1See Balcan et al. [3] for an extension to the case of a large finite supply.
2
Cole and Roughgarden [8] study single-item auctions with n bidders with valuations drawn from inde-
pendent (not necessarily identical) “regular” distributions (see Section 2), and prove upper and lower bounds
(polynomial in n and ǫ−1
) on the sample complexity of learning a (1 − ǫ)-approximate auction. While the
formalism in their work is inspired by learning theory, no formal connections are offered; in particular, both
their upper and lower bounds were proved from scratch. Our positive results include single-item auctions
as a very special case and, for bounded or MHR valuations, our sample complexity upper bounds are much
better than those in Cole and Roughgarden [8].
Huang et al. [15] consider learning the optimal price from samples when there is a single buyer and a
single seller; this problem was also studied implicitly in [10]. Our general positive results obviously cover the
bounded-valuation and MHR settings in [15], though the specialized analysis in [15] yields better (indeed,
almost optimal) sample complexity bounds, as a function of ǫ−1
and/or H.
Medina and Mohri [17] show how to use a combination of the pseudo-dimension and Rademacher com-
plexity to measure the sample complexity of selecting a single reserve price for the VCG mechanism to
optimize revenue. In our notation, this corresponds to analyzing a single set C of auctions (VCG with a
reserve). Medina and Mohri [17] do not address the expressivity vs. simplicity trade-off that is central to
this paper.
Dughmi et al. [11] also study the sample complexity of learning good auctions, but their main results are
negative (exponential sample complexity), for the difficult scenario of multi-parameter settings. (All settings
in this paper are single-parameter.)
Our work on t-level auctions also contributes to the literature on simple approximately revenue-maximizing
auctions (e.g., [6, 14, 7, 9, 21, 24, 2]). Here, one takes the perspective of a seller who knows the valuation
distribution F but is bound by a “simplicity constraint” on the auction deployed, thereby ruling out the
optimal auction. Our results that bound the representation error of t-level auctions (Theorems 3.4, 4.1, 5.4,
and 6.2) can be interpreted as a principled way to trade off the simplicity of an auction with its approxima-
tion guarantee. While previous work in this literature generally left the term “simple” safely undefined, this
paper effectively proposes the pseudo-dimension of an auction class as a rigorous and quantifiable simplicity
measure.
2 Preliminaries
This section reviews useful terminology and notation standard in Bayesian auction design and learning
theory.
Bayesian Auction Design We consider single-parameter settings with n bidders. This means that each
bidder has a single unknown parameter, its valuation or willingness to pay for “winning.” (Every bidder
has value 0 for losing.) A setting is specified by a collection X of subsets of {1, 2, . . ., n}; each such subset
represent a collection of bidders that can simultaneously “win.” For example, in a setting with k copies of
an item, where no bidder wants more than one copy, X would be all subsets of {1, 2, . . ., n} of cardinality at
most k.
A generalization of this case, studied in Section 5, is matroid settings. These satisfy: (i) whenever X ∈ X
and Y ⊆ X, Y ∈ X; and (ii) for two sets |I1| < |I2|, I1, I2 ∈ X, there is always an augmenting element
i2 ∈ I2 \I1 such that I1 ∪{i2} ∈ X, X. Section 6 also considers arbitrary single-parameter settings, where the
only assumption is that ∅ ∈ X. To ease comprehension, we often illustrate our main ideas using single-item
auctions (where X is the singletons and the empty set).
We assume bidders’ valuations are drawn from the continuous joint cumulative distribution F. Except in
the extension in Section 4, we assume that the support of F is limited to [1, H]n
. As in most of optimal auction
theory [18], we usually assume that F is a product distribution, with F = F1 ×F2 ×. . .×Fn and each vi ∼ Fi
drawn independently but not identically. The virtual value of bidder i is denoted by φi(vi) = vi − 1−Fi(vi)
fi(vi) .
A distribution satisfies the monotone-hazard rate (MHR) condition if fi(vi)/(1 − Fi(vi)) is nondecreasing;
intuitively, if its tails are no heavier than those of an exponential distribution. In a fundamental paper, [18]
3
proved that when every virtual valuation function is nondecreasing (the “regular” case), the auction that
maximizes expected revenue for n Bayesian bidders chooses winners in a way which maximizes the sum of
the virtual values of the winners. This auction is known as Myerson’s auction, which we refer to as M.
The result can be extended to the general, “non-regular” case by replacing the virtual valuation functions
by “ironed virtual valuation functions.” The details are well-understood but technical; see Myerson [18] and
Hartline [13] for details.
Sample Complexity, VC Dimension, and the Pseudo-Dimension This section reviews several
well-known definitions from learning theory. Suppose there is some domain Q, and let c be some unknown
target function c : Q → {0, 1}. Let D be an unknown distribution over Q. We wish to understand how
many labeled samples (x, c(x)), x ∼ D, are necessary and sufficient to be able to output a ĉ which agrees
with c almost everywhere with respect to D. The distribution-independent sample complexity of learning c
depends fundamentally on the “complexity” of the set of binary functions C from which we are choosing ĉ.
We define the relevant complexity measure next.
Let S be a set of m samples from Q. The set S is said to be shattered by C if, for every subset T ⊆ S,
there is some cT ∈ C such that cT (x) = 1 if x ∈ T and cT (y) = 0 if y /
∈ T . That is, ranging over all c ∈ C
induces all 2|S|
possible projections onto S. The VC dimension of C, denoted VC(C), is the size of the largest
set S that can be shattered by C.
Let errS(ĉ) = (
P
x∈S |c(x)−ĉ(x)|)/|S| denote the empirical error of ĉ on S, and let err(ĉ) = Ex∼D[|c(x)−
ĉ(x)|] denote the true expected error of ĉ with respect to D. A key result from learning theory [23] is:
for every distribution D, a sample S of size Ω(ǫ−2
(VC(C) + ln 1
δ )) is sufficient to guarantee that errS(ĉ) ∈
[err(ĉ) − ǫ, err(ĉ) + ǫ] for every ĉ ∈ C with probability 1 − δ. In this case, the error on the sample is close
to the true error, simultaneously for every hypothesis in C. In particular, choosing the hypothesis with the
minimum sample error minimizes the true error, up to 2ǫ. We say C is (ǫ, δ)-uniformly learnable with sample
complexity m if, given a sample S of size m, with probability 1 − δ, for all c ∈ C, |errS(c) − err(c)| < ǫ:
thus, any class C is (ǫ, δ)-uniformly learnable with m = Θ 1
ǫ2 VC(C) + ln 1
δ

samples. Conversely, for every
learning algorithm A that uses fewer than VC(C)
ǫ samples, there exists a distribution D′
and a constant q
such that, with probability at least q, A outputs a hypothesis ĉ′
∈ C with err(ĉ′
) > err(ĉ)+ ǫ
2 for some ĉ ∈ C.
That is, the true error of the output hypothesis is more than ǫ
2 larger the best hypothesis in the class.
To learn real-valued functions, we need a generalization of VC dimension (which concerns binary func-
tions). The pseudo-dimension [19] does exactly this. Formally, let c : Q → [0, H] be a real-valued function
over Q, and C the class we are learning over. Let S be a sample drawn from D, |S| = m, labeled according
to c. Both the empirical and true error of a hypothesis ĉ are defined as before, though |ĉ(x) − c(x)| can
now take on values in [0, H] rather than in {0, 1}. Let (r1
, . . . , rm
) ∈ [0, H]m
be a set of targets for S. We
say (r1
, . . . , rm
) witnesses the shattering of S by C if, for each T ⊆ S, there exists some cT ∈ C such that
fT (xi
) ≥ ri
for all xi
∈ T and cT (xi
) < ri
for all xi
/
∈ T . If there exists some ~
r witnessing the shattering
of S, we say S is shatterable by C. The pseudo-dimension of C, denoted dC, is the size of the largest set S
which is shatterable by C. The sample complexity upper bounds of this paper are derived from the follow-
ing theorem, which states that the distribution-independent sample complexity of learning over a class of
real-valued functions C is governed by the class’s pseudo-dimension.
Theorem 2.1 [E.g. [1]] Suppose C is a class of real-valued functions with range in [0, H] and pseudo-
dimension dC. For every ǫ > 0, δ ∈ [0, 1], the sample complexity of (ǫ, δ)-uniformly learning f with respect
to C is m = O

H
ǫ
2
dC ln H
ǫ

+ ln 1
δ

.
Moreover, the guarantee in Theorem 2.1 is realized by the learning algorithm that simply outputs the function
c ∈ C with the smallest empirical error on the sample.
4
Applying Pseudo-Dimension to Auction Classes For the remainder of this paper, we consider classes
of truthful auctions C.2
When we discuss some auction c ∈ C, we treat c : [0, H]n
→ R as the function that
maps (truthful) bid tuples to the revenue achieved on them by the auction c. Then, rather than minimizing
error, we aim to maximize revenue. In our setting, the guarantee of Theorem 2.1 directly implies that, with
probability at least 1 − δ (over the m samples), the output of the empirical revenue maximization learning
algorithm — which returns the auction c ∈ C with the highest average revenue on the samples — chooses
an auction with expected revenue (over the true underlying distribution F) that is within an additive ǫ of
the maximum possible.
3 Single-Item Auctions
To illustrate out ideas, we first focus on single-item auctions. The results of this section are generalized
significantly in Sections 5 and 6. Section 3.1 defines the class of t-level single-item auctions, gives an
example, and interprets the auctions as approximations to virtual welfare maximizers. Section 3.2 proves
that the pseudo-dimension of the set of such auctions is O(nt log nt), which by Theorem 2.1 implies a
sample-complexity upper bound. Section 3.3 proves that taking t = Ω(H
ǫ ) yields low representation error.
3.1 t-Level Auctions: The Single-Item Case
We now introduce t-level auctions, or Ct for short. Intuitively, one can think of each bidder as facing one of
t possible prices; the price they face depends upon the values of the other bidders. Consider, for each bidder
i, t numbers 0 ≤ ℓi,0 ≤ ℓi,1 ≤ . . . ≤ ℓi,t−1. We refer to these t numbers as thresholds. This set of tn numbers
defines a t-level auction with the following allocation rule. Consider a valuation tuple v:
1. For each bidder i, let ti(vi) denote the index τ of the largest threshold ℓi,τ that lower bounds vi (or -1
if vi < ℓi,0). We call ti(vi) the level of bidder i.
2. Sort the bidders from highest level to lowest level and, within a level, use a fixed lexicographical
tie-breaking ordering ≻ to pick the winner.3
3. Award the item to the first bidder in this sorted order (unless ti = −1 for every bidder i, in which case
there is no sale).
The payment rule is the unique one that renders truthful bidding a dominant strategy and charges 0 to
losing bidders — that is, the winning bidder pays the lowest bid at which she would continue to win. It
is important for us to understand this payment rule in detail; there are three interesting cases. Suppose
bidder i is the winner. In the first case, i is the only bidder who might be allocated the item (other bidders
have level -1), in which case her bid must be at least her lowest threshold. In the second case, there are
multiple bidders at her level, so she must bid high enough to be at her level (and, since ties are broken
lexicographically, this is her threshold to win). In the final case, she need not compete at her level: she can
choose to either pay one level above her competition (in which case her position in the tie-breaking ordering
does not matter) or she can bid at the same level as her highest-level competitors (in which case she only
wins if she dominates all of those bidders at the next-highest level according to ≻). Formally, the payment
p of the winner i (if any) is as follows. Let τ̄ denote the highest level τ such that there at least two bidders
at or above level τ, and I be the set of bidders other than i whose level is at least τ̄.
Monop If τ̄ = −1, then pi = ℓi,0 (she is the only potential winner, but must have level ≥ 0 to win).
Mult If ti(vi) = τ̄ then pi = ℓi,τ̄ (she needs to be at level τ̄).
Unique If ti(vi) > τ̄, if i ≻ i′
for all i′
∈ I, she pays pi = ℓi,τ̄ , otherwise she pays pi = ℓi,τ̄+1 (she either needs
to be at level τ̄ + 1, in which case her position in ≻ does not matter, or at level τ̄, in which case she
would need to be the highest according to ≻).
2An auction is truthful if truthful bidding is a dominant strategy for every bidder. That is: for every bidder i, and all
possible bids by the other bidders, i maximizes its expected utility (value minus price paid) by bidding its true value. In the
single-parameter settings that we study, the expected revenue of the optimal non-truthful auction (measured at a Bayes-Nash
equilibrium with respect to the prior distribution) is no larger than that of the optimal truthful auction.
3Our results hold also for some other tie-breaking rules.
5
We now describe a particular t-level auction, and demonstrate each case of the payment rule.
Example 3.1 Consider the following 4-level auction for bidders a, b, c. Let ℓa,· = [2, 4, 6, 8], ℓb,· = [1.5, 5, 9, 10],
and ℓc,· = [1.7, 3.9, 6, 7]. For example, if bidder a bids less than 2 she is at level −1, a bid in [2, 4) puts her
at level 0, a bid in [4, 6) at level 1, a bid in [6, 8) at level 2, and a bid of at least 8 at level 3. Let a ≻ b ≻ c.
Monop If va = 3, vb < 1.5, vc < 1.7, then b, c are at level −1 (to which the item is never allocated). So, a wins
and pays 2, the minimum she needs to bid to be at level 0.
Mult If va ≥ 8, vb ≥ 10, vc < 7, then a and b are both at level 3, and a ≻ b, so a will win and pays 8 (the
minimum she needs to bid to be at level 3).
Unique If va ≥ 8, vb ∈ [5, 9], vc ∈ [3.9, 6], then a is at level 3, and b and c are at level 1. Since a ≻ b and a ≻ c,
a need only pay 4 (enough to be at level 1). If, on the other hand, va ∈ [4, 6], vb = [5, 9] and vc ≥ 6, c
has level at least 2 (while a, b have level 1), but c needs to pay 6 since a, b ≻ c.
Remark 3.2 (Connection to virtual valuation functions) t-level auctions are naturally interpreted as
discrete approximations to virtual welfare maximizers, and our representation error bound in Theorem 3.4
makes this precise. Each level corresponds to a constraint of the form “If any bidder has level at least τ,
do not sell to any bidder with level less than τ.” We can interpret the ℓi,τ ’s (with fixed τ, ranging over
bidders i) as the bidder values that map to some common virtual value. For example, 1-level auctions treat
all values below the single threshold as having negative virtual value, and above the threshold uses values
as proxies for virtual values. 2-level auctions use the second threshold to the refine virtual value estimates,
and so on. With this interpretation, it is intuitively clear that as t → ∞, it is possible to estimate bidders’
virtual valuation functions and thus approximate the optimal auction to arbitrary accuracy.
3.2 The Pseudo-Dimension of t-Level Auctions
This section shows that the pseudo-dimension of the class of t-level single-item auctions with n bidders is
O(nt log nt). Combining this with Theorem 2.1 immediately yields sample complexity bounds (parameterized
by t) for learning the best such auction from samples.
Theorem 3.3 For a fixed tie-breaking order ≻, the pseudo-dimension of the set of n-bidder single-item
t-level auctions is O (nt log(nt)).
Proof: Recall from Section 2 that we need to upper bound the size of every set that is shatterable using
t-level auctions. Fix a set of samples S = v1
, . . . , vm

of size m and a potential witness R = r1
, . . . , rm

.
Each auction c induces a binary labeling of the samples vj
of S (whether c’s revenue on vj
is at least rj
or
strictly less than rj
). The set S is shattered with witness R if and only if the number of distinct labelings
of S given by any t-level auction is 2m
.
We upper-bound the number of distinct labelings of S given by t-level auctions (for some fixed potential
witness R), counting the labelings in two stages. Note that S involves nm numbers — one value vj
i for each
bidder for each sample. A t-level auction involves nt numbers — t thresholds ℓi,τ for each bidder. Call two
t-level auctions with thresholds {ℓi,τ } and {ℓ̂i,τ } equivalent if:
1. The relative order of the ℓi,τ ’s agrees with that of the ℓ̂i,τ ’s, in that both induce the same permutation
of {1, 2, . . ., n} × {0, 1, . . ., t − 1}.
2. Merging the sorted list of the vj
i ’s with the sorted list of the ℓi,τ ’s yields the same partition of the vj
i ’s
as does merging it with the sorted list of the ℓ̂i,τ ’s.
Note that this is an equivalence relation. If two t-level auctions are equivalent, every comparison between
two numbers (valuations or thresholds) is resolved identically by those auctions. Using the defining properties
of equivalence, a crude upper bound on the number of equivalence classes is
(nt)! ·

nm + nt
nt

≤ (nm + nt)2nt
. (1)
6
We now upper-bound the number of distinct labelings of S that can be generated by any auction in a single
equivalence class C. First, as all comparisons between two numbers (valuations or thresholds) are resolved
identically for all auctions in C, each bidder i in each sample vj
of S is assigned the same level (across
auctions in C), and the winner (if any) in each sample vj
is constant across all of C. By the same reasoning,
the identity of the parameter that gives the winner’s payment (some ℓi,τ ) is uniquely determined by pairwise
comparisons (recall Section 3.1) and hence is common across all auctions in C. The payments ℓi,τ , however,
can vary across auctions in the equivalence class.
For a bidder i and level τ ∈ {0, 1, 2, . . ., t− 1}, let Si,τ ⊆S be the subset of samples in which bidder i wins
and pays ℓi,τ . The revenue obtained by each auction in C on a sample of Si,τ is simply ℓi,τ (and independent
of all other parameters of the auction). Thus, ranging over all t-level auctions in C generates at most |Si,τ |
distinct binary labelings of Si,τ — the possible subsets of Si,τ for which an auction meets the corresponding
target rj
form a nested collection.
Summarizing, within the equivalence class C of t-level auctions, varying a parameter ℓi,τ generates at
most |Si,τ | different labelings of the samples Si,τ and has no effect on the other samples. Since the subsets
{Si,τ }i,τ are disjoint, varying all of the ℓi,τ ’s (i.e., ranging over C) generates at most
n
Y
i=1
t−1
Y
τ=0
|Si,τ | ≤ mnt
(2)
distinct labelings of S.
Combining (1) and (2), the class of all t-level auctions produces at most (nm + nt)3nt
distinct labelings
of S. Since shattering S requires 2m
distinct labelings, we conclude that 2m
≤ (nm + nt)3nt
, implying
m = O(nt log nt) as claimed. 
3.3 The Representation Error of Single-Item t-Level Auctions
In this section, we show that for every bounded product distribution, there exists a t-level auction with
expected revenue close to that of the optimal single-item auction when bidders are independent and bounded.
The analysis “rounds” an optimal auction to a t-level auction without losing much expected revenue. This
is done using thresholds to approximate each bidder’s virtual value: the lowest threshold at the bidder’s
monopoly reserve price, the next 1
ǫ thresholds at the values at which bidder i’s virtual value surpasses
multiples of ǫ, and the remaining thresholds at those values where bidder i’s virtual value reaches powers of
1 + ǫ. Theorem 3.4 formalizes this intuition.
Theorem 3.4 Suppose F is product distribution over [1, H]n
. If t = Ω 1
ǫ + log1+ǫ H

, then Ct contains a
single-item auction with expected revenue at least 1 − ǫ times the optimal expected revenue.
With an eye toward our generalizations, we prove the following more general result. Theorem 3.4 follows
immediately by taking α = γ = 1.
Lemma 3.5 Consider n bidders with valuations in [0, H] and with P[maxi vi > α] ≥ γ. Then, Ct contains
a single-item auction with expected revenue at least a 1 − ǫ times that of an optimal auction, for t =
Θ

1
γǫ + log1+ǫ
H
α

.
Proof: Consider a fixed bidder i. We define t thresholds for i, bucketing i by her virtual value, and prove
that the t-level auction A using these thresholds for each bidder closely approximates the expected revenue
of the optimal auction M. Let ǫ′
be a parameter defined later.
Set ℓi,0 = φ−1
i (0), bidder i’s monopoly reserve.4
For τ ∈ [1, ⌈ 1
γǫ′ ⌉], let ℓi,τ = φ−1
i (τ · αγǫ′
) (φi ∈ [0, 1]).
For τ ∈ [⌈ 1
γǫ′ ⌉, ⌈ 1
γǫ′ ⌉ + ⌈log1+ ǫ
2
H
α ⌉], let ℓi,τ = φ−1
i (α(1 + ǫ
2 )
τ−⌈ 1
γǫ′ ⌉
) (φi > 1).
4Recall from Section 2 that φi denotes the virtual valuation function of bidder i. For the non-regular case, φi denotes the
ironed virtual valuation functions. It is convenient to assume that these functions are strictly increasing (not just nondecreasing);
this can be enforced at the cost of losing an arbitrarily small amount of revenue.
7
Consider a fixed valuation profile v. Let i∗
denote the winner according to A, and i
′
the winner according
to the optimal auction M. If there is no winner, we interpret φi∗ (vi∗ ) and φi′ (vi′ ) as 0. Recall that M
always awards the item to a bidder with the highest positive virtual value (or no one, if no such bidders
exist). The definition of the thresholds immediately implies the following.
1. A only allocates to non-negative ironed virtual-valued bidders.
2. If there is no tie (that is, there is a unique bidder at the highest level), then i
′
= i∗
.
3. When there is a tie at level τ, the virtual value of the winner of A is close to that of M:
If τ ∈ [0, ⌈ 1
γǫ′ ⌉] then φi′ (vi′ ) − φi∗ (vi∗ ) ≤ αγǫ′
;
if τ ∈ [⌈ 1
γǫ′ ⌉, ⌈ 1
γǫ′ ⌉ + ⌈log1+ ǫ
2
H
α ⌉], φi∗ (vi∗ )
φi
′ (vi
′ ) ≥ 1 − ǫ
2 .
These facts imply that
Ev[Rev(A)] = Ev[φi∗ (vi∗ )] ≥ (1 − ǫ
2 ) · Ev[φi′ (vi′ )] − αγǫ′
= (1 − ǫ
2 ) · Ev[Rev(M)] − αγǫ′
. (3)
where the first and final equality follow from A and M’s allocations depending on ironed virtual values, not
on the values themselves, thus, the ironed virtual values are equal in expectation to the unironed virtual
values, thus the revenue, of the mechanisms (see [13], Chapter 3.5 for discussion).
The assumption that P[maxi vi > α] ≥ γ implies (since a feasible auction prices the good at α and awards
it to any bidder with value at least α, if any) that E[Rev(M)] ≥ αγ. Combining this with (3), and setting
ǫ′
= ǫ
2 implies Ev[Rev(A)] ≥ (1 − ǫ) Ev[Rev(M)]. 
Combining Theorems 2.1 and 3.4 yields the following Corollary 3.6.
Corollary 3.6 Let F be a product distribution with all bidders’ valuations in [1, H]. Assume that t =
Θ 1
ǫ + log1+ǫ H

and m = O

H
ǫ
2
nt log (nt) log H
ǫ + log 1
δ

= Õ

H2
n
ǫ3

. Then with probability at least
1 − δ, the single-item empirical revenue maximizer of Ct on a set of m samples from F has expected revenue
at least 1 − ǫ times that of the optimal auction.
4 Unbounded MHR Distributions
This section shows how to replace the assumption of bounded valuations by the assumption that each
valuation distribution satisfies the monotone hazard rate (MHR) condition, meaning that fi(vi)
1−Fi(vi) is non-
decreasing. Our resulting sample complexity bounds depend on the number of bidders n and the error
parameter ǫ only. bounded case, following ideas from This extension is based on previous work [5] that
effectively reduces the case of MHR valuations to the case of valuations lying in the interval

βǫ, 2β log 1
ǫ

for a suitable choice of β. Our analysis works with η-truncated t−level auctions, where each t-level auction
f is replaced with fη = min(f, η).
Theorem 4.1 Suppose F is a product distribution and each bidder’s valuation distribution satisfies the MHR
condition. Then, for each ǫ > 0, and each β̂ ≥ β such that P
h
maxi vi ≥ β̂
2
i
≥ 1 − 1
√
e
− ǫ′
, there is a t-level

β̂ log 1
ǫ′

-truncated auction with expected revenue at least 1 − ǫ times that of an optimal auction, where
t = Θ 1
ǫ′ + log1+ǫ′ log 1
ǫ′

and ǫ′
= O

ǫ
log 1
ǫ

.
Before proving Theorem 4.1, we quote a key fact about MHR distributions Cai and Daskalakis [5].
Theorem 4.2 (Theorem 19 and Lemma 38 of [5]) Let X1, . . . , Xn be a collection of independent ran-
dom variables whose distributions satisfy the MHR condition. Then there exists an anchoring point β such
that
P[max
i
Xi ≥
β
2
] ≥ 1 −
1
√
e
,
8
and for all ǫ > 0, Z ∞
2β log 1
ǫ
zfmaxi Xi (z)dz ≤ 36βǫ log
1
ǫ
.
Now, we proceed to prove Theorem 4.1.
Proof of Theorem 4.1: Fix ǫ′
, to be defined later. Conditioning on all bids being at most 2β̂ log 1
ǫ′ allows
us apply Lemma 3.5 as though the valuations are bounded. In particular, since P[maxi vi ≥ β̂
2 ] ≥ 1 −
1
√
e
− ǫ′
, Lemma 3.5 implies for α = β̂
2 , γ = 1 − 1
√
e
− ǫ′
and H = 2β̂ log 1
ǫ′ , implies the existence of a
t = O 1
ǫ′ + log1+ǫ′ log 1
ǫ′

-level truncated5
auction A such that:
E

Rev(A)| max
i
vi ≤ 2β̂ log
1
ǫ′

≥ (1 − ǫ′
)E

Rev(M)| max
i
vi ≤ 2β̂ log
1
ǫ′

(4)
Thus, we have
E [Rev(A)] ≥ E

Rev(A)| max
i
vi ≤ 2β̂ log
1
ǫ′

P

max
i
vi ≤ 2β̂ log
1
ǫ′

≥ (1 − ǫ′
)E

Rev(M)| max
i
vi ≤ 2β̂ log
1
ǫ′

P

max
i
vi ≤ 2β̂ log
1
ǫ′

≥ (1 − ǫ′
)E [Rev(M)] − 36β̂ǫ′
log
1
ǫ′
≥

1 − O

ǫ′
log
1
ǫ′

E [Rev(M)]
where the first inequality comes from the fact that A only sells to agents with non-negative virtual value
(so the revenue on a smaller region of bids is only less), the second from Equation 4 and probabilities being
at most 1, the penultimate from Theorem 4.2, and the final from the fact that Rev(M) ≥
1− 1
√
e
2 β̂. Setting
ǫ = ǫ′
log 1
ǫ′ and noticing that this implies ǫ′
≤ ǫ
log 1
ǫ
yields the desired result. 
Corollary 4.3 follows as a corollary of Theorems 2.1 and 4.1.
Corollary 4.3 With probability 1 − δ, the empirical revenue maximizer for a sample of size m of the class
of t-level η-truncated single-item auctions is a 1 − O (ǫ)-approximation of the optimal auction for n MHR
bidders, for t = O 1
ǫ′ + log1+ǫ′ log 1
ǫ′

, ǫ′
= ǫ
log 1
ǫ
and
m = O

1
ǫ′
2 
nt log (nt) ln
1
ǫ′
+ ln
1
δ
!
= Õ
 n
ǫ3

where η can be learned from the set of m samples.
Proof: We first argue that one can learn some η from the sample. Let Q(S, g) =
P
v∈S
ǫ′
I[maxi vi≥g]
|S| and
q(g) = P [maxi vi ≥ g] (the empirical and true probability that the maximum bid is at least g, respectively).
Given ǫ′
, consider a set of samples Sǫ′ of profiles, and compute the largest β̂ such that Q(Sǫ′ , β̂
2 ) ≥ 1− 1
√
e
−ǫ′
.
Standard VC-bounds imply that |q(ρ) − Q(Sǫ′ , ρ)| ≤ ǫ′
for all ρ with probability at least 1 − δ, provided
|Sǫ′ | ≥ Θ

1
ǫ′
2
ln 1
δ

.6
. In particular, with probability 1 − δ, we will have q(Sǫ′ , β
2 ) ≥ 1 − 1
√
e
− ǫ′
, so it will
be the case that β̂ ≥ β, and also q(β̂) ≥ 1 − 1
√
e
− 2ǫ′
. Then, let η = 2β̂ log 1
ǫ′ .
5Lemma 3.5 only implies the existence of such a t-level auction. However, when the bids are all below some η, one can
always find an η-truncated auction which is equivalent to each untruncated auction.
6This can be thought of as a class of binary classifiers with VC-dimension one.
9
Now, Theorem 4.1 implies the existence of a η-truncated t-level auction which (1 − ǫ′
)-approximates the
optimal auction. The argument is completed using the fact that, if the auctions’ values are upper-bounded
by η, one can equivalently think of the values being upper-bounded by η, so Theorem 2.1 implies the sample
complexity bound allowing additive error ǫ′
η = ǫ′
β̂
2 log 1
ǫ′ . This error is multiplicatively at most ǫ = ǫ′
log 1
ǫ′ ,
since Rev(M) = Ω

β̂
2

. 
Remark 4.4 (Near-optimality of sample complexity) Can we do better than Theorem 3.3? Can we
learn an approximately optimal auction from a simpler class — with pseudo-dimension poly(log H, log n, 1
ǫ ),
say — allowing for much smaller sample complexity than achieved here? The answer is negative: lower
bounds in Cole and Roughgarden [8] imply that approximate revenue maximization requires sample com-
plexityy at least linear in the number of bidder n, even when bidders’ valuations independently drawn from
MHR distributions. Thus, every class of auctions that is sufficiently expressive to guarantee expected revenue
at least 1 − ǫ times optimal must have pseudo-dimension that grows polynomially with n.
5 t-Level Matroid Auctions
This section extends the ideas and techniques from Section 3.2 to matroid environments. The straightforward
generalization of t-level auctions to matroid environments suffices: we order the bidders by level, breaking
ties within a level by some fixed linear ordering over agents ≻, and greedily choose winners according to this
ordering (subject to feasibility and to bids exceeding the lowest threshold). The next theorem bounds the
pseudo-dimension of this more general class of auctions.
Theorem 5.1 The pseudo-dimension of t-level matroid auctions with n bidders is O(nt log (nt)).
The proof is conceptually similar to that of Theorem 3.3, though we require a more general argument.
Our proof uses a couple of standard results from learning theory (see e.g. [16] for details). The first, also
known as Sauer’s Lemma, states that the number of distinct projections of a set S induced by a set system
with bounded VC dimension grows only polynomially in |S|.
Lemma 5.2 Let C be a set of functions from Q to {0, 1} with VC dimension d, and S⊆Q. Then
|{S ∩ {x ∈ Q : c(x) = 1} : c ∈ C}| ≤ |S|d
.
Recall that a linear separator in Rd
is defined by coefficients a1, . . . , ad, and assigns x ∈ Rd
the value 1
if
Pd
i=1 aixi ≥ 0 and the value 0 otherwise.
Lemma 5.3 The set of linear separators in Rd
has VC dimension d + 1.
Proof of Theorem 5.1: Consider a set of samples S of size m which can be shattered by t-level matroid
auctions with revenue targets (r1
, . . . , rm
). We upper-bound the number of labelings of S possible using
t-level auctions, which again yields an upper bound on m.
We partition auctions into equivalence classes, identically to the proof of Theorem 3.3. Recall that, across
all auctions in an equivalence class, all comparisons between two thresholds or a threshold and a bid are
resolved identically. Recall also that the number of equivalence classes is at most (nm + nt)2nt
. We now
upper bound the number of distinct labelings any fixed equivalence class C of auctions can generate.
Consider a class C of equivalent auctions. The allocation and payment rules are more complicated than
in the single-item case but still relatively simple. In particular, whether or not a bidder wins depends only
on the ordering of the bidders (by level) and the fixed tie-breaking rule ≻, and thus is a function only of
comparisons between bids and thresholds. This implies that, for every sample in S, all auctions in the class C
declare the same set of winners. It also implies that the payment of each winning bidder is a fixed threshold
ℓi∗,τ , and the identity of this parameter is the same across all auctions in C.
10
Now, encode each auction A ∈ C and sample vj
as an nt + 1-dimensional vector as follows. Let xA
i,τ
equal the value of ℓi,τ in the auction A. Define xA
nt+1 = 1 for every A ∈ C. Define yj
i,τ = 1 if bidder i is a
winner paying her threshold ℓi,τ for auctions in C and 0 otherwise. Finally, define yj
nt+1 = −rj
. The point
is that, for every auction A in the class C and sample vj
,
xA
· yj
≥ 0
if and only if Rev(A) ≥ rj
. Thus, the number of distinct labelings of the samples generated by auctions in
C is bounded above by the number of distinct sign patterns on m points in Rnt+1
generated by all linear
separators. (The yj
-vectors are constant across C and can be viewed as m fixed points in Rnt+1
; each
auction A ∈ C corresponds to the vector xA
of coefficients.) Applying Lemmas 5.2 and 5.3, t-level matroid
auctions can generate at most mnt+2
labelings per equivalence class, and hence at most (nm + nt)3nt+2
distinct labelings in total. This imposes the restriction
2m
≤ (nm + nt)3nt+2
;
solving for m yields the desired bound. 
We now extend our representation error bound for t-level single-item auctions to matroids.
Theorem 5.4 Consider an arbitrary matroid environment. Suppose F is a production distribution with
valuations in [1, H]. Provided t = Ω 1
ǫ + log1+ǫ H

, there exists a t-level matroid auction with expected
revenue at least a 1 − ǫ fraction of the optimal expected revenue.
The key new idea in the proof is to exhibit a bijection between the feasible sets I∗
(our winning set)
and I
′
(the optimal winning set) such that each bidder from I∗
has a level at least as high as their bijective
partner in I
′
. To implement this, we use the following property of matroids (e.g. [14, 20]).
Proposition 5.5 Let Opt denote the largest-weight set of a matroid, and let B be any other feasible set
such that |B| = |Opt|, and Opti, Bi denote the i-th largest element of Opt and B, respectively. Then
w(Opti) ≥ w(Bi) for all i.
Proof of Theorem 5.4: Define bidders’ thresholds exactly as in the proof of Theorem 3.4 and let A denote the
corresponding t-level auction. Fix an arbitrary valuation profile v. Let I∗
denote the set of winning bidders
in A and I
′
the set of winning bidders in M. Recall that the latter is the feasible set that maximizes the
sum of virtual valuations. Both sets are maximally independent amongst those bidders with non-negative
virtual value (M, by virtual of being welfare-maximal, and A, by definition). Then, we claim |I∗
| = |I
′
| (if
not, by the augmentation property of matroids, the smaller set could be extended to include an element of
the larger while maintaining independence, violating their maximality).
Notice that I∗
is lexicographically optimal with respect to the levels, rather than the exact weights.
Proposition 5.5 implies that I
′
is also lexicographically optimal with respect to the levels; thus, the level
of the ith largest bidder in I
′
has the same level as the ith largest bidder in I∗
. Then, by an accounting
argument identical to the one for the single-item case (comparing virtual values for the ith bidder in I∗
to
the ith bidder in I
′
) summing up over all bidders completes the proof. 
Thus, we have the following corollary about the sample complexity of 1 − ǫ-approximating Myerson in
matroid environments with t-level auctions, noting that the maximum revenue is now nH rather than H.
Corollary 5.6 With probability 1 − δ, the empirical revenue maximizer for a sample of size m of the class
of t-level single-item auctions is a 1 − O (ǫ)-approximation to Myerson for n bidders whose valuations are in
[1, H], for t = O 1
ǫ + log1+ǫ H

and
m = O

Hn
ǫ
2 
nt log(nt) ln
Hn
ǫ
+ ln
1
δ
!
= Õ

H2
n3
ǫ3

.
11
6 Single-parameter t-level auctions
In this section, we show how to extend the ideas and techniques from Section 3.2 to any single-parameter
environment which has the empty set as a feasible outcome. With this mild assumption, the results in this
section do not require the environment to be a matroid or even downwards-closed. Before we state this
result, we need a slight generalization of the t-level auction to this setting. Previously, no t-level auction
would allocate to any bidder whose value was below their lowest threshold, and this will not be a possibility
in environments which are not downwards-closed. Instead, in this setting, if any bidder fails to pass her
lowest threshold, we will assume A will choose the empty outcome. Moreover, a t-level auction will now need
more about what the various levels represent: previously, we implicitly used the kth threshold to correspond
to a value where each bidder’s virtual value would pass some quantity qk.
In this general setting, we make that connection explicit. There will still be nt numbers which define a
particular t-level auction, the t threshold locations per bidder. In addition, we will consider a fixed vector
Φ ∈ Rt
(not parameterizing the auction class) which, for all τ, intuitively assigns an estimate of φ−1
i (ℓi,τ ),
which is the same for all bidders i. Formally, Φτ will be used to assign a real value to a feasible set X ∈ X
with a valuation profile v as follows. Let eX =
P
i∈X Φti(vi), where ti(vi) as before is the level agent i’s bid
according to vi. Then, a particular t-level auction will choose the winning set X ∈ X which maximizes eX
(breaking ties in some fixed way which does not depend upon the bids). If, for each bidder i and level τ, the
threshold ℓi,τ is placed exactly at the value at which i’s virtual valuation surpasses Φτ , then this auction is
approximately optimizing the virtual surplus of the winning set.
Theorem 6.1 The pseudo-dimension of t-level single-parameter auctions with n bidders is O (nt log(nt)).
These auctions have slightly more complicated payment rules than those for matroids, where each agent
i was intuitively competing with (at most) one other bidder for inclusion in the winning set. Now, a bidder
i who is in the winning set X will have a payment of the following form. For a fixed assignment of levels
to bidders, sort the alternatives according to their values eY for all Y ∈ X. Let Y be the highest-ranked
alternative set which does not contain i. Then, i’s payment will be the threshold corresponding to the
minimal τ such that
P
i′∈X,i′6=i Φti′ (vi′ ) + Φτ ≥
P
i′∈Y Φti′ (vi′ ) (namely, the minimal bid which keeps X
preferred to Y in terms of the estimated virtual values)7
. While this rule is more complicated, it is still the
case that, once each bidder is assigned to some level, each of the bidders in the winning set’s payment is just
one of their thresholds. Thus, the proof of Theorem 6.1 is identical to the one of Theorem 5.1 without ties.
When considering non-downwards-closed environments, the optimal revenue may be arbitrarily close to
0, making it difficult to argue about multiplicative approximations to the optimal revenue. Instead, we
will give a weaker guarantee, namely, that the empirical revenue maximizer will have expected revenue
which is additively close to the optimal expected revenue. If one is willing to restrict the environment to
be downwards-closed, it is possible to achieve a multiplicative guarantee, since in that case the optimal
revenue is at least 1. We now state the Theorem which bounds the representation error of t-level auctions
for single-parameter environments.
Theorem 6.2 There is a t-level auction whose expected revenue is within an additive ǫ of optimal, in any
single-parameter setting X such that ∅ ∈ X, for t = O

Hn2
ǫ

, for n bidders with valuations distributions
which are product and bounded in [0, H].
Proof: Let t = Hn2
ǫ + Hn
ǫ . We will begin by defining Φ, the t-dimensional vector corresponding to the
estimated virtual values. Let Φ0 = −Hn (if any bidder has virtual value < −Hn, the virtual value of any set
containing her is negative, since virtual values are upper-bounded by values and the value of the remaining
set may be at most H(n − 1), so in this case one should allocate to ∅). Then, let Φτ = Φτ−1 + ǫ
n . Thus, we
partition the space of virtual values into additive sections of width ǫ
n .
7Since we assume ties are broken in a way which does not depend on the bids, we can ignore ties in the payment rule, and
agents will only ever pay thresholds.
12
Then, for each bidder i and τ, let ℓi,τ = φ−1
i (Φτ ), the value at which bidder i’s virtual value surpasses
Φτ . Then, consider a valuation profile v on which M and this particular A disagree on the winning sets
I∗
, I
′
∈ X. Notice that each bidder i’s virtual value is estimated correctly within an additive ǫ
n by Φti(vi)
(assuming no bidder has highly negative virtual value, in which case M and A both choose outcome ∅), and
are never overestimated. Thus, it is the case that
X
i∗∈I∗
φi∗ (vi∗ ) ≥
X
i∗∈I∗
Φti∗ (vi∗ ) ≥
X
i′
∈I′
Φti
′ (vi
′ ) ≥
X
i′
∈I′
φi′ (vi′ ) − ǫ
and the claim follows. 
Thus, we have the following sample complexity result for general single-parameter settings.
Corollary 6.3 With probability 1 − δ, the empirical revenue maximizer on m samples S from the class
of t-level auctions has true expected revenue within an additive ǫ of Myerson’s expected revenue, for the
single-parameter environment X when bidders have valuations in [0, H], for t = O

Hn2
ǫ

, and
m = O

Hn
ǫ
2 
nt log(nt) log
Hn
ǫ
+ log
1
δ
!
= Õ

H3
n5
ǫ3

.
Open Questions
There are some significant opportunities for follow-up research. First, there is much to do on the design of
computationally efficient (in addition to sample-efficient) algorithms for learning a near-optimal auction. The
present work focuses on sample complexity, and our learning algorithms are generally not computationally
efficient.8
The general research agenda here is to identify auction classes C for various settings such that:
1. C has low representation error;
2. C has small pseudo-dimension;
3. There is a polynomial-time algorithm to find an approximately revenue-maximizing auction from C on
a given set of samples.9
There are also interesting open questions on the statistical side, notably for multi-parameter problems.
While the negative result in [11] rules out a universally good upper bound on the sample complexity of
learning a near-optimal mechanism in multi-parameter settings, we suspect that positive results are possible
for several interesting special cases.
References
[1] Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press,
NY, NY, USA, 1999.
[2] Moshe Babaioff, Nicole Immorlica, Brendan Lucier, and S. Matthew Weinberg. A simple and approximately optimal
mechanism for an additive buyer. SIGecom Exch., 13(2):31–35, January 2015.
[3] Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Single price mechanisms for revenue maximization in unlimited
supply combinatorial auctions. Technical report, Carnegie Mellon University, 2007.
[4] Maria-Florina Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Reducing mechanism design to algorithm
design via machine learning. Jour. of Comp. and System Sciences, 74(8):1245–1270, 2008.
8There is a clear parallel with computational learning theory [22]: while the information-theoretic foundations of classification
(VC dimension, etc. [23]) have been long understood, this research area strives to understand which low-dimensional concept
classes are learnable in polynomial time.
9The sample-complexity and performance bounds implied by pseudo-dimension analysis, as in Theorem 2.1, hold with such
an approximation algorithm, with the algorithm’s approximation factor carrying through to the learning algorithm’s guarantee.
See also [4, 11].
13
[5] Yang Cai and Constantinos Daskalakis. Extreme-value theorems for optimal multidimensional pricing. In Foundations of
Computer Science (FOCS), 2011 IEEE 52nd Annual Symposium on, pages 522–531, Palm Springs, CA, USA., Oct 2011.
IEEE.
[6] Shuchi Chawla, Jason Hartline, and Robert Kleinberg. Algorithmic pricing via virtual valuations. In Proceedings of the
8th ACM Conf. on Electronic Commerce, pages 243–251, NY, NY, USA, 2007. ACM.
[7] Shuchi Chawla, Jason D. Hartline, David L. Malec, and Balasubramanian Sivan. Multi-parameter mechanism design and
sequential posted pricing. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, pages 311–320,
NY, NY, USA, 2010. ACM.
[8] Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceedings of the 46th Annual
ACM Symposium on Theory of Computing, pages 243–252, NY, NY, USA, 2014. SIAM.
[9] Nikhil Devanur, Jason Hartline, Anna Karlin, and Thach Nguyen. Prior-independent multi-parameter mechanism design.
In Internet and Network Economics, pages 122–133. Springer, Singapore, 2011.
[10] Peerapong Dhangwatnotai, Tim Roughgarden, and Qiqi Yan. Revenue maximization with a single sample. In Proceedings
of the 11th ACM Conf. on Electronic Commerce, pages 129–138, NY, NY, USA, 2010. ACM.
[11] Shaddin Dughmi, Li Han, and Noam Nisan. Sampling and representation complexity of revenue maximization. In Web
and Internet Economics, volume 8877 of Lecture Notes in Computer Science, pages 277–291. Springer Intl. Publishing,
Beijing, China, 2014.
[12] Edith Elkind. Designing and learning optimal finite support auctions. In Proceedings of the eighteenth annual ACM-SIAM
symposium on Discrete algorithms, pages 736–745. SIAM, 2007.
[13] Jason Hartline. Mechanism design and approximation. Jason Hartline, Chicago, Illinois, 2015.
[14] Jason D. Hartline and Tim Roughgarden. Simple versus optimal mechanisms. In ACM Conf. on Electronic Commerce,
Stanford, CA, USA., 2009. ACM.
[15] Zhiyi Huang, Yishay Mansour, and Tim Roughgarden. Making the most of your samples. abs/1407.2479:1–3, 2014. URL
http://arxiv.org/abs/1407.2479.
[16] Michael J. Kearns and Umesh V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge,
MA, 1994.
[17] Andres Munoz Medina and Mehryar Mohri. Learning theory and algorithms for revenue optimization in second price
auctions with reserve. In Proceedings of The 31st Intl. Conf. on Machine Learning, pages 262–270, 2014.
[18] Roger B Myerson. Optimal auction design. Mathematics of operations research, 6(1):58–73, 1981.
[19] David Pollard. Convergence of stochastic processes. David Pollard, New Haven, Connecticut, 1984.
[20] T. Roughgarden and O. Schrijvers. Ironing in the dark. Submitted, 2015.
[21] Tim Roughgarden, Inbal Talgam-Cohen, and Qiqi Yan. Supply-limiting mechanisms. In Proceedings of the 13th ACM
Conf. on Electronic Commerce, pages 844–861, NY, NY, USA, 2012. ACM.
[22] Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.
[23] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their proba-
bilities. Theory of Probability & Its Applications, 16(2):264–280, 1971.
[24] Andrew Chi-Chih Yao. An n-to-1 bidder reduction for multi-item auctions and its applications. In Proceedings of the
Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 92–109, San Diego, CA, USA., 2015. ACM.
14
