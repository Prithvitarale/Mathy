JournalofMachineLearningResearch15(2014)2773-2832Submitted2/13;Revised3/14;Published8/14TensorDecompositionsforLearningLatentVariableModelsAnimashreeAnandkumara.anandkumar@uci.eduElectricalEngineeringandComputerScienceUniversityofCalifornia,Irvine2200EngineeringHallIrvine,CA92697RongAnandkumar,Ge,Hsu,Kakade,andTelgarskyKeywords:latentvariablemodels,tensordecompositions,mixturemodels,topicmodels,methodofmoments,powermethod1.IntroductionThemethodofmomentsisaclassicalparameterestimationtechnique(Pearson,1894)fromstatisticswhichhasprovedinvaluableinanumberofapplicationdomains.Thebasicparadigmissimpleandintuitive:(i)computecertainstatisticsofthedataâ€”oftenempiricalmomentssuchasmeansandcorrelationsâ€”and(ii)findmodelparametersthatgiveriseto(nearly)thesamecorrespondingpopulationquantities.InanumberofTensorDecompositionsforLearningLatentVariableModelsOneapproachforobtainingtheorthogonaldecompositionisthetensorpowermethodofLathauweretal.(2000,Remark3).Weprovideaconvergenceanalysisofthismethodfororthogonallydecomposablesymmetrictensors,aswellasadetailedperturbationanalysisforarobust(andacomputationallytractable)variant(Theorem5.1).ThisperturbationanalysiscanbeviewedasananalogueofWedinâ€™sperturbationtheoremforsingularvectorsofmatrices(Wedin,1972),providingaboundontheerroroftherecovereddecompositionintermsoftheoperatornormofthetensorperturbation.Thisanalysisissubtleinatleasttwoways.First,unlikeformatrices(whereeverymatrixhasasingularvaluedecomposi-tion),anorthogonaldecompositionneednotexistfortheperturbedtensor.Ourrobustvariantusesrandomrestartsanddeflationtoextractanapproximatedecompositioninacomputationallytractablemanner.Second,theanalysisofthedeflationstepsisnon-trivial;anaÄ±Ìˆveargumentwouldentailerroraccumulationineachdeflationstep,whichweshowcaninfactbeavoided.Whenthismethodisappliedforparameterestimationinlatentvariablemodelspreviouslydiscussed,improvedsamplecomplexitybounds(overpreviouswork)canbeobtainedusingthisperturbationanalysis.Finally,wealsoaddresscomputationalissuesthatarisewhenapplyingthetensorde-compositionapproachestoestimatinglatentvariablemodels.Specifically,weAnandkumar,Ge,Hsu,Kakade,andTelgarskycorrespondtolocalextremaoftheso-calledâ€œcontrastfunctionsâ€whichdistinguishGaussianvariablesfromnon-Gaussianvariables.Thismethodcanbeeffectivelyimplementedusingfastdescentalgorithms(Hyvarinen,1999).Whenusingtheexcesskurtosis(i.e.,TensorDecompositionsforLearningLatentVariableModelsRegalia,2002)â€”thespecialsymmetricorthogonalstructureweconsiderpermitssimplealgorithmstoefficientlyandstablyrecoverthedesireddecomposition.Inparticular,agen-eralizationofthematrixpowermethodtosymmetrictensors,introducedbyLathauweretal.(2000,Remark3)andanalyzedbyKofidisandRegalia(2002),providessuchade-composition.ThisisinfactimpliedbythecharacterizationofZhangandGolub(2001),whichshowsthatiterativelyobtainingthebestrank-1approximationofsuchorthogonallydecomposabletensorsalsoyieldstheexactdecomposition.Wenotethatingeneral,ob-tainingsuchapproximationsforgeneral(symmetric)tensorsisNP-hard(HillarandLim,2013).1.2.2LatentVariableModelsThisworkfocusesontheparticularapplicationoftensordecompositionmethodstoestimat-inglatentvariablemodels,asignificantdeparturefrommanypreviousapproachesinthemachinelearningandstatisticsliterature.ByfarthemostpopularheuristicforparameterestimationforsuchmodelsistheExpectation-Maximization(EM)algorithm(Dempsteretal.,1977;RednerandWalker,1984).AlthoughEMhasanumberofmerits,itmaysufferfromslowconvergenceandpoorqualitylocaloptima(RednerandWalker,1984),requir-ingpractitionerstoemploymanyadditionalheuristicstoobtaingoodsolutions.Forsomemodelssuchaslatenttrees(Roch,2006)andtopicmodels(Aroraetal.,2012a),maximumlikelihoodestimationisNP-hard,whichsuggeststhatotherestimationapproachesmaybemoreattractive.Morerecently,algorithmsfromtheoreticalcomputerscienceandmachinelearninghaveaddressedcomputationalandsamplecomplexityissuesrelatedtoestimatingcertainlatentvariablemodelssuchasGaussianmixturemodelsandHMMs(Dasgupta,1999;AroraandKannan,2005;DasguptaandSchulman,2007;VempalaandWang,2004;Kannanetal.,2008;AchlioptasandMcSherry,2005;ChaudhuriandRao,2008;BrubakerandVempala,2008;Kalaietal.,2010;BelkinandSinha,2010;MoitraandValiant,2010;HsuandKakade,2013;Chang,1996;MosselandRoch,2006;Hsuetal.,2012b;Anandkumaretal.,2012c;Aroraetal.,2012a;Anandkumaretal.,2012a).SeetheworksbyAnandku-maretal.(2012c)andHsuandKakade(2013)foradiscussionofthesemethods,togetherwiththecomputationalandstatisticalhardnessbarriersthattheyface.Thepresentworkreviewsabroadrangeoflatentvariableswhereamildnon-degeneracyconditionimpliesthesymmetricorthogonaldecompositionstructureinthetensorsoflow-orderobservablemoments.Notably,anotherclassofmethods,basedonsubspaceidentification(OverscheeandMoor,1996)andobservableoperatormodels/multiplicityautomata(SchuÌˆtzenberger,1961;Jaeger,2000;Littmanetal.,2001),havebeenproposedforanumberoflatentvariablemodels.ThesemethodsweresuccessfullydevelopedforHMMsbyHsuetal.(2012b),andsubsequentlygeneralizedandextendedforanumberofrelatedsequentialandtreeMarkovmodelsmodels(Siddiqietal.,2010;Bailly,2011;Bootsetal.,2010;Parikhetal.,2011;Roduetal.,2013;Balleetal.,2012;BalleandMohri,2012),aswellascertainclassesofparsetreemodels(Luqueetal.,2012;Cohenetal.,2012;Dhillonetal.,2012).Thesemethodsuselow-ordermomentstolearnanâ€œoperatorâ€representationofthedistribution,whichcanbeusedfordensityestimationandbeliefstateupdates.Whilefinitesampleboundscanbegiventoestablishthelearnabilityofthesemodels(Hsuetal.,2012b),Anandkumar,Ge,Hsu,Kakade,andTelgarskyactuallygiveparameterestimates(e.g.,oftheemissionortransitionmatricesinthecaseofHMMs).1.3OrganizationTherestofthepaperisorganizedasfollows.Section2reviewssomebasicdefinitionsoftensors.Section3providesexamplesofanumberoflatentvariablemodelswhich,afterappropriatemanipulationsoftheirlowordermoments,shareacertainnaturaltensorstruc-ture.Section4reducestheproblemofparameterestimationtothatofextractingacertain(symmetricorthogonal)decompositionofatensor.WethenprovideadetailedanalysisofarobusttensorpowermethodandestablishananalogueofWedinâ€™sperturbationtheoremforthesingularvectorsofmatrices.ThediscussioninSection6addressesanumberofpracticalconcernsthatarisewhendealingwithmomentmatricesTensorDecompositionsforLearningLatentVariableModelsTherankofap-thordertensorAâˆˆNpRnisthesmallestnon-negativeintegerksuchthatA=Pkj=1u1,jâŠ—u2,jâŠ—Â·Â·Â·âŠ—up,jforsomeui,jâˆˆRn,iâˆˆ[p],jâˆˆ[k],andthesymmetricrankofasymmetricp-thordertensorAisthesmallestnon-negativeintegerksuchthatA=Pkj=1uâŠ—pjforsomeujâˆˆRn,jâˆˆ[k].1Thenotionofrankreadilyreducestotheusualdefinitionofmatrixrankwhenp=2,asrevealedbythesingularvaluedecomposition.Similarly,forsymmetricmatrices,thesymmetricrankisequivalenttothematrixrankasgivenbythespectraltheorem.Adecompositionintosuchrank-onetermsisknownasacanonicalpolyadicdecomposition(Hitchcock,1927a,b).Thenotionoftensor(symmetric)rankisconsiderablymoredelicatethanmatrix(sym-metric)rank.Forinstance,itisnotclearapriorithatthesymmetricrankofatensorshouldevenbefinite(Comonetal.,2008).Inaddition,removalofthebestrank-1approx-imationofa(general)tensormayincreasethetensorrankoftheresidual(StegemanandComon,2010).Throughout,weusekvk=(Piv2i)1/2todenotetheEuclideannormofavectorv,andkMktodenotethespectral(operator)normofamatrix.WealsousekTktodenotetheoperatornormofatensor,whichwedefinelater.3.TensorStructureinLatentVariableModelsInthissection,wegiveseveralexamplesoflatentvariablemodelswhoselow-ordermomentscanbewrittenassymmetrictensorsoflowsymmetricrank;someoftheseexamplescanbededucedusingthetechniquesdevelopedinthetextbyMcCullagh(1987).ThebasicformisdemonstratedinTheorem3.1forthefirstexample,andthegeneralpatternwillemergefromsubsequentexamples.3.1ExchangeableSingleTopicModelsWefirstconsiderasimplebag-of-wordsmodelfordocumentsinwhichthewordsinthedocumentareassumedtobeexchangeable.Recallthatacollectionofrandomvariablesx1,x2,...,x`areexchangeableiftheirjointprobabilitydistributionisinvarianttopermu-tationoftheindices.Thewell-knownDeFinettiâ€™stheorem(Austin,2008)impliesthatsuchexchangeablemodelscanbeviewedasmixturemodelsinwhichthereisalatentvariablehsuchthatx1,x2,...,x`areconditionallyi.i.d.givenh(seeFigure1(a)forthecorrespondinggraphicalmodel)andtheconditionaldistributionsareidenticalatallthenodes.Inoursimplifiedtopicmodelfordocuments,thelatentvariablehisinterpretedasthe(sole)topicofagivendocument,anditisassumedtotakeonlyafinitenumberofdistinctvalues.Letkbethenumberofdistincttopicsinthecorpus,dbethenumberofdistinctwordsinthevocabulary,and`â‰¥3bethenumberofwordsineachdocument.Thegenerativeprocessforadocumentisasfollows:thedocumentâ€™stopicisdrawnaccordingtotheAnandkumar,Ge,Hsu,Kakade,andTelgarskyGiventhetopich,thedocumentâ€™s`wordsaredrawnindependentlyaccordingtothedis-cretedistributionspecifiedbytheprobabilityvectorÂµhâˆˆâˆ†dâˆ’1.Itwillbeconvenienttorepresentthe`wordsinthedocumentbyd-dimensionalrandomvectorsx1,x2,...,x`âˆˆRd.Specifically,wesetxt=eiifandonlyifthet-thwordinthedocumentisi,tâˆˆ[`],wheree1,e2,...edisthestandardcoordinatebasisforRd.Oneadvantageofthisencodingofwordsisthatthe(cross)momentsoftheserandomvectorscorrespondtojointprobabilitiesoverwords.Forinstance,observethatE[x1âŠ—x2]=X1â‰¤i,jâ‰¤dPr[x1=ei,x2=ej]eiâŠ—ej=X1â‰¤i,jâ‰¤dPr[1stword=i,2ndword=j]eiâŠ—ej,sothe(i,j)-theentryofthematrixE[x1âŠ—x2]isPr[1stword=i,2ndword=j].Moregenerally,the(i1,i2,...,i`)-thentryinthetensorE[x1TensorDecompositionsforLearningLatentVariableModelsAswewillseeinSection4.3,thestructureofM2andM3revealedinTheorem3.1impliesthatthetopicvectorsÂµ1,Âµ2,...Anandkumar,Ge,Hsu,Kakade,andTelgarsky3.2.2SphericalGaussianMixtures:DifferingCovariancesThegeneralcaseiswhereeachcomponentmayhaveadifferentsphericalcovariance.Anobservationinthismodelisagainx=Âµh+z,butnowzâˆˆRdisarandomvectorwhoseconditionaldistributiongivenh=i(forsomeiâˆˆ[k])isamultivariateGaussianN(0,Ïƒ2iI)withzeromeanandsphericalcovarianceÏƒ2iI.Theorem3.3(HsuandKakade,2013)Assumedâ‰¥k.TheaveragevarianceÏƒÌ„2:=Pki=1wiÏƒ2iisthesmallesteigenvalueofthecovariancematrixE[xâŠ—x]âˆ’E[x]âŠ—E[x].LetvbeanyunitnormeigenvectorcorrespondingtotheeigenvalueÏƒÌ„2TensorDecompositionsforLearningLatentVariableModels(i.e.,Tisthefourthderivativetensorofthefunctionv7â†’8âˆ’1E[(v>x)2]2,soM4isthefourthcumulanttensor).LetÎºAnandkumar,Ge,Hsu,Kakade,andTelgarskyhx1x2Â·Â·Â·x`(a)Multi-viewmodelsh1h2Â·Â·Â·h`x1x2x`(b)HiddenMarkovmodelFigure1:Examplesoflatentvariablemodels.thecasewhereÎ±0issmall(e.g.,aconstantindependentofk),whereuponhtypicallyhasonlyafewlargeentries.Thiscorrespondstothesettingwherethedocumentsaremainlycomprisedofjustafewtopics.Theorem3.5(Anandkumaretal.,2012a)DefineM1:=E[x1]M2:=E[x1âŠ—x2]âˆ’Î±0Î±0+1M1âŠ—M1M3:=E[x1âŠ—x2âŠ—x3]âˆ’Î±0Î±0+2E[x1âŠ—x2âŠ—M1]+E[x1âŠ—M1âŠ—x2]+E[M1âŠ—x1âŠ—x2]+2Î±20(Î±0+2)(Î±0+1)M1âŠ—M1âŠ—M1.ThenM2=kXi=1Î±i(Î±0+1)Î±0ÂµiâŠ—ÂµiM3=kXi=12Î±i(Î±0+2)(Î±0+1)Î±0ÂµiâŠ—ÂµiâŠ—Âµi.NotethatÎ±0needstobeknowntoformM2andM3fromtherawmoments.This,however,isamuchweakerthanassumingthattheentiredistributionofhisknown(i.e.,knowledgeofthewholeparametervectorÎ±).3.3Multi-ViewModelsMulti-viewmodels(alsosometimescallednaÄ±ÌˆveBayesmodels)areaspecialclassofBayesiannetworksinwhichobservedvariablesx1,x2,...,x`areconditionallyindependentgivenalatentvariableh.Thisissimilartotheexchangeablesingletopicmodel,butherewedonotrequiretheconditionaldistributionsofthext,tâˆˆ[`]tobeidentical.TechniquesdevelopedforthisclasscanbeusedtohandleanumberofwidelyusedmodelsincludingTensorDecompositionsforLearningLatentVariableModelsAsbefore,welethâˆˆ[k]beadiscreterandomvariablewithPr[h=j]=wjforalljâˆˆ[k].Nowconsiderrandomvectorsx1âˆˆRdAnandkumar,Ge,Hsu,Kakade,andTelgarsky3.3.1MixturesofAxis-AlignedGaussiansandOtherProductDistributionsThefirstexampleisamixtureofkproductdistributionsinRnunderamildincoherenceas-sumption(Anandkumaretal.,2012c).Here,wealloweachofthekcomponentdistributionstohaveadifferentproductdistribution(e.g.,Gaussiandistributionwithanaxis-alignedco-variancematrix),butrequirethematrixofcomponentmeansA:=[Âµ1|Âµ2|Â·Â·Â·|Âµk]âˆˆRnÃ—ktosatisfyacertain(verymild)incoherencecondition.Theroleoftheincoherenceconditionisexplainedbelow.Foramixtureofproductdistributions,anypartitioningofthedimensions[n]intothreegroupscreatesthree(possiblyasymmetric)â€œviewsâ€whichareconditionallyindependentoncethemixturecomponentisselected.However,recallthatTheorem3.6requiresthatforeachview,thekconditionalmeansbelinearlyindependent.Ingeneral,thismaynotbeachievable;consider,forinstance,thecaseÂµi=eiforeachiâˆˆ[k].Suchcases,wherethecomponentmeansareveryalignedwiththecoordinatebasis,areprecludedbytheincoherencecondition.Definecoherence(A):=maxiâˆˆ[n]{e>iÎ Aei}tobethelargestdiagonalentryoftheorthog-onalprojectortotherangeofA,andassumeAhasrankk.Thecoherenceliesbetweenk/nand1;itislargestwhentherangeofAisspannedbythecoordinateaxes,anditisk/nwhentherangeisspannedbyasubsetoftheHadamardbasisofcardinalityk.Theincoherenceconditionrequires,forsomeÎµ,Î´âˆˆ(0,1),coherence(A)â‰¤(Îµ2/6)/ln(3k/Î´).Essentially,thisconditionensuresthatthenon-degeneracyofthecomponentmeansisnotisolatedinjustafewofthendimensions.Operationally,itimpliesthefollowing.Proposition3.2(Anandkumaretal.,2012c)AssumeAhasrankk,andcoherence(A)â‰¤Îµ2/6ln(3k/Î´)forsomeÎµ,Î´âˆˆ(0,1).Withprobabilityatleast1âˆ’Î´,arandompartitioningofthedimensions[n]intothreegroups(foreachiâˆˆ[n],independentlypicktâˆˆ{1,2,3}uniformlyatrandomandputiingroupt)hasthefollowingproperty.Foreachtâˆˆ{1,2,3}andjâˆˆ[k],letÂµt,jbetheentriesofÂµjputintogroupt,andletAt:=[Âµt,1|Âµt,2|Â·Â·Â·|Âµt,k].Thenforeachtâˆˆ{1,2,3},Athasfullcolumnrank,andthek-thlargestsingularvalueofAtisatleastp(1âˆ’Îµ)/3timesthatofA.Therefore,threeasymmetricviewscanbecreatedbyrandomlypartitioningtheobservedrandomvectorxintox1,x2,andx3,suchthattheresultingcomponentmeansforeachviewsatisfytheconditionsofTheorem3.6.3.3.2SphericalGaussianMixtures,RevisitedConsideragainthecaseofsphericalGaussianmixtures(cf.Section3.2).AsweshallseeinSection4.3,theprevioustechniques(basedonTheorem3.2andTheorem3.3)leadtoestimationprocedureswhenthedimensionofxiskorgreater(andwhenthekcomponentmeansarelinearlyindependent).WenowTensorDecompositionsforLearningLatentVariableModelsWeagainusearandomizedreduction.Specifically,wecreatethreeviewsby(i)applyingarandomrotationtox,andthen(ii)partitioningxâˆˆRnintothreeviewsxÌƒ1,Anandkumar,Ge,Hsu,Kakade,andTelgarskyProposition3.4(Anandkumaretal.,2012c)Defineh:=y2,wherey2isthesecondhiddenstateintheMarkovchain.Thenâ€¢x1,x2,x3areconditionallyindependentgivenh;â€¢thedistributionofhisgivenbythevectorw:=TÏ€âˆˆâˆ†kâˆ’1;â€¢foralljâˆˆ[k],E[x1|h=j]=Odiag(Ï€)T>diag(w)âˆ’1ejE[x2|h=j]=OejE[x3|h=j]=OTej.Notethematrixofconditionalmeansofxthasfullcolumnrank,foreachtâˆˆ{1,2,3},providedthat:(i)Ohasfullcolumnrank,(ii)Tisinvertible,and(iii)Ï€andTÏ€havepositiveentries.4.OrthogonalTensorDecompositionsWenowshowhowrecoveringtheÂµiâ€™sinouraforementionedproblemsreducestotheprob-lemoffindingacertainorthogonaltensordecompositionofasymmetrictensor.Westartbyreviewingthespectraldecompositionofsymmetricmatrices,andthendiscussageneraliza-tiontothehigher-ordertensorcase.Finally,weshowhoworthogonaltensordecompositionscanbeusedforestimatingthelatentvariablemodelsfromtheprevioussection.4.1Review:TheMatrixCaseWefirstbuildintuitionbyreviewingthematrixsetting,wherethedesireddecomposi-tionistheeigendecompositionofasymmetricrank-kmatrixM=VÎ›V>,whereV=[v1|v2|Â·Â·Â·|vk]âˆˆRnÃ—kisthematrixwithorthonormaleigenvectorsascolumns,andÎ›=diag(Î»1,Î»2,...,Î»k)âˆˆRkÃ—kisdiagonalmatrixofnon-zeroeigenvalues.Inotherwords,M=kXi=1Î»iviv>i=kXTensorDecompositionsforLearningLatentVariableModelsu1andu2spanningthesamesubspaceasv1andv2.Nevertheless,thedecompositionisuniquewhenÎ»1,Î»2,...,Î»Anandkumar,Ge,Hsu,Kakade,andTelgarsky4.2.1Fixed-PointCharacterizationForatensorT,considerthevector-valuedmapu7â†’T(I,u,u)(5)whichisthethird-ordergeneralizationof(2).ThiscanbeexplicitlywrittenasTTensorDecompositionsforLearningLatentVariableModelsAswenowsee,theseadditionaleigenvectorscanbeviewedasspurious.WesayaunitvectoruisarobusteigenvectorofTifthereexistsan>0suchthatAnandkumar,Ge,Hsu,Kakade,andTelgarskyTheproofofTheorem4.2isgiveninAppendixA.2.ItissimilartolocaloptimalityanalysisforICAmethodsusingfourth-ordercumulants(e.g.,DelfosseandLoubaton,1995;Friezeetal.,1996).Again,weseesimilardistinctionstothematrixcase.Inthematrixcase,theonlylocalmaximizersoftheRayleighquotientaretheeigenvectorswiththelargesteigenvalue(andthesemaximizerstakeonthegloballyoptimalvalue).Forthecaseoforthogonaltensorforms,therobusteigenvectorsarepreciselytheisolatedlocalmaximizers.Animportantimplicationofthetwocharacterizationsisthat,fororthogonallydecom-posabletensorsT,(i)thelocalmaximizersoftheobjectivefunctionu7â†’T(u,u,u)/(u>u)3/2correspondpreciselytothevectorsviinthedecomposition,and(ii)theselocalmaximizerscanbereliablyidentifiedusingasimplefixed-pointiteration(i.e.,thetensoranalogueofthematrixpowermethod).Moreover,asecond-derivativetestbasedonT(I,I,u)canbeemployedtotestforlocaloptimalityandruleTensorDecompositionsforLearningLatentVariableModels4.3.1TheReductionFirst,letWâˆˆRdÃ—kbealineartransformationsuchthatM2(W,W)=W>M2W=IwhereIisthekAnandkumar,Ge,Hsu,Kakade,andTelgarsky4.3.2LocalMaximizersof(CrossMoment)SkewnessThevariationalcharacterizationprovidesaninterestingperspectiveontherobusteigen-vectorsfortheselatentvariablemodels.Considertheexchangeablesingletopicmodels(Theorem3.1),andtheobjectivefunctionu7â†’E[(x>1u)(x>2u)(x>3u)]E[(x>1u)(x>2u)]3/2=M3(u,u,u)M2(u,u)3/2.Inthiscase,everylocalmaximizeruâˆ—satisfiesM2(I,uâˆ—)=âˆšwiÂµiforsomeiâˆˆ[k].Theobjectivefunctioncanbeinterpretedasthe(crossmoment)skewnessoftherandomvectorsx1,x2,x3alongdirectionu.5.TensorPowerMethodInthissection,weconsiderthetensorpowermethodofLathauweretal.(2000,Remark3)fororthogonaltensordecomposition.WefirststateasimpleconvergenceanalysisforanorthogonallydecomposabletensorT.WhenonlyanapproximationTÌ‚toanorthogonallydecomposabletensorTisavailable(e.g.,whenempiricalmomentsareusedtoestimatepopulationmoments),anorthogonaldecompositionneednotexistforthisperturbedtensor(unlikeforthecaseofmatrices),andamorerobustapproachisrequiredtoextracttheapproximatedecomposition.Here,weproposesuchavariantinAlgorithm1andprovideadetailedperturbationanalysis.Wenotethatalternativeapproachessuchassimultaneousdiagonalizationcanalsobeemployed(seeAppendixD).5.1ConvergenceAnalysisforOrthogonallyDecomposableTensorsThefollowinglemmaestablishesthequadraticconvergenceofthetensorpowermethodâ€”i.e.,repeatediterationof(6)â€”forextractingasinglecomponentoftheorthogonaldecom-position.NotethattheinitialvectorÎ¸0determineswhichrobusteigenvectorwillbetheconvergentpoint.Computationofsubsequenteigenvectorscanbecomputedwithdeflation,i.e.,bysubtractingappropriatetermsfromT.Lemma5.1LetTâˆˆN3Rnhaveanorthogonaldecompositionasgivenin(4).ForavectorÎ¸0âˆˆRn,supposethatthesetofnumbers|Î»1v>1Î¸0|,|Î»2v>2Î¸0|,...,|Î»kv>kÎ¸0|hasauniquelargestelement.Withoutlossofgenerality,say|Î»1v>1Î¸0|isthislargestvalueand|Î»2v>2Î¸0|isthesecondlargestvalue.Fort=1,2,...,letÎ¸t:=T(I,Î¸tâˆ’1,Î¸tâˆ’1)kT(I,Î¸tâˆ’1,Î¸tâˆ’1)k.Thenkv1âˆ’Î¸tk2â‰¤2Î»21kXi=2Î»âˆ’2iÂ·Î»2v>2Î¸0Î»1v>1Î¸02t+1.Thatis,repeatedTensorDecompositionsforLearningLatentVariableModelsToobtainalleigenvectors,wemaysimplyproceediterativelyusingdeflation,executingthepowermethodonTâˆ’PjÎ»jvâŠ—3jafterhavingobtainedrobusteigenvector/eigenvaluepairsAnandkumar,Ge,Hsu,Kakade,andTelgarskyAssumethatthesymmetrictensorTâˆˆRkÃ—kÃ—kisorthogonallydecomposable,andthatTÌ‚=T+E,wheretheperturbationEâˆˆRkÃ—kÃ—kisasymmetrictensorwithsmalloperatornorm:kEk:=supkÎ¸k=1|E(Î¸,Î¸,Î¸)|.Inourlatentvariablemodelapplications,TÌ‚isthetensorformedbyusingempiricalmo-ments,whileTistheorthogonallydecomposabletensorderivedfromthepopulationmo-mentsforthegivenmodel.Inthecontextofparameterestimation(asinSection4.3),Emustaccountforanyerroramplificationthroughoutthereduction,suchasinthewhiteningstep(see,e.g.,HsuandKakade,2013,forsuchananalysis).ThefollowingtheoremissimilartoWedinâ€™sperturbationtheoremforsingularvectorsofmatrices(Wedin,1972)inthatitboundstheerrorofthe(approximate)decompositionreturnedbyAlgorithm1oninputTÌ‚intermsofthesizeoftheperturbation,providedthattheperturbationissmallenough.Theorem5.1LetTÌ‚=T+EâˆˆRkÃ—kÃ—k,whereTTensorDecompositionsforLearningLatentVariableModelsorthogonaldecomposition).Furthermore,notethatAlgorithm1usesmultiplerestartstoensure(approximate)convergenceâ€”theintuitionisthatbyrestartingatmultiplepoints,weeventuallystartatapointinwhichtheinitialcontractionAnandkumar,Ge,Hsu,Kakade,andTelgarskyInpractice,oneshoulduseallofthewordsinadocumentforefficientestimationofthemoments.Onewaytodothisistoaverageoverall`3Â·3!orderedtriplesofwordsinadocumentoflength`.Atfirstblush,thisseemscomputationallyexpensive(when`islarge),butasitturnsout,theaveragingcanbedoneimplicitly,asshownbyZouetal.(2013).LetcâˆˆRdbethewordTensorDecompositionsforLearningLatentVariableModelsisapproximatelytherangeofM2.Fromhere,anapproximateSVDofU>M2UisusedtocomputetheapproximatewhiteningmatrixW.NotethatbothmatrixAnandkumar,Ge,Hsu,Kakade,andTelgarskyalgorithmsduetoacertainamplificationoftheerror(apolynomialfactorofk,whichisobservedinpractice).Usingtheperturbationanalysisforthetensorpowermethod,improvedsamplecomplex-ityboundscanbeobtainedforalloftheexamplesdiscussedinSection3.Theunderlyinganalysisremainsthesameasinpreviousworks(e.g.,Anandkumaretal.,2012a;HsuandKakade,2013),themaindifferencebeingtheaccuracyoftheorthogonaltensordecompo-sitionobtainedviathetensorpowermethod.Relativetothepreviouslycitedworks,thesamplecomplexityboundwillbeconsiderablyimprovedinitsdependenceontherankpa-rameterk,asTheorem5.1impliesthatthetensorestimationerror(e.g.,errorinestimatingfM3fromSection4.3)isnotTensorDecompositionsforLearningLatentVariableModels1.ThesetofÎ¸âˆˆRnwhichdonotconvergetosomeviunderrepeatediterationof(6)hasmeasurezero.2.ThesetofrobusteigenvectorsofTisAnandkumar,Ge,Hsu,Kakade,andTelgarskyMoreover,ifkuk<1andT(I,u,u)=Î»uforÎ»>0,thenu0=(1+Î´)uforasmallenoughÎ´âˆˆ(0,1)satisfiesku0kâ‰¤1andT(u0,u0,u0)=(1+Î´)3T(u,u,u)>T(u,u,u).ThereforealocalmaximizermusthaveT(I,u,u)=Î»uforsomeÎ»â‰¥0,andkuk=1wheneverÎ»>0.Extend{v1,v2,...,vk}toanorthonormalbasis{v1,v2,...,vn}ofRn.NowpickanystationaryTensorDecompositionsforLearningLatentVariableModelsuÌ„:=u0+w,wehaveT(uÌƒ,uÌƒ,uÌƒ)=T(u0,u0,u0)+âˆ‡uT(u,u,u)>(uÌƒâˆ’Anandkumar,Ge,Hsu,Kakade,andTelgarskyAppendixB.AnalysisofRobustPowerMethodInthissection,weproveTheorem5.1.Theproofisstructuredasfollows.InAppendixB.1,weshowthatwithhighprobability,atleastoneoutofLrandomvectorswillbeagoodinitializerforthetensorpoweriterations.Aninitializerisgoodifitsprojectionontoaneigenvectorisnoticeablylargerthanitsprojectionontoothereigenvectors.WethenanalyzeinAppendixB.2theconvergencebehaviorofthetensorpoweriterations.TensorDecompositionsforLearningLatentVariableModelsItsufficestoshowthatwithprobabilityatleast1/2,thereisacolumnjâˆ—âˆˆ[L]suchthat|Z1,jâˆ—|â‰¥11âˆ’Î³maxiâˆˆ[k]\{1}|Zi,jAnandkumar,Ge,Hsu,Kakade,andTelgarskyInthissubsection,weassumethatTÌƒhastheformTÌƒ=kXi=1Î»ÌƒivâŠ—3i+EÌƒ(11)where{v1,v2,...,vk}isanorthonormalbasis,and,withoutlossofgenerality,Î»Ìƒ1|Î¸1,t|=maxiâˆˆ[k]Î»Ìƒi|Î¸i,t|>0.Also,defineÎ»Ìƒmin:=min{Î»Ìƒi:iâˆˆ[k],Î»Ìƒi>0},Î»Ìƒmax:=max{Î»Ìƒi:iâˆˆ[k]}.WefurtherassumetheerrorEÌƒisasymmetrictensorsuchthat,forsomeconstantp>1,kEÌƒ(I,u,u)kâ‰¤Ëœ,âˆ€uâˆˆSkâˆ’1;(12)kEÌƒ(I,u,u)kâ‰¤ËœTensorDecompositionsforLearningLatentVariableModelsUsingthetriangleinequalityandthefactkE(vi,Î¸t,Î¸t)kâ‰¤Ëœ,wehaveÎ¸ÌŒi,t+1â‰¥Î»ÌƒiÎ¸2i,tâˆ’Ëœâ‰¥|Î¸i,t|Â·Î»Ìƒi|Î¸i,t|âˆ’Ëœ/|Î¸i,t|(17)and|Î¸ÌŒi,t+1|â‰¤|Î»ÌƒiÎ¸2i,t|+Ëœâ‰¤|Î¸i,t|Â·Î»Ìƒi|Î¸i,t|+Ëœ/|Î¸i,t|(18)foralliâˆˆ[k].Combining(17)and(18)givesri,t+1=Î»Ìƒ1Î¸1,t+1Î»Ìƒi|Î¸i,t+1|=Î»Ìƒ1Î¸ÌŒ1,t+1Î»Ìƒi|Î¸ÌŒi,t+1|â‰¥r2i,tÂ·1âˆ’Î´t1+ËœÎ»ÌƒiÎ¸2i,t=r2i,tÂ·1âˆ’Î´t1+(Î»Ìƒi/Î»Ìƒ1)Î´tr2i,tâ‰¥r2i,tÂ·1âˆ’Î´t1+ÎºÎ´tr2i,t.Moreover,bythetriangleinequalityandHoÌˆlderâ€™sinequality,nXi=2[Î¸ÌŒi,t+1]21/2=nXi=2Î»ÌƒiÎ¸2i,t+E(vi,Î¸t,Î¸t)21/2â‰¤nXi=2Î»Ìƒ2iÎ¸4i,t1/2+nXi=2E(vi,Î¸t,Î¸t)21/2â‰¤maxi6=1Î»Ìƒi|Î¸i,t|nXi=2Î¸2i,t1/2+Ëœ=(1âˆ’Î¸21,t)1/2Â·maxi6=1Î»Ìƒi|Î¸i,t|+Ëœ/(1âˆ’Î¸21,t)1/2.(19)Combining(17)and(19)gives|Î¸1,t+1|(1âˆ’Î¸21,t+1)1/2=|Î¸ÌŒ1,t+1|Pni=2[Î¸ÌŒi,t+1]21/2â‰¥|Î¸1,t|(1âˆ’Î¸21,t)1/2Â·Î»Ìƒ1|Î¸1,t|âˆ’Ëœ/|Î¸1,t|maxi6=1Î»Ìƒi|Î¸i,t|+Ëœ/(1âˆ’Î¸21,t)1/2.IntermsofRt+1,Rt,Î³t,andÎ´t,thisreadsRt+1â‰¥1âˆ’Î´t(1âˆ’Î³t)1âˆ’Î¸21,tÎ¸21,t1/2+Î´t=RtÂ·1âˆ’Î´t1âˆ’Î³t+Î´tRt=1âˆ’Î´t1âˆ’Î³tRt+Î´tâ‰¥1âˆ’Î´tÎºR2t+Î´twherethelastinequalityfollowsfromPropositionB.1.LemmaB.2FixanyÏ>1.Assume0â‰¤Î´t<minn12(1+2ÎºÏ2),1âˆ’1/Ï1+ÎºÏoandÎ³t>2(1+2ÎºÏ2)Î´t.1.IfAnandkumar,Ge,Hsu,Kakade,andTelgarsky2.IfÏ2<r2i,t,thenri,t+1â‰¥min{r2i,t/Ï,1âˆ’Î´tâˆ’1/ÏÎºÎ´t}.3.Î³t+1â‰¥min{Î³t,1âˆ’1/Ï}.4.Ifmini6=1r2i,t>(Ï(1âˆ’Î´t)âˆ’1)/(ÎºÎ´t),thenRt+1>1âˆ’Î´tâˆ’1/ÏÎºÎ´tÂ·Î»ÌƒminÎ»Ìƒ1Â·1âˆšk.5.IfRtâ‰¤1+2ÎºÏ2,thenRt+1â‰¥Rt1+Î³t3,Î¸21,t+1â‰¥Î¸21,t,andÎ´t+1â‰¤Î´t.ProofConsidertwo(overlapping)casesdependingonr2i,t.â€¢Case1:r2i,tTensorDecompositionsforLearningLatentVariableModelsLemmaB.3Assume0â‰¤Î´t<1/2andÎ³t>0.PickanyÎ²>Î±>0suchthatÎ±(1+Î±)(1+Î±2)â‰¥ËœAnandkumar,Ge,Hsu,Kakade,andTelgarskyB.2.1ApproximateRecoveryofaSingleEigenvectorWenowstatethemainresultregardingtheapproximaterecoveryofasingleeigenvectorusingthetensorpowermethodonTÌƒ.Here,weexploitthespecialpropertiesoftheerrorEÌƒâ€”both(12)and(13).LemmaB.4ThereexistsauniversalconstantC>0suchthatthefollowingholds.Letiâˆ—:=argmaxiâˆˆ[k]Î»Ìƒi|Î¸i,0|.IfËœ<Î³02(1+8Îº)Â·Î»ÌƒminÂ·Î¸2iâˆ—,0andNâ‰¥CÂ·log(kÎº)Î³0+loglogpÎ»Ìƒiâˆ—Ëœ,thenaftertâ‰¥NiterationsofthetensorpowermethodontensorTÌƒasdefinedin(11)andsatisfying(12)and(13),thefinalvectorÎ¸tsatisfiesÎ¸iâˆ—,tâ‰¥s1âˆ’3ËœpÎ»Ìƒiâˆ—2,kÎ¸tâˆ’viâˆ—kâ‰¤4ËœpÎ»Ìƒiâˆ—,|TÌƒ(Î¸t,Î¸t,Î¸t)âˆ’Î»Ìƒiâˆ—|â‰¤27ÎºËœpÎ»iâˆ—2+2Ëœp.ProofAssumewithoutlossofgeneralitythatiâˆ—=1.Weconsiderthreephases:(i)iterationsbeforethefirsttimetsuchthatRt>1+2ÎºÏ2=1+8Îº(usingÏ:=2),(ii)thesubsequentiterationsbeforethefirsttimetsuchthatRtâ‰¥1/Î±(whereÎ±willbedefinedbelow),andfinally(iii)theremainingiterations.Webeginbyanalyzingthefirstphase,i.e.,theiteratesinT1:={tâ‰¥0:Rtâ‰¤1+2ÎºÏ2=1+8Îº}.ObservethattheconditiononËœimpliesÎ´0=ËœÎ»Ìƒ1Î¸21,0<Î³02(1+8Îº)Â·Î»ÌƒminÎ»Ìƒ1â‰¤minÎ³02(1+2ÎºÏ2),1âˆ’1/Ï2(1+2ÎºÏ2),andhencethepreconditionsonÎ´tandÎ³tofLemmaB.2holdfort=0.ForalltâˆˆT1satisfyingthepreconditions,LemmaB.2impliesthatÎ´t+1â‰¤Î´tandÎ³t+1â‰¥min{Î³t,1âˆ’1/Ï},sothenextiterationalsosatisfiesthepreconditions.Hencebyinduction,thepreconditionsholdforalliterationsinT1.Moreover,foralliâˆˆ[k],wehave|ri,0|â‰¥11âˆ’Î³0;andwhiletâˆˆT1:(i)|ri,t|increasesatalinearratewhiler2i,tâ‰¤2Ï2,and(ii)|ri,t|increasesataquadraticratewhileÏ2â‰¤r2i,tâ‰¤1âˆ’Î´tâˆ’1/ÏÎºÎ´t.(Thespecificratesaregiven,respectively,inLemmaB.2,claims1and2.)Since1âˆ’Î´tâˆ’1/ÏÎºÎ´tâ‰¤Î»Ìƒ12ÎºËœ,itfollowsthatmini6=1r2i,tâ‰¤1âˆ’Î´tâˆ’1/ÏÎºÎ´tforatmost2Î³0lnp2Ï211âˆ’Î³0+lnlnÎ»Ìƒ12ÎºËœlnâˆš2=O1Î³0+loglogÎ»Ìƒ1Ëœ(22)iterationsinT1.Assoonasmini6=1r2i,t>1âˆ’Î´tâˆ’1/ÏÎºÎ´t,wehaveTensorDecompositionsforLearningLatentVariableModelsandallthewhileRtisgrowingatalinearrate(giveninLemmaB.2,claim5).Therefore,thereareatmostanadditional1+3Î³0ln1+8Îº7/âˆšk=Olog(kÎº)Î³0(23)iterationsinT1overthatcountedin(22).Therefore,bycombiningthecountsin(22)and(23),wehavethatthenumberofiterationsinthefirstphasesatisfies|T1|=OloglogÎ»Ìƒ1Ëœ+log(kÎº)Î³0.Wenowanalyzethesecondphase,i.e.,theiteratesinT2:={tâ‰¥0:t/âˆˆT1,Rt<1/Î±}.DefineÎ±:=3ËœÎ»Ìƒ1,Î²:=11+2ÎºÏ2=11+8Îº.Notethatfortheinitialiterationt0:=minT2,wehavethatRt0â‰¥1+2ÎºÏ2=1+8ÎºAnandkumar,Ge,Hsu,Kakade,andTelgarskyItcanbecheckedthatÎ´t00âˆˆ(0,1/2),Î³t00â‰¥1âˆ’3ËœÎº/Î»1>0,Î±(1+Î±)(1+Î±2)â‰¥Ëœp(1âˆ’3ËœÎº/Î»1)Î»Ìƒ1â‰¥ËœpÎ³t00Î»Ìƒ1,Î±2(1+Î±)(1+Î²2)â‰¥ËœpÎ»Ìƒ1.Therefore,thepreconditionsofLemmaB.3aresatisfiedfortheinitialiterationt00inthisfinalphase,andbythesameargumentsasbefore,thepreconditionsholdforallsubsequentiterationstâ‰¥t00.Initially,wehaveRt00â‰¥1/Î±â‰¥1/Î²,andbyLemmaB.3,wehavethatRtincreasesataquadraticrateinthisfinalphaseuntilRtâ‰¥1/Î±.SothenumberofiterationsbeforeRtâ‰¥1/Î±canbeboundedaslnln(1/Î±)ln((1/Î²)/(2Îº))=lnlnpÎ»Ìƒ13ËœlnÎ»13ËœÂ·12Îºâ‰¤lnlnpÎ»Ìƒ13Ëœ=OloglogpÎ»Ìƒ1Ëœ.OnceRtâ‰¥1/Î±,wehaveÎ¸21,tâ‰¥1âˆ’3ËœpÎ»Ìƒ12.Sincesign(Î¸1,t)=r1,tâ‰¥r21,tâˆ’1Â·(1âˆ’Î´tâˆ’1)/(1+ÎºÎ´tâˆ’1r21,tâˆ’1)=(1âˆ’Î´tâˆ’1)/(1+ÎºÎ´tâˆ’1)>0byPropositionB.2,wehaveÎ¸1,t>0.ThereforewecanconcludethatkÎ¸tâˆ’v1k=q2(1âˆ’Î¸1,t)â‰¤s21âˆ’q1âˆ’(3Ëœ/(pÎ»Ìƒ1))2â‰¤4Ëœ/(pÎ»Ìƒ1).Finally,|TÌƒ(Î¸t,Î¸t,Î¸t)âˆ’Î»Ìƒ1|=Î»Ìƒ1(Î¸31,tâˆ’1)+kXi=2Î»ÌƒiÎ¸3i,t+EÌƒ(Î¸t,Î¸t,Î¸t)â‰¤Î»Ìƒ1|Î¸31,tâˆ’1|+kXi=2Î»Ìƒi|Î¸i,t|Î¸2i,t+kEÌƒ(I,Î¸t,Î¸t)kâ‰¤Î»Ìƒ11âˆ’Î¸1,t+|Î¸1,t(1âˆ’Î¸21,t)|+maxi6=1Î»Ìƒi|Î¸i,t|kXi=2Î¸2i,t+kEÌƒ(I,Î¸t,Î¸t)kâ‰¤Î»Ìƒ11âˆ’Î¸1,t+|Î¸1,t(1âˆ’Î¸21,t)|+maxi6=1Î»Ìƒiq1âˆ’Î¸21,tkXi=2Î¸2i,t+kEÌƒ(I,Î¸t,Î¸t)k=Î»Ìƒ11âˆ’Î¸1,t+|Î¸1,t(1âˆ’Î¸21,t)|+maxi6=1Î»Ìƒi(1âˆ’Î¸21,t)3/2+kEÌƒ(I,Î¸t,Î¸t)kâ‰¤Î»Ìƒ1Â·33ËœpÎ»Ìƒ12+ÎºÎ»Ìƒ1Â·3ËœpÎ»Ìƒ13+Ëœpâ‰¤(27ÎºÂ·(Ëœ/pÎ»ÌƒTensorDecompositionsforLearningLatentVariableModelsB.3DeflationLemmaB.5FixsomeËœâ‰¥0.Let{v1,v2,...,vk}beanorthonormalbasisforRk,andAnandkumar,Ge,Hsu,Kakade,andTelgarskyWenowexpressEi(I,u,u)intermsofthecoordinatesystemdefinedbyviandvÌ‚âŠ¥i,depictedbelow.Defineai:=u>viandbi:=u>vÌ‚âŠ¥i/kvÌ‚âŠ¥ik.(Notethatthepartofulivinginspan{vi,vÌ‚âŠ¥i}âŠ¥isirrelevantforanalyzingEi(I,u,u).)WehaveEi(I,u,u)TensorDecompositionsforLearningLatentVariableModelsTherefore,usingtheinequalityfrom(25)andagain(x+y)2â‰¤2(x2+y2),tXi=1Ei(I,u,u)22â‰¤2tXi=1A2i+2tXi=1|Bi|2â‰¤4(5+11Ëœ/Î»min)2Ëœ2tXi=1a2i+64(1+Ëœ/Î»min)2Ëœ2tXi=1(Ëœ/Î»i)2+28(1+Ëœ/Î»min)(Ëœ2/Î»min)tXi=1a2i+32(1+Ëœ/Î»min)ËœtXi=1(Ëœ/Î»i)32â‰¤4(5+11Ëœ/Î»min)2Ëœ2tXi=1a2i+64(1+Ëœ/Î»min)2Ëœ2tXi=1(Ëœ/Î»i)2+128(1+ËœAnandkumar,Ge,Hsu,Kakade,andTelgarsky(NotethattheconditiononLholdswithL=poly(k)log(1/Î·).)SupposethatAlgorithm1isiterativelycalledktimes,wheretheinputtensorisTÌ‚inthefirstcall,andineachsubsequentcall,theinputtensoristhedeflatedtensorreturnedbythepreviouscall.Let(vÌ‚1,Î»Ì‚1),(vÌ‚2,Î»Ì‚2),...,(vÌ‚k,Î»Ì‚k)bethesequenceofestimatedeigenvector/eigenvaluepairsre-turnedinthesekcalls.Withprobabilityatleast1âˆ’Î·,thereexistsapermutationÏ€on[k]suchthatkvÏ€(j)âˆ’vÌ‚jkâ‰¤8/Î»Ï€(j),|Î»Ï€(j)âˆ’Î»Ì‚j|â‰¤5,âˆ€jâˆˆ[k],andTâˆ’kXj=1Î»Ì‚jvÌ‚âŠ—3jâ‰¤55.ProofWeprovebyinductionthatforeachiâˆˆ[k](correspondingtothei-thcalltoAlgorithm1),withprobabilityatleast1âˆ’iÎ·/k,thereexistsapermutationÏ€on[k]suchthatthefollowingassertionshold.1.Foralljâ‰¤i,kvÏ€(j)âˆ’vÌ‚jkâ‰¤8/Î»Ï€(j)and|Î»Ï€(j)âˆ’Î»Ì‚j|â‰¤12.2.TheerrortensorEÌƒi+1:=TÌ‚âˆ’Xjâ‰¤iÎ»Ì‚jvÌ‚âŠ—3jâˆ’Xjâ‰¥i+1Î»Ï€(j)vâŠ—3Ï€(j)=E+Xjâ‰¤iÎ»Ï€(j)vâŠ—3Ï€(j)âˆ’Î»Ì‚jvÌ‚âŠ—3jsatisfieskEÌƒi+1(I,u,u)kâ‰¤56,âˆ€uâˆˆSkâˆ’1;(26)kEÌƒi+1(I,u,u)kâ‰¤2,âˆ€uâˆˆSkâˆ’1s.t.âˆƒjâ‰¥i+1(u>vÏ€(j))2â‰¥1âˆ’(168/Î»Ï€(j))2.(27)Weactuallytakei=0asthebasecase,sowecanignorethefirstassertion,andjustobservethatfori=0,EÌƒ1=TÌ‚âˆ’kXj=1Î»ivâŠ—3i=E.WehavekEÌƒ1k=kEk=,andthereforethesecondassertionholds.Nowfixsomeiâˆˆ[k],andassumeastheTensorDecompositionsforLearningLatentVariableModelsObservethatTÌƒiâˆ’Ti=EÌƒi,whichsatisfiesthesecondassertionintheinductivehypothesis.WemaywriteTi=Pkl=1Î»ÌƒlvâŠ—3Anandkumar,Ge,Hsu,Kakade,andTelgarskywhichinturnimpliesÎ»Ï€(j)|Î¸Ï€(j),N|â‰¤34Î»Ï€(jâˆ—)|Î¸Ï€(jâˆ—),N|,j6=jâˆ—.ThismeansthatÎ¸Nis(1/4)-separatedrelativetoÏ€(jâˆ—).Also,observethat|Î¸Ï€(jâˆ—),N|â‰¥45Â·Î»Ï€(jmax)Î»Ï€(jâˆ—)â‰¥45,Î»Ï€(jmax)Î»Ï€(jâˆ—)â‰¤54.ThereforebyLemmaB.4(usingËœ/p:=2,Î³:=1/4,andÎº:=5/4),executinganotherNpoweriterationsstartingfromÎ¸NgivesavectorÎ¸Ì‚thatsatisfieskÎ¸Ì‚âˆ’vÏ€(jâˆ—)kâ‰¤8Î»Ï€(jâˆ—),|Î»Ì‚âˆ’Î»Ï€(jâˆ—)|â‰¤5.SincevÌ‚i=Î¸Ì‚andÎ»Ì‚i=Î»Ì‚,thefirstassertionoftheinductivehypothesisissatisfied,aswecanmodifythepermutationÏ€byswappingÏ€(i)andÏ€(jâˆ—)withoutaffectingthevaluesof{Ï€(j):TensorDecompositionsforLearningLatentVariableModelsthereexistsapermutationÏ€suchthattwoassertionsholdfori=k,withprobabilityatleast1âˆ’Î·.Fromthelastinductionstep(i=k),itisalsoclearAnandkumar,Ge,Hsu,Kakade,andTelgarskyWeassumeEÌƒisasymmetrictensorsuchthat,forsomeconstantp>1,kEÌƒ(I,u,u)kâ‰¤Ëœ,âˆ€uâˆˆSkâˆ’1;kEÌƒ(I,u,u)kâ‰¤Ëœ/p,âˆ€uâˆˆSkâˆ’1s.t.(u>v1)2â‰¥1âˆ’(3Ëœ/Î»Ìƒ1)2;kEÌƒkFâ‰¤ËœF.AssumethatnotallÎ»Ìƒiarezero,anddefineÎ»Ìƒmin:=min{Î»Ìƒi:iâˆˆ[k],Î»Ìƒi>0},Î»Ìƒmax:=max{Î»Ìƒi:iâˆˆ[k]},`:=|{iâˆˆ[k]:Î»Ìƒi>0}|,Î»Ìƒavg:=1`kXi=1Î»Ìƒ2i1/2.WeTensorDecompositionsforLearningLatentVariableModelsMoreover,kTÌƒkFâ‰¥kXi=1Î»ÌƒivâŠ—3iFâˆ’kEÌƒkF=kXj=1kXi=1Î»Ìƒiviv>i(v>ivj)2F1/2âˆ’kEÌƒkF=kXj=1Î»Ìƒjvjv>j2F1/2âˆ’kEÌƒkF=kXj=1Î»Ìƒ2j1/2âˆ’Anandkumar,Ge,Hsu,Kakade,andTelgarskyinequality,andthefactkAkFâ‰¤âˆškkAkforanymatrixAâˆˆRkÃ—k,(1+Î±)|Ï†1|â‰¥(1+Î±)Î¸>MÌƒÎ¸â‰¥kMÌƒkF(32)â‰¥kXi=1Î»ÌƒiÎ¸iviv>iFâˆ’EÌƒ(I,I,Î¸)Fâ‰¥kXi=1Î»Ìƒ2iÎ¸2i1/2âˆ’âˆškkEÌƒ(I,I,Î¸)kâ‰¥kXi=1Î»Ìƒ2iÎ¸2i1/2âˆ’âˆškËœ.Combiningtheseboundson|Ï†1|givesÎ»Ìƒ1|Î¸1|+Ëœâ‰¥11+Î±"kXi=1Î»Ìƒ2iÎ¸2i1/2âˆ’âˆškËœ#.(33)TheassumptionËœâ‰¤Î±Î»Ìƒmin/âˆškimpliesthatâˆškËœâ‰¤Î±Î»Ìƒminâ‰¤Î±kXi=1Î»Ìƒ2iÎ¸2i1/2.Moreover,sinceÎ»Ìƒ1|Î¸TensorDecompositionsforLearningLatentVariableModelsMoreover,bythetriangleinequality,|Î¸>MÌƒÎ¸|â‰¤kXi=1|Ï†i|(u>iÎ¸)2â‰¤|Ï†1|(u>1Î¸)2+maxi6=1Anandkumar,Ge,Hsu,Kakade,andTelgarskyforÎ±âˆˆ(0,1/20).Moreover,byassumptionwehaveTÌƒ(Î¸,Î¸,Î¸)â‰¥0,andTÌƒ(Î¸,Î¸,Î¸)=kXi=1Î»ÌƒiÎ¸3i+EÌƒ(Î¸,Î¸,Î¸)=Î»Ìƒ1Î¸31+kXi=2Î»ÌƒiÎ¸3i+EÌƒ(Î¸,Î¸,Î¸)â‰¤Î»Ìƒ1Î¸31+maxi6=1Î»Ìƒi|Î¸i|kXi=2Î¸2i+Ëœâ‰¤Î»Ìƒ1Î¸31+âˆš7Î±Î»Ìƒ1|Î¸1|(1âˆ’Î¸21)+Ëœ(bythesecondclaim)â‰¤Î»Ìƒ1|Î¸1|3sign(Î¸1)+âˆš7Î±(1âˆ’2Î±)2âˆ’âˆš7Î±+Î±(1âˆ’2Î±)3(since|Î¸1|â‰¥1âˆ’2Î±)<Î»Ìƒ1|Î¸1|3sign(Î¸1)+1sosign(Î¸1)>âˆ’1,meaningÎ¸1>0.ThereforeÎ¸1=|Î¸1|â‰¥1âˆ’2Î±.Thisprovesthefinalclaim.LemmaC.2FixÎ±,Î²âˆˆ(0,1).AssumeÎ»Ìƒiâˆ—=maxiâˆˆ[k]Î»ÌƒiandËœâ‰¤minÎ±5âˆšk+7,1âˆ’Î²7Â·Î»Ìƒiâˆ—,ËœFâ‰¤âˆš`Â·1âˆ’Î²2Î²Â·Î»Ìƒiâˆ—.TotheconclusionofLemmaB.4,itcanbeaddedthatthestoppingcondition(31)issatisfiedbyÎ¸=Î¸t.ProofWithoutlossofgenerality,assumeiâˆ—=1.BythetriangleinequalityandCauchy-Schwarz,kTÌƒ(I,I,Î¸t)kFâ‰¤Î»Ìƒ1|Î¸1,t|+Xi6=1Î»i|Î¸i,t|+kEÌƒ(I,I,Î¸t)kFâ‰¤Î»Ìƒ1|Î¸1,t|+Î»Ìƒ1âˆškXi6=1Î¸2i,t1/2+âˆškËœâ‰¤Î»Ìƒ1|Î¸1,t|+3âˆškËœp+âˆškËœ.wherethelaststepusesthefactthatÎ¸21,tâ‰¥1âˆ’(3Ëœ/(pÎ»Ìƒ1))2.Moreover,TÌƒ(Î¸t,Î¸t,Î¸t)â‰¥Î»Ìƒ1âˆ’27ËœpÎ»12+2ËœTensorDecompositionsforLearningLatentVariableModelsUsingthedefinitionofthetensorFrobeniusnorm,wehave1âˆš`kTÌƒkFâ‰¤1âˆš`kXi=1Î»ÌƒivâŠ—3iF+1âˆšAnandkumar,Ge,Hsu,Kakade,andTelgarskyAppendixD.SimultaneousDiagonalizationforTensorDecompositionAsdiscussedintheintroduction,anotherstandardapproachtocertaintensordecompositionproblemsistosimultaneouslydiagonalizeacollectionofsimilarmatricesobtainedfromthegiventensor.Wenowexaminethisapproachinthecontextofourlatentvariablemodels,whereM2=kXi=1wiÂµiâŠ—ÂµiM3=kXi=1wiÂµiâŠ—ÂµiâŠ—Âµi.LetV:=[Âµ1|Âµ2|Â·Â·Â·|Âµk]andD(Î·):=diag(Âµ>1Î·,Âµ>2Î·,...,Âµ>kÎ·),soM2=Vdiag(w1,w2,...wk)V>M3(I,I,Î·)=Vdiag(w1,w2,...wk)D(Î·)V>Thus,theproblemofdeterminingtheÂµicanbecastasasimultaneousdiagonalizationproblem:findamatrixXsuchthatXTensorDecompositionsforLearningLatentVariableModelscolumnsofï£®ï£¯ï£¯ï£¯ï£°Âµ>1Î·(1)Âµ>2Î·(1)Â·Â·Â·Âµ>kÎ·(1)Âµ>1Î·(2)Âµ>2Î·(2)Â·Â·Â·Âµ>kÎ·(2)............Âµ>1Î·(m)Âµ>2Î·(m)Â·Â·Â·Âµ>kÎ·(m)ï£¹ï£ºï£ºï£ºï£»aredistinct(i.e.,foreachpairofcolumnindicesi,j,thereexistsarowindexrsuchthatthe(r,i)-thand(r,j)-thentriesaredistinct).Thisisamuchweakerrequirementforuniqueness,andthereforemaytranslatetoanimprovedperturbationanalysis.Infact,usingthetechniquesdiscussedinSection4.3,wemayevenreducetheproblemtoanorthogonalsimultaneousdiagonalization,whichmaybeeasiertoobtain.Furthermore,anumberofrobustnumericalmethodsfor(approximately)simultaneouslydiagonalizingcollectionsofmatriceshavebeenproposedandusedsuccessfullyintheAnandkumar,Ge,Hsu,Kakade,andTelgarskyS.Arora,R.Ge,A.Moitra,andS.Sachdeva.ProvableICAwithunknownGaussiannoise,andimplicationsforGaussianmixturesandautoencoders.InAdvancesinNeuralInformationProcessingSystems25,2012b.T.Austin.Onexchangeablerandomvariablesandthestatisticsoflargegraphsandhyper-graphs.Probab.Survey,5:80â€“145,2008.R.Bailly.Quadraticweightedautomata:Spectralalgorithmandlikelihoodmaximization.JournalofMachineLearningResearch,2011.B.BalleandM.Mohri.Spectrallearningofgeneralweightedautomataviaconstrainedmatrixcompletion.InAdvancesinNeuralInformationProcessingSystems25,2012.B.Balle,A.Quattoni,andX.Carreras.Locallossoptimizationinoperatormodels:Anewinsightintospectrallearning.InTwenty-NinthInternationalConferenceonMachineLearning,2012.M.BelkinandK.Sinha.Polynomiallearningofdistributionfamilies.InFifty-FirstAnnualIEEESymposiumonFoundationsofComputerScience,pages103â€“112,2010.A.Bhaskara,M.Charikar,A.Moitra,andA.Vijayaraghavan.Smoothedanalysisoften-sordecompositions.InProceedingsofthe46thAnnualACMSymposiumonTheoryofComputing,2014.B.Boots,S.M.Siddiqi,andG.J.Gordon.Closingthelearning-planningloopwithpredic-tivestaterepresentations.InProceedingsoftheRoboticsScienceandSystemsConference,2010.S.C.BrubakerandS.Vempala.IsotropicPCAandaffine-invariantclustering.InForty-NinthAnnualIEEESymposiumonFoundationsofComputerScience,2008.A.Bunse-Gerstner,R.Byers,andV.Mehrmann.Numericalmethodsforsimultaneousdiagonalization.SIAMJournalonMatrixAnalysisandApplications,14(4):927â€“949,1993.J.-F.Cardoso.Super-symmetricdecompositionofthefourth-ordercumulanttensor.blindidentificationofmoresourcesthansensors.InAcoustics,Speech,andSignalProcessing,1991.ICASSP-91.,1991InternationalConferenceon,pages3109â€“3112.IEEE,1991.J.-F.Cardoso.Perturbationofjointdiagonalizers.TechnicalReport94D027,SignalDe-partment,TeÌleÌcomParis,1994.J.-F.CardosoandP.Comon.Independentcomponentanalysis,asurveyofsomealgebraicmethods.InIEEEInternationalSymposiumonCircuitsandSystems,pagesTensorDecompositionsforLearningLatentVariableModelsR.B.Cattell.Parallelproportionalprofilesandotherprinciplesfordeterminingthechoiceoffactorsbyrotation.Psychometrika,9(4):267â€“283,1944.J.T.Chang.FullreconstructionofMarkovmodelsonevolutionarytrees:IdentifiabilityandAnandkumar,Ge,Hsu,Kakade,andTelgarskyA.T.Erdogan.OntheconvergenceofICAalgorithmswithsymmetricorthogonalization.IEEETransactionsonSignalProcessing,57:2209â€“2221,2009.A.M.Frieze,M.Jerrum,andR.Kannan.Learninglineartransformations.InThirty-SeventhAnnualTensorDecompositionsforLearningLatentVariableModelsE.KofidisandP.A.Regalia.Onthebestrank-1approximationofhigher-ordersuper-symmetrictensors.SIAMJournalonMatrixAnalysisandApplications,23(3):863â€“884,2002.T.G.KoldaandB.W.Bader.Tensordecompositionsandapplications.SIAMreview,51(3):455,2009.T.G.KoldaandJ.R.Mayo.Shiftedpowermethodforcomputingtensoreigenpairs.SIAMJournalonMatrixAnalysisandApplications,32(4):1095â€“1124,October2011.J.B.Kruskal.Three-wayarrays:rankanduniquenessoftrilineardecompositions,withapplicationtoarithmeticcomplexityandstatistics.LinearAlgebraandAppl.,18(2):95â€“138,1977.L.D.Lathauwer,B.D.Moor,andJ.Vandewalle.Onthebestrank-1andrank-(R1,R2,...,Rn)approximationandapplicationsofhigher-ordertensors.SIAMJ.MatrixAnal.Anandkumar,Ge,Hsu,Kakade,andTelgarskyL.PachterandB.Sturmfels.AlgebraicStatisticsforComputationalBiology,volume13.CambridgeUniversityPress,2005.A.Parikh,L.Song,andE.P.Xing.Aspectralalgorithmforlatenttreegraphicalmodels.InTwenty-EighthInternationalConferenceonMachineLearning,2011.K.Pearson.Contributionstothemathematicaltheoryofevolution.PhilosophicalTrans-actionsoftheRoyalSociety,London,A.,page71,1894.L.Qi.Eigenvaluesofarealsupersymmetrictensor.JournalofSymbolicComputation,40(6):1302â€“1324,2005.R.A.Rednerand