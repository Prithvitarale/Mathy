JournalofMachineLearningResearch15(2014)2773-2832Submitted2/13;Revised3/14;Published8/14TensorDecompositionsforLearningLatentVariableModelsAnimashreeAnandkumara.anandkumar@uci.eduElectricalEngineeringandComputerScienceUniversityofCalifornia,Irvine2200EngineeringHallIrvine,CA92697RongAnandkumar,Ge,Hsu,Kakade,andTelgarskyKeywords:latentvariablemodels,tensordecompositions,mixturemodels,topicmodels,methodofmoments,powermethod1.IntroductionThemethodofmomentsisaclassicalparameterestimationtechnique(Pearson,1894)fromstatisticswhichhasprovedinvaluableinanumberofapplicationdomains.Thebasicparadigmissimpleandintuitive:(i)computecertainstatisticsofthedata—oftenempiricalmomentssuchasmeansandcorrelations—and(ii)findmodelparametersthatgiveriseto(nearly)thesamecorrespondingpopulationquantities.InanumberofTensorDecompositionsforLearningLatentVariableModelsOneapproachforobtainingtheorthogonaldecompositionisthetensorpowermethodofLathauweretal.(2000,Remark3).Weprovideaconvergenceanalysisofthismethodfororthogonallydecomposablesymmetrictensors,aswellasadetailedperturbationanalysisforarobust(andacomputationallytractable)variant(Theorem5.1).ThisperturbationanalysiscanbeviewedasananalogueofWedin’sperturbationtheoremforsingularvectorsofmatrices(Wedin,1972),providingaboundontheerroroftherecovereddecompositionintermsoftheoperatornormofthetensorperturbation.Thisanalysisissubtleinatleasttwoways.First,unlikeformatrices(whereeverymatrixhasasingularvaluedecomposi-tion),anorthogonaldecompositionneednotexistfortheperturbedtensor.Ourrobustvariantusesrandomrestartsanddeflationtoextractanapproximatedecompositioninacomputationallytractablemanner.Second,theanalysisofthedeflationstepsisnon-trivial;anaı̈veargumentwouldentailerroraccumulationineachdeflationstep,whichweshowcaninfactbeavoided.Whenthismethodisappliedforparameterestimationinlatentvariablemodelspreviouslydiscussed,improvedsamplecomplexitybounds(overpreviouswork)canbeobtainedusingthisperturbationanalysis.Finally,wealsoaddresscomputationalissuesthatarisewhenapplyingthetensorde-compositionapproachestoestimatinglatentvariablemodels.Specifically,weAnandkumar,Ge,Hsu,Kakade,andTelgarskycorrespondtolocalextremaoftheso-called“contrastfunctions”whichdistinguishGaussianvariablesfromnon-Gaussianvariables.Thismethodcanbeeffectivelyimplementedusingfastdescentalgorithms(Hyvarinen,1999).Whenusingtheexcesskurtosis(i.e.,TensorDecompositionsforLearningLatentVariableModelsRegalia,2002)—thespecialsymmetricorthogonalstructureweconsiderpermitssimplealgorithmstoefficientlyandstablyrecoverthedesireddecomposition.Inparticular,agen-eralizationofthematrixpowermethodtosymmetrictensors,introducedbyLathauweretal.(2000,Remark3)andanalyzedbyKofidisandRegalia(2002),providessuchade-composition.ThisisinfactimpliedbythecharacterizationofZhangandGolub(2001),whichshowsthatiterativelyobtainingthebestrank-1approximationofsuchorthogonallydecomposabletensorsalsoyieldstheexactdecomposition.Wenotethatingeneral,ob-tainingsuchapproximationsforgeneral(symmetric)tensorsisNP-hard(HillarandLim,2013).1.2.2LatentVariableModelsThisworkfocusesontheparticularapplicationoftensordecompositionmethodstoestimat-inglatentvariablemodels,asignificantdeparturefrommanypreviousapproachesinthemachinelearningandstatisticsliterature.ByfarthemostpopularheuristicforparameterestimationforsuchmodelsistheExpectation-Maximization(EM)algorithm(Dempsteretal.,1977;RednerandWalker,1984).AlthoughEMhasanumberofmerits,itmaysufferfromslowconvergenceandpoorqualitylocaloptima(RednerandWalker,1984),requir-ingpractitionerstoemploymanyadditionalheuristicstoobtaingoodsolutions.Forsomemodelssuchaslatenttrees(Roch,2006)andtopicmodels(Aroraetal.,2012a),maximumlikelihoodestimationisNP-hard,whichsuggeststhatotherestimationapproachesmaybemoreattractive.Morerecently,algorithmsfromtheoreticalcomputerscienceandmachinelearninghaveaddressedcomputationalandsamplecomplexityissuesrelatedtoestimatingcertainlatentvariablemodelssuchasGaussianmixturemodelsandHMMs(Dasgupta,1999;AroraandKannan,2005;DasguptaandSchulman,2007;VempalaandWang,2004;Kannanetal.,2008;AchlioptasandMcSherry,2005;ChaudhuriandRao,2008;BrubakerandVempala,2008;Kalaietal.,2010;BelkinandSinha,2010;MoitraandValiant,2010;HsuandKakade,2013;Chang,1996;MosselandRoch,2006;Hsuetal.,2012b;Anandkumaretal.,2012c;Aroraetal.,2012a;Anandkumaretal.,2012a).SeetheworksbyAnandku-maretal.(2012c)andHsuandKakade(2013)foradiscussionofthesemethods,togetherwiththecomputationalandstatisticalhardnessbarriersthattheyface.Thepresentworkreviewsabroadrangeoflatentvariableswhereamildnon-degeneracyconditionimpliesthesymmetricorthogonaldecompositionstructureinthetensorsoflow-orderobservablemoments.Notably,anotherclassofmethods,basedonsubspaceidentification(OverscheeandMoor,1996)andobservableoperatormodels/multiplicityautomata(Schützenberger,1961;Jaeger,2000;Littmanetal.,2001),havebeenproposedforanumberoflatentvariablemodels.ThesemethodsweresuccessfullydevelopedforHMMsbyHsuetal.(2012b),andsubsequentlygeneralizedandextendedforanumberofrelatedsequentialandtreeMarkovmodelsmodels(Siddiqietal.,2010;Bailly,2011;Bootsetal.,2010;Parikhetal.,2011;Roduetal.,2013;Balleetal.,2012;BalleandMohri,2012),aswellascertainclassesofparsetreemodels(Luqueetal.,2012;Cohenetal.,2012;Dhillonetal.,2012).Thesemethodsuselow-ordermomentstolearnan“operator”representationofthedistribution,whichcanbeusedfordensityestimationandbeliefstateupdates.Whilefinitesampleboundscanbegiventoestablishthelearnabilityofthesemodels(Hsuetal.,2012b),Anandkumar,Ge,Hsu,Kakade,andTelgarskyactuallygiveparameterestimates(e.g.,oftheemissionortransitionmatricesinthecaseofHMMs).1.3OrganizationTherestofthepaperisorganizedasfollows.Section2reviewssomebasicdefinitionsoftensors.Section3providesexamplesofanumberoflatentvariablemodelswhich,afterappropriatemanipulationsoftheirlowordermoments,shareacertainnaturaltensorstruc-ture.Section4reducestheproblemofparameterestimationtothatofextractingacertain(symmetricorthogonal)decompositionofatensor.WethenprovideadetailedanalysisofarobusttensorpowermethodandestablishananalogueofWedin’sperturbationtheoremforthesingularvectorsofmatrices.ThediscussioninSection6addressesanumberofpracticalconcernsthatarisewhendealingwithmomentmatricesTensorDecompositionsforLearningLatentVariableModelsTherankofap-thordertensorA∈NpRnisthesmallestnon-negativeintegerksuchthatA=Pkj=1u1,j⊗u2,j⊗···⊗up,jforsomeui,j∈Rn,i∈[p],j∈[k],andthesymmetricrankofasymmetricp-thordertensorAisthesmallestnon-negativeintegerksuchthatA=Pkj=1u⊗pjforsomeuj∈Rn,j∈[k].1Thenotionofrankreadilyreducestotheusualdefinitionofmatrixrankwhenp=2,asrevealedbythesingularvaluedecomposition.Similarly,forsymmetricmatrices,thesymmetricrankisequivalenttothematrixrankasgivenbythespectraltheorem.Adecompositionintosuchrank-onetermsisknownasacanonicalpolyadicdecomposition(Hitchcock,1927a,b).Thenotionoftensor(symmetric)rankisconsiderablymoredelicatethanmatrix(sym-metric)rank.Forinstance,itisnotclearapriorithatthesymmetricrankofatensorshouldevenbefinite(Comonetal.,2008).Inaddition,removalofthebestrank-1approx-imationofa(general)tensormayincreasethetensorrankoftheresidual(StegemanandComon,2010).Throughout,weusekvk=(Piv2i)1/2todenotetheEuclideannormofavectorv,andkMktodenotethespectral(operator)normofamatrix.WealsousekTktodenotetheoperatornormofatensor,whichwedefinelater.3.TensorStructureinLatentVariableModelsInthissection,wegiveseveralexamplesoflatentvariablemodelswhoselow-ordermomentscanbewrittenassymmetrictensorsoflowsymmetricrank;someoftheseexamplescanbededucedusingthetechniquesdevelopedinthetextbyMcCullagh(1987).ThebasicformisdemonstratedinTheorem3.1forthefirstexample,andthegeneralpatternwillemergefromsubsequentexamples.3.1ExchangeableSingleTopicModelsWefirstconsiderasimplebag-of-wordsmodelfordocumentsinwhichthewordsinthedocumentareassumedtobeexchangeable.Recallthatacollectionofrandomvariablesx1,x2,...,x`areexchangeableiftheirjointprobabilitydistributionisinvarianttopermu-tationoftheindices.Thewell-knownDeFinetti’stheorem(Austin,2008)impliesthatsuchexchangeablemodelscanbeviewedasmixturemodelsinwhichthereisalatentvariablehsuchthatx1,x2,...,x`areconditionallyi.i.d.givenh(seeFigure1(a)forthecorrespondinggraphicalmodel)andtheconditionaldistributionsareidenticalatallthenodes.Inoursimplifiedtopicmodelfordocuments,thelatentvariablehisinterpretedasthe(sole)topicofagivendocument,anditisassumedtotakeonlyafinitenumberofdistinctvalues.Letkbethenumberofdistincttopicsinthecorpus,dbethenumberofdistinctwordsinthevocabulary,and`≥3bethenumberofwordsineachdocument.Thegenerativeprocessforadocumentisasfollows:thedocument’stopicisdrawnaccordingtotheAnandkumar,Ge,Hsu,Kakade,andTelgarskyGiventhetopich,thedocument’s`wordsaredrawnindependentlyaccordingtothedis-cretedistributionspecifiedbytheprobabilityvectorµh∈∆d−1.Itwillbeconvenienttorepresentthe`wordsinthedocumentbyd-dimensionalrandomvectorsx1,x2,...,x`∈Rd.Specifically,wesetxt=eiifandonlyifthet-thwordinthedocumentisi,t∈[`],wheree1,e2,...edisthestandardcoordinatebasisforRd.Oneadvantageofthisencodingofwordsisthatthe(cross)momentsoftheserandomvectorscorrespondtojointprobabilitiesoverwords.Forinstance,observethatE[x1⊗x2]=X1≤i,j≤dPr[x1=ei,x2=ej]ei⊗ej=X1≤i,j≤dPr[1stword=i,2ndword=j]ei⊗ej,sothe(i,j)-theentryofthematrixE[x1⊗x2]isPr[1stword=i,2ndword=j].Moregenerally,the(i1,i2,...,i`)-thentryinthetensorE[x1TensorDecompositionsforLearningLatentVariableModelsAswewillseeinSection4.3,thestructureofM2andM3revealedinTheorem3.1impliesthatthetopicvectorsµ1,µ2,...Anandkumar,Ge,Hsu,Kakade,andTelgarsky3.2.2SphericalGaussianMixtures:DifferingCovariancesThegeneralcaseiswhereeachcomponentmayhaveadifferentsphericalcovariance.Anobservationinthismodelisagainx=µh+z,butnowz∈Rdisarandomvectorwhoseconditionaldistributiongivenh=i(forsomei∈[k])isamultivariateGaussianN(0,σ2iI)withzeromeanandsphericalcovarianceσ2iI.Theorem3.3(HsuandKakade,2013)Assumed≥k.Theaveragevarianceσ̄2:=Pki=1wiσ2iisthesmallesteigenvalueofthecovariancematrixE[x⊗x]−E[x]⊗E[x].Letvbeanyunitnormeigenvectorcorrespondingtotheeigenvalueσ̄2TensorDecompositionsforLearningLatentVariableModels(i.e.,Tisthefourthderivativetensorofthefunctionv7→8−1E[(v>x)2]2,soM4isthefourthcumulanttensor).LetκAnandkumar,Ge,Hsu,Kakade,andTelgarskyhx1x2···x`(a)Multi-viewmodelsh1h2···h`x1x2x`(b)HiddenMarkovmodelFigure1:Examplesoflatentvariablemodels.thecasewhereα0issmall(e.g.,aconstantindependentofk),whereuponhtypicallyhasonlyafewlargeentries.Thiscorrespondstothesettingwherethedocumentsaremainlycomprisedofjustafewtopics.Theorem3.5(Anandkumaretal.,2012a)DefineM1:=E[x1]M2:=E[x1⊗x2]−α0α0+1M1⊗M1M3:=E[x1⊗x2⊗x3]−α0α0+2E[x1⊗x2⊗M1]+E[x1⊗M1⊗x2]+E[M1⊗x1⊗x2]+2α20(α0+2)(α0+1)M1⊗M1⊗M1.ThenM2=kXi=1αi(α0+1)α0µi⊗µiM3=kXi=12αi(α0+2)(α0+1)α0µi⊗µi⊗µi.Notethatα0needstobeknowntoformM2andM3fromtherawmoments.This,however,isamuchweakerthanassumingthattheentiredistributionofhisknown(i.e.,knowledgeofthewholeparametervectorα).3.3Multi-ViewModelsMulti-viewmodels(alsosometimescallednaı̈veBayesmodels)areaspecialclassofBayesiannetworksinwhichobservedvariablesx1,x2,...,x`areconditionallyindependentgivenalatentvariableh.Thisissimilartotheexchangeablesingletopicmodel,butherewedonotrequiretheconditionaldistributionsofthext,t∈[`]tobeidentical.TechniquesdevelopedforthisclasscanbeusedtohandleanumberofwidelyusedmodelsincludingTensorDecompositionsforLearningLatentVariableModelsAsbefore,weleth∈[k]beadiscreterandomvariablewithPr[h=j]=wjforallj∈[k].Nowconsiderrandomvectorsx1∈RdAnandkumar,Ge,Hsu,Kakade,andTelgarsky3.3.1MixturesofAxis-AlignedGaussiansandOtherProductDistributionsThefirstexampleisamixtureofkproductdistributionsinRnunderamildincoherenceas-sumption(Anandkumaretal.,2012c).Here,wealloweachofthekcomponentdistributionstohaveadifferentproductdistribution(e.g.,Gaussiandistributionwithanaxis-alignedco-variancematrix),butrequirethematrixofcomponentmeansA:=[µ1|µ2|···|µk]∈Rn×ktosatisfyacertain(verymild)incoherencecondition.Theroleoftheincoherenceconditionisexplainedbelow.Foramixtureofproductdistributions,anypartitioningofthedimensions[n]intothreegroupscreatesthree(possiblyasymmetric)“views”whichareconditionallyindependentoncethemixturecomponentisselected.However,recallthatTheorem3.6requiresthatforeachview,thekconditionalmeansbelinearlyindependent.Ingeneral,thismaynotbeachievable;consider,forinstance,thecaseµi=eiforeachi∈[k].Suchcases,wherethecomponentmeansareveryalignedwiththecoordinatebasis,areprecludedbytheincoherencecondition.Definecoherence(A):=maxi∈[n]{e>iΠAei}tobethelargestdiagonalentryoftheorthog-onalprojectortotherangeofA,andassumeAhasrankk.Thecoherenceliesbetweenk/nand1;itislargestwhentherangeofAisspannedbythecoordinateaxes,anditisk/nwhentherangeisspannedbyasubsetoftheHadamardbasisofcardinalityk.Theincoherenceconditionrequires,forsomeε,δ∈(0,1),coherence(A)≤(ε2/6)/ln(3k/δ).Essentially,thisconditionensuresthatthenon-degeneracyofthecomponentmeansisnotisolatedinjustafewofthendimensions.Operationally,itimpliesthefollowing.Proposition3.2(Anandkumaretal.,2012c)AssumeAhasrankk,andcoherence(A)≤ε2/6ln(3k/δ)forsomeε,δ∈(0,1).Withprobabilityatleast1−δ,arandompartitioningofthedimensions[n]intothreegroups(foreachi∈[n],independentlypickt∈{1,2,3}uniformlyatrandomandputiingroupt)hasthefollowingproperty.Foreacht∈{1,2,3}andj∈[k],letµt,jbetheentriesofµjputintogroupt,andletAt:=[µt,1|µt,2|···|µt,k].Thenforeacht∈{1,2,3},Athasfullcolumnrank,andthek-thlargestsingularvalueofAtisatleastp(1−ε)/3timesthatofA.Therefore,threeasymmetricviewscanbecreatedbyrandomlypartitioningtheobservedrandomvectorxintox1,x2,andx3,suchthattheresultingcomponentmeansforeachviewsatisfytheconditionsofTheorem3.6.3.3.2SphericalGaussianMixtures,RevisitedConsideragainthecaseofsphericalGaussianmixtures(cf.Section3.2).AsweshallseeinSection4.3,theprevioustechniques(basedonTheorem3.2andTheorem3.3)leadtoestimationprocedureswhenthedimensionofxiskorgreater(andwhenthekcomponentmeansarelinearlyindependent).WenowTensorDecompositionsforLearningLatentVariableModelsWeagainusearandomizedreduction.Specifically,wecreatethreeviewsby(i)applyingarandomrotationtox,andthen(ii)partitioningx∈Rnintothreeviewsx̃1,Anandkumar,Ge,Hsu,Kakade,andTelgarskyProposition3.4(Anandkumaretal.,2012c)Defineh:=y2,wherey2isthesecondhiddenstateintheMarkovchain.Then•x1,x2,x3areconditionallyindependentgivenh;•thedistributionofhisgivenbythevectorw:=Tπ∈∆k−1;•forallj∈[k],E[x1|h=j]=Odiag(π)T>diag(w)−1ejE[x2|h=j]=OejE[x3|h=j]=OTej.Notethematrixofconditionalmeansofxthasfullcolumnrank,foreacht∈{1,2,3},providedthat:(i)Ohasfullcolumnrank,(ii)Tisinvertible,and(iii)πandTπhavepositiveentries.4.OrthogonalTensorDecompositionsWenowshowhowrecoveringtheµi’sinouraforementionedproblemsreducestotheprob-lemoffindingacertainorthogonaltensordecompositionofasymmetrictensor.Westartbyreviewingthespectraldecompositionofsymmetricmatrices,andthendiscussageneraliza-tiontothehigher-ordertensorcase.Finally,weshowhoworthogonaltensordecompositionscanbeusedforestimatingthelatentvariablemodelsfromtheprevioussection.4.1Review:TheMatrixCaseWefirstbuildintuitionbyreviewingthematrixsetting,wherethedesireddecomposi-tionistheeigendecompositionofasymmetricrank-kmatrixM=VΛV>,whereV=[v1|v2|···|vk]∈Rn×kisthematrixwithorthonormaleigenvectorsascolumns,andΛ=diag(λ1,λ2,...,λk)∈Rk×kisdiagonalmatrixofnon-zeroeigenvalues.Inotherwords,M=kXi=1λiviv>i=kXTensorDecompositionsforLearningLatentVariableModelsu1andu2spanningthesamesubspaceasv1andv2.Nevertheless,thedecompositionisuniquewhenλ1,λ2,...,λAnandkumar,Ge,Hsu,Kakade,andTelgarsky4.2.1Fixed-PointCharacterizationForatensorT,considerthevector-valuedmapu7→T(I,u,u)(5)whichisthethird-ordergeneralizationof(2).ThiscanbeexplicitlywrittenasTTensorDecompositionsforLearningLatentVariableModelsAswenowsee,theseadditionaleigenvectorscanbeviewedasspurious.WesayaunitvectoruisarobusteigenvectorofTifthereexistsan>0suchthatAnandkumar,Ge,Hsu,Kakade,andTelgarskyTheproofofTheorem4.2isgiveninAppendixA.2.ItissimilartolocaloptimalityanalysisforICAmethodsusingfourth-ordercumulants(e.g.,DelfosseandLoubaton,1995;Friezeetal.,1996).Again,weseesimilardistinctionstothematrixcase.Inthematrixcase,theonlylocalmaximizersoftheRayleighquotientaretheeigenvectorswiththelargesteigenvalue(andthesemaximizerstakeonthegloballyoptimalvalue).Forthecaseoforthogonaltensorforms,therobusteigenvectorsarepreciselytheisolatedlocalmaximizers.Animportantimplicationofthetwocharacterizationsisthat,fororthogonallydecom-posabletensorsT,(i)thelocalmaximizersoftheobjectivefunctionu7→T(u,u,u)/(u>u)3/2correspondpreciselytothevectorsviinthedecomposition,and(ii)theselocalmaximizerscanbereliablyidentifiedusingasimplefixed-pointiteration(i.e.,thetensoranalogueofthematrixpowermethod).Moreover,asecond-derivativetestbasedonT(I,I,u)canbeemployedtotestforlocaloptimalityandruleTensorDecompositionsforLearningLatentVariableModels4.3.1TheReductionFirst,letW∈Rd×kbealineartransformationsuchthatM2(W,W)=W>M2W=IwhereIisthekAnandkumar,Ge,Hsu,Kakade,andTelgarsky4.3.2LocalMaximizersof(CrossMoment)SkewnessThevariationalcharacterizationprovidesaninterestingperspectiveontherobusteigen-vectorsfortheselatentvariablemodels.Considertheexchangeablesingletopicmodels(Theorem3.1),andtheobjectivefunctionu7→E[(x>1u)(x>2u)(x>3u)]E[(x>1u)(x>2u)]3/2=M3(u,u,u)M2(u,u)3/2.Inthiscase,everylocalmaximizeru∗satisfiesM2(I,u∗)=√wiµiforsomei∈[k].Theobjectivefunctioncanbeinterpretedasthe(crossmoment)skewnessoftherandomvectorsx1,x2,x3alongdirectionu.5.TensorPowerMethodInthissection,weconsiderthetensorpowermethodofLathauweretal.(2000,Remark3)fororthogonaltensordecomposition.WefirststateasimpleconvergenceanalysisforanorthogonallydecomposabletensorT.WhenonlyanapproximationT̂toanorthogonallydecomposabletensorTisavailable(e.g.,whenempiricalmomentsareusedtoestimatepopulationmoments),anorthogonaldecompositionneednotexistforthisperturbedtensor(unlikeforthecaseofmatrices),andamorerobustapproachisrequiredtoextracttheapproximatedecomposition.Here,weproposesuchavariantinAlgorithm1andprovideadetailedperturbationanalysis.Wenotethatalternativeapproachessuchassimultaneousdiagonalizationcanalsobeemployed(seeAppendixD).5.1ConvergenceAnalysisforOrthogonallyDecomposableTensorsThefollowinglemmaestablishesthequadraticconvergenceofthetensorpowermethod—i.e.,repeatediterationof(6)—forextractingasinglecomponentoftheorthogonaldecom-position.Notethattheinitialvectorθ0determineswhichrobusteigenvectorwillbetheconvergentpoint.Computationofsubsequenteigenvectorscanbecomputedwithdeflation,i.e.,bysubtractingappropriatetermsfromT.Lemma5.1LetT∈N3Rnhaveanorthogonaldecompositionasgivenin(4).Foravectorθ0∈Rn,supposethatthesetofnumbers|λ1v>1θ0|,|λ2v>2θ0|,...,|λkv>kθ0|hasauniquelargestelement.Withoutlossofgenerality,say|λ1v>1θ0|isthislargestvalueand|λ2v>2θ0|isthesecondlargestvalue.Fort=1,2,...,letθt:=T(I,θt−1,θt−1)kT(I,θt−1,θt−1)k.Thenkv1−θtk2≤2λ21kXi=2λ−2i·λ2v>2θ0λ1v>1θ02t+1.Thatis,repeatedTensorDecompositionsforLearningLatentVariableModelsToobtainalleigenvectors,wemaysimplyproceediterativelyusingdeflation,executingthepowermethodonT−Pjλjv⊗3jafterhavingobtainedrobusteigenvector/eigenvaluepairsAnandkumar,Ge,Hsu,Kakade,andTelgarskyAssumethatthesymmetrictensorT∈Rk×k×kisorthogonallydecomposable,andthatT̂=T+E,wheretheperturbationE∈Rk×k×kisasymmetrictensorwithsmalloperatornorm:kEk:=supkθk=1|E(θ,θ,θ)|.Inourlatentvariablemodelapplications,T̂isthetensorformedbyusingempiricalmo-ments,whileTistheorthogonallydecomposabletensorderivedfromthepopulationmo-mentsforthegivenmodel.Inthecontextofparameterestimation(asinSection4.3),Emustaccountforanyerroramplificationthroughoutthereduction,suchasinthewhiteningstep(see,e.g.,HsuandKakade,2013,forsuchananalysis).ThefollowingtheoremissimilartoWedin’sperturbationtheoremforsingularvectorsofmatrices(Wedin,1972)inthatitboundstheerrorofthe(approximate)decompositionreturnedbyAlgorithm1oninputT̂intermsofthesizeoftheperturbation,providedthattheperturbationissmallenough.Theorem5.1LetT̂=T+E∈Rk×k×k,whereTTensorDecompositionsforLearningLatentVariableModelsorthogonaldecomposition).Furthermore,notethatAlgorithm1usesmultiplerestartstoensure(approximate)convergence—theintuitionisthatbyrestartingatmultiplepoints,weeventuallystartatapointinwhichtheinitialcontractionAnandkumar,Ge,Hsu,Kakade,andTelgarskyInpractice,oneshoulduseallofthewordsinadocumentforefficientestimationofthemoments.Onewaytodothisistoaverageoverall`3·3!orderedtriplesofwordsinadocumentoflength`.Atfirstblush,thisseemscomputationallyexpensive(when`islarge),butasitturnsout,theaveragingcanbedoneimplicitly,asshownbyZouetal.(2013).Letc∈RdbethewordTensorDecompositionsforLearningLatentVariableModelsisapproximatelytherangeofM2.Fromhere,anapproximateSVDofU>M2UisusedtocomputetheapproximatewhiteningmatrixW.NotethatbothmatrixAnandkumar,Ge,Hsu,Kakade,andTelgarskyalgorithmsduetoacertainamplificationoftheerror(apolynomialfactorofk,whichisobservedinpractice).Usingtheperturbationanalysisforthetensorpowermethod,improvedsamplecomplex-ityboundscanbeobtainedforalloftheexamplesdiscussedinSection3.Theunderlyinganalysisremainsthesameasinpreviousworks(e.g.,Anandkumaretal.,2012a;HsuandKakade,2013),themaindifferencebeingtheaccuracyoftheorthogonaltensordecompo-sitionobtainedviathetensorpowermethod.Relativetothepreviouslycitedworks,thesamplecomplexityboundwillbeconsiderablyimprovedinitsdependenceontherankpa-rameterk,asTheorem5.1impliesthatthetensorestimationerror(e.g.,errorinestimatingfM3fromSection4.3)isnotTensorDecompositionsforLearningLatentVariableModels1.Thesetofθ∈Rnwhichdonotconvergetosomeviunderrepeatediterationof(6)hasmeasurezero.2.ThesetofrobusteigenvectorsofTisAnandkumar,Ge,Hsu,Kakade,andTelgarskyMoreover,ifkuk<1andT(I,u,u)=λuforλ>0,thenu0=(1+δ)uforasmallenoughδ∈(0,1)satisfiesku0k≤1andT(u0,u0,u0)=(1+δ)3T(u,u,u)>T(u,u,u).ThereforealocalmaximizermusthaveT(I,u,u)=λuforsomeλ≥0,andkuk=1wheneverλ>0.Extend{v1,v2,...,vk}toanorthonormalbasis{v1,v2,...,vn}ofRn.NowpickanystationaryTensorDecompositionsforLearningLatentVariableModelsū:=u0+w,wehaveT(ũ,ũ,ũ)=T(u0,u0,u0)+∇uT(u,u,u)>(ũ−Anandkumar,Ge,Hsu,Kakade,andTelgarskyAppendixB.AnalysisofRobustPowerMethodInthissection,weproveTheorem5.1.Theproofisstructuredasfollows.InAppendixB.1,weshowthatwithhighprobability,atleastoneoutofLrandomvectorswillbeagoodinitializerforthetensorpoweriterations.Aninitializerisgoodifitsprojectionontoaneigenvectorisnoticeablylargerthanitsprojectionontoothereigenvectors.WethenanalyzeinAppendixB.2theconvergencebehaviorofthetensorpoweriterations.TensorDecompositionsforLearningLatentVariableModelsItsufficestoshowthatwithprobabilityatleast1/2,thereisacolumnj∗∈[L]suchthat|Z1,j∗|≥11−γmaxi∈[k]\{1}|Zi,jAnandkumar,Ge,Hsu,Kakade,andTelgarskyInthissubsection,weassumethatT̃hastheformT̃=kXi=1λ̃iv⊗3i+Ẽ(11)where{v1,v2,...,vk}isanorthonormalbasis,and,withoutlossofgenerality,λ̃1|θ1,t|=maxi∈[k]λ̃i|θi,t|>0.Also,defineλ̃min:=min{λ̃i:i∈[k],λ̃i>0},λ̃max:=max{λ̃i:i∈[k]}.WefurtherassumetheerrorẼisasymmetrictensorsuchthat,forsomeconstantp>1,kẼ(I,u,u)k≤˜,∀u∈Sk−1;(12)kẼ(I,u,u)k≤˜TensorDecompositionsforLearningLatentVariableModelsUsingthetriangleinequalityandthefactkE(vi,θt,θt)k≤˜,wehaveθ̌i,t+1≥λ̃iθ2i,t−˜≥|θi,t|·λ̃i|θi,t|−˜/|θi,t|(17)and|θ̌i,t+1|≤|λ̃iθ2i,t|+˜≤|θi,t|·λ̃i|θi,t|+˜/|θi,t|(18)foralli∈[k].Combining(17)and(18)givesri,t+1=λ̃1θ1,t+1λ̃i|θi,t+1|=λ̃1θ̌1,t+1λ̃i|θ̌i,t+1|≥r2i,t·1−δt1+˜λ̃iθ2i,t=r2i,t·1−δt1+(λ̃i/λ̃1)δtr2i,t≥r2i,t·1−δt1+κδtr2i,t.Moreover,bythetriangleinequalityandHölder’sinequality,nXi=2[θ̌i,t+1]21/2=nXi=2λ̃iθ2i,t+E(vi,θt,θt)21/2≤nXi=2λ̃2iθ4i,t1/2+nXi=2E(vi,θt,θt)21/2≤maxi6=1λ̃i|θi,t|nXi=2θ2i,t1/2+˜=(1−θ21,t)1/2·maxi6=1λ̃i|θi,t|+˜/(1−θ21,t)1/2.(19)Combining(17)and(19)gives|θ1,t+1|(1−θ21,t+1)1/2=|θ̌1,t+1|Pni=2[θ̌i,t+1]21/2≥|θ1,t|(1−θ21,t)1/2·λ̃1|θ1,t|−˜/|θ1,t|maxi6=1λ̃i|θi,t|+˜/(1−θ21,t)1/2.IntermsofRt+1,Rt,γt,andδt,thisreadsRt+1≥1−δt(1−γt)1−θ21,tθ21,t1/2+δt=Rt·1−δt1−γt+δtRt=1−δt1−γtRt+δt≥1−δtκR2t+δtwherethelastinequalityfollowsfromPropositionB.1.LemmaB.2Fixanyρ>1.Assume0≤δt<minn12(1+2κρ2),1−1/ρ1+κρoandγt>2(1+2κρ2)δt.1.IfAnandkumar,Ge,Hsu,Kakade,andTelgarsky2.Ifρ2<r2i,t,thenri,t+1≥min{r2i,t/ρ,1−δt−1/ρκδt}.3.γt+1≥min{γt,1−1/ρ}.4.Ifmini6=1r2i,t>(ρ(1−δt)−1)/(κδt),thenRt+1>1−δt−1/ρκδt·λ̃minλ̃1·1√k.5.IfRt≤1+2κρ2,thenRt+1≥Rt1+γt3,θ21,t+1≥θ21,t,andδt+1≤δt.ProofConsidertwo(overlapping)casesdependingonr2i,t.•Case1:r2i,tTensorDecompositionsforLearningLatentVariableModelsLemmaB.3Assume0≤δt<1/2andγt>0.Pickanyβ>α>0suchthatα(1+α)(1+α2)≥˜Anandkumar,Ge,Hsu,Kakade,andTelgarskyB.2.1ApproximateRecoveryofaSingleEigenvectorWenowstatethemainresultregardingtheapproximaterecoveryofasingleeigenvectorusingthetensorpowermethodonT̃.Here,weexploitthespecialpropertiesoftheerrorẼ—both(12)and(13).LemmaB.4ThereexistsauniversalconstantC>0suchthatthefollowingholds.Leti∗:=argmaxi∈[k]λ̃i|θi,0|.If˜<γ02(1+8κ)·λ̃min·θ2i∗,0andN≥C·log(kκ)γ0+loglogpλ̃i∗˜,thenaftert≥NiterationsofthetensorpowermethodontensorT̃asdefinedin(11)andsatisfying(12)and(13),thefinalvectorθtsatisfiesθi∗,t≥s1−3˜pλ̃i∗2,kθt−vi∗k≤4˜pλ̃i∗,|T̃(θt,θt,θt)−λ̃i∗|≤27κ˜pλi∗2+2˜p.ProofAssumewithoutlossofgeneralitythati∗=1.Weconsiderthreephases:(i)iterationsbeforethefirsttimetsuchthatRt>1+2κρ2=1+8κ(usingρ:=2),(ii)thesubsequentiterationsbeforethefirsttimetsuchthatRt≥1/α(whereαwillbedefinedbelow),andfinally(iii)theremainingiterations.Webeginbyanalyzingthefirstphase,i.e.,theiteratesinT1:={t≥0:Rt≤1+2κρ2=1+8κ}.Observethattheconditionon˜impliesδ0=˜λ̃1θ21,0<γ02(1+8κ)·λ̃minλ̃1≤minγ02(1+2κρ2),1−1/ρ2(1+2κρ2),andhencethepreconditionsonδtandγtofLemmaB.2holdfort=0.Forallt∈T1satisfyingthepreconditions,LemmaB.2impliesthatδt+1≤δtandγt+1≥min{γt,1−1/ρ},sothenextiterationalsosatisfiesthepreconditions.Hencebyinduction,thepreconditionsholdforalliterationsinT1.Moreover,foralli∈[k],wehave|ri,0|≥11−γ0;andwhilet∈T1:(i)|ri,t|increasesatalinearratewhiler2i,t≤2ρ2,and(ii)|ri,t|increasesataquadraticratewhileρ2≤r2i,t≤1−δt−1/ρκδt.(Thespecificratesaregiven,respectively,inLemmaB.2,claims1and2.)Since1−δt−1/ρκδt≤λ̃12κ˜,itfollowsthatmini6=1r2i,t≤1−δt−1/ρκδtforatmost2γ0lnp2ρ211−γ0+lnlnλ̃12κ˜ln√2=O1γ0+loglogλ̃1˜(22)iterationsinT1.Assoonasmini6=1r2i,t>1−δt−1/ρκδt,wehaveTensorDecompositionsforLearningLatentVariableModelsandallthewhileRtisgrowingatalinearrate(giveninLemmaB.2,claim5).Therefore,thereareatmostanadditional1+3γ0ln1+8κ7/√k=Olog(kκ)γ0(23)iterationsinT1overthatcountedin(22).Therefore,bycombiningthecountsin(22)and(23),wehavethatthenumberofiterationsinthefirstphasesatisfies|T1|=Ologlogλ̃1˜+log(kκ)γ0.Wenowanalyzethesecondphase,i.e.,theiteratesinT2:={t≥0:t/∈T1,Rt<1/α}.Defineα:=3˜λ̃1,β:=11+2κρ2=11+8κ.Notethatfortheinitialiterationt0:=minT2,wehavethatRt0≥1+2κρ2=1+8κAnandkumar,Ge,Hsu,Kakade,andTelgarskyItcanbecheckedthatδt00∈(0,1/2),γt00≥1−3˜κ/λ1>0,α(1+α)(1+α2)≥˜p(1−3˜κ/λ1)λ̃1≥˜pγt00λ̃1,α2(1+α)(1+β2)≥˜pλ̃1.Therefore,thepreconditionsofLemmaB.3aresatisfiedfortheinitialiterationt00inthisfinalphase,andbythesameargumentsasbefore,thepreconditionsholdforallsubsequentiterationst≥t00.Initially,wehaveRt00≥1/α≥1/β,andbyLemmaB.3,wehavethatRtincreasesataquadraticrateinthisfinalphaseuntilRt≥1/α.SothenumberofiterationsbeforeRt≥1/αcanbeboundedaslnln(1/α)ln((1/β)/(2κ))=lnlnpλ̃13˜lnλ13˜·12κ≤lnlnpλ̃13˜=Ologlogpλ̃1˜.OnceRt≥1/α,wehaveθ21,t≥1−3˜pλ̃12.Sincesign(θ1,t)=r1,t≥r21,t−1·(1−δt−1)/(1+κδt−1r21,t−1)=(1−δt−1)/(1+κδt−1)>0byPropositionB.2,wehaveθ1,t>0.Thereforewecanconcludethatkθt−v1k=q2(1−θ1,t)≤s21−q1−(3˜/(pλ̃1))2≤4˜/(pλ̃1).Finally,|T̃(θt,θt,θt)−λ̃1|=λ̃1(θ31,t−1)+kXi=2λ̃iθ3i,t+Ẽ(θt,θt,θt)≤λ̃1|θ31,t−1|+kXi=2λ̃i|θi,t|θ2i,t+kẼ(I,θt,θt)k≤λ̃11−θ1,t+|θ1,t(1−θ21,t)|+maxi6=1λ̃i|θi,t|kXi=2θ2i,t+kẼ(I,θt,θt)k≤λ̃11−θ1,t+|θ1,t(1−θ21,t)|+maxi6=1λ̃iq1−θ21,tkXi=2θ2i,t+kẼ(I,θt,θt)k=λ̃11−θ1,t+|θ1,t(1−θ21,t)|+maxi6=1λ̃i(1−θ21,t)3/2+kẼ(I,θt,θt)k≤λ̃1·33˜pλ̃12+κλ̃1·3˜pλ̃13+˜p≤(27κ·(˜/pλ̃TensorDecompositionsforLearningLatentVariableModelsB.3DeflationLemmaB.5Fixsome˜≥0.Let{v1,v2,...,vk}beanorthonormalbasisforRk,andAnandkumar,Ge,Hsu,Kakade,andTelgarskyWenowexpressEi(I,u,u)intermsofthecoordinatesystemdefinedbyviandv̂⊥i,depictedbelow.Defineai:=u>viandbi:=u>v̂⊥i/kv̂⊥ik.(Notethatthepartofulivinginspan{vi,v̂⊥i}⊥isirrelevantforanalyzingEi(I,u,u).)WehaveEi(I,u,u)TensorDecompositionsforLearningLatentVariableModelsTherefore,usingtheinequalityfrom(25)andagain(x+y)2≤2(x2+y2),tXi=1Ei(I,u,u)22≤2tXi=1A2i+2tXi=1|Bi|2≤4(5+11˜/λmin)2˜2tXi=1a2i+64(1+˜/λmin)2˜2tXi=1(˜/λi)2+28(1+˜/λmin)(˜2/λmin)tXi=1a2i+32(1+˜/λmin)˜tXi=1(˜/λi)32≤4(5+11˜/λmin)2˜2tXi=1a2i+64(1+˜/λmin)2˜2tXi=1(˜/λi)2+128(1+˜Anandkumar,Ge,Hsu,Kakade,andTelgarsky(NotethattheconditiononLholdswithL=poly(k)log(1/η).)SupposethatAlgorithm1isiterativelycalledktimes,wheretheinputtensorisT̂inthefirstcall,andineachsubsequentcall,theinputtensoristhedeflatedtensorreturnedbythepreviouscall.Let(v̂1,λ̂1),(v̂2,λ̂2),...,(v̂k,λ̂k)bethesequenceofestimatedeigenvector/eigenvaluepairsre-turnedinthesekcalls.Withprobabilityatleast1−η,thereexistsapermutationπon[k]suchthatkvπ(j)−v̂jk≤8/λπ(j),|λπ(j)−λ̂j|≤5,∀j∈[k],andT−kXj=1λ̂jv̂⊗3j≤55.ProofWeprovebyinductionthatforeachi∈[k](correspondingtothei-thcalltoAlgorithm1),withprobabilityatleast1−iη/k,thereexistsapermutationπon[k]suchthatthefollowingassertionshold.1.Forallj≤i,kvπ(j)−v̂jk≤8/λπ(j)and|λπ(j)−λ̂j|≤12.2.TheerrortensorẼi+1:=T̂−Xj≤iλ̂jv̂⊗3j−Xj≥i+1λπ(j)v⊗3π(j)=E+Xj≤iλπ(j)v⊗3π(j)−λ̂jv̂⊗3jsatisfieskẼi+1(I,u,u)k≤56,∀u∈Sk−1;(26)kẼi+1(I,u,u)k≤2,∀u∈Sk−1s.t.∃j≥i+1(u>vπ(j))2≥1−(168/λπ(j))2.(27)Weactuallytakei=0asthebasecase,sowecanignorethefirstassertion,andjustobservethatfori=0,Ẽ1=T̂−kXj=1λiv⊗3i=E.WehavekẼ1k=kEk=,andthereforethesecondassertionholds.Nowfixsomei∈[k],andassumeastheTensorDecompositionsforLearningLatentVariableModelsObservethatT̃i−Ti=Ẽi,whichsatisfiesthesecondassertionintheinductivehypothesis.WemaywriteTi=Pkl=1λ̃lv⊗3Anandkumar,Ge,Hsu,Kakade,andTelgarskywhichinturnimpliesλπ(j)|θπ(j),N|≤34λπ(j∗)|θπ(j∗),N|,j6=j∗.ThismeansthatθNis(1/4)-separatedrelativetoπ(j∗).Also,observethat|θπ(j∗),N|≥45·λπ(jmax)λπ(j∗)≥45,λπ(jmax)λπ(j∗)≤54.ThereforebyLemmaB.4(using˜/p:=2,γ:=1/4,andκ:=5/4),executinganotherNpoweriterationsstartingfromθNgivesavectorθ̂thatsatisfieskθ̂−vπ(j∗)k≤8λπ(j∗),|λ̂−λπ(j∗)|≤5.Sincev̂i=θ̂andλ̂i=λ̂,thefirstassertionoftheinductivehypothesisissatisfied,aswecanmodifythepermutationπbyswappingπ(i)andπ(j∗)withoutaffectingthevaluesof{π(j):TensorDecompositionsforLearningLatentVariableModelsthereexistsapermutationπsuchthattwoassertionsholdfori=k,withprobabilityatleast1−η.Fromthelastinductionstep(i=k),itisalsoclearAnandkumar,Ge,Hsu,Kakade,andTelgarskyWeassumeẼisasymmetrictensorsuchthat,forsomeconstantp>1,kẼ(I,u,u)k≤˜,∀u∈Sk−1;kẼ(I,u,u)k≤˜/p,∀u∈Sk−1s.t.(u>v1)2≥1−(3˜/λ̃1)2;kẼkF≤˜F.Assumethatnotallλ̃iarezero,anddefineλ̃min:=min{λ̃i:i∈[k],λ̃i>0},λ̃max:=max{λ̃i:i∈[k]},`:=|{i∈[k]:λ̃i>0}|,λ̃avg:=1`kXi=1λ̃2i1/2.WeTensorDecompositionsforLearningLatentVariableModelsMoreover,kT̃kF≥kXi=1λ̃iv⊗3iF−kẼkF=kXj=1kXi=1λ̃iviv>i(v>ivj)2F1/2−kẼkF=kXj=1λ̃jvjv>j2F1/2−kẼkF=kXj=1λ̃2j1/2−Anandkumar,Ge,Hsu,Kakade,andTelgarskyinequality,andthefactkAkF≤√kkAkforanymatrixA∈Rk×k,(1+α)|φ1|≥(1+α)θ>M̃θ≥kM̃kF(32)≥kXi=1λ̃iθiviv>iF−Ẽ(I,I,θ)F≥kXi=1λ̃2iθ2i1/2−√kkẼ(I,I,θ)k≥kXi=1λ̃2iθ2i1/2−√k˜.Combiningtheseboundson|φ1|givesλ̃1|θ1|+˜≥11+α"kXi=1λ̃2iθ2i1/2−√k˜#.(33)Theassumption˜≤αλ̃min/√kimpliesthat√k˜≤αλ̃min≤αkXi=1λ̃2iθ2i1/2.Moreover,sinceλ̃1|θTensorDecompositionsforLearningLatentVariableModelsMoreover,bythetriangleinequality,|θ>M̃θ|≤kXi=1|φi|(u>iθ)2≤|φ1|(u>1θ)2+maxi6=1Anandkumar,Ge,Hsu,Kakade,andTelgarskyforα∈(0,1/20).Moreover,byassumptionwehaveT̃(θ,θ,θ)≥0,andT̃(θ,θ,θ)=kXi=1λ̃iθ3i+Ẽ(θ,θ,θ)=λ̃1θ31+kXi=2λ̃iθ3i+Ẽ(θ,θ,θ)≤λ̃1θ31+maxi6=1λ̃i|θi|kXi=2θ2i+˜≤λ̃1θ31+√7αλ̃1|θ1|(1−θ21)+˜(bythesecondclaim)≤λ̃1|θ1|3sign(θ1)+√7α(1−2α)2−√7α+α(1−2α)3(since|θ1|≥1−2α)<λ̃1|θ1|3sign(θ1)+1sosign(θ1)>−1,meaningθ1>0.Thereforeθ1=|θ1|≥1−2α.Thisprovesthefinalclaim.LemmaC.2Fixα,β∈(0,1).Assumeλ̃i∗=maxi∈[k]λ̃iand˜≤minα5√k+7,1−β7·λ̃i∗,˜F≤√`·1−β2β·λ̃i∗.TotheconclusionofLemmaB.4,itcanbeaddedthatthestoppingcondition(31)issatisfiedbyθ=θt.ProofWithoutlossofgenerality,assumei∗=1.BythetriangleinequalityandCauchy-Schwarz,kT̃(I,I,θt)kF≤λ̃1|θ1,t|+Xi6=1λi|θi,t|+kẼ(I,I,θt)kF≤λ̃1|θ1,t|+λ̃1√kXi6=1θ2i,t1/2+√k˜≤λ̃1|θ1,t|+3√k˜p+√k˜.wherethelaststepusesthefactthatθ21,t≥1−(3˜/(pλ̃1))2.Moreover,T̃(θt,θt,θt)≥λ̃1−27˜pλ12+2˜TensorDecompositionsforLearningLatentVariableModelsUsingthedefinitionofthetensorFrobeniusnorm,wehave1√`kT̃kF≤1√`kXi=1λ̃iv⊗3iF+1√Anandkumar,Ge,Hsu,Kakade,andTelgarskyAppendixD.SimultaneousDiagonalizationforTensorDecompositionAsdiscussedintheintroduction,anotherstandardapproachtocertaintensordecompositionproblemsistosimultaneouslydiagonalizeacollectionofsimilarmatricesobtainedfromthegiventensor.Wenowexaminethisapproachinthecontextofourlatentvariablemodels,whereM2=kXi=1wiµi⊗µiM3=kXi=1wiµi⊗µi⊗µi.LetV:=[µ1|µ2|···|µk]andD(η):=diag(µ>1η,µ>2η,...,µ>kη),soM2=Vdiag(w1,w2,...wk)V>M3(I,I,η)=Vdiag(w1,w2,...wk)D(η)V>Thus,theproblemofdeterminingtheµicanbecastasasimultaneousdiagonalizationproblem:findamatrixXsuchthatXTensorDecompositionsforLearningLatentVariableModelscolumnsofµ>1η(1)µ>2η(1)···µ>kη(1)µ>1η(2)µ>2η(2)···µ>kη(2)............µ>1η(m)µ>2η(m)···µ>kη(m)aredistinct(i.e.,foreachpairofcolumnindicesi,j,thereexistsarowindexrsuchthatthe(r,i)-thand(r,j)-thentriesaredistinct).Thisisamuchweakerrequirementforuniqueness,andthereforemaytranslatetoanimprovedperturbationanalysis.Infact,usingthetechniquesdiscussedinSection4.3,wemayevenreducetheproblemtoanorthogonalsimultaneousdiagonalization,whichmaybeeasiertoobtain.Furthermore,anumberofrobustnumericalmethodsfor(approximately)simultaneouslydiagonalizingcollectionsofmatriceshavebeenproposedandusedsuccessfullyintheAnandkumar,Ge,Hsu,Kakade,andTelgarskyS.Arora,R.Ge,A.Moitra,andS.Sachdeva.ProvableICAwithunknownGaussiannoise,andimplicationsforGaussianmixturesandautoencoders.InAdvancesinNeuralInformationProcessingSystems25,2012b.T.Austin.Onexchangeablerandomvariablesandthestatisticsoflargegraphsandhyper-graphs.Probab.Survey,5:80–145,2008.R.Bailly.Quadraticweightedautomata:Spectralalgorithmandlikelihoodmaximization.JournalofMachineLearningResearch,2011.B.BalleandM.Mohri.Spectrallearningofgeneralweightedautomataviaconstrainedmatrixcompletion.InAdvancesinNeuralInformationProcessingSystems25,2012.B.Balle,A.Quattoni,andX.Carreras.Locallossoptimizationinoperatormodels:Anewinsightintospectrallearning.InTwenty-NinthInternationalConferenceonMachineLearning,2012.M.BelkinandK.Sinha.Polynomiallearningofdistributionfamilies.InFifty-FirstAnnualIEEESymposiumonFoundationsofComputerScience,pages103–112,2010.A.Bhaskara,M.Charikar,A.Moitra,andA.Vijayaraghavan.Smoothedanalysisoften-sordecompositions.InProceedingsofthe46thAnnualACMSymposiumonTheoryofComputing,2014.B.Boots,S.M.Siddiqi,andG.J.Gordon.Closingthelearning-planningloopwithpredic-tivestaterepresentations.InProceedingsoftheRoboticsScienceandSystemsConference,2010.S.C.BrubakerandS.Vempala.IsotropicPCAandaffine-invariantclustering.InForty-NinthAnnualIEEESymposiumonFoundationsofComputerScience,2008.A.Bunse-Gerstner,R.Byers,andV.Mehrmann.Numericalmethodsforsimultaneousdiagonalization.SIAMJournalonMatrixAnalysisandApplications,14(4):927–949,1993.J.-F.Cardoso.Super-symmetricdecompositionofthefourth-ordercumulanttensor.blindidentificationofmoresourcesthansensors.InAcoustics,Speech,andSignalProcessing,1991.ICASSP-91.,1991InternationalConferenceon,pages3109–3112.IEEE,1991.J.-F.Cardoso.Perturbationofjointdiagonalizers.TechnicalReport94D027,SignalDe-partment,TélécomParis,1994.J.-F.CardosoandP.Comon.Independentcomponentanalysis,asurveyofsomealgebraicmethods.InIEEEInternationalSymposiumonCircuitsandSystems,pagesTensorDecompositionsforLearningLatentVariableModelsR.B.Cattell.Parallelproportionalprofilesandotherprinciplesfordeterminingthechoiceoffactorsbyrotation.Psychometrika,9(4):267–283,1944.J.T.Chang.FullreconstructionofMarkovmodelsonevolutionarytrees:IdentifiabilityandAnandkumar,Ge,Hsu,Kakade,andTelgarskyA.T.Erdogan.OntheconvergenceofICAalgorithmswithsymmetricorthogonalization.IEEETransactionsonSignalProcessing,57:2209–2221,2009.A.M.Frieze,M.Jerrum,andR.Kannan.Learninglineartransformations.InThirty-SeventhAnnualTensorDecompositionsforLearningLatentVariableModelsE.KofidisandP.A.Regalia.Onthebestrank-1approximationofhigher-ordersuper-symmetrictensors.SIAMJournalonMatrixAnalysisandApplications,23(3):863–884,2002.T.G.KoldaandB.W.Bader.Tensordecompositionsandapplications.SIAMreview,51(3):455,2009.T.G.KoldaandJ.R.Mayo.Shiftedpowermethodforcomputingtensoreigenpairs.SIAMJournalonMatrixAnalysisandApplications,32(4):1095–1124,October2011.J.B.Kruskal.Three-wayarrays:rankanduniquenessoftrilineardecompositions,withapplicationtoarithmeticcomplexityandstatistics.LinearAlgebraandAppl.,18(2):95–138,1977.L.D.Lathauwer,B.D.Moor,andJ.Vandewalle.Onthebestrank-1andrank-(R1,R2,...,Rn)approximationandapplicationsofhigher-ordertensors.SIAMJ.MatrixAnal.Anandkumar,Ge,Hsu,Kakade,andTelgarskyL.PachterandB.Sturmfels.AlgebraicStatisticsforComputationalBiology,volume13.CambridgeUniversityPress,2005.A.Parikh,L.Song,andE.P.Xing.Aspectralalgorithmforlatenttreegraphicalmodels.InTwenty-EighthInternationalConferenceonMachineLearning,2011.K.Pearson.Contributionstothemathematicaltheoryofevolution.PhilosophicalTrans-actionsoftheRoyalSociety,London,A.,page71,1894.L.Qi.Eigenvaluesofarealsupersymmetrictensor.JournalofSymbolicComputation,40(6):1302–1324,2005.R.A.Rednerand