                                             A regression approach for explaining manifold embedding
                                                                   coordinates
                                                              Marina Meila‚àó    Samson Koelle        Hanyu Zhang
arXiv:1811.11891v1 [stat.ML] 29 Nov 2018
                                                                           Department of Statistics
                                                                          University of Washington
                                                                           Seattle, WA 98195-4322
                                                                     {mmp2,sjkoelle,hanyuz6}@uw.edu
                                                                                   November 30, 2018
                                                                                          Abstract
                                                    Manifold embedding algorithms map high dimensional data, down to coordinates in a
                                                much lower dimensional space. One of the aims of the dimension reduction is to find the
                                                intrinsic coordinates that describe the data manifold. However, the coordinates returned by the
                                                embedding algorithm are abstract coordinates. Finding their physical, domain related meaning
                                                is not formalized and left to the domain experts. This paper studies the problem of recovering
                                                the domain-specific meaning of the new low dimensional representation in a semi-automatic,
                                                principled fashion. We propose a method to explain embedding coordinates on a manifold
                                                as non-linear compositions of functions from a user-defined dictionary. We show that this
                                                problem can be set up as a sparse linear Group Lasso recovery problem, find sufficient recovery
                                                conditions, and demonstrate its effectiveness on data.
                                               Manifold learning algorithms, also known as embedding algorithms, map data from high di-
                                           mensional, possibly infinite dimensional spaces down to coordinates in a much lower dimensional
                                           space. In the sciences, one of the goals of dimension reduction is the discovery of descriptors of
                                           the data generating process. Both linear dimension reduction algorithms like Principal Component
                                           Analysis (PCA) and non-linear algorithms such as Diffusion Maps [CL06] are used in applications
                                           from genetics to chemistry to uncover collective coordinates describing large scale properties of the
                                           interrogated system. For example, in chemistry, a common objective is to discover so-called reaction
                                           coordinates describing evolution of molecular configurations [CNO00, NC17] .
                                               For example, Figure 1 shows the toluene molecule (C7 H8 ), consisting of Na = 15 atoms. By
                                           Molecular Dynamics (MD) simulation, configurations of this molecule are generated in D = 3 √ó Na =
                                           45 dimensions. They are then mapped into m = 2 dimensions by a manifold learning algorithm.
                                           The figure shows that this configuration space is well approximated by a one-dimensional manifold,
                                           and this manifold is parametrized by a geometric quantity, the torsion of the bond connecting the
                                           CH3 methyl group with the C6 H5 benzene group. Torsions are dihedral angles in tetrahedra with
                                           vertices at atoms of the molecule. In a molecule, any two atoms which are not too distant interact;
                                           hence the toluene molecule has many more interactions than the graph edges presented in Figure
                                             ‚àó www.stat.washington.edu/mmp
                                                                                              1

Figure 1: Explaining the collective coordinates in Molecular Dynamics (MD) simulations. In such
simulations, a molecular configuration is represented by the D = 3Na vector of spatial locations of the Na
atoms comprising the molecule (not shown). Left, top: The toluene molecule, with the bond defining the
important torsion œÑ marked in blue. Left, bottom: Embedding of the configurations of toluene into m = 2
dimensions, showing a manifold of d = 1. The color corresponds to the values of œÑ . Right: Embedding of the
configurations of the ethanol (C2 H5 OH) in m = 2 dimensions, showing a manifold of d = 2. This manifold
is colored by two bond torsions in the ethanol molecule; it can be seen that each torsion explains one of the
circles generating the torus.
1. Our problem is to select the torsion œÑ that explains the one-dimensional manifold of toluene
configurations out of the many torsions to be considered.
     Finding the meaning behind the coordinates output by an embedding algorithm through com-
parison to problem-relevant covariates is usually done via visual inspection by human experts. This
paper introduces a regression method for automatically establishing such relationships between the
abstract coordinates output by a manifold learning algorithm and functions of the data that have
meaning in the domain of the problem.
     The next section defines the problem formally, Section 2 presents useful background in manifold
estimation, while Sections 3, 4 and 5 develop our method. Section 3 is of independent interest as it
introduces Functional Lasso (FLasso), a novel method for sparsely representing a function f as a non-
linear composition of functions from a dictionary. Section 4 introduces a new differential-geometric
method for estimating the gradient of a function defined on a manifold. These two methods are
combined to obtain our main algorithm, ManifoldLasso in Section 5. The relationship to previous
work is discussed in Section 6. Section 8 presents experiments with FLasso and ManifoldLasso,
respectively. Recovery results are presented in Section 7.
1      Problem formulation, assumptions and challenges
We make the standard assumption that the observed data D = {Œæi ‚àà RD : i ‚àà 1 . . . n} are sampled
i.i.d. from a smooth manifold 1 M of intrinsic dimension d smoothly embedded in RD by the inclusion
map. In this paper, we will call smooth any function or manifold of class at least C 3 . We assume
that the intrinsic dimension d of M is known; for example, by having been estimated previously
    1 The reader is referred to [Lee03] for the definitions of the differential geometric terms used in this paper.
                                                              2

by one method in [KvL15]. The manifold M is a Riemannian manifold with Riemannian metric
inherited from the ambient space RD . Furthermore, we assume the existence of a smooth embedding
map œÜ : M ‚Üí œÜ(M) ‚äÇ Rm , where typically m << D. We call the coordinates œÜ(Œæi ) in this m
dimensional ambient space the embedding coordinates; let Œ¶ = [œÜ(Œæi )T ]i=1:n ‚àà Rn√óm . In practice,
the mapping of the data D onto œÜ(D) represents the output of an embedding algorithm, and we
only have access to M and œÜ via D and its image œÜ(D).
    In addition, we are given a dictionary of user-defined and domain-related smooth functions
G = {g1 , . . . gp , with gj : RD ‚Üí R}. Our goal is to express the embedding coordinate functions
œÜ1 . . . œÜm in terms of functions in G.
    More precisely, we assume that œÜ(x) = h(gj1 (x), . . . gjs (x)) with h : O ‚äÜ Rs ‚Üí Rm a smooth
function of s variables, defined on a open subset of Rs containing the range of gj1 , . . . gjs . Let
S = {j1 , . . . js }, and gS = [gj1 (x), . . . gjs ]T . The problem is to discover the set S ‚äÇ [p] such that
œÜ = h‚ó¶gS . We call S the functional support of h, or the explanation for the manifold M in terms of G.
For instance, in the toluene example, the functions in G are all the possible torsions in the molecule,
s = 1, and gS = œÑ is the explanation for the 1-dimensional manifold traced by the configurations.
Indeterminacies In differential geometric terms, the explanation gS is strongly related to finding
a parametrization of M. Hence, |S| ‚â• d. Since the function œÜ given by the embedding algorithm is
not unique, the function h cannot be uniquely determined. Moreover, whenever g1 = t(g2 ) where t
is a smooth monotonic function, the support S may not be unique either.
    In Section 7 we give sufficient and necessary conditions under which S can be recovered uniquely;
intuitively, they consist of functional independencies between the functions in G. For instance, it
is sufficient to assume that that the dictionary G is a functionally independent set, i.e. there is no
g ‚àà G that can be obtained as a smooth function of other functions in G.
    Hence, in this paper we resolve the indeterminacies w.r.t. the support S, and we limit our scope
to recovering S. We leave to future work the problem of recovering information on how the functions
gS combine to explain M.
2       Background on manifold estimation: neighborhood graph,
        Laplacian and estimating tangent subspaces
Manifold learning and intrinsic geometry Suppose we observe data points Œæi ‚àà RD that are
sampled from a smooth d-dimensional submanifold M ‚äÇ RD . The task of manifold learning is to
provide a diffeomorphism œÜ : M ‚Üí œÜ(M) ‚äÇ Rm where m  D. The Whitney Embedding Theorem
[Lee03] guarantees the existence of a map satisfying this property with m ‚â§ 2d. Hence, a good
manifold learner will identify a smooth map œÜ : M ‚Üí Rm with d ‚â§ m ‚â§ 2d  D.
The neighborhood graph and kernel matrix The neighborhood graph is a data structure that
associates to each data point Œæi ‚àà D its set of neighbors Ni = {i0 ‚àà [n], with ||Œæi0 ‚àí Œæi || ‚â§ rN , where
rN is a neighborhood radius parameter. The neighborhood relation is symmetric, and determines an
undirected graph with nodes represented by the data points Œæ1:n .
    Closely related to the neighborhood graph are the local position matrices Œûi = {Œæi0 : i0 ‚àà Ni } ‚àà
  |Ni |√ód
R         , local embedding coordinate matrices Œ¶i = {œÜ(Œæi0 ) : i0 ‚àà Ni } ‚àà R|Ni |√ód , and the kernel
                                                           3

matrix K ‚àà Rn√ón whose elements are
                                       (                     
                                          exp ‚àí ||Œæi‚àíŒæ
                                                     2
                                                        i0 ||
                                                                if i0 ‚àà Ni
                                Kii0 =               N                                            (1)
                                          0 otherwise.
Typically, the radius rN and the bandwidth parameter N are related by rN = cN with c a small
constant greater than 1. This ensures that K is close to its limit when rN ‚Üí ‚àû while remaining
sparse, with sparsity structure induced by the neighborhood graph. Rows of this matrix will be
denoted Ki,Ni to emphasize that when a particular row is passed to an algorithm, only |Ni | values
need to be passed. The neighborhood graph, local position matrices, and kernel matrix play crucial
roles in our manifold estimation tasks.
Estimating tangent spaces in the ambient space RD The tangent subspace at point Œæi in
the data can be estimated by Weighted Local Principal Component Analysis, as described in the
LocalPCA algorithm. The output of this algorithm is an orthogonal matrix Ti ‚àà RD√ód , representing
a basis for TŒæi M. For this algorithm and others we define the SVD algorithm SVD(X, d) of a
symmetrical matrix X as outputting V, Œõ, where Œõ and V are the largest d eigenvalues and their
eigenvectors, respectively, and denote a column vector of ones of length k by 1k . We denote |Ni | by
ki .
LocalPCA (local data Œûi , kernel row Ki,Ni , intrinsic dimension d)
  1: Compute weighted mean Œæ¬Øi = (Ki,Ni 1ki )‚àí1 Ki,Ni Œûi
  2: Compute weighted local difference matrix Zi = (Ki,Ni 1ki )‚àí1 Ki,Ni (Œûi ‚àí 1ki Œæ¬Øi )
  3: Compute Ti , Œõ ‚Üê SVD(ZiT Zi , d)
  4: Output Ti
The renormalized graph Laplacian The renormalized graph Laplacian, also known as the
sample Laplacian, or Diffusion Maps Laplacian L, constructed by Laplacian , converges to the
manifold Laplace operator ‚àÜM ; [CL06] shows that this estimator is unbiased w.r.t. the sampling
density on M (see also [HAvL05, HAvL07, THJ10]). The Laplacian L is a sparse matrix; row i of
L contains non-zeros only for i0 ‚àà Ni . Thus, as for K, rows of this matrix will be denoted Li,Ni .
Again, sparsity pattern is given by the neighborhood graph; construction of the neighborhood graph
is therefore the computationally expensive component of this algorithm.
Laplacian (neighborhoods Ni:n , local data Œû1:n , bandwidth N )
  1: Compute kernel matrix K using (1)
  2: Compute normalization weights wi ‚Üê Ki,Ni 1ki , i = 1, . . . n, W ‚Üê diag(wi i = 1 : n)
  3: Normalize LÃÉ ‚Üê W ‚àí1 KW ‚àí1
  4: Compute renormalization weights wÃÉi ‚Üê LÃÉi,Ni 1ki , i = 1, . . . n, WÃÉ = diag(wÃÉi i = 1 : n)
  5: Renormalize L ‚Üê 42 (WÃÉ ‚àí1 LÃÉ ‚àí In )
                         N
  6: Output Kernel matrix K, Laplacian L, [optionally wÃÉ1:n ]
     The m principal eigenvectors of the Laplacian L (or alternatively of the matrix LÃÉ of Algorithm
Laplacian), corresponding to its smallest eigenvalues, are sometimes used as embedding coordinates
                                                   4

Œ¶ of the data; the embedding obtained is known as the Diffusion Map [CL06] or the Laplacian
Eigenmap [BN02] of D. We use this embedding approach for convenience, but in general, any
algorithm which asymptotically generates a smooth embedding is acceptable.
The pushforward Riemannian metric Geometric quantities such as angles and lengths of
vectors in the tangent bundle T M, distances along curves in M, etc., are captured by Riemannian
geometry. We assume that (M, id) is a Riemannian manifold, with the metric id induced from
RD . Furthermore, we associate with œÜ(M) a Riemannian metric g which preserves the geometry of
(M, id). This metric is called the pushforward Riemannian metric and is defined by
                     hu, vig = hDœÜ‚àí1 (Œæ)u, DœÜ‚àí1 (Œæ)vi        for all u, v ‚àà TœÜ(Œæ) œÜ(M).            (2)
In the above, DœÜ‚àí1 (Œæ) maps vectors from TœÜ(Œæ) œÜ(M) to TŒæ M, and h, i is the Euclidean scalar
product.
    For each œÜ(Œæi ), the associated push-forward Riemannian metric expressed in the coordinates of
Rm , is a symmetric, semi-positive definite m √ó m matrix Gi of rank d. The scalar product hu, vig
takes the form uT Gi v.
    The matrices Gi can be estimated by the algorithm RMetric of [PM13]. The algorithm uses
only local information, and thus can be run efficiently. For notational simplicity, we refer to the m
embedding coordinates of the points in a neighborhood of a point i as Œ¶i .
RMetric (Laplacian row Li,Ni , local embedding coordinates Œ¶i , intrinsic dimension d)
 1:  Compute centered local embedding coordinates Œ¶ÃÉi ‚Üê Œ¶i ‚àí œÜ(Œæi )1Tki
 2:  Hikk0 = Li,Ni (Œ¶ÃÉi,k Œ¶ÃÉi,k0 ) for k, k 0 = 1 : m. Hi ‚Üê [Hikk0 ]k,k0 ‚àà1:m
 3:  Compute Vi , Œõi ‚Üê SVD(Hi , d)
 4:  Gi ‚Üê Vi Œõ‚àí1
               i Vi .
                     T
 5:  Output Gi
3     Flasso: sparse non-linear functional support recovery
Our approach for explaining manifold coordinates is predicated on the following general method
for decomposing a real-valued function f over a given dictionary G. The main idea is that when
f = h ‚ó¶ g, their differentials Df, Dh, Dg at any point are in the linear relationship Df = DhDg.
This is expressed more precisely as follows.
Proposition 1 (Leibniz Rule) Let f = h ‚ó¶ gS with gS = [g1 , g2 , ¬∑ ¬∑ ¬∑ , gs ]T : RD ‚Üí Rs , h : Rs ‚Üí R.
All maps are assumed to be smooth. Then at every point,
                                          ‚àÇh          ‚àÇh              ‚àÇh
                               ‚àáf =           ‚àág1 +       ‚àág2 + . . .     ‚àágs ,                    (3)
                                         ‚àÇg1          ‚àÇg2             ‚àÇgs
or, in matrix notation
                                                Df = DhDgS .
                                                       5

Given samples Œæ1:n ‚àà RD , dictionary G, and function f , we would like to discover that f is a function
of gS without knowing h. The usefulness of this decomposition is that even if f is a non-linear
function of g1:s , its gradient is a linear combination of the gradients ‚àág1:s at every point. That
is, proposition 1 transforms the functional, non-linear sparse recovery problem into a set of linear
sparse recovery problems, one for each data point.
Formulating the problem as Group Lasso Let ‚àáf (Œæi ) = yi and ‚àágj (Œæi )/||‚àágj (Œæ)||2 = xij ,
for i = 1 : n. These vectors in RD could be estimated, or available analytically. Then, by (3) we
construct the following linear model
                                  X p
                           yi =         Œ≤ij xij + i = Xi Œ≤i: + i ,        for i = 1 : n.                         (4)
                                   j=1
In the above, Œ≤i: ‚àà Rp is the vector [Œ≤ij ]j=1:p . If there exists some h such that f = h ‚ó¶ gS holds, then
non-zero Œ≤ij s are estimations of ‚àÇh  gj (Œæi )||‚àágj (Œæi )|| for j ‚àà S and zeros Œ≤ij indicate j ‚àà       / S . Hence, in
Œ≤i: , only s elements are non-zero. The term i is added to account for noise or model misspecification.
     The key characteristic of the functional support that we leverage is that the same set of elements
will be non-zero for all i‚Äôs. Denote by Œ≤j ‚àà Rn the vector [Œ≤ij ]i=1:n . Since finding the set S ‚äÇ [p] for
which Œ≤j 6‚â° 0, given y1:n and X1:n is underdetermined when for example s > d, we use Group Lasso
[YL06] regularization in order to zero out as many Œ≤j vectors as possible. In particular, each group
has size n. Thus, the support recovery can be reformulated as the globally convex optimization
problem
                                                         n
                                                       X                           ‚àö X
                    (FLasso) min JŒª (Œ≤) = 21                 ||yi ‚àí Xi Œ≤i ||22 + Œª/ n         ||Œ≤j ||,             (5)
                                    Œ≤
                                                       i=1                               j
with Œª > 0 a regularization parameter. This framework underscores why we normalize gradients
by k‚àágj k2 ; else we would favor dictionary functions that are scaled by large constants, since their
coefficients would be smaller. Note that normalization of f is not necessary since it rescales all
estimated partials the same. We call problem (5) Functional Lasso (FLasso). Experiments on
synthetic data in Section 8 illustrate the effectiveness of this algorithm.
Multidimensional FLasso The FLasso extends easily to vector valued functions f : RD ‚Üí Rm .
To promote finding a common support S for all m embedding coordinates, groups will overlap the
m embedding coordinates. Let
                                                                     ‚àÇhk
                                       yik = ‚àáfk (Œæi ) Œ≤ijk =              (Œæi )                                   (6)
                                                                     ‚àÇgj
and
              Œ≤j = vec(Œ≤ijk , i = 1 : n, k = 1 : m) ‚àà Rmn ,         Œ≤ik = vec(Œ≤ijk , j = 1 : p) ‚àà Rp .             (7)
Then the FLasso objective function becomes
                                          n     m                                 p
                                      1 XX                                    Œª X
                          JŒª (Œ≤) =                ||yik ‚àí Xi Œ≤ik ||2 + ‚àö             ||Œ≤j ||.                      (8)
                                      2 i=1                                  mn j=1
                                              k=1
In the above, Œ≤j is the vector of regression coefficients corresponding to the effect of function gj ,
which form group j in the multivariate group Lasso problem. The size of each group is mn.
                                                           6

4      Explaining manifold coordinates with FLasso on the tan-
       gent bundle
To explain the coordinates œÜ1:m with the FLasso method of Section 3, we need (i) to extend
FLasso to functions defined on a manifold M, and (ii) to reliably estimate their differentials DœÜ1:m
in an appropriate coordinate system.
    These tasks require bringing various ‚Äúgradients‚Äù values into the same coordinate system. For
function g defined on a neighborhood of M in RD , we denote ‚àáŒæ g the usual gradient ‚àÇg        ‚àÇŒæ . Similarly
for a function f defined on a neighborhood of œÜ(M) in Rm , the gradient is denoted ‚àáœÜ f . To define
the gradient of a function on a manifold, we recall that the differential of a scalar function f on
M at point Œæ ‚àà M is a linear functional Df (Œæ) : TŒæ M ‚Üí R. The gradient of f at Œæ, denoted
grad f (Œæ) ‚àà TŒæ M is defined by hgrad f (Œæ), vi = Df (Œæ)v for any v ‚àà TŒæ M. To distinguish between
gradients in the tangent bundles of M, respectively œÜ(M), we denote the former by gradT and the
latter by gradœÜ .
    In this section, we will be concerned with the coordinate functions œÜ1 , . . . œÜm seen as functions
on M, and with the gradients ‚àáŒæ gj and gradTi gj of the dictionary functions. It is easy to see that
for a choice of basis Ti ‚àà RD√ód in TŒæi M,
                                        gradT gj (Œæ) = TiT ‚àáŒæ gj (Œæi ).                                  (9)
4.1     FLasso for vector-valued functions on M
We start from the FLasso objective function JŒª for m-dimensional vector valued functions (8).
Since differentials of functions on a manifold M operate on the tangent bundle T M, the definitions
of yi and Xi from (FLasso) are replaced with the values of the respective gradients in T M. For the
moment we assume that these are given. In Section 4.2 we will concern ourselves with estimating
the parameters yik of JŒª from the data, while Xi is given in (12) below.
    For each i, we fix an orthogonal basis in TŒæi M , and denote the orthogonal D √ó d matrix
representing the respective basis vectors by Ti . Hence, any vector in TŒæi M can be expressed as
v ‚â° [v1 . . . vd ]T by its d coordinates in the chosen basis, or as Ti v ‚àà RD . For any vector w in
RD , TiT Œæ ‚àà Rd represents the coordinates of its projection onto TŒæi M; in particular, if w ‚àà TŒæi M,
TiT Ti w = w. These elementary identities are reflected in (9) above. Let
                                                                             ‚àÇhk
                        yik = ‚àáT œÜk (Œæi ) Yi = [yik ]m
                                                     k=1 ‚àà R
                                                               d√óm
                                                                      Œ≤ijk =      (Œæi )                (10)
                                                                             ‚àÇgj
and
             Œ≤j = vec(Œ≤ijk , i = 1 : n, k = 1 : m) ‚àà Rmn ,     Œ≤ik = vec(Œ≤ijk , j = 1 : p) ‚àà Rp .      (11)
In the above, the columns of Yi are the coordinates of gradT œÜk (Œæi ) in the chosen basis of TŒæi M; Œ≤j
is the vector of regression coefficients corresponding to the effect of function gj , which form group j
in the multivariate group Lasso problem. The size of each group is mn. By comparing (6) and (7)
with (10) and (11) we see, first, the change in the expression of yik due to projection. We also note
that in the definition of Œ≤ijk the normalization is omitted. For the present, we will assume that the
dictionary functions g1:p have been appropriately normalized; how to do this is deferred to Section 5.
    To obtain the matrices Xi , we project the gradients of the dictionary functions gj onto the
tangent bundle, i.e.
                                     Xi = TiT ‚àáŒæ gj (Œæi )]j=1:p ‚àà Rd√óp .                               (12)
                                                     7

The FLasso objective function is
                                            n    m                             p
                                       1 XX                                Œª X
                         JŒª (Œ≤) =                   ||yik ‚àí Xi Œ≤ik ||2 + ‚àö        ||Œ≤j ||,            (13)
                                       2 i=1                               mn j=1
                                                k=1
an expression identical to (8), but in which yi , Xi denote the quantities in (10) and (12). To note
that JŒª (Œ≤) is invariant to the change of basis Ti . Inded, let TÃÉi = Ti Œì be a different basis, with
Œì ‚àà Rd√ód a unitary matrix. Then, yÃÉik = ŒìT yik , XÃÉi = ŒìT Xi , and ||yÃÉik ‚àí XÃÉi Œ≤||2 = ||yik ‚àí Xi Œ≤||2 .
The next section is concerned with estimating the gradients yik from the data.
4.2     Estimating the coordinate gradients by pull-back
Since œÜ is implicitly determined by a manifold embedding algorithm, the gradients of œÜk are in
general not directly available, and œÜk is known only through its values at the data points. We
could estimate these gradients naively from differences œÜk (Œæi ) ‚àí œÜk (Œæi0 ) between neighboring points,
but we choose a differential geometric method inspired by [LSW09] and [PM13], that pulls back
the differentials of œÜ1:m (Œæi ) from Rm to TŒæi M, thus enabling them to be compared in the same
coordinate system with the derivatives gradT gj .
    If the range of œÜ were Rm , then the gradient of a coordinate function œÜk would be trivially
equal to the k-th basis vector, and ‚àáœÜ œÜ = Id . If d < m, by projecting the rows of Im onto the
tangent subspace TœÜ(Œæi ) œÜ(M), we get gradTœÜ œÜ. We then pull back the resulting vectors to TŒæi M to
get gradT œÜ(Œæi ) ‚â° Yi , as defined in (10).
Pulling back gradœÜ œÜ = I into TŒæi M If the embedding induced by œÜ were an isometry, the
estimation of TœÜ(Œæi ) œÜ(M) could be performed by LocalPCA, and the pull-back following [PM13].
Here we do not assume that œÜ is isometric. The method we introduce exploits the fact that, even
when œÜ is not an isometry, the distortion induced by œÜ can be estimated and corrected [PM13].
More precisely, we make use of the RMetric algorithm described in Section |refsec:background to
estimate for each œÜ(Œæi ), the associated push-forward Riemannian metric, expressed in the coordinates
of Rm by Gi , a symmetric, semi-positive definite m √ó m matrix of rank d. Additionally, since the
theoretical rank of Gi equals d, the d principal eigenvectors of Gi represent (an estimate of) an
orthonormal basis of TœÜ(Œæi ) œÜ(M).
    We now apply (2) to the set of vectors ProjTŒæ M (Œæi0 ‚àí Œæi ) for i0 ‚àà Ni . Let
                                                            i
                  h                        i
            Ai = ProjTŒæ      M (Œæ i0 ‚àí Œæi )
                                              0
                                                    ‚àà Rd√óki     Bi = [œÜ(Œæi0 ) ‚àí œÜ(Œæi )]i0 ‚ààNi ‚àà Rm√óki (14)
                           i                 i ‚ààNi
and Yi = gradT œÜ(Œæi ) ‚àà Rm√ód as defined in the previous section. Then, (2) implies
                                                  ATi Yi ‚âà BiT Gi I.                                  (15)
The equality is approximate to first order because the ProjTŒæ M operator above is a first order
                                                                           i
approximation to the logarithmic map ; the error tends to 0 with the neighborhood radius [dC92]. If
A is full rank d, for ki ‚â• d, we can obtain Yi by least-squares as
                                              Yi = (Ai ATi )‚àí1 Ai BiT Gi .                            (16)
This equation recovers gradT œÜ(Œæi ) in the local basis Ti of TŒæi M.
                                                           8

    If we desired to obtain gradx iœÜ(Œæi ) in the global RD coordinates, it suffices to express the columns
of Ai as vectors in RD (or equivalently to apply the appropriate linear transformation to Yi ).
    Algorithm PullBackDPhi summarizes the above steps. For the estimation of Gi , the Laplacian
must be available. Note also that in obtaining (15) explicit projection on TœÜ(Œæi ) œÜ(M) is not necessary,
because any component orthogonal to this subspace is in the null space of Gi .
PullBackDPhi (basis Ti , local data Œûi , local embedding coordinates Œ¶i , Laplacian row Li,Ni ,
intrinsic dimension d
 1: Compute pushforward metric Gi ‚Üê RMetric(Li,Ni , Œ¶i , d)
 2: Project Ai = TiT (Œûi ‚àí Œæi 1T   ki ).
                                           |Ni |√óm
 3: Compute Bi = Œ¶i ‚àí œÜ(Œæi )1T      ki ‚àà R         .
                               T         T
 4: Solve linear system Ai Yi = Bi Gi as in (16)
 5: Output Yi
5      The full ManifoldLasso algorithm
We are now ready to combine the algorithms of Section 2 with the results of Sections 3 and 4 into
the main algorithm of this paper. Algorithm ManifoldLasso takes as input data D sampled near
an unknown manifold M, a dictionary G of functions defined on M (or alternatively on an open
subset of the ambient space RD that contains M) and an embedding œÜ(D) in Rm . The output of
ManifoldLasso is a set S of indices in G, representing the functions G that explain M.
ManifoldLasso (Dataset D, dictionary G, embedding coordinates œÜ(D), intrinsic dimension d,
kernel bandwidth N , neighborhood cutoff size rN , regularization parameter Œª)
 1:  Construct Ni for i = 1 : n; i0 ‚àà Ni iff ||Œæi0 ‚àí Œæi || ‚â§ rN , and local data matrices Œû1:n
 2:  Construct kernel matrix and Laplacian K, L ‚Üê Laplacian(N1:n , Œû1:n , N )
 3:  [Optionally compute embedding: œÜ(Œæ1:n ) ‚ÜêEmbeddingAlg(D, N1:n , m, . . .)]
 4:  for i = 1, 2, . . . n (Prepare gradients for group lasso) do
 5:    Compute basis Ti ‚ÜêLocalPCA(Œûi , Ki,Ni , d)
 6:    Compute ‚àáŒæ gj (Œæi ) for j = 1, . . . p
 7:    Project Xi ‚Üê TiT ‚àáŒæ g1:p
 8:    Compute Yi ‚ÜêPullBackDPhi(Ti , Œûi , Œ¶i , Li,Ni , Ki,Ni , d)
 9:  end for
10:  Œ≤, S ‚Üê FLasso(Xi:n , Yi:n , Œª)
11:  Output S
Computation The first two steps of ManifoldLasso are construction of the neighborhood
graph, and estimation of the Laplacian L. As shown in Section 2, L is a sparse matrix, hence
RMetric can be run efficiently by only passing values corresponding to one neighborhood at a time.
Note that in our examples and experiments, Diffusion Maps is our chosen embedding algorithm, so
the neighborhoods and Laplacian are already available, though in general this is not the case.
    The second part of the algorithm estimates the gradients and constructs matrices Y1:n , X1:n . The
gradient estimation runtime is O(qd2 +nd3 ) where q is the number of edges in the neighborhood graph,
                                                      9

using Cholesky decomposition-based solvers. Finally, the last step is a call to the GroupLasso,
which estimates the support S of œÜ. The computation time of each iteration in GroupLasso is
O(n2 m2 pd). For large data sets, one can perform the ‚Äúfor‚Äù loop over a subset I ‚äÇ [n] of the original
data while retaining the geometric information from the full data set. This replaces the n in the
computation time with the smaller factor |I|.
Normalization In Group Lasso, the columns of the X matrix corresponding to each group are
rescaled to have unit Euclidean norm. This ensures that the FLasso algorithm will be invariant to
rescaling of dictionary functions by a constant. Since any multiplication of gj by a non-zero constant,
simultaneously with dividing its corresponding Œ≤j by the same constant leaves the reconstruction
error of all y‚Äôs invariant, but affects the norm ||Œ≤j ||, rescaling will favor the dictionary functions
that are scaled by large constants. Therefore, the relative scaling of the dictionary functions gj can
influence the support S recovered.
    When the dictionary functions gj are defined on M, but not outside M. In this case we follow
the standard Group Lasso recipe. We calculate the normalizing constant
                                                n
                                             1X
                                     Œ≥j2 =         k gradT gj (Œæi )||2 .                            (17)
                                             n i=1
The above is the finite sample version of k gradT gj kL2 (M) , integrated w.r.t. the data density on M.
Then we set gj ‚Üê gj /Œ≥j which has the same effect as the normalization for FLasso in Section 3.
    In the case when the dictionary functions are defined on a neighborhood around M in RD , we
compute the normalizing constant with respect to ‚àáŒæ gj , that is
                                                  n
                                               1X
                                        Œ≥j2 =        k‚àáŒæ gj (Œæi )k2 .                               (18)
                                               n i=1
Then, once again, we set gj ‚Üê gj /Œ≥j . Doing this favors the dictionary functions whose gradients
are parallel to the manifold M, and penalizes the gj ‚Äôs which have large gradient components
perpendicular to M.
Tuning As the tuning parameter Œª is increased, the cardinality of the support decreases. Tuning
parameters are often selected by cross-validation in Lasso-type problems, but this is not possible
in our unsupervised setting. We base our choice of Œª on matching the cardinality of the support
to d. The recovery results proved in Section 7.2 state that for a single chart, s = d functionally
independent dictionary functions suffice.
    In manifolds which cannot be represented by a single chart, the situation is more complicated,
and it depends on the topology of the co-domains of the functions gj . For example, in the toluene
data presented in Section 8, the manifold is a circle. This cannot be covered by a single chart, but it
can be parametrized by a single function in the dictionary, the angle of the bond torsion in Figure
5. Another case when s > d is the case when no single dictionary function can parametrize the
manifold. When G is not a functionally independent set, it is possible that the parametrization of
M by G is not unique. Hence, in general, the support size s may be equal to d or larger.
5.1     Variants and extensions
The approach utilized here can be extended in several interesting ways. First, our current approach
explains the embedding coordinates œÜ produced by a particular embedding algorithm. However, the
                                                    10

same approach can be used to directly explain the tangent subspace of M, independently of any
embedding.
    Second, one could set up FLasso problems that explain a single coordinate function. In general,
manifold coordinates do not have individual meaning, so it will not be always possible to find a good
explanation for a single œÜk . However, Figure 6 shows that for the ethanol molecule, whose manifold
is a torus, there exists a canonical system of coordinates in which each coordinate is explained by
one torsion.
    Third, manifold codomains of G can be used, eliminating certain issues of continuity and
chart-counting.
    Finally, when the gradients of the dictionary functions are not analytically available, they can
also be estimated from data.
6     Related work
To our knowledge, ours is the first solution to estimating a function f as a non-linear sparse
combination of functions in a dictionary. Below we cite some of the closest related work.
    Group Lasso has been widely used in sparse regression and variable selection. In [OPV+ 14] the
Functional Sparse Shrinkage and (FuSSO) is introduced to solve similar problem in Euclidean space
setting. FuSSO uses Group Lasso to recover F as a sparse linear combination of functions from an
infinite dictionary given implictly by the decomposition over the Fourier basis. More importantly,
this method imposes an additive model between the response and the functional covariates, which is
not true in our method.
    Gradient Learning [YX12]is also a similar work trying to recover non-zero partial derivatives via
Group Lasso type regression as methods of variable selection and dimension reduction. Their work
does not have functional covariate like us as input. Moreover, the goal of [YX12] is not explaining
the coordinates but instead predicting a response given the covariates.
    The Active Subspace method of [CDW14] uses the information in the gradient to discover a
subspace of maximum variation of f . This subspace is given by the principal subspace of the
matrix C = EœÅ [‚àáf ‚àáf T ], where œÅ is a weighting function averaging over a finite or infinite set of
points in the domain of f . While this method uses the gradient information, it can only find a
global subspace, which would not be adequate for function composition, or for functions defined on
non-linear manifolds.
    The work of [BPK16] is similar to ours in that it uses a dictionary. The goal is to identify the
functional equations of non-linear dynamical systems by regressing the time derivatives of the state
variables on a subset of functions in the dictionary, with a sparsity inducing penalty. The recovered
functional equation is linear in the dictionary functions, hence any non-linearity in the state variables
must be explicitly included in the dictionar. On the other hand, when the functional equation can
be expressed as a sum of dictionary functions, then the system is completely identified.
    With respect to parametrizing manifolds, the early work of [SR03, TR02] (and references therein)
proposes parametrizing the manifold by finite mixtures of local linear models, aligned in a way that
provides global coordinates, in a way reminiscent of LTSA[ZZ04].
    A point of view different from ours views a set of d eigenvectors of the Laplace-Beltrami operator
‚àÜM as a parametrization of M. Hence, the Diffusion Maps coordinates could be considered such
a parametrization [CL06, CLL+ 05, Gea12]. With [MN17] it was shown that principal curves and
surfaces can provide an approximate manifold parametrization. Our work differs from these in two
ways (1) first, obviously, the explanations we obtain are endowed with the physical meaning of
                                                   11

the domain specific dictionaries, (2) less obviously, descriptors like principal curves or Laplacian
eigenfunctions are generally still non-parametric (i.e exist in infinite dimensional function spaces),
while the parametrizations by dictionaries we obtain (e.g the torsions) are in finite dimensional spaces.
[DTCK18] tackles a related problem: choosing among the infinitely many Laplacian eigenfunctions d
which provide a d-dimensional parametrization of the manifold; the approach is to solve a set of Local
Linear Embedding [RS00] problems, each aiming to represent an eigenfunction as a combination of
the preceding ones.
7     Uniqueness and recovery results
7.1      Functional dependency and uniqueness of representation
Here we study the conditions under which f = h ‚ó¶ gS is uniquely represented over a dictionary G
that contains gS . Not surprisingly, we will show that these are functional (in)dependency conditions
on the dictionary.
    We first study when a group of functions on an open U ‚äÇ Rd can be represented with a subset
of functionally independent functions. The following lemma implies that if a group of non-full-rank
smooth functions has a constant rank in a neighborhood, then it can be showed that locally we can
choose a subset of these functions such that the others can be smoothly represented by them. This
is a direct result from the constant rank theorem.
Lemma 2 (Remark 2 after Zorich[Zor04] Theorem 2 in Section 8.6.2) Let f : U ‚Üí Rm
be a mapping defined in an neighborhood U ‚äÇ Rd of a point x ‚àà Rd . Suppose f ‚àà C ` and the rank
of the mapping f is k at every point of a neighborhood U and k < m. Moreover assume that the
principal minor of order k of the matrix Df is not zero. Then in some neighborhood of x ‚àà U there
exist m ‚àí k C ` functions g i , i = k + 1, ¬∑ ¬∑ ¬∑ , m such that
      f i (x1 , x2 , ¬∑ ¬∑ ¬∑ , xd ) = g i (f 1 (x1 , x2 , ¬∑ ¬∑ ¬∑ , xd ), f 2 (x1 , x2 , ¬∑ ¬∑ ¬∑ , xd ), ¬∑ ¬∑ ¬∑ , f k (x1 , x2 , ¬∑ ¬∑ ¬∑ , xd )) (19)
    Applying this lemma we can construct a local representation of a subset in gS . The following
classical result in differential geometry lets us expand the above lemma beyond local to global.
    We start with a definition. A smooth partition of unity subordiante to {UŒ± } is an indexed family
(œÜŒ± )Œ±‚ààA of smooth functions œÜŒ± : M ‚Üí R with the following properties:
   1. 0 ‚â§ œÜŒ± (x) for all Œ± ‚àà A and all x ‚àà M ;
   2. supp œÜŒ± ‚äÇ UŒ± for each Œ± ‚àà A;
   3. Every Œæ ‚àà M has a neighborhood that intersects supp œÜŒ± for only finitely many values of Œ±;
      P
   4.      Œ±‚ààA œÜŒ± (x) = 1 for all Œæ ‚àà M.
Lemma 3 (John 2013[Lee03] Theorem 2.23) Suppose M is a smooth manifold, and {UŒ± }Œ±‚ààA
is any indexed open cover of M . Then there exists a smooth partition of unity subordinate to {UŒ± }
    Now we state our main results.
                                                                       12

Theorem 4 Assume G and gS are defined as in Section 3, with g1:p are C ` functions in open set
U ‚äÇ Rd and let S 0 ‚äÇ [p], S 0 6= S, |S 0 | < d be another subset of C ` functions. Then there exists a C `
                    0
mapping œÑ : Rs ‚Üí Rs on U such that gS = œÑ ‚ó¶ gS 0 iff
                                                            
                                                       DgS
                                              rank               = rank DgS 0 on U                                       (20)
                                                       DgS 0
Proof If DgS 0 is not full rank on U , we replace S 0 with a subset of S 0 so that DgS 0 is full rank
globally on U . The existence of such subsets are guaranteed by step-wise eliminating functions in
gS 0 , which will result in a zero matrix in r.h.s if such a subset does not exist and thus leads to a
contradiction. Since s0 = |S 0 | ‚â§ d, rank DgS 0 = s0 . Let
                                                                                
                                                                       gS 0 (Œæ)
                                                      gS ‚à™S (Œæ) =
                                                          0                                                              (21)
                                                                       gS (Œæ)
and DgS 0 ‚à™S denote the l.h.s. matrix in (20). When the rank of DgS‚à™S 0 equals the rank of
DgS0 , according to Lemma 2, there exists some neighborhood Ux ‚àà Rd of x and C ` functions
œÑxi , i = s0 + 1, s0 + 2, ¬∑ ¬∑ ¬∑ , s0 + s
                 gSi 0 ‚à™S (Œæ) = œÑxi (g1 (Œæ), ¬∑ ¬∑ ¬∑ , gs0 (Œæ)), for i = s0 + 1, s0 + 2, ¬∑ ¬∑ ¬∑ , s0 + s, Œæ ‚àà Ux            (22)
Here we should notice that œÑxi is defined only on a neighborhood of x. We denote such a neighborhood
by Ux and then since this holds for every x ‚àà U , therefore we can find an open cover {Ux } of the
original open set U . Since each open set in Rd is a manifold, the classical result of partition of unity
in Lemma 3 holds, that U admits a smooth partition of unity subordinate to the cover {Ux }. We
denote this partition of unity by œÜx (¬∑).
      Hence we can define
                                                 X
                                      œÑ i (Œæ) =        œÜx (Œæ)œÑxi (gj1 (Œæ), ¬∑ ¬∑ ¬∑ , gj|S‚à™S0 | (Œæ)),                       (23)
                                                   x
where the functions in gS‚à™S 0 are taken without repetition. For each x ‚àà U , the product œÜx œÑxi is C `
on the neighborhood Ux . According to the properties of partition of unity, this is a locally finite
sum, which means that we do not have to deal with the problem of convergence. Also this will be a
C ` function.
      Therefore, globally in U we have
                    i               i                                       0         0            0
                  gS‚à™S   0 (Œæ) = œÑ (g1 (Œæ), ¬∑ ¬∑ ¬∑ , gs0 (Œæ)), for i = s + 1, s + 2, ¬∑ ¬∑ ¬∑ , s + s, Œæ ‚àà U                 (24)
      Now we prove the converse implication. If rank DgS‚à™S 0 > s0 , then there is j ‚àà S, so that
Dgj 6‚àà rowspan DgS 0 . Pick Œæ 0 ‚àà U such that Dgj (Œæ 0 ) 6= 0; such an Œæ 0 must exist because otherwise it
will be in rowspan DgS 0 . By the theorem‚Äôs assumption, DgS = DœÑ DgS 0 . This implies that (DgS )T
is in rowspan(DgS 0 )T for any Œæ. But this is impossible at Œæ 0 .
      A direct corollary of this theorem is that in a single chart scenario, we can use exactly d
functionally independent functions in the dictionary to give the explanation.
Corollary 5 Let G, gS defined as before. M is a smooth manifold with dimension d embedded
in RD . Suppose that œÜ : M ‚äÇ RD ‚Üí Rm is also an embedding of M and has a decomposition
œÜ(Œæ) = h ‚ó¶ gS (Œæ) for each Œæ ‚àà M. If there is a C ` diffeomorphism œï : Rd ‚Üí M, then there is a
subset of functions gS 0 ‚äÇ gS with |S 0 | = d such that for some C ` function e                                       h ‚ó¶ gS 0
                                                                                                    h we can find œÜ = e
on each Œæ ‚àà M
                                                                  13

Proof Since œï is a diffeomorphism, we can write uniquely Œæ = œï(Œ∑) for each Œæ ‚àà M. And also
œï‚àí1 (M) = Rd . Then the original decomposition of œÜ(Œæ) = h ‚ó¶ gS on each Œæ ‚àà M is actually
                                           œÜ ‚ó¶ œï(Œ∑) = h ‚ó¶ gS ‚ó¶ œï(Œ∑)                                            (25)
and geS = gS ‚ó¶ œï is a C ` mapping Rd ‚Üí Rs . Now we choose S 0 ‚äÜ S such that De                gS 0 is full rank and
rank De  gS = rank De gS 0 . From                      
                                                 DegS
                                         rank             = rank De gS 0                                       (26)
                                                De gS 0
and the previous theorem, we know that there exists a C ` function œÑ such that geS = œÑ ‚ó¶ geS 0 on each
Œ∑ ‚àà Rd . Finally let eh = h ‚ó¶ œÑ then
               œÜ(Œæ) = œÜ ‚ó¶ œï ‚ó¶ œï‚àí1 (Œæ) = h ‚ó¶ geS ‚ó¶ œï‚àí1 (Œæ) = h ‚ó¶ œÑ ‚ó¶ geS 0 ‚ó¶ œï‚àí1 (Œæ) = e    h ‚ó¶ gS 0 (Œæ)        (27)
holds for each Œæ ‚àà M. Now we determine the number of functions in S 0 . On one hand we can select
S 0 so that De gS 0 = DgS 0 Dœï is full rank, hence |S 0 | = rank DgS 0 = s0 ‚â§ d. On the other hand, since
œÜ is an embedding, which means that œÜ(M) is also an d‚àídimension manifold, we could consider
another diffeomorphism œà : œÜ(M) ‚Üí Rd such that
                                        œà ‚ó¶ œÜ ‚ó¶ œï(Œ∑) = œà ‚ó¶ e h ‚ó¶ geS 0 (Œ∑)                                     (28)
is Rd ‚Üí Rd and one-to-one.Therefore the Jacobian of l.h.s. is of rank d, which should be less than
the rank of each of the r.h.s. differentials. Therefore s0 ‚â• d. To combine these results we have s0 = d.
     We say that S itself is functionally independent when that every g ‚àà GS is functionally independent
of GS \ {g}. Corollary 5 states that S can be uniquely recovered iff S is a functionally independent
subset of G.
     In a finite sample setting, Theorem 4 states that S and S 0 are equivalent explanations for f
whenever (20) holds on open sets around the sample points. For example, Theorem 4 does not have
to hold globally.
     For low dimensional settings, i.e. when s ‚â• d, the Theorem implies that in general there will be
many equivalent explanations of f in G. P      Assuming no noise, the solution to (13) will be the subset
                                           Œª     p
S that minimizes the penalty term ‚àömn            j=1 kŒ≤j k on the data set.
7.2      Recovery guarantees for FLasso
We now give recovery guarantees for the (FLasso) problem. The guarantees are deteriministic,
but they depend on the noise sample covariance, hence they can lead to statistical guarantees
holding w.h.p. in the usual way. The first theorem deals with support recovery, proving that all
coefficients outside the support are zeroed out, under conditions that depend only on the dictionary,
the true support S and the noise. The second result completes the previous with error bounds on
the estimates Œ≤ÃÇj , assuming an additional condition on the magnitude of the true Œ≤j coefficients.
Since these coefficients are partial derivatives w.r.t. the dictionary functions, the condition implies
that the the dependence on each function must be strong enough to allow the accurate estimation
of the partial derivatives.
     Introduce the following quantitites
                              S-incoherence in G    ¬µ =         max          |xTij xij 0 |                     (29)
                                                           i=1:n,j‚ààS,j 0 6‚ààS
                                                        14

                                                                                 X n
                                          noise level œÉ defined by                    ||i ||2 = ndœÉ 2 ,                        (30)
                                                                                 i=1
and internal colinearity ŒΩ, defined as follows. Let
                              Œ£i = xTij xij 0 j,j 0 ‚ààS = XTiS XiS ,
                                                   
                                                                                     and Œ£ = diag{Œ£1:n }                        (31)
and denote by ŒΩ ‚â§ 1 the maximum eigenvalue of Œ£‚àí1 ; a smaller ŒΩ means that the xij gradients are
closer to being orthogonal at each datapoint i.
Theorem 6 (Support recovery) Assume that equation (4) holds, and that ||xij‚àö|| = 1 for all
                                                                                                              ‚àö
i = 1 : n, j = 1 : p. Denote by Œ≤ÃÑ the solution of (5) for some Œª > 0. If ¬µŒΩ s + œÉ Œªnd < 1, then
Œ≤ÃÑij = 0 for j 6‚àà S and all i = 1, . . . n.
     Proof We structure equation (4) in the form
                                     ¬Ø Œ≤ÃÑ ‚àó + ¬Ø with y = [y ]                       nd                       np
                            y = XÃÑ                                   i i=1:n ‚àà R , Œ≤ÃÑ = [Œ≤i ]i=1:n ‚àà R ,                        (32)
XÃÉij ‚àà Rnd is obtained from xij by padding with zeros for the entries not in the i-th segment,
 ¬Ø = [[XÃÉ ]                                nd√ónp             ¬Ø = [XÃÉ ]                       nd√ón                            ¬Ø that
XÃÑ j           tj j=1:p ]i=1:n ‚àà R                , and XÃÑ     j           ij i=1:n ‚àà R            collects the colums of XÃÑ
correspond to the j-th dictionary entry. Note that
                               XÃÉijT
                                     XÃÉij 0 = xTij xij 0      and XÃÉij     T
                                                                             XÃÉi0 j 0 = 0 whenever i 6= i0 .                    (33)
The proof is by the Primal Dual Witness method, following [ESJ+ 17, OWJ11]. It can be shown
[ESJ+ 17, Wai09] that Œ≤ÃÑ is a solution to (FLasso) iff, for all j = 1 : p,
     ¬Ø T XÃÑ
    XÃÑ    ¬Ø (Œ≤ÃÑ ‚àí Œ≤ÃÑ ‚àó ) ‚àí XÃÑ ¬Ø T ¬Ø + Œªz = 0 ‚àà Rn with z = Œ≤j if Œ≤ 6= 0 and ||z || < 1 otherwise. (34)
      j                        j            j                            j                    j            j
                                                                                ||Œ≤j ||
The matrix XÃÑ             ¬Ø is a diagonal matrix with n blocks of size 1 √ó p, hence the first term in (34)
                     ¬Ø T XÃÑ
                      j
becomes
                                                      [xTij Xi (Œ≤ÃÑi: ‚àí Œ≤ÃÑi:‚àó )]i=1:n ‚àà Rn .                                     (35)
                ¬Ø T ¬Ø = [xT w ]                  n
Similarly XÃÑ     j           ij i i=1:n ‚àà R .
     We now consider the solution Œ≤ÃÇ to problem (5) (FLasso) under the additional constraint that
Œ≤tj 0 = 0 for j 0 6‚àà S. In other words, Œ≤ÃÇ is the solution we would obtain if S was known. Let zÃÇ be the
optimal dual variable for this problem, and let zÃÇS = [zÃÇj ]j‚ààS .
     We will now complete zÃÇS to a z ‚àà Rnp so that the pair (Œ≤ÃÇ, z) satisfies (34). If we succeed, then
we will have proved that Œ≤ÃÇ is the solution to the original FLasso problem, and in particular that
the support of Œ≤ÃÇ is included in S.
     From (34) we obtain values for zj when j 6‚àà S.
                                                            ‚àí1 ¬Ø T h ¬Ø T                        i
                                                 zj =            XÃÑj XÃÑ (Œ≤ÃÇ ‚àí Œ≤ÃÑ ‚àó ) ‚àí ¬Ø .                                     (36)
                                                             Œª
In the same time, if we consider all j ‚àà S, we obtain from (34) that XÃÑ                             ¬Ø = [XÃÑ¬Ø ]     (here the vectors
                                                                                                     S       j j‚ààS
Œ≤S , Œ≤S ‚àó and all other vectors are size ns, with entries sorted by j, then by i).
                                                     ¬Ø
                                                ¬Ø T XÃÑ               ‚àó        ¬Ø T ¬Ø + ŒªzÃÇ = 0.
                                              XÃÑ S S (Œ≤ÃÇS ‚àí Œ≤S ) ‚àí XÃÑS                   S                                     (37)
                                                                        15

Solving for Œ≤ÃÇS ‚àí Œ≤S‚àó in (37), we obtain
                                                                                                          
                          Œ≤ÃÇS ‚àí Œ≤S‚àó = (XÃÑ     ¬Ø T XÃÑ
                                                   ¬Ø ‚àí1 XÃÑ     ¬Ø T ¬Ø ‚àí ŒªzÃÇ        = Œ£‚àí1 XÃÑ   ¬Ø T ¬Ø ‚àí ŒªzÃÇ .
                                                S S)             S           S                 S           S                       (38)
After replacing the above in (36) we have
          ‚àí1 ¬Ø T h ¬Ø ‚àí1 ¬Ø T                    ¬Ø Œ£‚àí1 ŒªzÃÇ ‚àí ¬Ø = XÃÑ
                                                                   i
                                                                                  ¬Ø Œ£‚àí1 zÃÇ + 1 XÃÑ
                                                                             ¬Ø T XÃÑ                  ¬Ø T (I ‚àí XÃÑ¬Ø Œ£‚àí1 XÃÑ   ¬Ø T )¬Ø
   zj =        XÃÑj XÃÑS Œ£ XÃÑS w ‚àí XÃÑ             S           S                 j    S       S                      S         S . (39)
           Œª                                                                                    Œª j
Finally, by noting that Œ† = I ‚àí XÃÑ            ¬Ø Œ£‚àí1 XÃÑ   ¬Ø T is the projection operator on the subspace span(XÃÑ                   ¬Ø )‚ä• ,
                                                S          S                                                                       S
we obtain that
                                                 ¬Ø T XÃÑ
                                                      ¬Ø )Œ£‚àí1 zÃÇ +        1 ¬ØT
                                      zj = (XÃÑ          S          S       XÃÑ Œ†¬Ø    , for j 6‚àà S.                                 (40)
                                                  j
                                                                         Œª j
We must show that ||zj || < 1 for j 6‚àà S. To bound the first term, we note that XÃÑ                                ¬Ø is n √ó ns, block
                                                                                                             ¬Ø T XÃÑ
                                                                                                              j    S
diagonal, with blocks of size 1√ós, and with all non-zero entries bounded in absolute                              value   by ¬µ. Hence,
for any vector v = [vi ]i=1:n ‚àà Rns , ||XÃÑ                   ¬Ø v||2 = ||[(xT x )v ]
                                                        ¬Ø T XÃÑ                                     ||2
                                                                                                            Pn           T         2
    Pn                                                    j    S                  ij  iS  i i=1:n       =      i=1 ‚àö ij xiS )vi || ‚â§
                                                                                                                    ||(x
  2              2        2     2                                    ‚àí1                            ‚àí1
¬µ     i=1 ||vi || = ¬µ ||v|| . But in our case v = Œ£                     zÃÇS , hence ||v|| ‚â§ ||Œ£ ||||zÃÇS || = ŒΩ s.
                                                                 ¬Ø
    To bound the second term, we note that ||XÃÑj || = ||xij || = 1, andPthat ||Œ†¬Ø                         || ‚â§ ||¬Ø|| because Œ† is a
                                                                                               n
projection. Hence, the norm squared of this term is bounded above i=1 ||i ||2 ||xij ||2 /Œª2 = ndœÉ 2 /Œª2 .
    Replacing these bounds in (40) we obtain that
                                                                                            ‚àö
                                 ¬Ø  T ¬Ø    ‚àí1               1 ¬ØT                   ‚àö     œÉ dn
                   ||zj || ‚â§ ||XÃÑj XÃÑS Œ£ zÃÇS || + || XÃÑj Œ†¬Ø           || ‚â§ ¬µŒΩ s +                 for any j 6‚àà S.                 (41)
                                                            Œª                                Œª
                                                                                                                                     
Theorem 7 Assume that equation (5) holds, and that ||xij || = 1 for all i = 1 : n, j = 1 : p.                                 ‚àö Denote
                                                                                          ‚àö
by Œ≤ÃÇ the solution to problem (FLasso)     ‚àö           for‚àösome Œª > 0. If (1) ¬µŒΩ s < 1, (2) Œª = (c ‚àí 1)œÉ dn with
             1‚àö                  ‚àó
c > 1 + 1‚àí¬µŒΩ       s
                     , (3)   ||Œ≤ j || > cœÉ     dn(1   +     s) for all j ‚àà S, then the support S is recovered exactly
               ‚àó
                            ‚àö           ‚àö
and ||Œ≤ÃÇj ‚àí Œ≤j || < cœÉ dn(1 + s) for all j ‚àà S.
    Proof According to Theorem 6, Œ≤ÃÇj = 0 for j 6‚àà S. It remains to prove the error bound for j ‚àà S.
According to Lemma V.2 of [ESJ+ 17], for any j ‚àà S,
              ||Œ≤ÃÇj ‚àí Œ≤j‚àó || ‚â§ ||XÃÑ      ¬Ø T ¬Ø|| + ||XÃÑ¬Ø T ¬Ø|| + Œª(1 + ‚àös)                                                       (42)
                                           j              S
                                                                                    ‚àö          ‚àö
                                  ‚â§ (||XÃÑ ¬Ø || + ||XÃÑ  ¬Ø ||)||¬Ø || + œÉ(c ‚àí 1) dn(1 + s)                                           (43)
                                             j           S
                                               ‚àö ‚àö                           ‚àö           ‚àö              ‚àö            ‚àö
                                  ‚â§ (1 + s)œÉ dn + œÉ(c ‚àí 1) dn(1 + s) = cœÉ dn(1 + s)                                                (44)
                            ‚àö            ‚àö
Hence, if ||Œ≤j‚àó || > cœÉ dn(1 + s), Œ≤ÃÇj 6= 0 and the support is recovered.                                                            
                                                ‚àö             ‚àö                      ‚àö
                                               œÉ dn                      1
    Note that for this value of c, Œª + ¬µŒΩ s = c‚àí1 + ¬µŒΩ s < 1.
    The assumptions we make are not probabilistic, while those in [ESJ+ 17] are. In spite of this
difference, the assumptions in our work bear comparison with [ESJ+ 17]. Specifically, our conditions
only concern the internal collinearity, for functions j ‚àà S, while [ESJ+ 17] requires bounds on the
intra-block coherence (the equivalent condition) for the whole dictionary. Second, we only require
incoherence between gS and the functions not in S; this property is most similar to the inter-block
coherence bound from [ESJ+ 17]. In other words, in a deterministic framework incoherence between
functions outside the support is not necessary. This conclusion can be extended in a straightforward
manner to other group Lasso recovery proofs.
                                                                     16

8       Experimental results
8.1      FLasso on synthetic data
We illustrate the effectiveness of FLasso for sparse functional support recovery where the output
function is composed of products of dictionary functions. In such situations, methods which only
consider linear combinations of dictionary functions will not be able to recover the support. The
ability to distinguish such multiplied signals is therefore a key advantage of our approach. These
examples have analytically known true solutions in the absence of noise, and also show the robustness
of FLasso to noise.
8.1.1      Example 1
Consider the following case where D = 4 and m = 1 and Œæ is drawn from a standard normal
distribution with covariance .2.
                                               f (Œæ) = Œæ1 Œæ2
                                                ‚àáf = [Œæ2 , Œæ1 , 0, 0]
We consider the algorithm output on the following two dictionaries.
                                           G1 : Œæ 7‚Üí {Œæ1 , Œæ2 , Œæ3 , Œæ4 }
                                                     Ô£Æ                  Ô£π
                                                      1 0 0 0
                                                     Ô£Ø0 1 0 0Ô£∫
                                            ‚àáG1 = Ô£Ø  Ô£∞0 0 1 0Ô£ª
                                                                        Ô£∫
                                                      0 0 0 1
                                           G2 : Œæ 7‚Üí {Œæ1 , Œæ2 , Œæ1 Œæ2 , 0}
                                                     Ô£Æ                    Ô£π
                                                       1 0 0 0
                                                     Ô£Ø 0 1 0 0Ô£∫
                                            ‚àáG2 = Ô£Ø  Ô£∞Œæ2 Œæ1 0 0Ô£ª
                                                                          Ô£∫
                                                       0 0 0 0
For the first dictionary, the only loss zero partials are ‚àÇh            ‚àÇg = [Œæ2 , Œæ1 , 0, 0], while for the second
              ‚àÇh1                         ‚àÇh2
dictionary, ‚àÇg = [x2 , x1 , 0, 0] and ‚àÇg = [0, 0, Œ≥3 , 0] both result in loss-zero solutions. However,
       k1,2 < k ‚àÇh
                                                              R ‚àÇh2
k ‚àÇh                                                             k ‚àÇg k1,2 ¬µ(dx) = Œ≥3 = (x22 + x21 )1/2 ¬µ(dx) =
                                                                                              R
                ‚àÇg k1,2R for all distributions of x, since
     2            1
R ‚àÇg‚àÇh1
  k ‚àÇg k1,2 ¬µ(dx) = (x2 + x1 )¬µ(dx) and 1‚àínorm is bounded by 2‚àínorm. . Figure 2 shows that
FLasso recovers the correct support in both dictionaries.
                                                        17

Figure 2: Regularization path of FLasso for Example 1 with 100 points. The first row shows support
recovery over a range of Œª values for dictionary G1 , and the second for G2 . On the left are results with no
noise, and on the right, independent Gaussian noise with œÉ 2 = 0.01 was added to the differentials of g and f .
8.1.2    Example 2
This example shows that FLasso is effective at selecting between more complex functional covariates.
As in the previous example, the function is a product of two dictionary functions. Œæ is drawn from a
standard normal distribution with covariance .2 and d = 8, 20. We have experimented with other
distributions and found that the sampling of Œæ has little or no effect, as long as it is not restricted to
specific low dimensional sets. Let
       f = g1 gd+2 = sin(Œæ1 + Œæ2 )(Œæ32 + Œæ52 )
     ‚àáf = [cos(Œæ1 + Œæ2 )(Œæ32 + Œæ52 ), cos(Œæ1 + Œæ2 )(Œæ32 + Œæ52 ), 2Œæ3 sin(Œæ1 + Œæ2 ), 0, 2Œæ5 sin(Œæ1 + Œæ2 ), 0, . . . ]
      G1 = {gk = sin(Œæk + Œæk+1 ), k = 1 : d ‚àí 1}
      G2 = {gd‚àí1+k = Œæk2 + Œæk+22
                                   , k = 1 : d ‚àí 2}
       G = G1 ‚à™ G2 .
Here, we have not included ‚àáG for brevity, but it can be readily derived.
                                                        18

Figure 3: Regularization path of FLasso for Example 2 with 100 points. The first row shows support
recovery over a range of Œª values for d = 8, and the second for d = 20. To produce the results on the left,
independent Gaussian noise with œÉ 2 = 0.05 was added to the differentials of g and f , while on the right
œÉ 2 = 0.1.
    FLasso coefficients for the true support are highest. However, one of the elements of the true
support decays rapidly along the solution path, so our variable selection procedure is not quite
robust enough to detect the functional support.
8.2     Manifold FLasso on synthetic data
8.2.1    Articulated ethanol skeleton
This simulation is on an ethanol skeleton with simplified behavior. This skeleton articulates in a
full rotation around the C ‚àí C and C ‚àí O bonds. Configurations are distributed uniformly around
these two angles of rotation. Thus, each configuration is determined by angles of rotation around
the C ‚àí C and C ‚àí O bonds, and the noise (independent Gaussian with œÉ 2 = .01) that is added to
the position of each atom. The learned manifold is a twisted torus explained by these two angles.
Our dictionary consists of four torsions, two which correspond to the articulated angles, and two
corresponding to noise. FLasso is able to detect which of these torsions explain the embedding
coordinates with respect to the manifold.
                                                    19

  A                                                                    B
  C
Figure 4: A) The learned manifold on the entire data set colored by the first angle of rotation. B) A radial
slice of the learned manifold colored by the other angle of rotation. C) Regularization path of FLasso for 8
repeats of 200 randomly selected points. The first two embedding coordinates are explained by torsion A
(corresponding to A), while the last coordinate is explained by torsion B (corresponding to B). The final
panel corresponds to the combined norm used for variable selection.
8.3      Manifold FLasso on molecular dynamics data
These experiments are based off of molecular dynamics simulations of two small molecules. In
molecular dynamics simulations, individual atoms are moved according to the interatomic force
fields acting upon them. Each data point corresponds to a position along such a trajectory. We
                                                     20

apply a manifold learning algorithm to identify the low-dimensional manifold these trajectories
define, seek an explanation of this manifold in terms of bond torsions. A major complicating factor
in this is that the true configuration space is R3 \SE(3) (the 3 ‚àí D special Euclidean group), since
configurations which differ by rotations and translations (but not mirror images) are equivalent. To
respect this symmetry, we parametrize each configuration by the angles of the triangles formed by
each combination of three atoms. Torsions are computed with respect to the angular representation.
8.3.1    Toluene
The toluene molecule has one rotational degree of freedom. The manifold learning algorithm therefore
discovers that the data manifold is a circle. Out of a functional dictionary consisting of the putatively
significant dihedral angle and several other dihedral angles, FLasso then identifies the bond torsion
that explains the circle.
Figure 5: Regularization path of FLasso for molecular dynamics simulation of toluene with 3 repeats
selecting 25 random points out of 50000 used to learn the manifold.
8.3.2    Ethanol
The ethanol molecule has two rotational degree of freedom. The manifold learning algorithm
therefore discovers that the data manifold is a torus. Out of a functional dictionary consisting of
the two putatively significant dihedral angles and two other dihedral angles, FLasso then identifies
the bond torsions that explain this torus.
                                                   21

Figure 6: Regularization path of FLasso for molecular dynamics simulation ethanol with 8 repeats of
selecting 200 random points out of 50000 used to learn the manifold.
References
[BN02]      M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
            clustering. In Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002.
            MIT Press.
[BPK16] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations
            from data by sparse identification of nonlinear dynamical systems. Proceedings of the National
            Academy of Sciences, 113(15):3932‚Äì3937, 2016.
[CDW14] P. Constantine, E. Dow, and Q. Wang. Active subspace methods in theory and practice:
            Applications to kriging surfaces. SIAM Journal on Scientific Computing, 36(4):A1500‚ÄìA1524,
            2014.
[CL06]      R. R. Coifman and S. Lafon. Diffusion maps. Applied and Computational Harmonic Analysis,
            30(1):5‚Äì30, 2006.
[CLL+ 05] R. R. Coifman, S. Lafon, A. Lee, Maggioni, Warner, and Zucker. Geometric diffusions as a tool
            for harmonic analysis and structure definition of data: Diffusion maps. In Proceedings of the
            National Academy of Sciences, pages 7426‚Äì7431, 2005.
[CNO00] C. Clementi, H. Nymeyer, and J.N. Onuchic. Topological and energetic factors: what determines
            the structural details of the transition state ensemble and ‚Äúen-route‚Äù intermediates for protein
            folding? an investigation for small globular proteins. Journal of molecular biology, 2000. says
            topology (of protein) more important than energy wells.
[dC92]      Manfredo do Carmo. Riemannian Geometry. Springer, 1992.
[DTCK18] Carmeline J Dsilva, Ronen Talmon, Ronald R Coifman, and Ioannis G Kevrekidis. Parsimonious
            representation of nonlinear dynamical systems through manifold learning: A chemotaxis case
            study. Appl. Comput. Harmon. Anal., 44(3):759‚Äì773, May 2018.
                                                       22

[ESJ+ 17] M.K. Elyaderani, S.Jain, J.M.Druce, S.Gonella, and J.D.Haupt. Improved support recov-
          ery guarantees for the group lasso with applications to structural health monitoring. CoRR,
          abs/1708.08826, 2017.
[Gea12]   C W Gear. Parameterization of non-linear manifolds. August 2012.
[HAvL05] Matthias Hein, Jean-Yves Audibert, and Ulrike von Luxburg. From graphs to manifolds -
          weak and strong pointwise consistency of graph laplacians. In Learning Theory, 18th Annual
          Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30, 2005, Proceedings,
          pages 470‚Äì485, 2005.
[HAvL07] Matthias Hein, Jean-Yves Audibert, and Ulrike von Luxburg. Graph laplacians and their
          convergence on random neighborhood graphs. Journal of Machine Learning Research, 8:1325‚Äì
          1368, 2007.
[KvL15]   Matth√§us Kleindessner and Ulrike von Luxburg. Dimensionality estimation without distances.
          In AISTATS, 2015.
[Lee03]   John M. Lee. Introduction to Smooth Manifolds. Springer-Verlag New York, 2003.
[LSW09]   Chuanjiang Luo, Issam Safa, and Yusu Wang. Approximating gradients for meshes and point
          clouds via diffusion metric. Comput. Graph. Forum, 28(5):1497‚Äì1508, July 2009.
[MN17]    Kitty Mohammed and Hariharan Narayanan. Manifold learning using kernel density estimation
          and local principal components analysis. arxiv, 1709.03615, 2017.
[NC17]    Frank No√© and Cecilia Clementi. Collective variables for the study of long-time kinetics from
          molecular trajectories: theory and methods. Curr. Opin. Struct. Biol., 43:141‚Äì147, April 2017.
[OPV+ 14] Junier Oliva, Barnabas Poczos, Timothy Verstynen, Aarti Singh, Jeff Schneider, Fang-Cheng
          Yeh, and Wen-Yih Tseng. FuSSO: Functional Shrinkage and Selection Operator. In International
          Conference on AI and Statistics(AISTATS), 2014.
[OWJ11]   Guillaume Obozinski, Martin J. Wainwright, and Michael I. Jordan. Support union recovery in
          high-dimensional multivariate regression. The Annals of Statistics, 39(1):1‚Äì47, 2011.
[PM13]    D. Perraul-Joncas and M. Meila. Non-linear dimensionality reduction: Riemannian metric
          estimation and the problem of geometric discovery. ArXiv e-prints, May 2013.
[RS00]    Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding.
          Science, 290(5500):2323‚Äì2326, December 2000.
[SR03]    Lawrence K. Saul and Sam T. Roweis. Think globally, fit locally: Unsupervised learning of low
          dimensional manifolds. J. Mach. Learn. Res., 4:119‚Äì155, December 2003.
[THJ10]   Daniel Ting, Ling Huang, and Michael I. Jordan. An analysis of the convergence of graph
          laplacians. In Proceedings of the 27th International Conference on Machine Learning (ICML-10),
          pages 1079‚Äì1086, 2010.
[TR02]    Yee Whye Teh and Sam T. Roweis. Automatic alignment of local representations. In NIPS, 2002.
[Wai09]   Martin J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
          `1 -constrained quadratic programming (lasso). IEEE Transactions on Information Theory,
          55:2183‚Äì2202, 2009.
[YL06]    M Yuan and Y Lin. Model selection and estimation in regression with grouped variables. J. R.
          Stat. Soc. Series B Stat. Methodol., 2006.
[YX12]    Gui-Bo Ye and Xiaohui Xie. Learning sparse gradients for variable selection and dimension
          reduction. Machine Learning, 87(3):303‚Äì355, Jun 2012.
[Zor04]   Vladimir A. Zorich. Mathematical Analysis I. Springer-Verlag Berlin Heidelberg, 2004.
[ZZ04]    Zhenyue Zhang and Hongyuan Zha. Principal manifolds and nonlinear dimensionality reduction
          via tangent space alignment. SIAM J. Scientific Computing, 26(1):313‚Äì338, 2004.
                                                     23

