JMLR: Workshop and Conference Proceedings vol 35:1‚Äì17, 2014
       lil‚Äô UCB : An Optimal Exploration Algorithm for Multi-Armed
                                                       Bandits ‚àó
Kevin Jamieson                                                                            KGJAMIESON @ WISC . EDU
Matthew Malloy                                                                               MMALLOY @ WISC . EDU
Robert Nowak                                                                               NOWAK @ ECE . WISC . EDU
University of Wisconsin
SeÃÅbastien Bubeck                                                                      SBUBECK @ PRINCETON . EDU
Princeton University
                                                        Abstract
      The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with
      the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number
      of total samples. The procedure cannot be improved in the sense that the number of samples
      required to identify the best arm is within a constant factor of a lower bound based on the law of
      the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly
      account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time
      for the algorithm we avoid a union bound over the arms that has been observed in other UCB-
      type algorithms. We prove that the algorithm is optimal up to constants and also show through
      simulations that it provides superior performance with respect to the state-of-the-art.
      Keywords: Multi-armed bandit, upper confidence bound (UCB), iterated logarithm
1. Introduction
This paper introduces a new algorithm for the best arm problem in the stochastic multi-armed bandit
(MAB) setting. Consider a MAB with n arms, each with unknown mean payoff ¬µ1 , . . . , ¬µn in
[0, 1]. A sample of the ith arm is an independent realization of a sub-Gaussian random variable
with mean ¬µi . In the fixed confidence setting, the goal of the best arm problem is to devise a
sampling procedure with a single input Œ¥ that, regardless of the values of ¬µ1 , . . . , ¬µn , finds the arm
with the largest mean with probability at least 1 ‚àí Œ¥. More precisely, best arm procedures must
satisfy sup¬µ1 ,...,¬µn P(bi 6= i‚àó ) ‚â§ Œ¥, where i‚àó is the best arm, bi an estimate of the best arm, and the
supremum is taken over all set of means such that there exists a unique best arm. In this sense,
best arm procedures must automatically adjust sampling to ensure success when the mean of the
best and second best arms are arbitrarily close. Contrast this with the fixed budget setting where the
total number of samples remains a constant and the confidence in which the best arm is identified
within the given budget varies with the setting of the means. While the fixed budget and fixed
confidence settings are related (see Gabillon et al. (2012) for a discussion) this paper focuses on the
fixed confidence setting only.
     The best arm problem has a long history dating back to the ‚Äô50s with the work of Paulson
(1964); Bechhofer (1958). In the fixed confidence setting, the last decade has seen a flurry of
   ‚àó
     Part of the research described here was carried out at the Simons Institute for the Theory of Computing. We are
     grateful to the Simons Institute for providing a wonderful research environment.
 c 2014 K. Jamieson, M. Malloy, R. Nowak & S. Bubeck.

                                    JAMIESON M ALLOY N OWAK B UBECK
activity providing new upper and lower bounds. In 2002, the successive                  elimination procedure of
Even-Dar et al. (2002) was shown to find the best arm with order i6=i‚àó ‚àÜ‚àí2                          ‚àí2
                                                                            P
                                                                                          i log(n‚àÜi ) samples,
where ‚àÜi = ¬µi‚àó ‚àí ¬µi , coming within a logarithmic factor of the lower bound of i6=i‚àó ‚àÜ‚àí2
                                                                                              P
                                                                                                       i , shown
in 2004 in Mannor and Tsitsiklis (2004). A similar bound was also obtained using a procedure
known as LUCB1 that was originally designed for finding the m-best arms (Kalyanakrishnan et al.,
2012). Recently,Jamiesonet al. (2013) proposed a procedure called PRISM which succeeds with
P ‚àí2               P ‚àí2
                               or i ‚àÜ‚àí2              ‚àí2
                                     P                   
   i ‚àÜi log log       j ‚àÜj                  i log ‚àÜi        samples depending on the parameterization of
the algorithm, improving the result of Even-Dar et al. (2002) by at least a factor of log(n). The best
sample complexity result for the fixed confidence setting comes from a procedure similar to PRISM,
called exponential-gap elimination    P (Karnin   et al., 2013), which guarantees best arm identification
                                              ‚àí2
with high probability using order i ‚àÜi log log ‚àÜ‚àí2         i samples, coming within a doubly logarithmic
factor of the lower bound of Mannor and Tsitsiklis (2004). While the authors of Karnin et al. (2013)
conjecture that the log log term cannot be avoided, it remained unclear as to whether the upper
bound of Karnin et al. (2013) or the lower bound of Mannor and Tsitsiklis (2004) was loose.
     The classic work of Farrell (1964) answers    P this     question. It shows that the doubly logarith-
                                                           ‚àí2
mic factor is necessary, implying that order i ‚àÜi log log ‚àÜ‚àí2           i     samples are necessary and suf-
ficient in the sense that no procedure can satisfy sup‚àÜ1 ,...,‚àÜn P(bi 6= i‚àó ) ‚â§ Œ¥ and use fewer than
P ‚àí2                ‚àí2
   i ‚àÜi log log ‚àÜi samples in expectation for all ‚àÜ1 , . . . , ‚àÜn . The doubly logarithmic factor is a
consequence of the law of the iterated logarithm (LIL) (Darling and Robbins, 1985). The LIL states
that ifPX` are i.i.d. sub-Gaussian random variables with E[X` ] = 0, E[X`2 ] = œÉ 2 and we define
St = t`=1 X` then
                                  St                                            St
                 lim sup ‚àö                    = 1 and lim inf t‚Üí‚àû ‚àö                         = ‚àí1
                   t‚Üí‚àû      2œÉ 2 t log log(t)                             2œÉ 2 t log log(t)
almost surely. Here is the basic intuition behind the lower bound. Consider the two-arm problem
and let ‚àÜ be the difference between the means. In this case, it is reasonable to sample both arms
equally and consider the sum of differences of the samples, which is‚àöa random walk with drift
‚àÜ. The deterministic drift crosses the LIL bound above when t ‚àÜ = 2t log log t. Solving this
equation for t yields t ‚âà 2‚àÜ‚àí2 log log ‚àÜ‚àí2 . This intuition will be formalized in the next section.
     The LIL also motivates a novel approach to the best arm problem. Specifically, the LIL sug-
gests a natural scaling for confidence bounds on empirical means, and we follow this intuition to
develop a new algorithm for the best-arm problem. The algorithm is an Upper Confidence Bound
(UCB) procedure (Auer et al., 2002) based on a finite sample version of the LIL. The new algo-
rithm, called lil‚ÄôUCB, is described in Figure 1. By explicitly accounting for the log log factor in
the confidence bound and using a novel stopping criterion, our analysis of lil‚ÄôUCB avoids taking
naive union bounds over time, as encountered in some UCB algorithms (Kalyanakrishnan et al.,
2012; Audibert et al., 2010), as well as the wasteful ‚Äúdoubling trick‚Äù often employed in algorithms
that proceed in epochs, such as the PRISM and exponential-gap elimination procedures (Even-Dar
et al., 2002; Karnin et al., 2013; Jamieson et al., 2013). Also, in some analyses of best arm algo-
rithms the upper confidence bounds of each arm are designed to hold with high probability for all
arms uniformly, incurring a log(n) term in the confidence bound as a result of the necessary union
bound over the n arms (Even-Dar et al., 2002; Kalyanakrishnan et al., 2012; Audibert et al., 2010).
However, our stopping time allows for a tighter analysis so that arms with larger gaps are allowed
larger confidence bounds than those arms with smaller gaps where higher confidence is required.
Like exponential-gap elimination, lil‚ÄôUCB is order optimal in terms of sample complexity.
                                                       2

          LIL‚Äô UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
     It is easy to show that without the stopping condition (and with the right Œ¥) our algorithm
achieves a cumulative regret of the same order as standard UCB. Thus for the expert it may be
surprising that such an algorithm can achieve optimal sample complexity for the best arm identifi-
cation problem given the lower bound of Bubeck et al. (2009). As it was empirically observed in the
latter paper there seems to be a transient regime, before this lower bound applies, where the perfor-
mance in terms of best arm identification is excellent. In some sense the results in the present paper
can be viewed as a formal proof of this transient regime: if stopped at the right time performance of
UCB for best arm identification is near-optimal (or even optimal for lil‚ÄôUCB).
     One of the main motivations for this work was to develop an algorithm that exhibits great
practical performance in addition to optimal sample complexity. While the sample complexity
of exponential-gap elimination is optimal up to constants, and PRISM up to small log log factors,
the empirical performance of these methods is rather disappointing, even when compared to non-
sequential sampling. Both PRISM and exponential-gap elimination employ median elimination
(Even-Dar et al., 2002) as a subroutine. Median elimination is used to find an arm that is within
Œµ > 0 of the largest, and has sample complexity within a constant factor of optimal for this sub-
problem. However, the constant factors tend to be quite large, and repeated applications of median
elimination within PRISM and exponential-gap elimination are extremely wasteful. On the con-
trary, lil‚ÄôUCB does not invoke wasteful subroutines. As we will show, in addition to having the best
theoretical sample complexities bounds known to date, lil‚ÄôUCB also exhibits superior performance
in practice with respect to state-of-the-art algorithms.
2. Lower Bound
Before introducing the lil‚ÄôUCB algorithm, we show that the log log factor in the sample complexity
is necessary for best-arm identification. It suffices to consider a two armed bandit problem with a
gap ‚àÜ. If a lower bound on the gap is unknown, then the log log factor is necessary, as shown by
the following result.
Theorem 1 Consider the best arm problem in the fixed confidence setting with n = 2, differ-
ence between the two means ‚àÜ, and expected number of samples E‚àÜ [T ]. Any procedure with
sup‚àÜ6=0 P(bi 6= i‚àó ) ‚â§ Œ¥, Œ¥ ‚àà (0, 1/2), then has
                                               E‚àÜ [T ]
                                  lim sup ‚àÜ‚àí2 log log ‚àÜ‚àí2
                                                             ‚â• 2 ‚àí 4Œ¥.
                                    ‚àÜ‚Üí0
Proof The proof follows readily from Theorem 1 of Farrell (1964) by considering a reduction of the
best arm problem with n = 2 in which the value of one arm is known. In this case, the only strategy
available is to sample the other arm some number of times to determine if it is less than or greater
than the known value. We have reduced the problem to the setting of (Farrell, 1964, Theorem 1),
and stated it in Appendix A.
     Theorem 1 implies that in the fixed Pconfidence    setting, no best arm procedure can have sup P(bi 6=
i‚àó )                                          ‚àí2
     ‚â§ Œ¥ and use fewer than (2 ‚àí 4Œ¥) i ‚àÜi log log ‚àÜ‚àí2         i samples in expectation for all ‚àÜi .
     In brief, the result of Farrell follows by showing a generalized sequential probability ratio test,
which compares the running empirical mean of X after t samples against a series of thresholds,
                                                       3

                                     JAMIESON M ALLOY N OWAK B UBECK
                                                                                                 p
is an optimal test. In the limit as t increases, if the thresholds are not at least (2/t) log log(t)
then the LIL implies the procedure will fail withpprobability approaching 1/2 for small values of
‚àÜ. Setting the thresholds to be just greater than (2/t) log log(t), in the limit, one can show the
expected number of samples must scale as ‚àÜ‚àí2 log log ‚àÜ‚àí2 . As the proof in Farrell (1964) is quite
involved, we provide a short argument for a slightly simpler result than above in Appendix A.
3. Procedure
This section introduces lil‚ÄôUCB. The procedure operates by sampling the arm with the largest upper
confidence bound; the confidence bounds are defined to account for the implications of the LIL.
The procedure terminates when one of the arms has been sampled more than a constant times
the number of samples collected from all other arms combined. Fig. 1 details the algorithm and
Theorem 2 quantifies performance. In what follows, let Xi,s , s = 1, 2, . . . denote independent
samples from arm i and let Ti (t) denote the number of times arm i has been sampled up to time
                                  PTi (t)
t. Define ¬µ  bi,Ti (t) := Ti1(t) s=1       Xi,s to be the empirical mean of the Ti (t) samples from arm i
up to time t. The algorithm of Fig. 1 assumes that the centered realizations of the ith arm are
sub-Gaussian1 with known scale parameter œÉ.
       lil‚Äô UCB
       input: confidence Œ¥ > 0, algorithm parameters Œµ, Œª, Œ≤ > 0
       initialize: sample eachPof the n arms once, set Ti (t) = 1 for all i and set t = n
       while Ti (t) < 1 + Œª j6=i Tj (t) for all i
           sample arm               Ô£±                               v                                   Ô£º
                                                                    u 2œÉ 2 (1 + Œµ) log log((1+Œµ)Ti (t)) Ô£¥
                                                                    u
                                                              ‚àö t
                                    Ô£¥
                                    Ô£≤                                                                     Ô£Ω
                                                                                                Œ¥
               It   = argmax ¬µ        bi,Ti (t) + (1 + Œ≤)(1 + Œµ)                                             .
                        i‚àà{1,...,n} Ô£¥
                                    Ô£≥                                                 Ti (t)              Ô£¥
                                                                                                          Ô£æ
           set Ti (t + 1) = Ti (t) + 1 if It = i, otherwise set Ti (t + 1) = Ti (t).
       else stop and output arg maxi‚àà{1,...,n} Ti (t)
                                       Figure 1: The lil‚Äô UCB algorithm.
Define
                                     X 1                        X log log+ (1/‚àÜ2 )
                                                                                        i
                             H1 =                  and   H3 =
                                          ‚àó
                                            ‚àÜ2i                      ‚àó
                                                                              ‚àÜ 2
                                                                                i
                                     i6=i                       i6=i
where log log+ (x) = log log(x) if x ‚â• e, and 0 otherwise. Our main result is the following.
Theorem 2 For Œµ ‚àà (0, 1), let cŒµ = 2+Œµ          Œµ (1/ log(1 + Œµ))
                                                                   1+Œµ and fix Œ¥ ‚àà (0, log(1 + Œµ)/(ec )). Then
                                                                                                         ‚àö Œµ
for any Œ≤ ‚àà (0, 3], there exists a constant Œª > 0 such that with probability at least 1 ‚àí 4 cŒµ Œ¥ ‚àí 4cŒµ Œ¥
lil‚Äô UCB stops after at most c1 H1 log(1/Œ¥) + c3 H3 samples and outputs the optimal arm, where
c1 , c3 > 0 are known constants that depend only on Œµ, Œ≤, œÉ 2 .
  1. A zero-mean random variable X is said to be sub-Gaussian with scale parameter œÉ if for all t ‚àà R we have
     E[exp{tX}] ‚â§ exp{œÉ 2 t2 /2}. If a ‚â§ X ‚â§ b almost surely than it suffices to take œÉ 2 = (b ‚àí a)2 /4.
                                                          4

            LIL‚Äô UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
      Note that the algorithm obtains the optimal query complexity of H1 log(1/Œ¥) + H3 up to con-
stant factors. We remark that the theorem holds with any value of Œª satisfying (7). Inspection of (7)
                                                              2
shows that as Œ¥ ‚Üí 0 we can let Œª tend to 2+Œ≤              Œ≤       . We point out that the sample complexity bound
in the theorem can be optimized by choosing Œµ and Œ≤. For a setting of these parameters in a way that
                                                                                                                     2
is more or less faithful to the theory, we recommend taking Œµ = 0.01, Œ≤ = 1, and Œª = 2+Œ≤                          Œ≤       . For
improved performance in practice, we recommend applying footnote 2 and setting Œµ = 0, Œ≤ = 0.5,
Œª = 1 + 10/n and Œ¥ ‚àà (0, 1), which do not meet the requirements of the theorem, but work very
well in our experiments presented later. We prove the theorem via two lemmas, one for the total
number of samples taken from the suboptimal arms and one for the correctness of the algorithm. In
the lemmas we give precise constants.
4. Proof of Theorem 2
Before stating the two main lemmas that imply the result, we first present a finite form of the law
of iterated logarithm. This finite LIL bound is necessary for our analysis and may also prove useful
for other applications.
Lemma 3 Let X1 , X2 , . . . be i.i.d. centered sub-Gaussian random variables with scale param-
eter œÉ. For any Œµ ‚àà (0, 1) and Œ¥ ‚àà (0, log(1 + Œµ)/e)2 one has with probability at least 1 ‚àí
                  1+Œµ
2+Œµ           Œ¥
  Œµ      log(1+Œµ)         for all t ‚â• 1,
                                                      s
                              t
                                               ‚àö
                                                                                                 
                            X                                                    log((1 + Œµ)t)
                                 Xs ‚â§ (1 +         Œµ)    2œÉ 2 (1  + Œµ)t log                         .
                                                                                         Œ¥
                             s=1
                                                                 r                      
                                 Pt                                    2          log(x)
Proof We denote St =                 s=1 Xs , and œà(x) =            2œÉ x log         Œ¥     . We also define by induction
the sequence of integers (uk ) as follows: u0 = 1, uk+1 = d(1 + Œµ)uk e.
Step 1: Control of Suk , k ‚â• 1. The following inequalities hold true thanks to an union bound
together with Chernoff‚Äôs bound, the fact that uk ‚â• (1 + Œµ)k , and a simple sum-integral comparison:
                                                                ‚àû
                                 ‚àö                                                                  
                                                                   exp ‚àí(1 + Œµ) log log(u         k)
                                                             X
        P ‚àÉk ‚â• 1 : Suk ‚â•            1 + Œµ œà(uk )        ‚â§                                       Œ¥
                                                              k=1
                                                                ‚àû               1+Œµ                            1+Œµ
                                                              X
                                                                           Œ¥                      1
                                                                                                         Œ¥
                                                        ‚â§            k log(1+Œµ)          ‚â§ 1+     Œµ    log(1+Œµ)         .
                                                              k=1
Step 2: Control of St , t ‚àà (uk , uk+1 ). Adopting the notation [n] = {1, . . . , n}, recall that Hoeffd-
ing‚Äôs maximal inequality3 states that for any m ‚â• 1 and x > 0 one has
                                                                                      2
                                                                                          
                                      P(‚àÉ t ‚àà [m] s.t. St ‚â• x) ‚â§ exp ‚àí 2œÉx2 m .
  2. Note Œ¥ is restricted to guarantee that log( log((1+Œµ)t)
                                                      Œ¥
                                                             ) is well defined. This makes the analysis cleaner but in practice
     one can allow the full range of Œ¥ by using log( log((1+Œµ)t+2)
                                                              Œ¥
                                                                     ) instead and obtain the same theoretical guarantees.
  3. It is an easy exercise to verify that Azuma-Hoeffding holds for martingale differences with sub-Gaussian increments,
     which implies Hoeffding‚Äôs maximal inequality for sub-Gaussian distributions.
                                                                 5

                                      JAMIESON M ALLOY N OWAK B UBECK
Thus the following inequalities hold true (by using trivial manipulations on the sequence (uk )):
                                                             ‚àö              
    P ‚àÉ t ‚àà {uk + 1, . . . , uk+1 ‚àí 1} : St ‚àí Suk ‚â• Œµ œà(uk+1 )
                                                  ‚àö                       
                                                                                     u
                                                                                                  
                                                                                                    log(uk+1 )
                                                                                                               
      = P ‚àÉ t ‚àà [uk+1 ‚àí uk ‚àí 1] : St ‚â• Œµ œà(uk+1 ) ‚â§ exp ‚àíŒµ uk+1k+1                     ‚àíuk ‚àí1 log       Œ¥
                                                                 1+Œµ
                                    log(uk+1 )                Œ¥
      ‚â§ exp ‚àí(1 + Œµ) log                Œ¥         ‚â§ (k+1) log(1+Œµ)           .
Step 3: By putting together the results of Step 1 and Step 2 we obtain that with probability at least
                      1+Œµ
1 ‚àí 2+Œµ
      Œµ
                 Œ¥
            log(1+Œµ)        , one has for any k ‚â• 0 and any t ‚àà {uk + 1, . . . , uk+1 },
                                    St = St ‚àí Suk + Suk
                                               ‚àö                 ‚àö
                                          ‚â§      Œµ œà(uk+1 ) + 1 + Œµ œà(uk )
                                               ‚àö                    ‚àö
                                          ‚â§      Œµ œà((1 + Œµ)t) + 1 + Œµ œà(t)
                                                    ‚àö
                                          ‚â§ (1 + Œµ) œà((1 + Œµ)t),
which concludes the proof.
Without loss of generality we assume that ¬µ1 > ¬µ2 ‚â• . . . ‚â• ¬µn . To shorten notation we denote
                                                      r
                                                  ‚àö         2
                                                                                      
                               U (t, œâ) = (1 + Œµ) 2œÉ (1+Œµ)    t     log   log((1+Œµ)t)
                                                                               œâ         .
    The following events will be useful in the analysis:
                                    Ei (œâ) = {‚àÄt ‚â• 1, |b ¬µi,t ‚àí ¬µi | ‚â§ U (t, œâ)}
        bi,t = 1t tj=1 xi,j . Note that Lemma 3 shows P(Ei (œâ)c ) = O(œâ). The following trivial
                     P
where ¬µ
inequalities will also be useful (the second one is derived from the first inequality and the fact that
x+a      a
 x+b ‚â§ b for a ‚â• b, x ‚â• 0). For t ‚â• 1, Œµ ‚àà (0, 1), c > 0, 0 < œâ ‚â§ 1,
                                                                                             
                     1       log((1 + Œµ)t)                     1       2 log((1 + Œµ)/(cœâ))
                       log                       ‚â• c ‚áí t ‚â§ log                                    ,               (1)
                     t               œâ                         c                  œâ
and for t ‚â• 1, s ‚â• 3, Œµ ‚àà (0, 1), c ‚àà (0, 1], 0 < œâ ‚â§ Œ¥ ‚â§ e‚àíe ,
                                                                                                      1
                                                                                                     
  1        log((1 + Œµ)t)           c        log((1 + Œµ)s)                              s log 2 log cœâ      /œâ
    log                         ‚â• log                            and œâ ‚â§ Œ¥ ‚áí t ‚â§                                . (2)
  t                 œâ              s               Œ¥                                   c       log(1/Œ¥)
                                                                                              ‚àö
Lemma 4 Let Œ≤, Œµ, Œ¥ be set as in Theorem 2 and let Œ≥ = 2(2 + Œ≤)2 (1 + Œµ)2 œÉ 2 (1 + Œµ) and
                       1+Œµ
                   1
cŒµ = 2+ŒµŒµ     log(1+Œµ)        . Then we have with probability at least 1 ‚àí 2cŒµ Œ¥ and any t ‚â• 1,
             n                                         n
           X                                          X      log(2 max{1, log(Œ≥(1 + Œµ)/‚àÜ2i /Œ¥)})
                 Ti (t) ‚â§ n + 5Œ≥H1 log(e/Œ¥) +             Œ≥                                              .
            i=2                                       i=2
                                                                                 ‚àÜ2i
                                                          6

        LIL‚Äô   UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
The proof relies crucially on the fact that the realizations from each arm are independent of each
other. This means that if we condition on the event that the realizations from the optimal arm are
well-behaved, it is shown that the number of times the ith suboptimal arm is pulled is an independent
sub-exponential random variable with mean on the order of ‚àÜ‚àí2                          ‚àí2
                                                                        i log(log(‚àÜi )/Œ¥). We then apply a
standard tail bound to the sum of independent sub-exponential random variables to obtain the result.
Proof We decompose the proof in two steps.
Step 1. Let i > 1. Assuming that E1 (Œ¥) and Ei (œâ) hold true and that It = i one has
¬µi +U (Ti (t), œâ)+(1+Œ≤)U (Ti (t), Œ¥) ‚â• ¬µ                                        b1,T1 (t) +(1+Œ≤)U (T1 (t), Œ¥) ‚â• ¬µ1 ,
                                                bi,Ti (t) +(1+Œ≤)U (Ti (t), Œ¥) ‚â• ¬µ
                                                                                    ‚àö
which implies (2 + Œ≤)U (Ti (t), min(œâ, Œ¥)) ‚â• ‚àÜi . If Œ≥ = 2(2 + Œ≤)2 (1 + Œµ)2 œÉ 2 (1 + Œµ) then using
                ‚àÜ 2
(1) with c = Œ≥i one obtains that if E1 (Œ¥) and Ei (œâ) hold true and It = i then
                                                 2 log(Œ≥(1 + Œµ)/‚àÜ2i / min(œâ, Œ¥))
                                                                                   
                                        Œ≥
                       Ti (t) ‚â§            log
                                       ‚àÜ2i                    min(œâ, Œ¥)
                                                                                   
                                             Œ≥          log(e/œâ)             2Œ≥        1
                                 ‚â§ œÑi + 2 log                      ‚â§ œÑi + 2 log             ,
                                            ‚àÜi              œâ               ‚àÜi         œâ
                         2 max{1,log(Œ≥(1+Œµ)/‚àÜ2i /Œ¥)}
                                                       
where œÑi = ‚àÜŒ≥2 log                      Œ¥                 .
                i
    Since Ti (t) only increases when i is played the above argument shows that the following in-
equality is true for any time t ‚â• 1:
                                                                                
                                                                       2Œ≥        1
                              Ti (t)1{E1 (Œ¥) ‚à© Ei (œâ)} ‚â§ 1 + œÑi + 2 log              .                      (3)
                                                                      ‚àÜi         œâ
Step 2. We define the following random variable:
                                      ‚Ñ¶i = max{œâ ‚â• 0 : Ei (œâ) holds true}.
Note that ‚Ñ¶i is well-defined and by Lemma 3 it holds that P(‚Ñ¶i < œâ) ‚â§ cŒµ œâ where cŒµ =
               1+Œµ
2+Œµ       1
  Œµ   log(1+Œµ)        . Furthermore one can rewrite (3) as
                                                                              
                                                                 2Œ≥          1
                                   Ti (t)1{E1 (Œ¥)} ‚â§ 1 + œÑi + 2 log              .                          (4)
                                                                 ‚àÜi         ‚Ñ¶i
We use this equation as follows:
           n                     n                                 n                 n
                                             !                                                          !
         X                      X                                 X                 X
     P         Ti (t) > x +         (œÑi + 1)      ‚â§ cŒµ Œ¥ + P          Ti (t) > x +       (œÑi + 1) E1 (Œ¥)
         i=2                    i=2                               i=2               i=2
                                                                   n                      !
                                                                  X    2Œ≥         1
                                                  ‚â§ cŒµ Œ¥ + P               log         >x .                 (5)
                                                                  i=2
                                                                      ‚àÜ2i        ‚Ñ¶i
                      c‚àí1
                          
           2Œ≥
Let Zi =   ‚àÜ2i
               log     Œµ
                       ‚Ñ¶i    , i ‚àà [n] \ 1. Observe that these are independent random variables and since
P(‚Ñ¶i < œâ) ‚â§ cŒµ œâ it holds that P(Zi > x) ‚â§ exp(‚àíx/ai ) with ai = 2Œ≥/‚àÜ2i . Using standard
                                                            7

                                    JAMIESON M ALLOY N OWAK B UBECK
techniques to bound the sum of sub-exponential random variables one directly obtains that
      n
                         !                      2                                             2                  
     X                                               z             z                                    z         z
P        (Zi ‚àí ai ) ‚â• z ‚â§ exp ‚àí min                          ,
                                                           2 4kak              ‚â§  exp    ‚àí  min               ,
                                                                                                            2 4kak          .
     i=2
                                                   4kak    2          ‚àû                               4kak  1        1
                                                                                                                         (6)
Putting together (5) and (6) with z = 4kak1 log(1/(cŒµ Œ¥)), x = z + ||a||1 log(ecŒµ ) one obtains
                               n              n                                   !
                              X             X        4Œ≥ log(e/Œ¥)
                        P         Ti (t) >                              + œÑi + 1        ‚â§ 2cŒµ Œ¥,
                              i=2           i=2
                                                            ‚àÜ2i
which concludes the proof.
                                                                                          1+Œµ
                                                                         2+Œµ         1
Lemma 5 Let Œ≤, Œµ, Œ¥ be set as in Theorem 2 and let cŒµ =                    Œµ     log(1+Œµ)        . If
                                                            2+Œ≤ 2
                                                                    
                                              log 2 log   (   Œ≤  ) /Œ¥
                                                                                     2
                                          1+
                                                                              
                                                 ‚àö    log(1/Œ¥)                  2+Œ≤
                                 Œª‚â•                                               Œ≤     ,                                (7)
                                       1‚àí(cŒµ Œ¥)‚àí    (cŒµ Œ¥)1/4 log(1/(cŒµ Œ¥))
                                                                                      P
then for all i = 2, . . . , n and t = 1, 2, . . . , we have Ti (t) < 1 + Œª               j6=i Tj (t)  with probability at
                     ‚àö
least 1 ‚àí 2cŒµ Œ¥ + 4 cŒµ Œ¥.
Note that the right hand side of (7) can be bound by a universal constant for all allowable Œ¥ which
leads to the simplified statement of Theorem 2. Moreover, for any ŒΩ > 0 there exists a sufficiently
                                                                                                               2
small Œ¥ ‚àà (0, 1) such that the right hand side of (7) is less than or equal to (1 + ŒΩ) 2+Œ≤                   Œ≤     .
     Essentially, the proof relies on the fact that given any two arms j < i (i.e. ¬µj ‚â• ¬µi ), Ti (t)
cannot be larger than a constant times Tj (t) with probability at least 1 ‚àí Œ¥. Considering                 Pi‚àí1 this fact, it
is reasonable to suppose that the probability that Ti (t) is larger than a constant times j=1 Tj (T ) is
decreasing exponentially fast in i. Consequently, our stopping condition is not based on a uniform
confidence bound for all arms. Rather, it is based on confidence bounds that grow in size as the arm
index i increases.
Proof We decompose the proof in two steps.
Step 1. Let i > j. Assuming that Ei (œâ) and Ej (Œ¥) hold true and that It = i one has
 ¬µi + U (Ti (t), œâ) + (1 + Œ≤)U (Ti (t), Œ¥) ‚â• ¬µ           bi,Ti (t) + (1 + Œ≤)U (Ti (t), Œ¥)
                                                  ‚â• ¬µ    bj,Tj (t) + (1 + Œ≤)U (Tj (t), Œ¥) ‚â• ¬µj + Œ≤U (Tj (t), Œ¥),
                                                                                                                    2
                                                                                                                 Œ≤
which implies (2 + Œ≤)U (Ti (t), min(œâ, Œ¥)) ‚â• Œ≤U (Tj (t), Œ¥). Thus using (2) with c = 2+Œ≤                                one
obtains that if Ei (œâ) and Ej (Œ¥) hold true and It = i then
                                                               2                        
                                                             2+Œ≤
                                        2  log 2 log         Œ≤
                                                                     / min(œâ,Œ¥) / min(œâ,Œ¥)
                                    2+Œ≤
                    Ti (t) ‚â§          Œ≤                            log(1/Œ¥)                    Tj (t).
     Similarly to Step 1 in the proof of Lemma 4 we use the fact that Ti (t) only increases when It is
played and the above argument to obtain the following inequality for any time t ‚â• 1:
                                                                            2                         
                                                                           2+Œ≤
                                                      2    log 2 log       Œ≤
                                                                                  / min(œâ,Œ¥)   / min(œâ,Œ¥)
                                                  2+Œ≤
        (Ti (t) ‚àí 1)1{Ei (œâ) ‚à© Ej (Œ¥)} ‚â§           Œ≤                            log(1/Œ¥)                     Tj (t).     (8)
                                                             8

         LIL‚Äô  UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
Step 2. Using (8) with œâ = Œ¥ i‚àí1 we see that
                                 i‚àí1
                            1 X                                                                     X
           1{Ei (Œ¥ i‚àí1 )}             1{Ej (Œ¥)} > 1 ‚àí Œ± ‚áí (1 ‚àí Œ±)(Ti (t) ‚àí 1) ‚â§ Œ∫                        Tj (t)
                          i‚àí1
                                 j=1                                                                j6=i
                                            2    
                                           2+Œ≤
                              log 2 log           /Œ¥
                   2                                   
                2+Œ≤                         Œ≤
where Œ∫ =        Œ≤        1+          log(1/Œ¥)               . This implies the following, using that P(Ei (œâ)) ‚â•
1 ‚àí cŒµ œâ,
       Ô£´                                                                                        Ô£∂
                                                                                  X
    P Ô£≠‚àÉ (i, t) ‚àà {2, . . . , n} √ó {1, . . . } : (1 ‚àí Œ±)(Ti (t) ‚àí 1) ‚â• Œ∫                Tj (t)Ô£∏
                                                                                   j6=i
           Ô£´                                                                              Ô£∂
                                                              i‚àí1
                                                        1    X
     ‚â§ P Ô£≠‚àÉ i ‚àà {2, . . . , n} : 1{Ei (Œ¥ i‚àí1 )}                     1{Ej (Œ¥)} ‚â§ 1 ‚àí Œ±Ô£∏
                                                   i‚àí1
                                                             j=1
                                                          Ô£´                                                           Ô£∂
          n                                      n                     i‚àí1
        X                                       X               1     X
     ‚â§       P(Ei (Œ¥ i‚àí1 ) does not hold) +           PÔ£≠                   1{Ej (Œ¥)} ‚â§ 1 ‚àí cŒµ Œ¥ ‚àí (Œ± ‚àí cŒµ Œ¥)Ô£∏ .
                                                             i‚àí1
        i=2                                     i=2                   j=1
Let Œ¥ 0 = cŒµ Œ¥. Note that by a simple Hoeffding‚Äôs inequality and a union bound one has
       Ô£´                                                    Ô£∂
                 i‚àí1
            1    X
    PÔ£≠               1{Ej (Œ¥)} ‚â§ 1 ‚àí Œ¥ 0 ‚àí (Œ± ‚àí Œ¥ 0 )Ô£∏ ‚â§ min((i ‚àí 1)Œ¥ 0 , exp(‚àí2(i ‚àí 1)(Œ± ‚àí Œ¥ 0 )2 ),
         i‚àí1
                 j=1
and thus if we define j‚àó = dŒ¥ 0‚àí1/4 /2e we obtain with the above calculations
    Ô£´                                                                                                                       Ô£∂
                                                                                                           X
  P Ô£≠‚àÉ (i, t) ‚àà {2, . . . , n} √ó {1, . . . } : 1 ‚àí Œ¥ 0 ‚àí Œ¥ 01/4 log(1/Œ¥ 0 ) (Ti (t) ‚àí 1) ‚â• Œ∫
                                                                p
                                                                                                                    Tj (t)Ô£∏
                                                                                                              j6=i
       n                                                                                           ‚àí2j‚àó Œ¥ 0 1/4 log( Œ¥10 )
      X                                             0 1/4 log( Œ¥10 )
                                                                            Œ¥0                  e
   ‚â§        Œ¥ 0i‚àí1 + min (i ‚àí 1)Œ¥ 0 , e‚àí2(i‚àí1)Œ¥                           ‚â§          + Œ¥ 0 j‚àó2 +
                                                                            1 ‚àí Œ¥0                            0 1/4
                                                                                                 1 ‚àí e‚àí2Œ¥ log( Œ¥0 )
                                                                                                                          1
      i=2
        Œ¥0        9 01/2    3 03/4
                                                   p
   ‚â§           +   Œ¥     +    Œ¥     ‚â§   2cŒµ Œ¥ +  4      cŒµ Œ¥.
      1 ‚àí Œ¥0 4              2
Treating Œµ, œÉ 2 and factors of log log(Œ≤) as constants, Lemma 4 says that the total number of times
the suboptimal arms are sampled does not exceed (Œ≤ + 2)2 (c1 H1 log(1/Œ¥) + c3 H3 ). Lemma 5
                                                                                                        2
states that only the optimal arm will meet the stopping condition with Œª = cŒª 2+Œ≤                    Œ≤          for some cŒª
constant defined in the lemma. Combining these results, we observe that the total                   number          of times
                                                                                                                         2 
                                                                  2                                                 2+Œ≤
all the arms are sampled does not exceed (Œ≤ + 2) (c1 H1 log(1/Œ¥) + c3 H3 ) 1 + cŒª Œ≤                                            ,
completing the proof of the theorem. We also observe using the approximation cŒª = 1, the optimal
choice of Œ≤ ‚âà 1.66.
                                                              9

                                     JAMIESON M ALLOY N OWAK B UBECK
5. Implementation and Simulations
In this section we investigate how the state of the art methods for solving the best arm problem
behave in practice. Before describing each of the algorithms in the comparison, we briefly describe
a LIL-based stopping criterion that can be applied to any of the algorithms.
      LIL Stopping (LS) : For any algorithm and i ‚àà [n], after the t-th time we have that the i-th
       arm has been sampled Ti (t) times and accumulated a mean ¬µ              bi,Ti (t) . We can apply Lemma 3
                                                                                              1+Œµ
       (with a union bound) so that with probability at least 1 ‚àí 2+Œµ        Œµ
                                                                                          Œ¥
                                                                                    log(1+Œµ)
                                                                  r                                       
                                                                                      2 log((1+Œµ)Ti (t)+2)
                                                              ‚àö      2œÉ 2 (1+Œµ) log           Œ¥/n
                      bi,Ti (t) ‚àí ¬µi ‚â§ Bi,Ti (t) := (1 + Œµ)
                      ¬µ                                                               Ti (t)                            (9)
       for all t ‚â• 1 and all i ‚àà [n]. We may then conclude that if bi := arg maxi‚àà[n] ¬µ                      bi,Ti (t) and
       bbi,Tb(t) ‚àí Bbi,Tb(t) ‚â• ¬µ
       ¬µ                        bj,Tj (t) + Bj,Tj (t) ‚àÄj 6= bi then with high probability we have that bi = i‚àó .
            i           i
The LIL stopping condition is somewhat naive but often quite effective in practice for smaller size
problems when log(n) is negligible. To implement the strategy for any algorithm with fixed confi-
dence ŒΩ, simply run the algorithm with ŒΩ/2 in place of ŒΩ and assign the other ŒΩ/2 confidence to the
LIL stopping criterion. Note that to for the LIL bound to hold with probability at least 1 ‚àí ŒΩ, one
                                      1/(1+Œµ)
                                   ŒΩŒµ
should use Œ¥ = log(1 + Œµ) 2+Œµ                   . The algorithms compared were:
    ‚Ä¢ Nonadaptive + LS : Draw a random permutation of [n] and sample the arms in an order
       defined by cycling through the permutation until the LIL stopping criterion is met.
    ‚Ä¢ Exponential-Gap Elimination (+LS) (Karnin et al., 2013) : This procedure proceeds in stages
       where at each stage, median elimination (Even-Dar et al., 2002) is used to find an Œµ-optimal
       arm whose mean is guaranteed (with large probability) to be within a specified Œµ > 0 of the
       mean of the best arm, and then arms are discarded if their empirical mean is sufficiently below
       the empirical mean of the Œµ-optimal arm. The algorithm terminates when there is only one
       arm that has not yet been discarded (or when the LIL stopping criterion is met).
    ‚Ä¢ Successive Elimination (Even-Dar et al., 2002) : This procedure proceeds in the same spirit as
       Exponential-Gap Elimination except the Œµ-optimal arm is equal to bi := arg maxi‚àà[n] ¬µ                     bi,Ti (t) .
    ‚Ä¢ lil‚ÄôUCB (+LS)‚àö : The procedure        of Figure 1 is run with Œµ = 0.01, Œ≤ = 1, Œª = (2+Œ≤)2 /Œ≤ 2 = 9,
                  ( 1+ŒΩ(/2)‚àí1)    2
       and Œ¥ =            4cŒµ       for input confidence ŒΩ. The algorithm terminates according to Fig. 1
       (or when the LIL stopping criterion is met). Note that Œ¥ is defined as prescribed by Theorem 2
       but we approximate the leading constant in (7) by 1 to define Œª.
    ‚Ä¢ lil‚ÄôUCB Heuristic : The procedure of Figure 1 is run with Œµ = 0, Œ≤ = 1/2, Œª = 1 + 10/n,
       and Œ¥ = ŒΩ/5 for input confidence ŒΩ. These parameter settings do not satisfy the conditions of
       Theorem 2, and thus there is no guarantee that this algorithm will find the best arm.
    ‚Ä¢ LUCB1 (+ LS) (Kalyanakrishnan et al., 2012) : This procedure pulls two arms at each time:
       the arm with the highest empirical mean and the arm with the highest upper confidence bound
       among the remaining arms. The upper confidence bound was of the form prescribed in the
       simulations section of Kaufmann and Kalyanakrishnan (2013) and is guaranteed to return the
       arm with the highest mean with confidence 1 ‚àí Œ¥.
                                                          10

           LIL‚Äô UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
We did not compare to PRISM of Jamieson et al. (2013) because the algorithm and its empirical per-
formance are very similar to Exponential-Gap Elimination so its inclusion in the comparison would
provide very little added value. We remark that the first three algorithms require O(1) amortized
computation per time step, the lil‚ÄôUCB algorithms require O(log(n)) computation per time step
using smart data structures4 , and LUCB1 requires O(n) computation per time step. LUCB1 was
not run on all problem sizes due to poor computational scaling with respect to the problem size.
      Three problem scenarios were considered over a variety problem sizes (number of arms). The
‚Äú1-sparse‚Äù scenario sets ¬µ1 = 1/2 and ¬µi = 0 for all i = 2, . . . , n resulting in a hardness of
H1 = 4n. The ‚ÄúŒ± = 0.3‚Äù and ‚ÄúŒ± = 0.6‚Äù scenarios consider n + 1 arms with ¬µ0 = 1 and
¬µi = 1 ‚àí (i/n)Œ± for all i = 1, . . . , n with respective hardnesses of H1 ‚âà 3/2n and H1 ‚âà 6n1.2 .
That is, the Œ± = 0.3 case should be about as hard as the sparse case with increasing problem size
while the Œ± = 0.6 is considerably more challenging and grows super linearly with the problem size.
See Jamieson et al. (2013) for an in-depth study of the Œ± parameterization. All experiments were
run with input confidence Œ¥ = 0.1. All realizations of the arms were Gaussian random variables
with mean ¬µi and variance 1/45 .
      Each algorithm terminates at some finite time with high probability so we first consider the
relative stopping times of each of the algorithms in Figure 2. Each algorithm was run on each
problem scenario and problem size, repeated 50 times. The first observation is that Exponential-
Gap Elimination (+LS) appears to barely perform better than nonadaptive sampling with the LIL
stopping criterion. This confirms our suspicion that the constants in median elimination are just
too large to make this algorithm practically relevant. While the LIL stopping criterion seems to
have measurably improved the lil‚ÄôUCB algorithm, it had no impact on the lil‚ÄôUCB Heuristic variant
(not plotted). While lil‚ÄôUCB Heuristic has no theoretical guarantees of outputting the best arm, we
remark that over the course of all of our tens of thousands of experiments, the algorithm never failed
to terminate with the best arm. The LUCB algorithm, despite having worse theoretical guarantees
than the lil‚ÄôUCB algorithm, performs surprisingly well. We conjecture that this is because UCB
style algorithms tend to lean towards exploiting the top arm versus focusing on increasing the gap
between the top two arms, which is the goal of LUCB.
      In reality, one cannot always wait for an algorithm to run until it terminates on its own so we now
explore how the algorithms perform if the algorithm must output an arm at every time step before
termination (this is similar to the setting studied in Bubeck et al. (2009)). For each algorithm, at each
time we output the arm with the highest empirical mean. Clearly, the probability that a sub-optimal
arm is output by any algorithm should very close to 1 in the beginning but then eventually decrease to
at least the desired input confidence, and likely, to zero. Figure 3 shows the ‚Äúanytime‚Äù performance
of the algorithms for the three scenarios and unlike the empirical stopping times of the algorithms,
we now observe large differences between the algorithms. Each experiment was repeated 5000
times. Again we see essentially no difference between nonadaptive sampling and the exponential-
gap procedure. While in the stopping time plots of Figure 2 the successive elimination appears
competitive with the UCB algorithms, we observe in Figure 3 that the UCB algorithms are collecting
  4. The sufficient statistic for lil‚ÄôUCB to decide which arm to sample depends only on ¬µ       bi,Ti (t) and Ti (t) which only
     changes for an arm if that particular arm is pulled. Thus, it suffices to maintain an ordered list of the upper confidence
     bounds in which deleting, updating, and reinserting the arm requires just O(log(n)) computation. Contrast this with
     a UCB procedure in which the upper confidence bounds depend explicitly on t so that the sufficient statistics for
     pulling the next arm changes for all arms after each pull, requiring ‚Ñ¶(n) computation per time step.
  5. The variance was chosen such that the analyses of algorithms that assumed realizations were in [0, 1] and used
     Hoeffding‚Äôs inequality were still valid using sub-Gaussian tail bounds with scale parameter 1/2.
                                                             11

                                 JAMIESON M ALLOY N OWAK B UBECK
        1-sparse, H1 = 4n                Œ± = 0.3, H1 ‚âà 32 n               Œ± = 0.6, H1 ‚âà 6n1.2
Figure 2: Stopping times of the algorithms for three scenarios for a variety of problem sizes. The
            problem scenarios from left to right are the 1-sparse problem (¬µ1 = 0.5, ¬µi = 0 ‚àÄi > 1),
            Œ± = 0.3 (¬µi = 1 ‚àí (i/n)Œ± , i = 0, 1, . . . , n), and Œ± = 0.6.
sufficient information to output the best arm at least twice as fast as successive elimination. This
tells us that the stopping conditions for the UCB algorithms are still too conservative in practice
which motivates the use of the lil‚ÄôUCB Heuristic algorithm which appears to perform very strongly
across all metrics. The LUCB algorithm again performs strongly here suggesting that LUCB-style
algorithms are very well-suited for exploration tasks.
6. Discussion
This paper proposed a new procedure for identifying the best arm in a multi-armed bandit problem
in the fixed confidence setting, a problem of pure exploration. However, there are some scenarios
where one wishes to balance exploration with exploitation and the metric of interest is the cumu-
lative regret. We remark that the techniques developed here can be easily extended to show that
the lil‚ÄôUCB algorithm obtains bounded regret with high probability, improving upon the result of
Abbasi-Yadkori et al. (2011).
     In this work we proved upper and lower bounds over the class of distributions with bounded
means and sub-Guassian realizations and presented our results just in terms of the difference be-
tween the means of the arms. In contrast to just considering the means of the distributions, Kauf-
mann and Kalyanakrishnan (2013) studied the Chernoff information between distributions, a quan-
tity related to the KL divergence, that is sharper and can result in improved rates in identifying the
best arm in theory and practice (for instance if the realizations from the arms have very different
variances). Pursuing methods that exploit distributional characteristics beyond the mean is a good
direction for future work.
     Finally, an obvious extension of this work is to consider finding the top-m arms instead of
just the best arm. This idea has been explored in both the fixed confidence setting Kaufmann and
Kalyanakrishnan (2013) and the fixed budget setting Bubeck et al. (2012) but we believe both of
these sample complexity results to be suboptimal. It may be possible to adapt the approach devel-
oped in this paper to find the top-m arms and obtain gains in theory and practice.
                                                   12

             LIL‚Äô   UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
               1-sparse, H1 = 4n              Œ± = 0.3, H1 ‚âà 32 n           Œ± = 0.6, H1 ‚âà 6n1.2
 n = 10
 n = 100
 n = 1000
 n = 10000
Figure 3: At every time, each algorithm outputs an arm iÃÇ that has the highest empirical mean. The
          P(iÃÇ 6= i‚àó ) is plotted with respect to the total number of pulls by the algorithm. The prob-
          lem sizes (number of arms) increase from top to bottom. The problem scenarios from left
          to right are the 1-sparse problem (¬µ1 = 0.5, ¬µi = 0 ‚àÄi > 1) , Œ± = 0.3 (¬µi = 1 ‚àí (i/n)Œ± ,
          i = 0, 1, . . . , n), and Œ± = 0.6. The arrows indicate the stopping times (if not shown,
          those algorithms did not terminate within the time window shown). Note that LUCB1
          is not plotted for n = 10000 due to computational constraints (see text for explanation).
          Also note that in some plots it is difficult to distinguish between the nonadaptive sampling
          procedure, the exponential-gap algorithm, and successive elimination due to the curves
          being on top of each other.
                                                    13

                                JAMIESON M ALLOY N OWAK B UBECK
References
Yasin Abbasi-Yadkori, Csaba SzepesvaÃÅri, and David Tax. Improved algorithms for linear stochastic
   bandits. In Advances in Neural Information Processing Systems, pages 2312‚Äì2320, 2011.
Jean-Yves Audibert, SeÃÅbastien Bubeck, and ReÃÅmi Munos. Best arm identification in multi-armed
   bandits. COLT 2010-Proceedings, 2010.
Peter Auer, NicoloÃÄ Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
   problem. Machine learning, 47(2-3):235‚Äì256, 2002.
Robert E Bechhofer. A sequential multiple-decision procedure for selecting the best one of several
   normal populations with a common unknown variance, and its use with various experimental
   designs. Biometrics, 14(3):408‚Äì429, 1958.
S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Pro-
   ceedings of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.
SeÃÅbastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed
   bandits. arXiv preprint arXiv:1205.3181, 2012.
DA Darling and Herbert Robbins. Iterated logarithm inequalities. In Herbert Robbins Selected
   Papers, pages 254‚Äì258. Springer, 1985.
Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC bounds for multi-armed bandit and markov
   decision processes. In Computational Learning Theory, pages 255‚Äì270. Springer, 2002.
R. H. Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals
   of Mathematical Statistics, 35(1):pp. 36‚Äì72, 1964. ISSN 00034851.
Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Team SequeL. Best arm iden-
   tification: A unified approach to fixed budget and fixed confidence. In NIPS, pages 3221‚Äì3229,
   2012.
Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. On finding the largest
   mean among many. arXiv preprint arXiv:1306.3917, 2013.
Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC subset selection in
   stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine
   Learning (ICML-12), pages 655‚Äì662, 2012.
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.
   In Proceedings of the 30th International Conference on Machine Learning, 2013.
Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-
   tion. COLT, 2013.
Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
   problem. The Journal of Machine Learning Research, 5:623‚Äì648, 2004.
Edward Paulson. A sequential procedure for selecting the population with the largest mean from k
   normal populations. The Annals of Mathematical Statistics, 35(1):174‚Äì180, 1964.
                                                 14

         LIL‚Äô UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
Appendix A. Condensed Proof of Lower Bound
We first re-state the main result of Farrell (1964).
                                                   i.i.d.
Theorem 6 (Farrell, 1964, Theorem 1). Let Xi ‚àº N (‚àÜ, 1), where ‚àÜ 6= 0 is unknown. Consider
testing whether ‚àÜ > 0 or ‚àÜ < 0. Let Y ‚àà {‚àí1, 1} be the decision of any such test based on T
samples (possibly a random number) and let Œ¥ ‚àà (0, 1/2). If sup‚àÜ6=0 P(Y 6= sign(‚àÜ)) ‚â§ Œ¥, then
                                                E‚àÜ [T ]
                                   lim sup ‚àÜ‚àí2 log log ‚àÜ‚àí2
                                                             ‚â• 2 ‚àí 4Œ¥.
                                     ‚àÜ‚Üí0
     In the following we show a weaker result than what is shown in Farrell (1964); nonetheless, it
shows the log log term is necessary.
                      i.i.d.
Theorem 7 Let Xi ‚àº N (‚àÜ, 1), where ‚àÜ 6= 0 is unknown. Consider testing whether ‚àÜ > 0 or
‚àÜ < 0. Let Y ‚àà {‚àí1, 1} be the decision of any such test based on T samples (possibly a random
number). If sup‚àÜ6=0 P(Y 6= sign(‚àÜ)) < 1/2, then
                                                 E[T ]
                                  lim sup                          > 0.
                                    ‚àÜ‚Üí0     ‚àÜ‚àí2 log log ‚àÜ‚àí2
     We rely on two intuitive facts, each which justified more formally in Farrell (1964).
Fact 1. The form of an optimal test is a generalized sequential probability ratio test (GSPRT),
       which continues sampling while
                                                  X  t
                                           ‚àíBt ‚â§        Xi ‚â§ Bt
                                                   j=1
       and stops otherwise, declaring ‚àÜ > 0 if tj=1 Xj ‚â• Bt , and ‚àÜ < 0 if tj=1 Xj ‚â§ ‚àíBt
                                                   P                                 P
       where Bt > 0 is non-decreasing in t. This is made formal in Farrell (1964).
Fact 2. If
                                                     Bt
                                          lim ‚àö                ‚â§1                                 (10)
                                         t‚Üí‚àû     2t log log t
       then Y , the decision output by the GSPRT, satisfies sup‚àÜ6=0 P‚àÜ (Y 6= sign ‚àÜ) = 1/2.
       This follows from the LIL and a continuity argument (and note the limit exists as Bt is
       non-decreasing). Intuitively, if the thresholds satisfy (10), a zero mean random walk will
       eventually hit either the upper or lower threshold. The upper threshold is crossed first with
       probability one half, as is the lower. By arguing that the error probabilities are continuous
       functions of ‚àÜ, one concludes this assertion is true.
     The argument proceeds as follows. If (10) is holds, then the error probability is 1/2. So we can
                                                          Bt
focus on threshold sequences satisfying limt‚Üí‚àû ‚àö2t log       log t
                                                                    ‚â• (1 + Œµ) for some Œµ > 0. In other
words, for all t > t1 some Œµ > 0, some sufficiently large t1
                                                  p
                                    Bt ‚â• (1 + Œµ) 2t log log t.
                                                   15

                                     JAMIESON M ALLOY N OWAK B UBECK
Define the function
                                                 Œµ2 ‚àÜ‚àí2              ‚àÜ‚àí2
                                                                        
                                    t0 (‚àÜ) =               log log
                                                    2                 2
and let T be the stopping time:
                                                              t
                                              (                          )
                                                            X
                                 T := inf        t‚ààN:            Xi ‚â• Bt   .
                                                            i=1
      (‚àÜ)                          iid
Let St = tj=1 Xj for Xj ‚àº N (‚àÜ, 1). Without loss of generality, assume ‚àÜ > 0. Additionally,
            P
suppose ‚àÜ is sufficiently small, such that both t0 (‚àÜ) > t1 (Œµ) and ‚àÜ ‚â§ Œµ (in the following steps we
consider the limit as ‚àÜ ‚Üí 0). We have
  P‚àÜ (T ‚â• t0 (‚àÜ))
           Ô£´                           Ô£∂
             t0 (‚àÜ)‚àí1
                           (‚àÜ)
                 \
    = PÔ£≠                |St | < Bt Ô£∏
                 t=1
           Ô£´                                                                                    Ô£∂
              t1 (Œµ)                      t0 (‚àÜ)‚àí1
                         (‚àÜ)                            (0)                  (0)
               \                              \
    =     PÔ£≠        {|St     | < Bt } ‚à©              {St     < Bt ‚àí ‚àÜt} ‚à© {St    > ‚àíBt ‚àí ‚àÜt}Ô£∏
               t=1                       t=t1 (Œµ)+1
           Ô£´                                                                          Ô£∂
              t1 (Œµ)                      t0 (‚àÜ)‚àí1
                         (‚àÜ)                             (0)
               \                              \                           p
    ‚â•     PÔ£≠        {|St     | < Bt } ‚à©              {|St | < (1 + Œµ/2) 2t log log t}Ô£∏                    (11)
               t=1                       t=t1 (Œµ)+1
           Ô£´                      Ô£∂ Ô£´                                                                     Ô£∂
             t1 (Œµ)                         t0 (‚àÜ)‚àí1                                  t1 (Œµ)
                      (‚àÜ)                                 (0)                                  (0)
              \                                 \                         p            \
    = PÔ£≠            |St    | < Bt Ô£∏ P Ô£≠                |St | ‚â§ (1 + Œµ/2) 2t log log t        |St | < Bt Ô£∏
              t=1                          t=t1 (Œµ)+1                                  t=1
           Ô£´                      Ô£∂ Ô£´                                                Ô£∂
             t1 (Œµ)                             ‚àû
                      (‚àÜ)                                 (0)
              \                                 \                         p
    ‚â• PÔ£≠            |St |    < Bt Ô£∏ P Ô£≠                |St | < (1 + Œµ/2) 2t log log tÔ£∏                    (12)
              t=1                          t=t1 (Œµ)+1
where (11) holds when Œµ ‚â• ‚àÜ and (12) holds by removing the conditioning, and then by increasing
                                                                                                     2
the number of terms in the intersection. To see that (11) holds, note that 2 logtlog t ‚â• 2‚àÜ        Œµ    for all
t ‚â§ t0 (‚àÜ), which is easily verified when Œµ ‚â• ‚àÜ since
                                             2 ‚àí2                ‚àí2 
                                  log log Œµ ‚àÜ2 log log ‚àÜ2
                                                        
                                                            ‚àí2
                                                                         ‚â• 1.
                                               log log ‚àÜ2
Taking the limit as ‚àÜ ‚Üí 0, for any Œµ > 0, gives
                                      lim P‚àÜ (T ‚â• t0 (‚àÜ)) ‚â• c(Œµ) > 0
                                     ‚àÜ‚Üí0
where c(Œµ) is a non-zero constant, and the inequality follows from (12), as the first term is non-zero
for any ‚àÜ (including ‚àÜ = 0) since t1 (Œµ) < ‚àû and Bt > 0, and the second term is non-zero by the
LIL for any Œµ > 0. Note that a finite bound on the second term can be obtained as in Section 2.
                                                           16

        LIL‚Äô UCB : A N O PTIMAL E XPLORATION A LGORITHM FOR M ULTI -A RMED BANDITS
    By Markov, E‚àÜ [T ]/t0 (‚àÜ) ‚â• P‚àÜ (T ‚â• t0 (‚àÜ)), and we conclude
                                      E‚àÜ [T ]
                             lim                  ‚â• Œµ2 c(Œµ) > 0
                            ‚àÜ‚Üí0   ‚àÜ‚àí2 log log ‚àÜ‚àí2
for any test with sup‚àÜ6=0 P(Y 6= sign(‚àÜ)) < 1/2.
                                               17

