                                                                   On Finding the Largest Mean Among Many
                                                         Kevin Jamieson† , Matthew Malloy†∗, Robert Nowak† , and Sébastien Bubeck‡
                                                                   †
                                                                     Department of Electrical and Computer Engineering,
                                                                             University of Wisconsin-Madison
arXiv:1306.3917v1 [stat.ML] 17 Jun 2013
                                                                                   ‡
                                                                                     Princeton University,
                                                                Department of Operations Research and Financial Engineering
                                                                                                         Abstract
                                                       Sampling from distributions to find the one with the largest mean arises in a broad range of applica-
                                                   tions, and it can be mathematically modeled as a multi-armed bandit problem in which each distribution
                                                   is associated with an arm. This paper studies the sample complexity of identifying the best arm (largest
                                                   mean) in a multi-armed bandit problem. Motivated by large-scale applications, we are especially in-
                                                   terested in identifying situations where the total number of samples that are necessary and sufficient to
                                                   find the best arm scale linearly with the number of arms. We present a single-parameter multi-armed
                                                   bandit model that spans the range from linear to superlinear sample complexity. We also give a new
                                                   algorithm for best arm identification, called PRISM, with linear sample complexity for a wide range of
                                                   mean distributions. The algorithm, like most exploration procedures for multi-armed bandits, is adaptive
                                                   in the sense that the next arms to sample are selected based on previous samples. We compare the sam-
                                                   ple complexity of adaptive procedures with simpler non-adaptive procedures using new lower bounds.
                                                   For many problem instances, the increased sample complexity required by non-adaptive procedures is a
                                                   polynomial factor of the number of arms.
                                          1 Introduction
                                          This paper studies the sample complexity of finding the best arm in a multi-armed bandit problem. Consider
                                          n + 1 arms with mean payoffs µ0 > µ1 > · · · > µn . The mean values and the ordering of the arms are
                                          unknown. The goal is to identify the arm with the largest mean (i.e., the “best arm”) by sampling the arms.
                                          A sample of arm i is an independent realization of a random variable Xi ∈ [0, 1] with mean µi ∈ [0, 1]
                                          (it is straightforward to extend all results presented in this paper to sub-Gaussian realizations with bounded
                                          means and variances).
                                               The main focus of this paper is identifying necessary and sufficient conditions under which the sample
                                          complexity (total number of samples) of finding the best arm grows linearly in the number of arms. This
                                          is motivated by applications involving very large numbers of arms, such as virus replication experiments
                                          testing thousands of cell strains [3], cognitive radio problems searching over hundreds of communication
                                          channels [1, 2], and network surveillance of large social networks. These applications are time-consuming
                                          and/or costly, so minimizing the number of samples required to find the most influential genes, best chan-
                                          nels, or malicious agents is crucial. This paper quantifies the minimum number of samples needed in such
                                          applications and gives a new algorithm called PRISM that succeeds using a total number of samples within
                                          a negligible factor of the minimum.
                                            ∗
                                                The first two authors are listed in alphabetical order as both contributed equally.
                                                                                                              1

       Mannor and Tsitsiklis [4] showed that for any procedure thatPfinds the best arm with probability at least
1 − δ requires on the order of H log(1/δ) samples, where H := ni=1 (µ0 − µi )−2 . This lower bound shows
that the sample complexity can be much greater than the number of arms. For example, if the gap between
µ0 and µ1 is 1/n, then H, and the sample complexity, are at least O(n2 ). On the other hand, scenarios can
arise in which H grows linearly with n. For instance, if µ0 − µi is greater than a positive constant for all i,
then H = O(n). This case is “sparse” in the sense that µ0 is bounded away from all others, and recent work
has shown that O(n) samples are sufficient in such cases [5].
       Most bandit exploration algorithms are sequential and adaptive in the sense that the selection of the arms
to sample next is based on previous samples. This is necessary in order to achieve linear sample complexity
in the sparse case mentioned above. Non-adaptive methods, in which every arm is sampled an equal number
of times, require at least O(n log n)1 samples [6]. The factor of log n is significant if n is large, which is
one motivation for adaptive strategies like the one used in [3]. Contrasting the differences between adaptive
and non-adaptive sampling is a second focus of this paper.
       Of particular interest here is the scaling of the sample complexity as a function of the number of arms
and the behavior of the gaps between their means. The sparse model discussed above is an enlightening
idealization, but unlikely to arise in practice. A smoothly decaying distribution of means may be a more
reasonable model for the biological and radio applications discussed above. Fig. 1 depicts the means in
three different arm configurations. The left plot (a) represents a sparse model in which µ0 is bounded
away from all others means by a fixed constant (all gaps greater than a fixed constant). In this case, the
sample complexities of non-adaptive and adaptive strategies differs by a factor of log n. The other two plots
represent cases in which the gaps between means are shrinking as n increases. In Figure 1(b) µ0 − µi =
( ni ).49 and in (c) µ0 − µi = ni . The difference between the sample complexities is much more significant
for these non-sparse cases. Also note that there are non-sparse cases in which adaptive strategies can find
the best arm in O(n) samples like the one shown in Fig. 1(b).
           adaptive – O(n)                                                                                  adaptive – O(n 2 )
                                                          adaptive – O(n)
           non-adaptive – O(n logn)                                                                         non-adaptive – O(n 3 )
                                                          non-adaptive – O(n 1.98)
                     (a)                                          (b)                                              (c)
Figure 1: Three different configurations of means, each ordered µ0 > µ1 > · · · > µn . In (a) µ0 − µi ≥ 0.95, in (b)
µ0 − µi = ( ni ).49 , and in (c) µ0 − µi = ni , for i = 1, . . . , n. The necessary sample complexities (sufficient to within
log log n factors) of non-adaptive and adaptive strategies are indicated in each case. Finding the best arm becomes
increasingly difficult as the gaps between the means decrease, but in all cases adaptive strategies have significantly
lower sample complexities.
     1
       In this paper our focus is on how the number of samples scales with n, not necessarily the probability of failure δ. For instance,
if we were to say an algorithm requires just O(n) samples then it is understood that this many samples suffices to find the best arm
with a fixed probability of error. However, in the theorem statements the dependence on δ is explicit.
                                                                   2

1.1 Contributions and Organization
The paper is organized as follows. In Sec. 2, we present a single-parameter model for the distribution of
means that spans the range from linear to superlinear sample complexity. In Sec. 3 we present an algorithm
for best arm identification with sample complexity O(H log(1/δ)) for a wide range of mean distributions.
In particular, we show that the algorithm has linear sample complexity for all of the single-parameter dis-
tributions satisfying H = O(n). Our bounds apply to the PAC (probably approximately correct) setting
(generalizing the algorithm to the fixed budget case of [7, 8] is a challenging open problem). While com-
pleting this paper2 , we became aware of independent work on the best arm problem to appear at ICML 2013
[9]. The algorithm and theoretical analysis in that paper are essentially the same as ours, but in fact the
upper bound on sample complexity bound given in [9] is slightly tighter.
      Sec. 4 discusses the limitations of non-adaptive sampling strategies and shows that any non-adaptive
sampling strategy may require drastically more samples than our adaptive algorithm. Using new lower
bounds for non-adaptive sampling procedures, we show that there exist problems in which the difference
between the sample complexities of non-adaptive and adaptive procedures grows polynomially with the
number of arms. This is somewhat surprising, as the as the advantage of adaptivity in the sparse setting
(where the gaps are bound by a fixed constant) is known to be a factor of log n at best. To be more concrete,
consider the following. We demonstrate problem instances where adaptive procedures, for example, suc-
ceed with just O(n) samples, but all non-adaptive procedures fail without at least O(n1.98 ) samples. This
observation is crucial since it shows that adaptive designs can be vastly superior to simpler non-adaptive
methods often used in practice (e.g., biological applications mentioned above). The take-away message is
that the added implementational burden of adaptive sampling methods may be well worth the investment.
      Notation, in general, follows convention. Since the order and means are unknown, we denote the index
of the best arm as i∗ throughout the paper. An estimate of the best arm is denoted as bi. Proofs of all
Theorems are found in the Appendix.
2 A Single-Parameter Family of Mean Distributions
A lower bound on the sample complexity of finding the best arm follows in a straightforward way from [4,
Theorem 5]; see Theorem 3 in Sec. 4 for the derivation. To find the best arm with probability at least 1 − δ
requires at least
                                                        c1 H log(1/δ)
samples, where c1 > 0 is a universal constant. The quantity H, refereed to as the hardness of the problem,
is given by
                                                               X n
                                                        H=          ∆−2i .                                                         (1)
                                                               i=1
where
                                                        ∆i = µ0 − µi                                                               (2)
is the gap between the best arm and the ith arm.
    2
      The main results of this paper were presented in a lecture (but not as a publication) at the Information Theory and Applications
(ITA) Workshop in San Diego in February 2013.
                                                                   3

    In this paper we focus on a specific parametric family that spans the hardness of the best arm problem
with a single parameter. Consider a model in which the means are given by
                                             µi = µ0 − (i/n)α                                              (3)
for i = 1, . . . , n and some α ≥ 0. We refer to this model as an α-parameterization. Under this model,
∆i = (i/n)α . The α-parameterization spans the range from “hard” problems in which the gaps ∆i are
shrinking quickly as n grows, to “easy” or sparse cases in which the gaps are greater than a constant (when
α = 0). Theorem 3 in Sec. 4 yields the following lower bounds on the sample complexity of identifying the
best arm (ignoring constant and log(1/δ) factors):
                                                     
                                                     
                                                      n               if α < 1/2
                                                     
                                                     
                            sample complexity ≥          n log n       if α = 1/2                          (4)
                                                     
                                                     
                                                     
                                                      n2α             if α > 1/2.
    The focus of this paper is on finding sufficient conditions under which the best arm can be found with
O(n) samples. If the gaps follow the α-parameterization with α ≥ 1/2, (4) implies that the best arm cannot
be found with O(n) samples. Conversely, if the gaps satisfy ∆i ≥ C( ni )α , for α < 1/2, the lower bound
in (4) does not preclude the possibility that order n samples are sufficient to find the best arm. In the next
section, we show that when α < 1/2, order n samples are indeed sufficient.
3 PRISM Algorithm for Best Arm Identification
To show that a linear number of pulls is sufficient for a number of problem instances, we propose and analyze
the algorithm for best arm identification outlined in Fig. 2. The algorithm follow a multi-phase approach,
with a specific allocation of confidence and sampling budgets across phases. The algorithm relies on the
output of Median Elimination [10] to establish a threshold on each phase. We mention again independent
work to appear at ICML 2013 [9], which proposes and analyzes essentially the same algorithm.
                                                            q
                                                               log(1/δ)
  Input δ. Let A1 = {0, 1, . . . , n}, nℓ = ℓ2ℓ , and εℓ =        2ℓ
                                                                        .
  For each phase ℓ = 1, 2, . . . ,
                                                                                              
    (1) Let iℓ be the output of Median Elimination [10] run on Aℓ with accuracy εℓ , δℓ .
    (2) For each arm i ∈ Aℓ , sample nℓ times arm i and let µ     bi (ℓ) be the corresponding average.
    (3) Let
                                         Aℓ+1 = {i ∈ Aℓ : µ  bi (ℓ) ≥ µ biℓ − 2εℓ } .
  Stop when Aℓ contains a unique element bi and output bi.
                         Figure 2: PRISM algorithm for the best arm identification problem.
    The following theorem is our main result. The sample complexity of identifying the best arm with
probability at least 1 − δ is bounded in terms of H and a novel measure of complexity denoted by G :=
                                                         4

Pn        −2        −2
   i=1 ∆i log2 (∆i ).     In general H ≤ G ≤ H log(H) but in many cases, G = H and the bound implies
that the best arm can be found using O(H) samples with a fixed probability of error. This is the best known
bound for the best arm problem. The proof is left to the appendix.
                                            P
Theorem 1. Let δ ∈ (0, 1). Let H = ni=1 ∆−2         i where ∆1 is the minimum gap. Then with probability at
            3δ2     δ        4δ2
least 1 − 1−δ  2 − 1−δ  −  (1−δ2 )2
                                    , the PRISM algorithm   of Fig. 2 stops after at most
                                           "                      n
                                                                                      #!
                                                                 X
                           O log(1/δ) H log(log(1/δ)) +               ∆−2          −2
                                                                        i log 2 (∆i )
                                                                 i=1
samples and outputs arm bi = i∗ .
Corollary 1. Consider the problem instance µ0 = 1 and µi = 1 − (i/n)α , for i = 1, . . . , n and some
0 < α. Then for a fixed probability error, using the PRISM algorithm of Fig. 2 we have that
                                                     
                                                     
                                                        O(n)                if α < 1/2
                                                     
                                                     
                    number of total samples =            O(n log2 n)         if α = 1/2                   (5)
                                                     
                                                     
                                                     
                                                      O(n2α log(n))         if α > 1/2.
and the algorithm outputs arm bi = i∗ .
                       α
Proof. For ∆i = ni         we have the relevant quantities given in the following table (ignoring lower order
terms):
                                                            Pn       −2         −2
                                                  H           i=1 ∆i log2 (∆i )
                                               2α     2α       (2α)2 2α
                                  α > 1/2     2α−1 n           2α−1 n log(n)
                                  α = 1/2     n log(n)           2αn log2 (n)
                                                 2α                   2α
                                  α < 1/2      1−2α n              (1−2α)2
                                                                           n
The result follows from plugging the above quantities into Theorem 1.
Conservative
p                PRISM. Consider the algorithm of Fig. 2, but in the first line set nℓ = 2ℓ and εℓ =
   log(ℓ /δ)/2ℓ and for item (2), run Median Elimination with input (εℓ , δ/ℓ2 ).
         2
                                                                         2      4
Theorem 2.                   With probability at least 1 − 2δ − 6δ − 6δ , Conservative PRISM stops after
            Let δ∈ (0, 0.6].
                     log(H)
at most O H log         δ       pulls and outputs bi = i∗ .
     Theorem 1 matches the lower bound when α < 1/2 and comes within a log(n) factor for α ≥ 1/2. On
the other hand, Theorem 2 comes within a factor of log log n of the lower bound for all α > 0. We note that
the upper bound of [9] also comes no closer than log log n of the lower bound for α ≥ 1/2.
4 Lower Bounds on the Sample Complexity of Non-Adaptive Algorithms
Here we examine the limitations of non-adaptive sampling strategies (which sample all arms an equal num-
ber of times), since these simpler procedures are not uncommon in applications like the biological problems
that partially motivate this paper. A non-adaptive procedure is any procedure that samples each arm m
                                                          5

times, where m is fixed a-priori, and outputs a single arm as an estimate of the best arm. We show that all
non-adaptive methods may require drastically more samples than PRISM. The take-away message is that
the small added difficulty (for the practitioner) in applying PRISM may be well worth the investment.
    We begin by formally stating the adaptive lower bound developed in [4].
Theorem 3. Adaptive Lower Bound [4, Theorem 5]. For every set of means, {µi }ni=0 , µi ∈ (3/8, 1/2],
there exists a joint distribution on the arms such that the arms take mean values {µ0 , . . . , µn }, each arm is
sub-Gaussian, and any adaptive procedure with fewer than
                                                                 1
                                                       c1 H log                                                      (6)
                                                                 8δ
samples in expectation has P(bi 6= i∗ ) ≥ δ for any δ ∈ (0, e−8 /8) and some constant c1 .
Note that the restriction on the means to (3/8, 1/2] in Theorem 3 can be relaxed (see [4] for details). We
proceed with the non-adaptive lower bounds which allow us to compare the best non-adaptive procedures
against adaptive procedures.
Theorem 4. Non-Adaptive Lower Bound. Consider any δ ∈ (0, e−3 /24). For every set of means
{µ1 , . . . , µn }, there exists a joint distribution on the arms such that the arms take mean values {µ1 , . . . , µn },
each arm is sub-Guassian, and any non-adaptive procedure with fewer than
                                                              n 
                                                      H log
                                                               25δ
samples in expectation has P(bi 6= i∗ ) ≥ δ. Moreover, for anyPvalue of H there exists a joint sub-Gaussian
distribution over arms with means {µ0 , . . . , µn } satisfying ni=1 (µ0 − µi )−2 = H, such that any non-
adaptive procedure with fewer than
                                                                   
                                                    Hn           1
                                                          log
                                                      2         24δ
samples in expectation has P(î 6= i∗ ) ≥ δ.
    The lower bound of Theorem 4 consists of two statements, the first which implies that for any set
of means, the sample complexity must be at least order H log n. The second statement, on the other hand,
implies the existence of particular problem instances that are especially difficult for non-adaptive procedures,
requiring order Hn samples. Inspecting the proofs of the two parts of Theorem 4 one sees that the minimum
gap ∆1 , not H, is governing the query complexity for non-adaptive procedures. Using this fact we have the
following Theorem which is proved in the appendix.
Theorem 5. Non-adaptive Lower Bound, α-parameterization. Consider arms with mean values accord-
ing to the parameterization of (3) for some α ≥ 0. There exists a joint sub-Gaussian distribution on the
arms such that any non-adaptive procedure with fewer than
                                            (             
                                                       n
                                               n log 25δ           if α = 0
                                                 2α+1        1
                                                               
                                               n      log 24δ      if α > 0
samples to find the best arm with a fixed probability of failure.
                                                               6

    To see that Theorem 5 is indeed tight, it is straightforward to show that the non-adaptive procedure
which chooses the arm with the largest empirical mean after sampling each arm the same number of times
does indeed meet the lower bound. Letting m be the number of times each arm is sampled. Then
                          X                      X
          P bi 6= i∗   ≤                    bi ) ≤
                                     µ i∗ ≤ µ
                                  P (b                   (P (b                         µi ≥ µi + ∆i /2))
                                                               µi∗ ≤ µi∗ − ∆i /2) + P (b
                            i6=i∗                  i6=i∗
                            X                         X                   2α !
                                                                           i
                       ≤          2 exp(−m∆2i )    =         2 exp −m
                                                                          n
                            i6=i∗                     i6=i ∗
which follow from a union bound and Hoeffding’s inequality. For α 6= 0, if m ≥ n2α (which implies the
total number of samples is greater than n2α+1 ) the above sum is convergent, and the probability that the
wrong arm is returned is controlled. The case where α = 0 is also controlled if m ≥ log n.
    We conclude that for α ∈ (0, 1/2), when compared to adaptive procedures that require just O(n) sam-
ples, any non-adaptive procedure requires a factor of n2α more samples to identify the best arm. The
implications of this observation can be somewhat surprising: for many problem instances, the improvement
in the sample complexity resulting from adaptivity is polynomial in n, compared with the typical log(n)
improvement observed for sparse problems (α = 0).
References
 [1] Simon Haykin. Cognitive radio: brain-empowered wireless communications. Selected Areas in Com-
      munications, IEEE Journal on, 23(2):201–220, 2005.
 [2] David López-Pérez, Alvaro Valcarce, Guillaume De La Roche, and Jie Zhang. OFDMA femtocells: A
      roadmap on interference avoidance. Communications Magazine, IEEE, 47(9):41–48, 2009.
 [3] L. Hao, A. Sakurai, T. Watanabe, E. Sorensen, C. Nidom, M. Newton, P. Ahlquist, and Y. Kawaoka.
      Drosophila RNAi screen identifies host genes important for influenza virus replication. Nature, page
      8903, 2008.
 [4] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
      problem. The Journal of Machine Learning Research, 5:623–648, 2004.
 [5] Matthew L Malloy and Robert Nowak.                 Sequential testing for sparse recovery.   arXiv preprint
      arXiv:1212.1801, 2012.
 [6] Matt Malloy and Robert Nowak. On the limits of sequential testing in high dimensions. In Signals,
      Systems and Computers (ASILOMAR), 2011 Conference Record of the Forty Fifth Asilomar Conference
      on, pages 1245–1249. IEEE, 2011.
 [7] J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In Proceed-
      ings of the 23rd Annual Conference on Learning Theory (COLT), 2010.
 [8] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings
      of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.
 [9] Z. Karnin, T. Koren, and O. Somekh. Almost optimal exploration in multi-armed bandits. Proceedings
      of the 30th International Conference on Machine Learning, June 2013.
                                                             7

[10] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the
       multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research,
       7:1079–1105, 2006.
[11] G. Abreu. Very simple tight bounds on the q-function. Communications, IEEE Transactions on,
       60(9):2415–2420, 2012.
[12] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer Science and Business Media,
       2006.
A      Appendix
A.1 Proof of Theorem 1
Proof. It will be useful to consider the following ‘slicing’ of arms:
                                                    √                    √
                                Ωs = {i ∈ [n] : 5 2 εs+1 < ∆i ≤ 5 2 εs }, s ≥ 1.
Note that
                                          X                                     X
                          25 log (1/δ)        ∆−2      s
                                                 i ≤ 2 |Ωs | ≤ 50 log (1/δ)         ∆−2i .                       (7)
                                         i∈Ωs                                  i∈Ωs
Step 1: A good event. In this step we describe the event of probability 1 − δ on which we will prove the
result. We want the following to hold:
                        bi∗ (ℓ) − µ∗ ≥ −εℓ , ∀ℓ ≥ 1 ,
                        µ                                                                                        (8)
                          µiℓ (ℓ) − µiℓ | ≤ εℓ , ∀ℓ ≥ 1 ,
                         |b                                                                                      (9)
                         max µj − µiℓ ≤ εℓ , ∀ℓ ≥ 1 ,                                                          (10)
                         j∈Aℓ
                                                                     |Aℓ ∩ Ωs |
                            i ∈ Aℓ ∩ Ωs : µ bi (ℓ) ≥ µ
                                                     biℓ − 2εℓ     ≤             , ∀ℓ ≥ s ≥ 1.                 (11)
                                                                           4
We first bound the probability that the above events do not hold. By Hoeffding’s inequality we have
                                  P (µ̂i (ℓ) − µi ≥ εℓ ) ≤ exp(−2nℓ ε2ℓ ) = δ2ℓ
for any i and note that an analogous inequality holds for the deviation away from its mean in the other
direction. Thus, applying Hoeffding’s and a union bound we have that the probability that 8) is not satisfied
              δ2
is less than 1−δ 2 . The probability that (9) is not satisfied is bound in the exact same manner with an additional
factor of two to satisfy both inequality directions. The only subtlety is that after the union bound one needs
to condition on the value of iℓ before using Hoeffding’s inequality, and this is possible since the random
variables obtain in Step (2) of the algorithm are independent of iℓ . By the properties of median elimination
                                                                                            δ
and a union bound we have that the probability that (10) is not satisfied is less than 1−δ     . Observe that by (8)
and (9) one always has:
                                   bi∗ (ℓ) ≥ µ∗ − εℓ ≥ µiℓ − εℓ ≥ µ
                                   µ                                   biℓ (ℓ) − 2εℓ .
which implies that the best arm is never removed from the set.
                                                            8

    It remains to bound the probability that (11) is not satisfied while (8), (9) and (10) are satisfied.
                                                                                                              
                                                                       |Aℓ ∩ Ωs |
                  P      i ∈ Aℓ ∩ Ωs : µ bi (ℓ) ≥ µ  biℓ − 2εℓ >                      |(8), (9), (10), Aℓ , iℓ
                                                                              4
                                                                                            
                                                          ∗                |Aℓ ∩ Ωs |
                   ≤P        i ∈ Aℓ ∩ Ωs : µ  bi (ℓ) ≥ µ − 4εℓ >                         |Aℓ
                                                                                  4
                                                                                         
                           4                                          ∗
                   ≤             E      i ∈ Aℓ ∩ Ωs : µ     bi (ℓ) ≥ µ − 4εℓ | Aℓ
                      |Aℓ ∩ Ωs |
                           4       X
                   =                       P (bµi (ℓ) ≥ µ∗ − 4 εℓ | Aℓ )
                      |Aℓ ∩ Ωs |
                                 i∈Aℓ ∩Ωs
                   ≤ 4 exp(−2nℓ ε2ℓ ) = 4δ2ℓ .
                                                                                                   √
where the last inequality follows since all elements of Ωs have gaps greater thanP5 2εP                 s+1 = 5εs . Summing
                                                                                                                          4δ2
over all s ≤ ℓ and then over ℓ ≥ 1 gives the probability that (11) is not satisfied: ∞             ℓ=1
                                                                                                                   2ℓ
                                                                                                           s≤ℓ 4δ = (1−δ2 )2 .
In the next steps we will assume that (8)-(9)-(10)-(11) are satisfied as they all hold with probability at least
      3δ2       δ        4δ2
1 − 1−δ   2 − 1−δ − (1−δ 2 )2 .
Step 2: Bound on the total number of phases. It suffices to bound the number of phases given that (8), (9),
(10) and (11) hold. Let L denote the first phase such that |Aℓ | = 1 (if there is no such phase then L = +∞).
Observe that using (11) one can show by induction that
                                                         X  ℓ             +∞
                                                                          X
                                                                |Ωs |
                                          |Aℓ | ≤ 1 +                 +          |Ωs |.                                     (12)
                                                                4ℓ−s
                                                          s=1            s=ℓ+1
Define s∗ = log2 (∆−2                                                     ∗
                       1 log(1/δ)) so that Ωs = ∅ for all s > s . By definition, when ℓ ≥ s the third term in
                                                                                                             ∗
the equation immediately above is equal to zero so that
                                                          s ∗
                                                         X
                                                     −ℓ
                                                                                   ∀ℓ ≥ s∗
                                                                    ∗
                                    |Aℓ | ≤ 1 + 2              2s−s 2s |Ωs |
                                                         s=1
where we have
         s ∗                                   ∞
        X         ∗                  ∆21 X 2s
             2s−s 2s |Ωs | =                       2 |Ωs |
        s=1
                                 log(1/δ) s=1
                                               ∞         n
                                                                (       r                                r              )
                                     ∆21 X 2s X                    √        log(1/δ)                √        log(1/δ)
                             =                     2          1 5 2                     < ∆i ≤ 5 2
                                 log(1/δ)                                      2s+1                              2s
                                              s=1       i=1
                                             X n
                                     ∆21           502 log(1/δ)2
                             ≤                                        = 502 log(1/δ)H∆21 .
                                 log(1/δ)
                                              i=1
                                                           ∆2i
We conclude that for
                                                                                           
  L := 1 + max s∗ , log2 (502 log(1/δ)H∆21 ) = log2 (2 log(1/δ)) + max log2 (∆−2                                      2
                                                                                                       1 ), log 2 (50 H∆1 )
                                                                                                                           2
we have that |Aℓ | < 2 whenever ℓ ≥ L. Hence, L is an upper bound on the stopping time.
Step 3: Bound on the total number of pulls. Recall that Median Elimination applied to a set Aℓ with
parameters εℓ , δℓ takes no more than cεME                                     ℓ
                                             2 |Aℓ |ℓ log(1/δ) = cME ℓ2 |Aℓ | pulls. Thus, the total number of pulls
                                             ℓ
                                                                 9

on phase ℓ is bounded by cℓ2ℓ |Aℓ | pulls with c = cME + 1. Using the results from the previous step (in
particular (12) and the stopping time L) one has that the total number of pulls is bounded from above by
               L                 L                  ℓ               ∞
                                                                              !
              X                 X                 X    |Ωs |      X
                     ℓ                 ℓ
                  cℓ2 |Aℓ | ≤       cℓ2 1 +              ℓ−s
                                                               +         |Ωs |
                                                  s=1
                                                       4
              ℓ=1               ℓ=1                              s=ℓ+1
                                              +∞ X
                                              X       +∞  s                       
                                    L+1                       2            2ℓ
                             ≤ cL2       +c         ℓ            1s≤ℓ + s 1s>ℓ 2s |Ωs |
                                                              2ℓ           2
                                              ℓ=1 s=1
                                               X+∞
                             ≤ cL2L+1 + 3c               s
                                                      s2 |Ωs | + 2c(n − 1)
                                                s=1
                             = cL2L+1 + 150c log(1/δ) [log2 (50 log(1/δ))H + G] + 2c(n − 1)
where
                                             X n
                                                                       
                                H ≤ G :=            ∆−2
                                                      i log2 ∆i
                                                                    −2
                                                                          ≤ H log(H)
                                              i=1
which follows directly from
                 ∞                ∞        n
                                                  (         r                           r          )
                X                X       X             √       log(1/δ)               √   log(1/δ)
                    s2s |Ωs | =      s2s       1 5 2               s+1
                                                                           < ∆i ≤ 5 2
                                                                 2                           s2s
                s=1              s=1     i=1
                                               X  n                              
                                                       1           50 log(1/δ)
                              ≤ 50 log(1/δ)                log2                     .
                                                i=1
                                                      ∆2i               ∆2i
Evaluating L2L+1 and collecting terms obtains the result.
A.2 Proof of Theorem 3
Proof. Assume some procedure has P(bi 6= i∗ ) ≤ δ and requires fewer than c1 H log 8δ          1
                                                                                                 samples for some
{µi }ni=0 , µi ∈ (3/8, 1/2]. This procedure is by definition (ε, δ) PAC (probably approximately correct) for
any ε ∈ (0, ∆1 ). [4, Theorem 5] implies any (ε, δ), procedure, ε ∈ (0, ∆1 ), requires more than
                                                 X          1           1
                                             c1                   log
                                                       µ i∗ − µ i      8δ
                                                 i∈N
samples in expectation, where
                                       (                                            )
                                                                          ε + µ i∗
                                 N =     i : µi ≤ µi∗ − ε, µi ≥              p
                                                                       1 + 1/2
Since µi ∈ (3/8, 1/2], N := [n]. Any procedure requires more than
                                                              1
                                                   ci H log
                                                              8δ
samples in expectation. This negates the original assumption.
                                                           10

A.3 Proof of Theorem 4
Proof. We restrict our attention to reward distributions of the form N (µi , 1). Assume that µi , i = 0, . . . , n,
are know up to a permutation, and let each arm be assigned a mean uniformly at random. We first show
that the test with minimum average probability P              of error simply picks the largest empirical mean among all
            b
arms, i.e., i = arg maxi µ   bi , where µ      bi = 1/m m        j=1 Xi,j , and Xi,j represents the reward of arm i on the
jth play of that arm, and m is the total number of samples of each arm. This can be seen by considering
the maximum a-posteriori (MAP) estimator of the best arm, which by definition has the smallest probably
of error. Under the assumption that the arms are assigned means uniformly at random, the MAP estimator
reduces to the maximum likelihood (ML) estimator:
                                    biMAP = biML = arg max P (X0m , . . . , Xnm |Hi ),
                                                                   i
where Xim = Xi,1 , . . . , Xi,m and Hi is event that arm i is the best arm. Consider comparing between events
                                                                                ′
Hi and Hi′ , i 6= i′ : the ML test is P (X0m , . . . , Xnm |Hi ) ≶ii P (X0m , . . . , Xnm |Hi′ ). By the independence
across arms, it is straightforward to show this test reduces to:
                                                                  ′
                             P (Xim |Hi )P (Xim′ |Hi ) ≶ii P (Xim |Hi′ )P (Xim′ |Hi′ )                                               (13)
The distribution of   Xim , given Hi , is simply
                                                         1                                        
                               P (Xim |Hi ) = √ exp − ||Xim − µi∗ 1||2 /2                                                            (14)
                                                          2π
where Xim = [Xi,1 , . . . , Xi,m ]T ∈ Rm . The marginal distribution on arm i′ , given that arm i is the largest,
follows a mixture distribution:
                                                      1 X
                                                             n                                       
                           P (Xim′ |Hi ) = √                     exp − ||Xim′ − µj 1||2 /2 .                                         (15)
                                                  n 2π j=1
Combining (13), (14), and (15), after a number of straightforwardP                     manipulations,Pexcluded for brevity, it can
be shown that the ML estimate prefers arm i to i if and only if m
                                                               ′
                                                                                          ℓ=1 Xi,ℓ >
                                                                                                             m
                                                                                                             ℓ=1 Xi′ ,ℓ , or equivalently,
bi > µ
µ     bi′ . The estimate with minimum probability of error is simply bi = arg maxi µ                          bi .
    We continue by bounding the probability of error of the maximum likelihood test. For any estimator,
                                                          
                                   [
         P(bi 6= i∗ ) ≥ P               b i∗ − µ
                                         µ        bi ≤ 0
                                  i6=i∗
                                                                                               
                                                        [
                      = P(b   µ i∗ ≥ µ i∗ ) P               bi∗ − µ
                                                             µ        bi ≤ 0 µ    bi∗ ≥ µi∗ 
                                                       i6=i∗
                                                                                                              
                                                                         [
                                        +     P(b µ i∗ < µ i∗ ) P            b i∗ − µ
                                                                              µ        bi ≤ 0 µ   b i∗ < µ i∗ 
                                                                        i6=i∗
                                                                                                 
                            1         [                        1                   \
                      ≥       P             µbi ≥ µi∗  = 1 − P                        bi ≤ µi∗ 
                                                                                         µ
                            2                                   2
                                      i6=i∗                                        i6=i∗
                                                                                                                            
                                         Y           q                                 Y                                  
                            1                                                1                          1                  
                      =           1−            FN         m∆2i  ≥ 1 −                          1−        exp −m∆2i               (16)
                            2                                                 2                         12
                                         i6=i ∗                                          i6=i ∗
                                                                                                            
                                                                    Y                                     
                                                         1                          1                   
                      ≥               minP 1                 1−             1−          exp −m∆2i                                   (17)
                            ∆1 ,...,∆n : i 2 =H          2                         12
                                              ∆
                                                i
                                                                   i6=i ∗
                                                                    11

where FN (x) is the standard Gaussian CDF. The inequality in (16) follows since FN (x) ≤ 1−exp(−x2 )/12
for x ≥ 0 [11, Eqn. 13]. The next step in the proof will be showing that error probabilities     p smaller than a
fixed constant, (17) is minimized when the gaps are equal, i.e., when ∆1 = ∆2 = ... = n/H. First define
z ∈ Rn+ with elements zi := 1/(m∆2i ). We can recover the minimum of (17) by solving
                                                 X n                                 
                                                                    1
                                  argmax              log 1 −           exp (−1/zi ) .                            (18)
                             z∈Rn      T
                                  + :1 z=H/m i=1
                                                                   12
Define the Lagrangian of (18) as
                                     X n                                 
                                                        1          −1
                                                                        
                     L(z, λ) = −           log 1 −         exp zi           − λ(1T z − H/m).
                                                       12
                                     i=1
From [12, p. 321], any z that maximizes (18) necessarily satisfies
                                   ∂L                    zi−2
                                            =                           − λzi = 0 ∀ i                             (19)
                                   ∂zi          12 exp(zi−1 ) − 1
                                  1T z = H.
The above system of equations is satisfied by pairs (z, λ) that satisfy
                                                                          
                                                            zi−3
                                         zi : λ =                              ∀i                                 (20)
                                                    12 exp(zi−1 ) − 1
and 1T z = H simultaneously. Differentiation of (20) shows the function λ(zi ) is monotonically increasing
in zi for zi ≤ 1/3. First, consider a solution to (19) which has one or more zi ≥ 1/3. This would imply
m∆2i ≤ 3 for some i, and from (17), P(bi 6= i∗ ) ≥ 24            1
                                                                    exp(−3). For any λ, (20) is satisfied by at most
one zi ∈ (0, 1/3] by the monotonicity of the function on this range; this implies implies either 1) the z that
maximizes (18) has the form z1 = z2 = · · · = zn , or 2) P(bi 6= i∗ ) ≥ 24          1
                                                                                       exp(−3). We focus our attention
                                                                                P                    p
on the case when z1 = · · · = zn (and thus ∆1 = · · · = ∆n ). Since i 1/∆2i = H, ∆i = n/H for all i.
(17) gives
                                                  1
                                                           
                                                                      1       mn n 
                               b       ∗
                            P(i 6= i ) ≥              1− 1−               exp −             .
                                                  2                  12           H
                                                                                                      
Recall the total number of samples is given by nm. If mn ≤ H(log n + log (25δ)−1 ) , then for δ ∈
(0, e−3 /24)
                                                                       n 
                                     ∗          1                  25δ
                           P(bi 6= i ) ≥             1− 1−                                                        (21)
                                                2                  12n
                                                                         
                                                1                     25δ
                                           ≥         1 − exp −                   for all n ≥ 1                    (22)
                                                2                     12
                                           ≥ δ                                                                    (23)
which completes the proof of the first statement of the theorem.
     To prove the second statement of the theorem, consider the following set of gaps –
                                                   q
                                                    2               i=1
                                            ∆i = q H
                                                    2(n−1) i > 1.
                                                            H
                                                              12

                                     P
Note that {∆1 , ..., ∆n } satisfy ni=1 1/∆2i = H. From (16), and by considering only the arm with the
smallest gap,
                                                                     
                                        Y                                              
                              1                     1                     1         2m
                  b
               P(i 6= i) ≥          1−           1−    exp −m∆i  2     ≥     exp −         .            (24)
                              2                     12                     24         H
                                        i6=i ∗
                                             
If m ≤   H
         2 log    1
                 24δ   , we have P bi 6= i∗ ≥ δ. This implies that if the total number of measurements is less
                  
than  Hn
       2 log    1
               24δ ,  then P(bi 6= i∗ ) ≥ δ, completing the proof of the second statement of Thm. 4.
A.4 Proof of Corollary 2
Proof. When α = 0, Theorem 4 implies the result. When α > 0, we can bound (16) by dropping all terms
in the product except the term corresponding to the smallest gap. This gives
                                             1                1                 
                             P(bi 6= i) ≥       exp −m∆21 =       exp −mn−2α                             (25)
                                            24                 24
                          1
                             
Setting m ≤ n2α log 24δ        implies the result.
                                                          13

