                         Matrix Completion from a Few Entries
                 Raghunandan H. Keshavanâˆ—, Andrea Montanariâˆ—â€ , and Sewoong Ohâˆ—
                                                  March 17, 2009
                                                     Abstract
           Let M be a random nÎ± Ã— n matrix of rank r â‰ª n, and assume that a uniformly random
       subset E of its entries is observed. We describe an efficient algorithm that reconstructs M from
       |E| = O(r n) observed entries with relative root mean square error
                                                               1/2
                                                             nr
                                          RMSE â‰¤ C(Î±)                .
                                                            |E|
       Further, if r = O(1), M can be reconstructed exactly from |E| = O(n log n) entries. These results
       apply beyond random matrices to general low-rank incoherent matrices.
           This settles (in the case of bounded rank) a question left open by CandeÌ€s and Recht and
       improves over the guarantees for their reconstruction algorithm. The complexity of our algorithm
       is O(|E|r log n), which opens the way to its use for massive data sets. In the process of proving
       these statements, we obtain a generalization of a celebrated result by Friedman-Kahn-SzemereÌdi
       and Feige-Ofek on the spectrum of sparse random matrices.
1      Introduction
Imagine that each of m customers watches and rates a subset of the n movies available through a
movie rental service. This yields a dataset of customer-movie pairs (i, j) âˆˆ E âŠ† [m] Ã— [n] and, for
each such pair, a rating Mij âˆˆ R. The objective of collaborative filtering is to predict the rating for
the missing pairs in such a way as to provide targeted suggestions.1 The general question we address
here is: Under which conditions do the known ratings provide sufficient information to infer the
unknown ones? Can this inference problem be solved efficiently? The second question is particularly
important in view of the massive size of actual data sets.
1.1     Model definition
A simple mathematical model for such data assumes that the (unknown) matrix of ratings has rank
r â‰ª m, n. More precisely, we denote by M the matrix whose entry (i, j) âˆˆ [m] Ã— [n] corresponds
to the rating user i would assign to movie j. We assume that there exist matrices U , of dimensions
m Ã— r, and V , of dimensions n Ã— r, and a diagonal matrix Î£, of dimensions r Ã— r such that
                                                 M = U Î£V T .                                                    (1)
   âˆ—
     Department of Electrical Engineering, Stanford University
   â€ 
     Departments of Statistics, Stanford University
   1
     Indeed, in 2006, Netflix made public such a dataset with m â‰ˆ 5 Â· 105 , n â‰ˆ 2 Â· 104 and |E| â‰ˆ 108 and challenged
the research community to predict the missing ratings with root mean square error below 0.8563 [Net].
                                                         1

For justification of these assumptions and background on the use of low rank matrices in information
retrieval, we refer to [BDJ99]. Since we are interested in very large data sets, we shall focus on the
limit m, n â†’ âˆž with m/n = Î± bounded away from 0 and âˆž.
    We further assume that the factors U , V are unstructured. This notion is formalized by the
incoherence condition introduced by CandeÌs and Recht [CR08], and defined in Section 2. In particular
the incoherence condition is satisfied with high probability if M = U Î£V T with U and V uniformly
random matrices with U T U = m1 and V T V = n1. Alternatively, incoherence holds if the entries of
U and V are i.i.d. bounded random variables.
    Out of the m Ã— n entries of M , a subset E âŠ† [m] Ã— [n] (the user/movie pairs for which a rating
is available) is revealed. We let M E be the m Ã— n matrix that contains the revealed entries of M ,
and is filled with 0â€™s in the other positions
                                           
                                     E        Mi,j if (i, j) âˆˆ E ,
                                   Mi,j =                                                            (2)
                                                 0 otherwise.
The set E will be uniformly random given its size |E|.
1.2    Algorithm
A naive algorithm consists of the following projection operation.
Projection. Compute the singular value decomposition (SVD) of M E (with Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ 0)
                                                min(m,n)
                                                  X
                                            E
                                         M    =          Ïƒi xi yiT ,                                 (3)
                                                  i=1
                                              P
and return the matrix Tr (M E ) = (mn/|E|) ri=1 Ïƒi xi yiT obtained by setting to 0 all but the r largest
singular values. Notice that, apart from the rescaling factor (mn/|E|), Tr (M E ) is the orthogonal
projection of M E onto the set of rank-r matrices. The rescaling factor compensates the smaller
average size of the entries of M E with respect to M .
    It turns out that, if |E| = Î˜(n), this algorithm performs very poorly. The reason is that the
matrix M E contains columns and rows withpÎ˜(log n/ log log n) non-zero (revealed) entries. The
largest singular values of M E are of order Î˜( log n/ log log n). The corresponding singular vectors
are highly concentrated on high-weight column or row indices (respectively, for left and right singular
vectors). Such singular vectors are an artifact of the high-weight columns/rows and do not provide
useful information about the hidden entries of M . This motivates the definition of the following
operation (hereafter the degree of a column or of a row is the number of its revealed entries).
Trimming. Set to zero all columns in M E with degree larger that 2|E|/n. Set to 0 all rows with
degree larger than 2|E|/m.
    Figure 1 shows the singular value distributions of M E and M     fE for a random rank-3 matrix M .
The surprise is that trimming (which amounts to â€˜throwing out informationâ€™) makes the underlying
rank-3 structure much more apparent. This effect becomes even more important when the number
of revealed entries per row/column follows a heavy tail distribution, as for real data.
    In terms of the above routines, our algorithm has the following structure.
     Spectral Matrix Completion( matrix M E )
     1: Trim M E , and let M  fE be the output;
     2: Project M  fE to Tr (M fE );
     3: Clean residual errors by minimizing the discrepancy F (X, Y ).
                                                    2

                                                              35
      40
                                                              30
      35
                                                              25
      30
      25                                                      20
      20
                                                              15
      15
                                                              10
      10
                                Ïƒ4 Ïƒ3 Ïƒ2      Ïƒ1                                             Ïƒ3 Ïƒ2 Ïƒ1
                                                               5
       5
       0                                                       0
        0     10     20    30    40      50     60              0  5  10 15     20  25 30 35   40   45
Figure 1: Histogram of the singular values of a partially revealed matrix M E before trimming (left) and after
trimming (right) for 104 Ã— 104 random rank-3 matrix M with Ç« = 30 and Î£ = diag(1, 1.1, 1.2). After trimming
the underlying rank-3 structure becomes clear. Here the number of revealed entries per row follows a heavy
tail distribution with P{N = k} = const./k 3 .
     The last step of the above algorithm allows to reduce (or eliminate) small discrepancies between
Tr (MfE ) and M , and is described below.
Cleaning. Various implementation are possible, but we found the following one particularly appeal-
ing. Given X âˆˆ RmÃ—r , Y âˆˆ RnÃ—r with X T X = m1 and Y T Y = n1, we define
                                 F (X, Y ) â‰¡        min F(X, Y, S) ,                                      (4)
                                                   SâˆˆRrÃ—r
                                                   1 X
                               F(X, Y, S) â‰¡                  (Mij âˆ’ (XSY T )ij )2 .                       (5)
                                                   2
                                                     (i,j)âˆˆE
The cleaning step consists in writing Tr (M fE ) = X0 S0 Y T and minimizing F (X, Y ) locally with initial
                                                                 0
condition X = X0 , Y = Y0 .
     Notice that F (X, Y ) is easy to evaluate since it is defined by minimizing the quadratic function
S 7â†’ F(X, Y, S) over the low-dimensional matrix S. Further it depends on X and Y only through
their column spaces. In geometric terms, F is a function defined over the cartesian product of
two Grassmann manifolds (we refer to Section 5 for background and references). Optimization
over Grassmann manifolds is a well understood topic [EAS99] and efficient algorithms (in particular
Newton and conjugate gradient) can be applied. To be definite, we assume that gradient descent
with line search is used to minimize F (X, Y ).
     Finally, the implementation proposed here implicitly assumes that the rank r is known. In
practice this is a non-issue. Since r â‰ª n, a loop over the value of r can be added at little extra cost.
For instance, in collaborative filtering applications, r ranges between 10 and 30.
1.3      Main results
Notice that computing Tr (M   fE ) only requires to find the first r singular vectors of a sparse matrix.
Our main result establishes that this simple procedure achieves arbitrarily small relative root mean
                                                          3

square error from O(nr) revealed entries. We define the relative root mean square error as
                                                                   1/2
                                             1             fE )||2F
                               RMSE â‰¡           ||M âˆ’ Tr (M              .                           (6)
                                           mnr
where we denote by ||A||F the Frobenius norm of matrix A. Notice that the factor (1/mn) corresponds
to the usual normalization by the number of entries. The factor (1/r) is instead necessary because
                                                                         âˆš
(as described in Section 2), the typical size of the entries of M is r.
Theorem 1.1. Assume M to be a rank r â‰¤ n1/2 matrix that satisfies the incoherence condition A2
(in particular, this is the case for random orthogonal matrices U, V ). Then with high probability
                                   1             fE )||2F â‰¤ C(Î±) nr .
                                      ||M âˆ’ Tr (M                                                    (7)
                                 mnr                               |E|
    The proof is provided in Section 3.
    Notice that the top r singular values and singular vectors of the sparse matrix M       fE can be
computed efficiently by subspace iteration [Ber92]. Each iteration requires O(|E|r) operations. As
proved in Section 3, the (r + 1)-th singular value is smaller than one half of the r-th one. As a
consequence, subspace iteration converges exponentially. A simple calculation shows that O(log n)
iterations are sufficient to ensure the error bound mentioned.
    The â€˜cleaningâ€™ step in the above pseudocode improves systematically over Tr (M   fE ) and, for large
enough |E|, reconstructs M exactly.
Theorem 1.2. Assume M to be a rank r â‰¤ n1/2 matrix that satisfies the incoherence conditions A1
and A2. Further, assume Î£min â‰¤ Î£1 , . . . , Î£r â‰¤ Î£max with Î£min , Î£max bounded away from 0 and âˆž.
Then there exists C â€² (Î±) such that, if
                                    |E| â‰¥ C â€² (Î±)nr max{log n, r} ,                                  (8)
then the cleaning procedure in Spectral Matrix Completion converges, with high probability, to
the matrix M .
    The proof is provided in Section 5. The basic intuition is that, for |E| â‰¥ C â€² (Î±)nr max{log n, r},
    fE ) is so close to M that the cost function is well approximated by a quadratic function.
Tr (M
    Theorem 1.1 is optimal: the number of degrees of freedom in M is of order nr, without the same
number of observations is impossible to fix them. The extra log n factor in Theorem 1.2 is due to a
coupon-collector effect [CR08, KMO08, KOM09]: it is necessary that E contains at least one entry
per row and one per column and this happens only for |E| â‰¥ Cn log n. As a consequence, for rank r
bounded, Theorem 1.2 is optimal. It is suboptimal by a polylogarithmic factor for r = O(log n).
1.4    Related work
Beyond collaborative filtering, low rank models are used for clustering, information retrieval, machine
learning, and image processing. In [Faz02], the NP-hard problem of finding a matrix of minimum
rank satisfying a set of affine constraints was addresses through convex relaxation. This problem is
analogous to the problem of finding the sparsest vector satisfying a set of affine constraints, which
is at the heart of compressed sensing [Don06, CRT06]. The connection with compressed sensing was
emphasized in [RFP07], that provided performance guarantees under appropriate conditions on the
constraints.
                                                     4

     In the case of collaborative filtering, we are interested in finding a matrix M of minimum rank
that matches the known entries {Mij : (i, j) âˆˆ E}. Each known entry thus provides an affine
constraint. CandeÌ€s and Recht [CR08] introduced the incoherent model for M . Within this model,
they proved that, if E is random, the convex relaxation correctly reconstructs M as long as |E| â‰¥
C r n6/5 log n. On the other hand, from a purely information theoretic point of view (i.e. disregarding
algorithmic considerations), it is clear that |E| = O(n r) observations should allow to reconstruct M
with arbitrary precision. Indeed this point was raised in [CR08] and proved in [KMO08], through a
counting argument.
     The present paper describes an efficient algorithm that reconstructs a rank-r matrix from O(n r)
random observations. The most complex component of our algorithm is the SVD in step 2. We were
able to treat realistic data sets with n â‰ˆ 105 . This must be compared with the O(n4 ) complexity of
semidefinite programming [CR08].
     Cai, CandeÌ€s and Shen [CCS08] recently proposed a low-complexity procedure to solve the convex
program posed in [CR08]. Our spectral method is akin to a single step of this procedure, with the
important novelty of the trimming step that improves significantly its performances. Our analysis
techniques might provide a new tool for characterizing the convex relaxation as well.
     Theorem 1.1 can also be compared with a copious line of work in the theoretical computer science
literature [FKV04, AFK+ 01, AM07]. An important motivation in this context is the development of
fast algorithms for low-rank approximation. In particular, Achlioptas and McSherry [AM07] prove
a theorem analogous to 1.1, but holding only for |E| â‰¥ (8 log n)4 n (in the case of square matrices).
     A short account of our results was submitted to the 2009 International Symposium on Information
Theory [KOM09]. While the present paper was under completion, CaÌndes and Tao posted online
a preprint proving a theorem analogous to 1.2 [CT09]. Once more, their approach is substantially
different from ours.
1.5     Open problems and future directions
It is worth pointing out some limitations of our results, and interesting research directions:
     1. Optimal RMSE with O(n) entries. Numerical simulations with the Spectral Matrix Com-
pletion algorithm suggest that the RMSE decays much faster with the number of observations
per degree of freedom (|E|/nr), than indicated by Eq. (7). This improved behavior is a product of
the cleaning step in the algorithm. It would be important to characterize the decay of RMSE with
(|E|/nr).
     2. Threshold for exact completion. As pointed out, Theorem 1.2 is order optimal for r bounded.
It would nevertheless be useful to derive quantitatively sharp estimates in this regime. A systematic
numerical study was initiated in [KMO08]. It appears that available theoretical estimates (including
the recent ones in [CT09]) are for larger values of the rank, we expect that our arguments can be
strenghtened to prove exact reconstruction for |E| â‰¥ C â€² (Î±)nr log n for all values of r.
     3. More general models. The model studied here and introduced in [CR08] presents obvious
limitations. In applications to collaborative filtering, the subset of observed entries E is far from
uniformly random. A recent paper [SC09] investigates the uniqueness of the solution of the matrix
completion problem for general sets E. In applications to fast low-rank approximation, it would be
desirable to consider non-incoherent matrices as well (as in [AM07]).
                                                     5

2       Incoherence property and some notations
The matrix M to be reconstructed takes the form (1) where U âˆˆ RmÃ—r , V âˆˆ RnÃ—r . We write
                                                                                                                  âˆš
U = [u1 , u2 , . . . , ur ] and V = [v1 , v2 , . . . , vr ] for the columns of the two factors, with ||ui || = m,
          âˆš
||vi || = n and uTi uj = 0, viT vj = 0 for i 6= j (there is no loss of generality in this, since normalizations
can be adsorbed by redefining Î£). We shall further write Î£ = diag(Î£1 , . . . , Î£r ) with Î£1 â‰¥ Î£2 â‰¥
Â· Â· Â· â‰¥ Î£r â‰¥ 0.
      The matrices U , V and Î£ will be said to be incoherent if they satisfy the following properties:
                                                                                                      P
 A1. There exists a constant Âµ0 > 0 such that for all i âˆˆ [m], j âˆˆ [n] we have rk=1 Ui,k                      2 â‰¤ Âµ r,
                                                                                                                   0
        Pr       2
          k=1 Vi,k â‰¤ Âµ0 r.
                                        Pr
 A2. There exists Âµ1 such that |           k=1 Ui,k Î£k Vj,k |      â‰¤ Âµ1 r 1/2 .
      Apart from difference in normalization, these assumptions coincide with the ones in [CR08].
      In the proof of Theorem 1.1, we only require the second incoherence assumption A2. In the
following, whenever we write that a property A holds with high probability (w.h.p.), we mean that
there exists a function f (n) = f (n; Î±, Âµ1 ) such that P(A) â‰¥ 1 âˆ’ f (n) and f (n) â†’ 0 for Âµ1 bounded
away from 0 and âˆž. In the case of exact completion (i.e. in the proof of Theorem 1.2) f ( Â· ) can also
depend on Âµ0 , Î£min , Î£max , and f (n) â†’ 0 for Âµ0 , Î£min , Î£max bounded away from 0 and âˆž.
      Probability is taken with respect to the uniformly random subset E âŠ† [m] Ã— [n]. It is convenient
                                                                                                            âˆš
to work with a model in which each entry         âˆš is revealed        independently
                                                                              âˆš         with probability Ç«/ mn. Since,
                                   âˆš                             âˆš
with high probability |E| âˆˆ [Ç« Î± n âˆ’ A n log n, Ç« Î± n âˆ’ A n log n], any guarantee on the algorithm
performances that holds within one model, holds within the other model as well if we allow for a
vanishing shift in Ç«. Finally, we will use C, C â€² etc. to denote generic constants that depend uniquely
on Î±, Âµ1 and, when proving Theorem 1.2, Âµ0 , Î£min , Î£max .
                                                                                                           â€²
      Given a vector x âˆˆ Rn , ||x|| will denote its Euclidean norm. For a matrix X âˆˆ RnÃ—n , ||X||F is its
Frobenius norm, and ||X||2 its operator norm (i.e. ||X||2 = supu6=0 ||Xu||/||u||). The standard scalar
product between vectors or matrices will sometimes be indicated by hx, yi or hX, Y i, respectively.
Finally, we use the standard combinatorics notation [N ] = {1, 2, . . . , N } to denote the set of first N
integers.
3       Proof of Theorem 1.1 and technical results
As explained in the previous section, the crucial idea is to consider the singular value decomposition
of the trimmed matrix M       fE instead of the original matrix M E , as in Eq. (3). We shall then redefine
{Ïƒi }, {xi }, {yi }, by letting
                                                         min(m,n)
                                                             X
                                             MfE =                  Ïƒi xi yiT .                                    (9)
                                                             i=1
Here ||xi || = ||yi || = 1, xTi xj = yiT yj = 0 for i 6= j and Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ 0.
      We will prove Theorem 1.1 by controlling the operator norm of M âˆ’ Tr (M                   fE ).
Theorem 3.1. There exists C(Î±) < âˆž such that, with high probability,
                                     1                                        r 1/2
                                   âˆš         M âˆ’ Tr (M      fE )      â‰¤C              .                           (10)
                                     mn                             2           Ç«
                                                                 6

    This result is proved later in this section. The proof of Theorem 1.1 is a direct application the
of above theorem.
Proof. (Theorem 1.1) Since M âˆ’ Tr (M     fE ) has rank at most 2r, it immediately follows that
                                                                                          
                      1                                  âˆš           1
                   âˆš                   f  E
                            M âˆ’ Tr (M ) F â‰¤                 2r    âˆš                  fE
                                                                            M âˆ’ Tr (M ) 2
                     mnr                                            mnr
                                                              r 1/2
                                                    â‰¤ C                ,
                                                               Ç«
which implies the thesis.
    To prove Theorem 3.1, we will use following key technical results.
Lemma 3.2. There exists a constant C > 0 such that, with high probability
                                                            
                                            Ç«                            âˆš
                                 x T
                                        âˆš      M âˆ’M    f   E
                                                               y â‰¤ C rÇ« ,                                   (11)
                                           mn
for any x âˆˆ Rm and y âˆˆ Rn such that ||x|| = ||y|| = 1.
    The proof of this lemma is given in Section 4. The next lemma, which is a direct consequence of
Lemma 3.2, relates the singular values of the trimmed matrix M             fE to the singular values of M .
Lemma 3.3. There exists a constant C > 0 such that, with high probability
                                         Ïƒq                  r 1/2
                                              âˆ’ Î£q â‰¤ C                 ,                                    (12)
                                          Ç«                   Ç«
where it is understood that Î£q = 0 for q > r.
    This lemma is proved later in this section. We can now prove Theorem 3.1.
Proof. (Theorem 3.1) Consider x âˆˆ Rn and y âˆˆ Rm such that ||x|| = ||y|| = 1. Then,
                                                   âˆš                        âˆš                   
          T           f E               T              mn fE                T     mn fE        f E
        x (M âˆ’ Tr (M ))y â‰¤ x                  Mâˆ’            M      y + x             M âˆ’ Tr (M ) y
                                                       Ç«                          Ç«
                                       âˆš       r 1 âˆš            Ïƒr+1
                                                     2
                                â‰¤ C mn                  + mn
                                                 Ç«                  Ç«
                                        âˆš       r1
                                                      2
                                â‰¤ C â€² mn                 ,
                                                  Ç«
where we used Lemma 3.2 for the second inequality and Lemma 3.3 for the last inequality. This
                                                    1
               1
implies that âˆšmn    M âˆ’ Tr (MfE ) â‰¤ C â€² (r/Ç«) 2 .
                                     2
    We end this section with the proof of Lemma 3.3.
Proof. (Lemma 3.3) Recall the variational characterization of the singular values.
                              Ïƒq =              min               max      ||MfE y||                        (13)
                                         H,dim(H)=nâˆ’q+1 yâˆˆH,||y||=1
                                   =          max           min       ||MfE y|| .                           (14)
                                         H,dim(H)=q yâˆˆH,||y||=1
Here H is understood to be a linear subspace of Rn .
                                                          7

     Using Eq. (13) with H the orthogonal complement of span(v1 , . . . , vqâˆ’1 ), we have, by Lemma 3.2,
               Ïƒq â‰¤      max       fE y
                                   M
                      yâˆˆH,||y||=1
                                                                                           
                         Ç«                                               T   fE        Ç«
                    â‰¤ âˆš           max      My       +      max          x    M âˆ’âˆš          M y
                         mn yâˆˆH,||y||=1               yâˆˆH,||y||=||x||=1                mn
                                âˆš
                    â‰¤ Ç«Î£q + C rÇ«
     In order to get a lower bound, we use Eq. (14) with H = span(v1 , . . . , vq ). By Lemma 3.2 we
have
               Ïƒq â‰¥      min       fE y
                                   M
                      yâˆˆH,||y||=1
                                                                                           
                         Ç«                                               T   fE        Ç«
                    â‰¥ âˆš           min      My       âˆ’      max          x    M âˆ’âˆš          M y
                         mn yâˆˆH,||y||=1               yâˆˆH,||y||=||x||=1                mn
                                âˆš
                    â‰¥ Ç«Î£q âˆ’ C rÇ«
Combining the upper and lower bounds, we get the desired result.
4      Proof of Lemma 3.2
We want to show that |xT (M     fE âˆ’ âˆš Ç« M )y| â‰¤ C âˆšrÇ« for each x âˆˆ Rm , y âˆˆ Rn such that ||x|| =
                                        mn
||y|| = 1. Following the technique of [FKS89], this will be done by first reducing ourselves to x, y
belonging to finite sets. We define
                                                       n               
                                                     âˆ†
                                  Tn =       xâˆˆ âˆš Z            : ||x|| â‰¤ 1 ,
                                                      n
Notice that Tn âŠ† Sn â‰¡ {x âˆˆ Rn : ||x|| â‰¤ 1}. The next two remarks are proved in [FKS89, FO05],
and relate the original problem to the discretized one.
Remark 4.1. Let R âˆˆ RmÃ—n be a matrix. If |xT Ry| â‰¤ B for all x âˆˆ Tm and y âˆˆ Tn , then
|xâ€²T Ry â€² | â‰¤ (1 âˆ’ âˆ†)âˆ’2 B for all xâ€² âˆˆ Sm and y â€² âˆˆ Sn .
Remark 4.2. |Tm | â‰¤ (10/âˆ†)m .
     Hence it is enough to show that |xT (M  fE âˆ’ âˆš Ç« M )y| â‰¤ C âˆšrÇ« for all x âˆˆ Tm and y âˆˆ Tn . There
                                                      mn
are two parts to the proof of this claim. One bounds the contribution of light couples L âŠ† [m] Ã— [n],
defined as
                                                             rÇ« 1/2 
                                L = (i, j) : |xi Mij yj | â‰¤                ,
                                                                mn
and the other bounds the contribution of its complement L, which we call heavy couples. We have
                                         X                                     X
             x T   fE âˆ’ âˆš Ç« M
                   M                y â‰¤             fE yj âˆ’ âˆš Ç« xT M y +
                                                 xi M                                     fE yj
                                                                                       xi M             (15)
                                                     ij                                    ij
                           mn                                   mn
                                         (i,j)âˆˆL                               (i,j)âˆˆL
                                                                                                         âˆš
In the next two subsections, we will prove that the first contribution is upper bounded by C1 rÇ«
                        âˆš                                                                fE âˆ’ âˆš Ç« M )y|, this
and the second by C2 rÇ« for all x âˆˆ Tm , y âˆˆ Tn . Applying Remark 4.1 to |xT (M                 mn
proves the thesis.
                                                       8

4.1     Bounding the contribution of light couples
Let us define the subset of row and column indices which have not been trimmed as Al and Ar :
                                                                       2Ç«
                                  Al = {i âˆˆ [m] : deg(i) â‰¤ âˆš } ,
                                                                         Î±
                                                                         âˆš
                                  Ar = {j âˆˆ [n] : deg(j) â‰¤ 2Ç« Î±} ,
where deg(Â·) denotes the degree (number of revealed entries) of a row or a column. Notice that
A = (Al , Ar ) is a function of the random set E. It is easy to get a rough estimate of the sizes of Al ,
Ar .
Remark 4.3. There exists C1 and C2 depending only on Î± such that, with probability larger than
1 âˆ’ 1/n3 , |Al | â‰¥ m âˆ’ max{eâˆ’C1 Ç« m, C2 Î±}, and |Ar | â‰¥ n âˆ’ max{eâˆ’C1 Ç« n, C2 }.
     For the proof of this claim, we refer to Appendix A. For any E âŠ† [m] Ã— [n] and A = (Al , Ar )
with Al âŠ† [m], Ar âŠ† [n], we define M E,A by setting to zero the entries of M that are not in E, those
whose row index is not in Al , and those whose column index not in Ar . Consider the event
                           ï£±                                                              ï£¼
                           ï£²             X           E,A           Ç«     T            âˆš ï£½
                H(E, A) = âˆƒ x, y :               xi Mij yj âˆ’ âˆš        x M y > C1 rÇ« ,                    (16)
                           ï£³                                      mn                      ï£¾
                                       (i,j)âˆˆL
where it is understood that x and y belong, respectively, to Tm and Tn . Note that M            fE = M E,A ,
and hence we want to bound P{H(E, A)}. We proceed as follows
                                          X
                      P {H(E, A)} =             P {H(E, A), A = A}
                                            A
                                                X                                    1
                                     â‰¤                   P {H(E, A), A = A} +
                                                                                     n3
                                          |Al |â‰¥m(1âˆ’Î´),
                                           |Ar |â‰¥n(1âˆ’Î´)
                                                                                        1
                                     â‰¤ 2(n+m)H(Î´)            max      P {H(E; A)} +        ,             (17)
                                                        |Al |â‰¥m(1âˆ’Î´),                   n3
                                                         |Ar |â‰¥n(1âˆ’Î´)
with Î´ â‰¡ max{eâˆ’C1 Ç« , C2 /n} and H(x) the binary entropy function.
     We are now left with the task of bounding P {H(E; A)} uniformly over A where H is defined as
in Eq. (16). The key step consists in proving the following tail estimate
                                            P
Lemma 4.4. Let x âˆˆ Sm , y âˆˆ Sn , Z = (i,j)âˆˆL xi MijE,A yj âˆ’ âˆšmn        Ç«
                                                                           xT M y, and assume |Al | â‰¥ m(1âˆ’Î´),
|Ar | â‰¥ n(1 âˆ’ Î´) with Î´ small enough. Then
                                  âˆš              n nÎ±1/2                     o
                         P Z > L rÇ« â‰¤ exp âˆ’                     L âˆ’ 2Âµ21 âˆ’ Âµ1      .
                                                          2
Proof. We begin by bounding the mean of Z as follows (for the proof of this statement we refer to
Appendix B).
                                    âˆš
Remark 4.5. |E [Z]| â‰¤ Âµ1 + Âµ21         rÇ«.
                                                        9

    For A = (Al , Ar ), let M A be the matrix obtained from M by setting to zero those entries whose
row index is not in Al , and those whose column index not in Ar . Define the potential contribution
of the light couples aij and independent random variables Zij as
                                       (
                                          xi MijA yj if |xi MijA yj | â‰¤ (rÇ«/mn)1/2 ,
                            aij =
                                                     0 otherwise,
                                                          âˆš
                                          ai,j w.p. Ç«/ mn,
                            Zij =                                âˆš
                                              0 w.p. 1 âˆ’ Ç«/ mn,
           P                                                                P              P                 2
Let Z1 =                                   âˆš Ç« xT M y. Note that                   a2ij â‰¤           x   M Ay     â‰¤ Âµ21 r. Fix
              i,j Zij so that Z = Z1 âˆ’        mn                               i,j             i,j     i ij j
Î» = (mn/4rÇ«)1/2 so that |Î»ai,j | â‰¤     1/2, whence e     Î»a ij âˆ’ 1 â‰¤ Î»aij +        2(Î»aij )2 . It then follows that
                                      n Ç« X                       X                     Î»Ç« T           o
                    E[eÎ»Z ] = exp âˆš                    Î»ai,j + 2         (Î»ai,j )2 âˆ’ âˆš           x My
                                          mn                                               mn
                                                   i,j               i,j
                                      n                      Î»2 o
                            â‰¤ exp Î»E[Z] + 2rÇ«Âµ21 1/2 .
                                                          nÎ±
The thesis follows by Chernoff bound P(Z > a) â‰¤ eâˆ’Î»a E[eÎ»Z ] after simple calculus.
                            âˆš
    Note that P (âˆ’Z > L rÇ«) can also be bounded analogously. We can now finish the upper bound
on the light couples contribution. Consider the error event Eq. (16). Using Remark 4.2, we can
apply union bound over Tm and Tn to Eq. (17) to obtain
                                                      n+m
                                                   20                      2        1/2 n/2       1
                         P{H(E, A)} â‰¤ 2                        eâˆ’(C1 âˆ’2Âµ1 âˆ’Âµ1 )Î±            +        ,
                                                    âˆ†                                            n3
If C1 is a large enough constant, the first term is of order eâˆ’Î˜(n) (for, say, Ç« â‰¥ r) thus finishing the
proof.
4.2    Bounding the contribution of heavy couples
Let Q be an mÃ—n matrix with Qij = 1 if (i, j) âˆˆ E and i 6âˆˆ Ar , j 6âˆˆ Al (i.e. entry (i, j) is not trimmed
by our algorithm), and Qij = 0 otherwise. Duep             to the incoherence assumption A2, |Mij | â‰¤ Âµ1 r 1/2
                                                                                   âˆš
and therefore the heavy couples satisfy |xi yj | â‰¥ Ç«/(Âµ21 mn) = C Ç«/n. We then have
                                X                           âˆš X
                                         fijE yj
                                      xi M           â‰¤ Âµ1 r               Qij |xi yj |
                              (i,j)âˆˆL                            (i,j)âˆˆL
                                                            âˆš          X
                                                     â‰¤ Âµ1 r                      Qij |xi yj | .
                                                                    (i,j)âˆˆE:
                                                                           âˆš
                                                                |xi yj |â‰¥C Ç«/n
Notice that Q is the adjacency matrix of a random bipartite graph with vertex sets [m] and [n]
and maximum degree bounded by 2Ç« max(Î±1/2 , Î±âˆ’1/2 ). The following remark strengthens a result of
[FO05].
                                                 â€²                                          âˆš
Remark 4.6. Given vectors x,       P   y, let L = {(i, j) âˆˆ E : |xi yj | â‰¥ C Ç«/n}. Then there exist a
                                                                  âˆš
constant C â€² such that, w.h.p., (i,j)âˆˆLâ€² Qij |xi yj | â‰¤ C â€² Ç«, for all x âˆˆ Tm , y âˆˆ Tn .
                                                          10

    For the readerâ€™s convenience, a proof of this fact is proposed in Appendix C. The analogous result
in [FO05] (for the adjacency matrix of a non-bipartite graph) is proved to hold only with probability
larger than 1 âˆ’ eâˆ’CÇ« . The stronger statement quoted here can be proved using concentration of
measure inequalities. The last remark implies that for all x âˆˆ Tm , y âˆˆ Tn and for large enough C
                                                                  âˆš
the contribution of heavy couples is, w.h.p., bounded by C2 rÇ« for some C2 < âˆž.
5     Minimization on Grassmann manifolds and proof of Theorem 1.2
The function F (X, Y ) defined in Eq. (4) and to be minimized in the last part of the algorithm
can naturally be viewed as defined on Grassmann manifolds. Here we recall from [EAS99] a few
important facts on the geometry of Grassmann manifold and related optimization algorithms. We
then prove Theorem 1.2. Technical calculations are deferred to section Sections 6, 7, and to the
appendices.
    We recall that, for the proof of Theorem 1.2, it is assumed that Î£min , Î£max are bounded away from
0 and âˆž. Constants (denoted by C, C â€² , . . . ) depend implicitly on Î£min , Î£max . Finally, throughout
this section, we use the notation X (i) âˆˆ Rr to refer to the i-th row of the matrix X âˆˆ RmÃ—n or
X âˆˆ RnÃ—r .
5.1    Geometry of the Grassmann manifold
Denote by O(d) the orthogonal group of d Ã— d matrices. The Grassmann manifold is defined as the
quotient G(n, r) â‰ƒ O(n)/O(r) Ã— O(n âˆ’ r). In other words, a point in the manifold is the equivalence
class of an n Ã— r orthogonal matrix A
                                          [A] = {AQ : Q âˆˆ O(r)} .                                   (18)
For consistency with the rest of the paper, we will assume the normalization AT A = n 1. To represent
a point in G(n, r), we will use an explicit representative of this form. More abstractly, G(n, r) is the
manifold of r-dimensional subspaces of Rn .
    It is easy to see that F (X, Y ) depends on the matrices X, Y only through their equivalence
classes [X], [Y ]. We will therefore interpret it as a function defined on the manifold M(m, n) â‰¡
G(m, r) Ã— G(n, r):
                                           F : M(m, n) â†’ R ,                                        (19)
                                              ([X], [Y ]) 7â†’ F (X, Y ) .                            (20)
In the following, a point in this manifold will be represented as a pair x = (X, Y ), with X an n Ã— r
orthogonal matrix and Y an m Ã— r orthogonal matrix. Boldface symbols will be reserved for elements
of M(m, n) or of its tangent space, and we shall use u = (U, V ) for the point corresponding to the
matrix M = U Î£V T to be reconstructed.
    Given x = (X, Y ) âˆˆ M(m, n), the tangent space at x is denoted by Tx and can be identified with
the vector space of matrix pairs w = (W, Z), W âˆˆ RmÃ—r , Z âˆˆ RnÃ—r such that W T X = Z T Y = 0.
The â€˜canonicalâ€™ Riemann metric on the Grassmann manifold corresponds to the usual scalar product
hW, W â€² i â‰¡ Tr(W T W â€² ). The induced scalar product on Tx between w = (W, Z) and wâ€² = (W â€² , Z â€² )
is hw, wâ€² i = hW, W â€² i + hZ, Z â€² i.
    This metric induces a canonical notion of distance on M(m, n) which we denote by d(x1 , x2 )
(geodesic or arc-length distance). If x1 = (X1 , Y1 ) and x2 = (X2 , Y2 ) then
                                                p
                                  d(x1 , x2 ) â‰¡ d(X1 , X2 )2 + d(Y1 , Y2 )2                         (21)
                                                         11

where the arc-length distances d(X1 , X2 ), d(Y1 , Y2 ) on the Grassmann manifold can be defined ex-
plicitly as follows. Let cos Î¸ = (cos Î¸1 , . . . , cos Î¸r ), Î¸i âˆˆ [âˆ’Ï€/2, Ï€/2] be the singular values of X1T X2 /n.
Then
                                               d(X1 , X2 ) = ||Î¸||2 .                                        (22)
The Î¸i â€™s are called the â€˜principal anglesâ€™ between the subspaces spanned by the columns of X1 and
X2 . It is useful to introduce two equivalent notions of distance:
                                1
              dc (X1 , X2 ) = âˆš        min ||X1 Q1 âˆ’ X2 Q2 ||F                  (chordal distance),          (23)
                                 n Q1 ,Q2âˆˆO(r)
                                 1
              dp (X1 , X2 ) = âˆš ||X1 X1T âˆ’ X2 X2T ||F                           (projection distance).       (24)
                                 2n
Notice that dc and dp do not depend on the specific representatives X1 , X2 , but only on the equiv-
alence classesp[X1 ] and [X2 ]. Distances on M(m, n) are defined through Pythagorean theorem, e.g.
dc (x1 , x2 ) = dc (X1 , X2 )2 + dc (Y1 , Y2 )2 .
Remark 5.1. The geodesic, chordal and projection distance are equivalent, namely
                 2                 1
                   d(X1 , X2 ) â‰¤ âˆš dc (X1 , X2 ) â‰¤ dp (X1 , X2 ) â‰¤ dc (X1 , X2 ) â‰¤ d(X1 , X2 ) .             (25)
                Ï€                   2
    For the readerâ€™s convenience, a proof of this fact is proposed in Section D.
    An important remark is that geodesics with respect to the canonical Riemann metric admit an
explicit and efficiently computable form. Given u âˆˆ M(m, n), w âˆˆ Tx the corresponding geodesic
is a curve t 7â†’ x(t), with x(t) = u + wt + O(t2 ) which minimizes arc-length. If u = (U, V ) and
w = (W, Z) then x(t) = (X(t), Y (t)) where X(t) can be expressed in terms of the singular value
decomposition W = LÎ˜RT [EAS99]:
                                   X(t) = U R cos(Î˜t)RT + L sin(Î˜t)RT ,                                      (26)
which can be evaluated in time of order O(nr). An analogous expression holds for Y (t).
5.2      Gradient and incoherence
The gradient of F at x is the vector grad F (x) âˆˆ Tx such that, for any smooth curve t 7â†’ x(t) âˆˆ
M(m, n) with x(t) = x + w t + O(t2 ), one has
                                F (x(t)) = F (x) + hgrad F (x), wi t + O(t2 ) .                              (27)
In order to write an explicit representation of the gradient of our cost function F , it is convenient to
introduce the projector operator
                                                    
                                                       Mij if (i, j) âˆˆ E,
                                      PE (M )ij =                                                            (28)
                                                       0       otherwise.
The two components of the gradient are then
                               grad F (x)X     = PE (XSY T âˆ’ M )Y S T âˆ’ XQX ,                                (29)
                                                                T       T
                               grad F (x)Y     = PE (XSY           âˆ’ M ) XS âˆ’ Y QY ,                         (30)
where QX , QY âˆˆ RrÃ—r are determined by the condition grad F (x) âˆˆ Tx . This yields
                                                  1 T
                                      QX   =        X PE (M âˆ’ XSY T )Y S T ,                                 (31)
                                                 m
                                                 1 T
                                      QY   =       Y PE (M âˆ’ XSY T )T XS .                                   (32)
                                                 n
                                                            12

5.3     Algorithm
At this point the gradient descent algorithm is fully specified. It takes as input the factors of Tr (M fE ),
to be denoted as x0 = (X0 , Y0 ), and minimizes a regularized cost function
                Fe(X, Y ) = F (X, Y ) + Ï G(X, Y )                                                       (33)
                                               m
                                                                    !     n
                                                                                                !
                                             X          ||X (i) ||2      X          ||Y (j) ||2
                             â‰¡ F (X, Y ) + Ï      G1                  +Ï      G1                  ,      (34)
                                                          2Âµ0 r                       2Âµ0 r
                                              i=1                        j=1
where X (i) denotes the i-th row of X, and Y (j) the j-th row of Y . The role of the regularization is
to force x to remain incoherent during the execution of the algorithm.
                                           
                                              0               if z â‰¤ 1,
                                  G1 (z) =     (zâˆ’1)2                                                    (35)
                                              e       âˆ’ 1 if z â‰¥ 1.
We will take Ï = nÇ«. Notice that G(X, Y ) is again naturally defined on the Grassmann manifold,
i.e. G(X, Y ) = G(XQ, Y Qâ€² ) for any Q, Qâ€² âˆˆ O(r).
     Let
                              n                                                      o
                    K(Âµâ€² ) â‰¡ (X, Y ) such that ||X (i) ||2 â‰¤ Âµâ€² r, ||Y (j) ||2 â‰¤ Âµâ€² r .                  (36)
We have G(X, Y ) = 0 on K(2Âµ0 ). Notice that u âˆˆ K(Âµ0 ) by the incoherence property. Without loss
of generality we can assume x0 âˆˆ K(2Âµ0 ), because otherwise we can rescale all lines of X0 , Y0 that
violate the constraint.
      Gradient descent( matrix M E , factors x0 )
      1: For k = 0, 1, . . . do:
      2:    Compute wk = grad Fe(xk );
      4:    Let t 7â†’ xk (t) be the geodesic with xk (t) = xk + wk t + O(t2 );
      5:    Minimize t 7â†’ Fe (xk (t)) for t â‰¥ 0, subject to d(xk (t), x0 ) â‰¤ Î³;
      6:    Set xk+1 = xk (tk ) where tk is the minimum location;
      7: End For.
     In the above, Î³ must be set in such a way that d(u, x0 ) â‰¤ Î³. The next remark determines the
correct scale.
Remark 5.2. Let U, X âˆˆ RnÃ—r with U T U = X T X = m1, V, Y âˆˆ RmÃ—r with V T V = Y T Y = n1,
and M = U Î£V T , M  c = XSY T for Î£ = diag(Î£1 , . . . , Î£r ) and S âˆˆ RrÃ—r . If Î£1 , . . . , Î£r â‰¥ Î£min , then
                                1           c||F                          1             c||F
               dp (U, X) â‰¤          ||M âˆ’ M        ,     dp (V, Y ) â‰¤          ||M âˆ’ M                   (37)
                             mÎ£min                                     nÎ£min
                                                                                                    âˆš
     As a consequence of this remark and Theorem 1.1, we can assume that d(u, x0 ) â‰¤ C r/ Ç«. We
                          âˆš
shall then set Î³ = C â€² r/ Ç« (the value of C â€² is set in the course of the proof).
     Before passing to the proof of Theorem 1.2, it is worth discussing a few important points con-
cerning the gradient descent algorithm.
   (i) The appropriate choice of Î³ might seem to pose a difficulty. In reality, this parameter is
       introduced only to simplify the proof. We will see that the constraint d(xk (t), x0 ) â‰¤ Î³ is, with
       high probability, never saturated.
                                                      13

  (ii) Indeed, the line minimization instruction 5 (which might appear complex to implement) can
       be replaced by a standard step selection procedure, such as the one in [Arm66].
 (iii) Similarly, there is no need to know the actual value of Âµ0 in the regularization term. One can
       start with Âµ0 = 1 and then repeat the optimization doubling it at each step.
 (iv) The Hessian of F can be computed explicitly as well. This opens the way to quadratically
       convergent minimization algorithms (e.g. the Newton method).
5.4     Proof of Theorem 1.2
The proof of Theorem 1.2 breaks down in two lemmas. The first one implies that, in a sufficiently
small neighborhood of u, the function x 7â†’ F (x) is well approximated by a parabola.
Lemma 5.3. Assume Ç« â‰¥ A max{r log n, r 2 } with A large enough. Then there exists constants
C1 , C2 , Î´ > 0 (independent of m, n, Ç« and r) such that, with high probability
                                                     
                        C1 nÇ« d(x, u)2 + ||S âˆ’ Î£||2F â‰¤ F (x) â‰¤ C2 nÇ« d(x, u)2                        (38)
for all x âˆˆ M(m, n) âˆ© K(3Âµ0 ) such that d(x, u) â‰¤ Î´ (where S âˆˆ RrÃ—r is the matrix realizing the
minimum in Eq. (4)).
     The second Lemma implies that x 7â†’ F (x) does not have any other stationary point (apart from
u) within such a neighborhood.
Lemma 5.4. Assume Ç« â‰¥ A max{r log n, r 2 } with A large enough. Then there exists constants
C, Î´ > 0 (independent of m, n, Ç« and r) such that, with high probability
                                   ||grad Fe(x)||2 â‰¥ C nÇ«2 d(x, u)2                                  (39)
for all x âˆˆ M(m, n) âˆ© K(3Âµ0 ) such that d(x, u) â‰¤ Î´.
     We can now prove Theorem 1.2.
Proof. (Theorem 1.2) Let Î´ > 0 be such that Lemma 5.3        p and Lemma 5.4 are verified, and C1 , C2
be defined as in Lemma 5.3. We further assume Î´ â‰¤ (e1/4 âˆ’ 1)/C1 . Take Ç« large enough that
                  âˆš
d(u, x0 ) â‰¤ C r/ Ç« â‰¤ min(1, (C1 /2C2 )1/2 )Î´/10. Further, set the algorithm parameter to Î³ = Î´/4.
     We make the following claims:
    1. xk âˆˆ K(3Âµ0 ) for all k.
       Indeed x0 âˆˆ K(2Âµ0 ) whence Fe (x0 ) = F (x0 ) â‰¤ C2 nÇ« Î´2 . The claim follows because Fe(xk ) is
       non-increasing and Fe(x) â‰¥ Ï G(X, Y ) â‰¥ nÇ«(e1/4 âˆ’ 1) for x 6âˆˆ K(3Âµ0 ).
    2. d(xk , u) â‰¤ Î´/10 for all k.
       Indeed by triangular inequality we can assume to have d(xk , u) â‰¤ Î´/2. Since d(x0 , u) â‰¤
       (C1 /2C2 )1/2 Î´/10, we have Fe(x) â‰¥ F (x) â‰¥ F (x0 ) for all x such that d(x, u) âˆˆ [Î´/10, Î´]. Since
       Fe(xk ) is non-increasing and Fe(x0 ) = F (x0 ), the claim follows.
     Notice that, by the last observation, the constraint d(xk (t), x0 ) â‰¤ Î³ is never saturated, and
therefore our procedure is just gradient discent with exact line search. Therefore [Arm66] this must
converge to the unique stationary point of Fe in K(3Âµ0 ) âˆ© {x : d(x, u) â‰¤ Î´/10, which, by Lemma 5.4,
is u.
                                                     14

6     Proof of Lemma 5.3
We will make use of the following Lemma.
Lemma 6.1. Assume Ç« = A log n with A large enough. Then there exists C > 0 such that with high
probability
                                  X                   CÇ«                     âˆš
                                          xi yj â‰¤         ||x||1 ||y||1 + C Ç«||x||2 ||y||2 .                                  (40)
                                                      n
                                (i,j)âˆˆE
for all x âˆˆ Rm , y âˆˆ Rn .
                                             P
Proof. Write xi = x0 + xâ€²i where                    â€²
                                                 i xi = 0. Then
                                    X                      X                    X
                                            xi yj = x0          deg(j)yj +            xâ€²i yj ,                                (41)
                                  (i,j)âˆˆE                 jâˆˆ[n]               (i,j)âˆˆE
                                                                                                           P
where we recall that deg(j) = {i âˆˆ [m] : such that (i, j) âˆˆ E}. Further |x0 | = |                             i xi /n| â‰¤ ||x||1 /n.
The first term is upper bounded by
                                x0 max deg(j)||y||1 â‰¤ max deg(j)||x||1 ||y||1 /n .                                            (42)
                                       j                          j
For Ç« = A log n, the maximum degree is with high probability of the same order as the average one,
and therefore this term is at most CÇ«||x||1 ||y||1 /n.
                                                              âˆš
    The second term is upper bounded by C Ç«||xâ€² ||2 ||y||2 using Theorem 1.1 in [FO05] or, equiva-
lently, Theorem 3.3 in the case r = 1. The thesis follows because ||xâ€² ||2 â‰¤ ||x||2 .
Proof. (Lemma 5.3) Throughout the proof we assume m = n to simplify notations.
    Let w = (W, Z) âˆˆ Tu , and t 7â†’ (X(t), Y (t)) be the geodesic such that (X(t), Y (t)) = (U, V ) +
(W, Z)t+O(t2 ). By setting (X, Y ) = (X(1), Y (1)), we establish a one-to-one correspondence between
the points x as in the statement and a neighborhood of the origin in Tu . If we let W = LÎ˜RT be
the singular value decomposition of W (with LT L = n1 and RT R = 1), the explicit expression for
geodesics in Eq. (26) yields
                          X =U +W ,                 W = U R(cos Î˜ âˆ’ 1)RT + L sin Î˜RT .                                        (43)
An analogous expression can obviously be written for Y = V + Z. Notice that, if u, x âˆˆ K(3Âµ0 ), then
                                                                                                            (i)
(W , Z) âˆˆ K(12Âµ0 ) and w âˆˆ K(48Âµ0 /Ï€ 2 ). In the first case this follows from ||W ||2 â‰¤ 2||X (i) ||2 +
2||U (i) ||2 . In order to prove w âˆˆ K(48Âµ0 /Ï€ 2 ), we notice that
                                                         4
                    ||W (i) ||2 = ||Î˜L(i) ||2 â‰¤             || sin Î˜L(i) ||2
                                                         Ï€2
                                        4       (i)                 T (i) 2       8  (i) 2             (i) 2
                                                                                                              
                                â‰¤          ||X      âˆ’  R  cos  Î˜R     U   ||  â‰¤       ||X      || + ||U    ||   .
                                       Ï€2                                         Ï€2
The claim follows by showing a similar bound for ||Z (i) ||2 .
    Denote by S âˆˆ RrÃ—r the matrix realizing the minimum in Eq. (4). We will start by proving the
lower bound in Eq. (38):
                                         1 X                                     T                         T
                      F (X, Y ) =                    (U (S âˆ’ Î£)V T + U SZ + W SV T + W SZ )2ij
                                         2
                                            (i,j)âˆˆE
                                         1 2
                                  â‰¥        A âˆ’ B2
                                         4
                                                                   15

where in we used Cauchy-Schwarz inequality to argue that (1/2)(A + B)2 â‰¥ (A2 /4) âˆ’ B 2 and defined
                                         X                                T
                               A2 â‰¡             (U (S âˆ’ Î£)V T + U SZ + W SV T )2ij ,
                                       (i,j)âˆˆE
                                         X             T
                               B2 â‰¡             (W SZ )2ij .
                                       (i,j)âˆˆE
We will show that, with high probability A2 â‰¥ CnÇ«||S âˆ’ Î£||2F + CnÇ«d(x, u)2 and B 2 â‰¤ (C/100)nÇ« (1+
||S âˆ’ Î£||2F )d(x, u)2 whence the lower bound in Eq. (38) follows.
    Lower bound on A. By Theorem 4.1 in [CR08], we have A2 â‰¥ (1 âˆ’ Î¾)E{A2 } with high probability
for each Î¾ > 0. Further
                      Ç«                            T
      E{A2 } =          ||U (S âˆ’ Î£)V T + U SZ + W SV T ||2F =
                      n
                      Ç«                          Ç«       T         Ç«
               =        ||U (S âˆ’ Î£)V T ||2F + ||U SZ ||2F + ||W SV T ||2F
                      n                         n                  n
                        2Ç«        T                2Ç«                             2Ç«       T
                                            T
                     + hU SZ , W SV i + hU (S âˆ’ Î£)V T , W SV T i + hU SZ , U (S âˆ’ Î£)V T i .
                         n                         n                              n
The first term is equal to nÇ«||S âˆ’ Î£||2F . The second and third terms are lower bounded by
            Ç«Ïƒmin (S)2 (||Z||2F + ||W ||2F ) â‰¥ CÏƒmin (S)2 nÇ«dc (x, u)2 â‰¥ C â€² Ïƒmin (S)2 nÇ«d(x, u)2 .
    The absolute value of the fourth term can be written as
                  Ç«          T                Ç«        T                    Ç«                T
       E4 =         |hU SZ , W SV T i| â‰¤ ||U SZ ||F ||W SV T ||F â‰¤ Ïƒmax (S)2 ||W U ||F ||V T Z||F
                 n                            n                             n
                  Ç«            2     T    2         T   2
             â‰¤      Ïƒmax (S) (||W U ||F + ||V Z||F ) .
                 n
In order proceed, consider Eq. (43). Since by tangency condition U T L = 0, we have U T W =
nR(cos Î˜ âˆ’ 1)RT whence
                                                       n                    n
                    ||U T W ||F = n|| cos Î¸ âˆ’ 1|| =       ||4 sin2 (Î¸/2)|| â‰¤ ||2 sin(Î¸/2)||2             (44)
                                                       2                    2
(here Î¸ = (Î¸1 , . . . , Î¸r ) is the vector containing the diagonal elements of Î˜). A similar calculation
reveals that ||W ||2F = n||2 sin(Î¸/2)||2 thus proving ||U T W ||2F â‰¤ ||W ||4F /4 â‰¤ CnÎ´2 ||W ||2F . The bound
||V T Z||2F â‰¤ CnÎ´2 ||Z||2F is proved in the same way, thus yielding
                                        E4 â‰¤ CnÇ«Ïƒmax (S)2 Î´2 d(x, u)2 .
Proceeding analogously for the other terms in the expression of E{A2 }, we proved that with high
probability
                                                                                
                A2 â‰¥ nÇ«||S âˆ’ Î£||2F + nÇ« CÏƒmin (S)2 âˆ’ C â€² Î´2 Ïƒmax (S)2 d(x, u)2                           (45)
                                               2            2            2       2 2
                                                                                               2
                        â‰¥ nÇ«(1 âˆ’ Cd(x, u) )||S âˆ’ Î£||F + nÇ« CÎ£min âˆ’ CÎ´ Î£max d(x, u) ,                     (46)
where we used the bounds Ïƒmin (S)2 â‰¥ Î£2min /2 âˆ’ ||S âˆ’ Î£||2F and Ïƒmax (S)2 â‰¤ 2Î£2max + 2 ||S âˆ’ Î£||2F .
The above inequality implies the desired claim if we take d(x, u) â‰¤ Î´ small enough.
                                                            16

    Upper bound on B. By Lemma 6.1 we have
                    X X              2 2
 B 2 â‰¤ Ïƒmax (S)2                W ia Z jb                                                                         (47)
                    a,b (i,j)âˆˆE
                                                                                            !1/2 ï£«               ï£¶1/2
          CÇ«           X          (i)       (j)                    âˆš X X               (i)          X      (j) 4
      â‰¤      Ïƒmax (S)2       ||W ||2 ||Z ||2 + CÏƒmax (S)2 Ç«                      ||W ||4         ï£­     ||Z    || ï£¸(48)
           n
                        i,j                                            a,b    i                     j
                                                                                           !1/2 ï£«             ï£¶1/2
          CÇ«           X          (i)       (j)                    âˆš       X       (i)           X (j)
      â‰¤      Ïƒmax (S)2       ||W ||2 ||Z ||2 + C â€² Ïƒmax (S)2 Ç«r                ||W ||2          ï£­    ||Z ||2 ï£¸ (49) .
           n
                        i,j                                                 i                     j
where in the last step we used the incoherence condition. We conclude therefore that
                                                                 âˆš 
                        B 2 â‰¤ nÏƒmax (S)2 CÇ« Î´2 + C â€² Ç«r d(x, u)2                                                  (50)
                                                                           âˆš
                               â‰¤ CnÇ«(Î£2max + ||S âˆ’ Î£||2F ) max(r/ Ç«, Î´2 )d(x, u)2                                 (51)
                                   âˆš
which implies the thesis for r/ Ç«, Î´ small enough.
    In order to prove the upper bound in Eq. (38) we can set Î£ = S, thus obtaining
                                              1 X              T                        T
                          F (X, Y ) =                    (U Î£Z + W Î£V T + W Î£Z )2ij
                                              2
                                                 (i,j)âˆˆE
                                        â‰¤ A +Bb2       b2 ,
where we defined
                                                    X           T
                                      b2 â‰¡
                                      A                   (U Î£Z + W Î£V T )2ij ,
                                                  (i,j)âˆˆE
                                                    X            T
                                      b2 â‰¡
                                      B                   (W Î£Z )2ij .
                                                  (i,j)âˆˆE
Bounds for these two quantities are derived as for A2 and B 2 . More precisely, by Theorem 4.1 in
[CR08], we have A b2 â‰¤ (1 âˆ’ Î¾)E{A      b2 } and
                               b2 } =       Ç«           T
                           E{A                ||U Î£Z + W Î£V T ||2F =
                                            n
                                            2Ç«           T      2Ç«
                                       =        ||U Î£Z ||2F + ||W Î£V T ||2F
                                             n                   n
                                       â‰¤ Ç«Î£2max (||Z||2F + ||W ||2F ) â‰¤ CnÇ«d(x, u)2 .
Further by setting S = Î£ in the derivation for B we get B            b 2 â‰¤ (C/100)nÇ« d(x, u)2 .
7     Proof of Lemma 5.4
Throughout this proof we assume m = n to lighten the notation.
    As in the proof of Lemma 5.3, we let t 7â†’ x(t) = (X(t), Y (t)) be the geodesic starting at x(0) = u
with velocity xÌ‡(0) = w = (W, Z) âˆˆ Tu . We also define x = x(1) = (X, Y ) with X = U + W and
Y = V + Z. Let w  b = xÌ‡(1) = (W        b be its velocity when passing through x. An explicit expression
                                   c , Z)
                                                            17

is obtained in terms of the singular value decomposition of W and Z. If we let W = LÎ˜RT , we
obtain
                                 c = âˆ’U RÎ˜ sin Î˜ RT + LÎ˜ cos Î˜ RT .
                                 W                                                                           (52)
An analogous expression holds for Z.   b Since LT U = 0, we have ||W       c ||2 = n||Î˜ sin Î˜||2 +n||Î˜ cos Î˜||2 =
                                                                                 F               F            F
      2
n||Î¸|| . Hence ||w|| b  2            2
                          = nd(x, u) . In order to prove the thesis, it is therefore sufficient to show
that hgrad Fe(x), wi b â‰¥ CnÇ« d(x, u)2 . In the following we will indeed show that hgrad F (x), wi           b â‰¥
             2
CnÇ« d(x, u) , and hgrad G(x), wi   b â‰¥ 0.
    As a preliminary remark, we notice that w        b âˆˆ K((3Ï€ 2 /2 + 96/Ï€ 2 )Âµ0 ). Indeed
             c (i) ||2 â‰¤ 2||Î˜ sin Î˜RT U (i) ||2 + 2||Î˜ cos Î˜L(i) ||2 â‰¤     Ï€ 2 (i) 2
           ||W                                                                 ||U || + 2||W (i) ||2 ,       (53)
                                                                            2
By assumption we have ||U (i) ||2 â‰¤ 3Âµ0 r and we proved ||W (i) ||2 â‰¤ 48Âµ0 r/Ï€ 2 in the previous section.
7.1     Lower bound on grad F (x)
Recalling that PE is the projector defined in Eq. (28), and using the expression (29), (30), for the
gradient, we have
               b = hPE (XSY T âˆ’ M ), (XS ZbT + W
  hgrad F (x), wi                                         c SY T )i
                                        T                          T                                   c SZ T )i
         = hPE (U (S âˆ’ Î£)V T + U SZ + W SV T + W SZ ), (U S ZbT + W                 c SV T + W S ZbT + W
         â‰¥ A âˆ’ B1 âˆ’ B2 âˆ’ B3                                                                                  (54)
where we defined
                                             T
                         A = hPE (U SZ + W SV T ), (U S ZbT + W           c SV T )i ,                        (55)
                                              T                              c SZ T )i| ,
                       B1 = |hPE (U SZ + W SV T ), (W S ZbT + W                                              (56)
                                                                  T
                       B2 = |hPE (U (S âˆ’ Î£)V T + W SZ ), (U S ZbT + W              c SV T )i| ,              (57)
                                                                  T           bT + Wc SZ T )i| .
                       B3 = |hPE (U (S âˆ’ Î£)V T + W SZ ), (W S Z                                              (58)
At this point the proof becomes very similar to the one in the previous section and consists in lower
bounding A and upper bounding B1 , B2 , B3 . One important fact that we will use is that W              c is well
approximated by W or by W , and Zb is well approximated by Z or by Z. Before proceeding, it is
worth deriving a few estimates of this type. In particular, using Eqs. (43) and (52) we get
                                          c ||2
                                        ||W         = ||W ||2F = n||Î¸||2 ,                                   (59)
                                                F
                                        ||W ||2F    = n||2 sin Î¸/2|| ,  2
                                                                                                             (60)
                                                           X r
                                        c, W i = n
                                       hW                      Î¸a sin Î¸a ,                                   (61)
                                                           a=1
                                                           X r
                                        c, W i = n
                                       hW                      Î¸a2 cos Î¸a ,                                  (62)
                                                           a=1
and therefore
                                        X r
                      c âˆ’ W ||2
                    ||W            = n        [(2 sin Î¸a /2)2 + Î¸a2 âˆ’ 2Î¸a sin Î¸a ]                           (63)
                               F
                                         i=a
                                        X r
                                                                        n             n
                                   â‰¤ n        (Î¸a âˆ’ 2 sin Î¸a /2)2 â‰¤       2
                                                                            ||Î¸||4 â‰¤ 2 d(u, x)4 .            (64)
                                                                      24             24
                                         i=a
                                                          18

Analogously
                                          Xr
                      c âˆ’ W ||2F
                    ||W             = n       [2Î¸a2 âˆ’ 2Î¸a2 cos Î¸a ] â‰¤ n ||Î¸||4 â‰¤ n d(u, x)4               (65)
                                          i=a
The last inequality implies in particular
                                  c ||F = ||U T (W âˆ’ W
                            ||U T W                    c )||F â‰¤ nd(u, x)2 .                               (66)
Similar bounds hold of course for Z, Z, b Z (for instance we have ||V T Z||     b F â‰¤ nd(u, x)2 ). Finally, we
                                                2               2
shall use repeatedly the fact that ||S âˆ’ Î£||F â‰¤ Cd(x, u) , which follows from Lemma 5.3. This in
turns implies
                                    Ïƒmax (S) â‰¤ Î£max + C d(x, u)2 ,                                        (67)
                                                                         2
                                    Ïƒmin (S) â‰¥ Î£min âˆ’ C d(x, u) .                                         (68)
    We can now bound the various terms on Eq. (54).
    Lower bound on A. Using Theorem 4.1 in [CR08] we obtain, with high probability for any Î¾ > 0:
                    Ç«        T
           A â‰¥        h(U SZ + W SV T ), (U S ZbT + W   c SV T )i                                         (69)
                   n
                      Î¾Ç«        T
                   âˆ’ ||U SZ + W SV T ||F ||U S ZbT + W       c SV T ||F â‰¥ (1 âˆ’ Î¾)A0 âˆ’ (1 + Î¾)B0           (70)
                       n
where
                           Ç«         T
                 A0 =         ||U SZ + W SV T ||2F                                                        (71)
                           n
                           Ç«         T                             b T + (W âˆ’ W  c )SV T ||F .
                 B0 =         ||U SZ + W SV T ||F ||U S(Z âˆ’ Z)                                            (72)
                           n
The term A0 is lower bounded analogously to E{A2 } in the proof of Lemma 5.3 (see Eq. (44) and
below). Using the eigenvalue bounds Eq. (67) and (68), we obtain A0 â‰¥ CnÇ«d(x, u)2 . As for the
second term we notice that
               B0 â‰¤ 2Ç«Tr(SS T (Z âˆ’ Z)(Z   b         b T ) + 2Ç«Tr(S T S(W âˆ’ W
                                                 âˆ’ Z)                           c )(W âˆ’ W c )T )          (73)
                     â‰¤ 2Ç«Ïƒmax (S)(||Z âˆ’ Z|| b 2F + ||W âˆ’ W   c ||2F ) â‰¤ CnÇ«d(x, u)4 .                     (74)
Therefore for Î´ â‰¥ d(x, u) small enough A0 > 2B0 , whence A0 â‰¥ CnÇ«d(x, u)2 /4.
    Upper bound on B1 . We begin by using Cauchy-Schwarz inequality:
                                      T                              bT + Wc SZ T )||F ,
                   B1 â‰¤ ||PE (U SZ + W SV T )||F ||PE (W S Z                                              (75)
Proceeding as above we obtain (with high probability)
                         T                                Ç«         T
             ||PE (U SZ + W SV T )||2F      â‰¤ (1 + Î¾) ||U SZ + W SV T ||2F                                (76)
                                                         n
                                                         2Ç«           T
                                            â‰¤ (1 + Î¾) (||U SZ ||2F + ||W SV T ||2F )                      (77)
                                                          n                                     
                                                                         T                  T
                                            â‰¤ (1 + Î¾)2Ç« Tr(S T SZ Z) + Tr(SS T W W )                      (78)
                                            â‰¤ CnÇ«d(x, u)2 .                                               (79)
                                                      19

In order to estimate the second factor in Eq. (75), we first notice that ||PE (W S ZbT + W             c SZ T )||2 â‰¤
                                                                                                                 F
2||PE (W S ZbT )||2 + 2||PE (W c SZ T )||2 and then bound each of the two terms in the same way. Con-
                  F                        F
sider, to be definite, the first one:
                                               X                               Xr     X
                             bT )||2                                                         2
                 ||PE (W S Z       F    =           (W S ZbT )2ij â‰¤ Ïƒmax (S)2              W ia Zbjb
                                                                                                  2
                                                                                                     .
                                             (ij)âˆˆE                           a,b=1 (ij)âˆˆE
At this point we can apply Lemma the same argument as after Eq. (47). Bounding Ïƒmax (S)2 and
using the fact that both W , Zb âˆˆ K(CÂµ0 ) we get
                                                                    âˆš
                            ||PE (W S ZbT )||2F = CnÇ« max(r/ Ç«, Î´2 )d(x, u)2 .
Putting together Eqs. (79) and (80) we finally get
                                   B1 = CnÇ« max(r 1/2 /Ç«1/4 , Î´)d(x, u)2 ,
                                             âˆš
which is smaller than A/100 for Î´, r/ Ç« small enough.
    Upper bound on B2 . We have
             B2 â‰¤ ||PE (U S Z      bT + Wc SV T )||F ||W SZ T ||F + |hPE (U S ZbT ), U (S âˆ’ Î£)V T i|
                           +|hPE (W c SV T ), U (S âˆ’ Î£)V T i|
                   â‰¡ B2â€² + B2â€²â€² + B2â€²â€²â€²
The upper bound on B2â€² is obtained similarly to the the one on B1 . Indeed proceeding as above we
obtain
                        ||PE (U S ZbT + W  c SV T )||2F   â‰¤ CnÇ«d(x, u)2 ,                                        (80)
                                                   T                        âˆš
                                          ||W SZ ||2F     â‰¤ CnÇ« max(r/ Ç«, Î´)d(x, u)2 ,                           (81)
                                                âˆš
whence B2â€² â‰¤ CnÇ«d(x, u)2 /100 for Î´, r/ Ç« small enough.
    Consider now B2â€²â€² . By Theorem 4.1 in [CR08], we have
                              Ç«                                   Î¾Ç«
                   B2â€²â€² â‰¤       |hU S ZbT , U (S âˆ’ Î£)V T i| + ||U S ZbT ||F ||U (S âˆ’ Î£)V T ||F                   (82)
                              n                                   n
                         â‰¤ Ïƒmax (S)Ç«||S âˆ’ Î£||F ||ZbT V ||F + Î¾Ç«||U S ZbT ||F ||S âˆ’ Î£||F .                        (83)
The first term is bounded using (the analogous of) Eq. (66) and ||S âˆ’ Î£||F â‰¤ d(x, u). For the second
                b 2 â‰¤ Cnd(x, u)2 , thus getting
term we use ||Z||   F
                        B2â€²â€² â‰¤ CnÇ«d(x, u)3 + CnÇ«Î¾d(x, u)2 â‰¤ CnÇ«(Î´ + Î¾)d(x, u)2                                   (84)
whence B2â€²â€² â‰¤ CnÇ«d(x, u)2 /100 for Î´ small enough. The same argument applies to B2â€²â€²â€² thus proving
the desired bound.
    Upper bound on B3 . Finally for the last term it is sufficient to use a crude bound
                                                                                                       
    B3 â‰¤ 4 ||PE (W S Z   bT )||F + ||PE (W c SZ T )||F ||PE (U (S âˆ’ Î£)V T )||F + ||PE (W SZ T )||F ,             (85)
and all of the factors have been estimated above.
                                                           20

7.2    Lower bound on grad G(x)
By the definition of G in Eq. (34), we have
                                  m
                                                        !                          n
                                                                                                       !
                            1 X â€² ||X (i) ||2                (i) c (i)         1 X â€²       ||Y (i) ||2
   hgrad G(x), wi  b =               G1                   hX , W i +                  G1                 hY (i) , Zb(i) i . (86)
                           Âµ0 r                2Âµ0 r                        Âµ0 r             2Âµ0 r
                                 i=1                                              j=1
It is therefore sufficient to show that if ||X (i) ||2 > 2Âµ0 r, then hX (i) , W           c (i) i > 0, and if ||Y (j) ||2 >
2Âµ0 r, then hY (j) , Zb(j) i > 0. We will just consider the first statement, the second being completely
symmetrical.
    From the explicit expressions (43) and (52) we get
                                                     n                                o
                                     X (i) = R cos Î˜ RT U (i) + sin Î˜ L(i) ,                                                (87)
                                                     n                                   o
                                     c (i) = R Î˜ cos Î˜L(i) âˆ’ Î˜ sin Î˜RT U (i) .
                                     W                                                                                      (88)
From the first expression it follows that
                            || sin Î˜ L(i) ||2 â‰¤ ||X (i) ||2 + || cos Î˜ RT U (i) ||2 â‰¤ 3 Âµ0 r .                              (89)
On the other hand, by taking the difference of Eqs. (87) and (88) we have
                 ||X (i) âˆ’ Wc (i) || â‰¤ ||(sin Î˜ âˆ’ Î˜ cos Î˜)L(i) || + ||(cos Î˜ + Î˜ sin Î˜)RT U (i) ||                          (90)
                                                                                       p          âˆš
                                       â‰¤ max(Î¸i2 )|| sin Î˜L(i) || + ||U (i) || â‰¤ Î´ 3Âµ0 r + Âµ0 r .                           (91)
                                               i
where we used the inequality (sin Ï‰ âˆ’ Ï‰ cos Ï‰) â‰¤ Ï‰ 2 sin Ï‰ valid for Ï‰ âˆˆ [0, Ï€/2]. For Î´ small enough
we have therefore ||X (i) âˆ’ W       c (i) || â‰¤ (9/10)âˆš2Âµ0 r. To conclude, for ||X (i) || â‰¥ 2Âµ0 r
                                                                                  p                  p
                                                            c (i) || â‰¥ ||X (i) ||( 2Âµ0 r âˆ’ (9/10) 2Âµ0 r) â‰¥ 0 .
               c (i) i â‰¥ ||X (i) ||2 âˆ’ ||X (i) || ||X (i) âˆ’ W
      hX (i) , W                                                                                                            (92)
Acknowledgements
We thank Emmanuel CandeÌs and Benjamin Recht for stimulating discussions on the subject of this
paper. This work was partially supported by a Terman fellowship and an NSF CAREER award
(CCF-0743978).
A      Proof of Remark 4.3
The proof consists in showing that |Al | > max{eâˆ’C1 Ç« m, C2 Î±} with probability                     less than 1/2n3 , using
                                                                    âˆš
Chernoff bound. In the case of large Ç«, when Ç« > 2 Î± log(n),                     we have P |Al | > C2 Î± â‰¤ 1/2n3 , for
                                                          âˆš                                                                 âˆš
C2 > 4/Î±. In the case of small Ç«, when Ç« â‰¤ 2 Î± log(n), P |Al | > eâˆ’C1 Ç« m â‰¤ 1/2n3 , for C1 < 1/4 Î±,
which proves the thesis.                           
    Analogously, we can prove that P |Ar | > max{eâˆ’C1 Ç« n, C2 } â‰¤ 1/2n3 , which finishes the proof
of Remark 4.3.
                                                                21

B     Proof of Remark 4.5
The expectation of the contribution of light couples, when each edge is independently revealed with
                âˆš
probability Ç«/ mn, is
                                                       ï£«                                    ï£¶
                                                 Ç« ï£­       X
                               E[Z] = âˆš                            xi MijA yj âˆ’ xT M y ï£¸ ,
                                                 mn
                                                         (i,j)âˆˆL
where we define M A by setting to zero the rows of M whose index is not in Al and the columns of
M whose index is not P    in Ar .
    In order to bound (i,j)âˆˆL xi MijA yj âˆ’ xT M y, we write,
                    X                                                                  X
                          xi MijA yj âˆ’ xT M y        =     xT M A âˆ’ M y âˆ’                      xi MijA yj
                  (i,j)âˆˆL                                                              (i,j)âˆˆL
                                                                                         X
                                                     â‰¤     xT M A âˆ’ M y +                        xi MijA yj .
                                                                                         (i,j)âˆˆL
    The first term can be bounded by noting that |(M A âˆ’ M )ij | is positive only if i âˆˆ                                 / Ar
                                                                                                               / Al or j âˆˆ
                     A                   âˆš
in which case |(M âˆ’ M )ij | â‰¤ Âµ1 r by the incoherence condition A2. Also, by Remark 4.3, there
exists Î´ â‰¤ max{eâˆ’C1 Ç« , C2 /n} such that |i : i âˆˆ      / Al | â‰¤ Î´m and |j : j âˆˆ         / Ar | â‰¤ Î´n. Denoting by I( Â· ) the
indicator function, we have
                                   X                                               âˆš
          xT M A âˆ’ M y â‰¤                    xi yj I(i âˆˆ    / Al ) + I(j âˆˆ    / Ar ) Âµ1 r
                                      ij
                                     ï£«                                                                        ï£¶
                                        X                       X             X                      X             âˆš
                                = ï£­            xi I(i âˆˆ/ Al )         yj +           yj I(j âˆˆ / Ar )       xi ï£¸ Âµ 1 r
                                          i                      j              j                      i
                                     âˆš âˆš              âˆš âˆš  âˆš
                                â‰¤        Î´m n + Î´n m Âµ1 r
                                         âˆš
                                     Âµ1 mnr
                                â‰¤        âˆš         .
                                             Ç«
         1
for Î´ â‰¤  4Ç« . We can bound the second term as follows
                                                                                   2
                                 X                            X        xi MijA yj
                                       xi MijA yj     â‰¤
                              (i,j)âˆˆL                       (i,j)âˆˆL     xi MijA yj
                                                            r
                                                                mn X                        2
                                                      â‰¤                        xi MijA yj
                                                                 rÇ«
                                                                      (i,j)âˆˆL
                                                            r
                                                                mn        X                     2
                                                      â‰¤                             xi MijA yj
                                                                 rÇ«
                                                                      iâˆˆ[m],jâˆˆ[n]
                                                               2 âˆš
                                                            Âµ1 mnr
                                                      â‰¤          âˆš         ,
                                                                    Ç«
                                                              22

where the second inequality follows from the definition of heavy couples and the last inequality is
due to incoherence condition A2.
    Hence summing two contributions, we get
                                                                 âˆš
                                           |E [Z]| â‰¤ Âµ1 + Âµ21 rÇ« .
C     Proof of Remark 4.6
We can associate to the matrix Q a bipartite graph G = ([m], [n], E). The proof is similar to the one
in [FKS89, FO05] and is based on two properties of the graph cG:
   1. The graph G has maximum degree bounded by a constant times the average degree:
                                                                2Ç«
                                                   deg(i) â‰¤   âˆš ,                                      (93)
                                                                  Î±
                                                                  âˆš
                                                  deg(j) â‰¤ 2Ç« Î± ,                                      (94)
      for all i âˆˆ [m] and j âˆˆ [n].
   2. Discrepancy. We will say that G (equivalently, the adjacency matrix Q) has the discrepancy
      property if, for any A âŠ† [m] and B âŠ† [n], one of the following is true:
                              e(A, B)
                          1.          â‰¤ Î¾1 ,                                                           (95)
                              Âµ(A, B)
                                         e(A, B)                                 âˆš          
                                                                                      mn
                          2. e(A, B) ln              â‰¤ Î¾2 max{|A|, Î±|B|} ln                      .     (96)
                                          Âµ(A, B)                               max{|A|, Î±|B|}
      for two numerical constants Î¾1 , Î¾2 (independent of n and Ç«). Here e(A, B) denotes the number
      of edges between A and B and Âµ(A, B) = |A||B||E|/mn denotes the average number of edges
      between A and B before trimming.
We will prove that the discrepancy property holds with high probability later in this section, see
Lemma C.1.
    Let us partition row and column indices with respect to the value of xu and yv :
                                                       âˆ†                    âˆ†
                                Ai = {u âˆˆ [m] : âˆš 2iâˆ’1 â‰¤ |xu | < âˆš 2i } ,
                                                        m                    m
                                                      âˆ† jâˆ’1               âˆ† j
                                Bj = {v âˆˆ [n] : âˆš 2          â‰¤ |yv | < âˆš 2 } ,
                                                       n                   n
                             âˆš                                            âˆš
for i âˆˆ {1, 2, . . . , âŒˆlog ( m/âˆ†)/ log 2âŒ‰}, and j âˆˆ {1, 2, . . . , âŒˆlog ( n/âˆ†)/ log 2âŒ‰}, and we denote the
size of subsets Ai and Bj by ai and bj respectively. Furthermore, we define ei,j to be the number
                                                                             âˆš
of edges between two subsets Ai and Bj , and we let Âµi,j = ai bj (Ç«/ mn). Notice that all indices u
of non zero xu fall into one of the subsets Ai â€™s defined above, since, by discretization, the smallest
                                                           âˆš
non-zero element of x âˆˆ Tm in absolute value is âˆ†/ m. The same applies for the entries of y âˆˆ Tn .
                                                        23

    By grouping the summation into Ai â€™s and Bj â€™s, we get
                              X                                        X                âˆ†2i âˆ†2j
                                        Quv |xu yv | â‰¤                             ei,j âˆš âˆš
                                                                              âˆš           m n
                             (u,v): âˆš                          (i,j):2i+j â‰¥ 4C Î±Ç«
                                                                              âˆ†2
                         |xu yv |â‰¥ Cn Ç«
                                                                    X             Ç« ei,j 2i 2j
                                                          = âˆ†2           ai bj âˆš           âˆš âˆš
                                                                                 mn Âµi,j m n
                                                                        X 22i 22j ei,j âˆšÇ«
                                                                  2âˆš
                                                          = âˆ† Ç«              ai     bj                 .
                                                                                                 2i+j
                                                                             | {zm} | {zn} |Âµi,j{z    }
                                                                               Î±i      Î²j      Ïƒi,j
Note that, by definition, we have
                                                 X
                                                      Î±i â‰¤ 4||x||2 /âˆ†2 ,                                        (97)
                                                  i
                                                 X
                                                      Î²i â‰¤ 4||y||2 /âˆ†2 .                                        (98)
                                                    i
                                                  P
We are now left with task of bounding                  Î±i Î²j Ïƒi,j , for Q that satisfies bounded degree property and
discrepancy property.
    Define,
                                                          âˆš                                          
                                              i+j     4C Î±Ç«
                  C1 â‰¡          (i, j) : 2        â‰¥                and (Ai , Bj ) satisfies (95) ,              (99)
                                                         âˆ†2
                                                          âˆš                                          
                                                      4C Î±Ç«
                  C2 â‰¡          (i, j) : 2i+j â‰¥                    and (Ai , Bj ) satisfies (96) \ C1 .        (100)
                                                         âˆ†2
                          P
We need to show that (i,j)âˆˆC1 âˆªC2 Î±i Î²j Ïƒi,j is bounded.
    For theâˆš terms in C1 this bound is easy. Since summation is over pairs of indices (i, j) such that
                                                             âˆš                                           P
2i+j â‰¥ 4Câˆ†2Î±Ç« , it follows that Ïƒi,j â‰¤ Î¾1 âˆ†2 /4C Î±. By Eqs. (97) and (98), we have C1 Î±i Î²j Ïƒi,j â‰¤
          âˆš
(Î¾1 âˆ†2 /4C Î±)(2/âˆ†)4 = O(1).
    For the terms in C2 the bound is more complicated. We assume ai â‰¤ Î±bj for simplicity and
the other case can be treated in the same manner. By change of notation the second discrepancy
condition becomes
                                                                                âˆš           
                                    ei,j                                             mn
                       ei,j log               â‰¤ Î¾2 max{ai , Î±bj } log                               .          (101)
                                    Âµi,j                                      max{ai , Î±bj }
We start by changing variables on both sides of Eq. (101).
                                                                                  2j 
                                 ei,j ai bj Ç«         ei,j                             2
                                     âˆš         log              â‰¤ Î¾2 Î±bj log            âˆš       .
                                 Âµi,j mn              Âµi,j                           Î²j Î±
                                         âˆš
Now, multiply each side by 2i /bj Ç«2j to get
                                                
                                            ei,j        Î¾2 2i                          âˆš 
                          Ïƒi,j Î±i log                â‰¤ âˆš j log(22j ) âˆ’ log(Î²j Î±) .                             (102)
                                           Âµi,j            Ç«2
To achieve the desired bound, we partition the analysis into 5 cases:
                                                                24

                                                              P
   1. Ïƒi,j â‰¤ 1 : By Eqs. (97) and (98), we have                  Î±i Î²j Ïƒi,j â‰¤ (2/âˆ†)4 = O(1).
              âˆš                                                                                        âˆš
   2. 2i > Ç«2j : By the bounded degree propertyP                 in Eq. (94), we have ei,j â‰¤ aiP   2Ç«/ Î±, which implies
                                                                                   âˆš           âˆš                    âˆš
      that ei,j /Âµi,j â‰¤ 2n/bj . For a fixed i we have, j Î²j Ïƒi,j I(2i > Ç«2j ) â‰¤ 2 Ç« j 2jâˆ’i I(2i > Ç«2j ) â‰¤
                   P
      4. Then,         Î±i Î²j Ïƒi,j â‰¤ 16/âˆ†2 = O(1).
                                                   âˆš 
   3. log (ei,j /Âµi,j ) > 14 log(22j ) âˆ’ log(Î²j Î±) : From Eq.(102), it immediately follows that Ïƒi,j Î±i â‰¤
      4Î¾    i                                                     âˆš j
       âˆš 2 2j . Because of case 2, we can assume 2i â‰¤                Ç«2 , which implies that for a fixed j we have the
        Ç«2
                                   P                    P 2i           i     âˆš j
      following inequality :         i Ïƒi,j Î±i â‰¤ 4Î¾2        i Ç«2j I(2 â‰¤       Ç«2 ) â‰¤ 8Î¾2 . Then it follows by Eq. (98)
                                                              âˆš
               P                       2
      that       Î±i Î²j Ïƒi,j â‰¤ 32Î¾2 /âˆ† = O(1).
                                âˆš                                                                                        âˆš 
   4. log(22j ) â‰¥ âˆ’ log(Î²j Î±) : Because of case 3, we can assume log (ei,j /Âµi,j ) â‰¤ 41 log(22j ) âˆ’ log(Î²j Î±) ,
      which implies that log (ei,j /Âµi,j ) â‰¤ log(2j ). Further, because of case 1, we assume 1 < Ïƒi,j =
           âˆš                                                                           âˆš
      ei,j Ç«/Âµi,j 2i+j . Combining those two inequalities, we get 2i â‰¤ Ç«.
      Since in defining C2 we excluded C1 , if (i, j) âˆˆ C2 then log (ei,j /Âµi,j ) â‰¥ 1. Applying Eq. (102)
                                                                   âˆš                        âˆš                 âˆš
      we get Ïƒi,j Î±i â‰¤ Ïƒi,j Î±i log (ei,j /Âµi,j ) â‰¤ (Î¾2 2iâˆ’j / Ç«) log(22j ) âˆ’ log(Î²j Î±) â‰¤ 4Î¾2 2i / Ç«.
                                                                    P                 P 2i           âˆš
      Combining above two results, it follows that i Ïƒi,j Î±i â‰¤ 4Î¾2 i âˆš                      Ç«
                                                                                              I(2i â‰¤ Ç«) â‰¤ 8Î¾2 . Then,
                                            P
      we have the desired bound :              Î±i Î²j Ïƒi,j â‰¤ 32Î¾    2
                                                                âˆ†2 = O(1).
                                 âˆš                                                                          âˆš
   5. log(22j ) < âˆ’ log(Î²j Î±) : Because of case 4, we assume                      log(22j ) â‰¤ âˆ’ log(Î²   j Î±). Then it
                                                                                                   âˆš                  âˆš
      follows, since weâ€™re not in case 3, that log (ei,j /Âµi,j ) â‰¤ 14 log(22j ) âˆ’ log(Î²j Î±) â‰¤ âˆ’ log(Î²j Î±).
                                   âˆš                                           âˆš                âˆš     âˆš
      Hence, ei,j /Âµi,j â‰¤ 1/Î²j Î±. This implies that Ïƒi,j = ei,j Ç«/Âµi,j 2i+j â‰¤ Ç«/Î²j Î±2i+j . Since the
                                                                                   âˆš                    P              âˆ†2
      summation is over pairs of indices (i, j) such that 2i+j â‰¥ 4C Î±Ç«/âˆ†2 , we have j Ïƒi,j Î²j â‰¤ 2Î±C                       .
                                  P                 2
      Then it follows that          Î±i Î²j Ïƒi,j â‰¤ Î±C = O(1).
    Summing up the results, we get that there exists a constant C â€² â‰¤                   16
                                                                                       âˆ†4
                                                                                            + Câˆ†4Î¾21âˆšÎ± + âˆ† 16   32Î¾2    2
                                                                                                            2 + âˆ†2 + Î±C ,
such that
                                                 X
                                                              Î±i Î²j Ïƒi,j â‰¤ C â€² .
                                                         âˆš
                                                           Î±Ç«
                                         (i,j):2i+j â‰¥ 4C
                                                        âˆ†2
This finishes the proof of Remark 4.6.
Lemma C.1. The adjacency matrix Q has discrepancy property with probability at least 1 âˆ’ 1/n.
Proof. The proof is a generalization of analogous result in [FKS89, FO05] which is proved to hold only
with probability larger than 1âˆ’eâˆ’CÇ« . The stronger statement quoted here is a result of the observation
that, when we trim the graph the number of edges between any two subsets does not increase. Define
Q0 to be the adjacency matrix corresponding to original random matrix M E before trimming. If
the discrepancy assumption holds for Q0 , then it also holds for Q, since eQ (A, B) â‰¤ eQ0 (A, B), for
A âŠ† [m] and B âŠ† [n].
    Now we need to show that the desired property is satisfied for Q0 . This is proved for the case
of non-bipartite graph in Section 2.2.5 of [FO05], and analogous analysis for bipartite graph shows
that for all subsets A âŠ† [m] and B âŠ† [n], with probability at least 1 âˆ’ 1/n, the discrepancy condition
                                                                          12
holds with Î¾1 = max{4, 2e} and Î¾2 = max{12 + 15                Î± , 15 + Î± }.
                                                               25

D      Proof of remarks 5.1 and 5.2
Proof. (Remark 5.1.) Let Î¸ = (Î¸1 , . . . , Î¸p ), Î¸i âˆˆ [âˆ’Ï€/2, Ï€/2] be the principal angles between the
planes spanned by the columns of X1 and X2 . It is known that dc (X1 , X2 ) = ||2 sin(Î¸/2)||2 and
dp (X1 , X2 ) = || sin Î¸||2 . The thesis follows from the elementary inequalities
                                  1      âˆš
                                    Î± â‰¤ 2 sin(Î±/2) â‰¤ sin Î± â‰¤ 2 sin(Î±/2)                           (103)
                                  Ï€
valid for Î± âˆˆ [0, Ï€/2].
Proof. (Remark 5.2.) We start by observing that
                                                    1
                                    dp (V, Y ) = âˆš        min ||V âˆ’ Y A||F .                      (104)
                                                     n AâˆˆRrÃ—r
Indeed the minimization on the right hand side can be performed explicitly (as ||V âˆ’ Y A||2F is a
quadratic function of A) and the minimum is achieved at A = Y T V /n. The inequality follows by
simple algebraic manipulations.
    Take A = S T X T U Î£âˆ’1 /n. Then
                        ||V âˆ’ Y A||F     =        sup    hB, (V âˆ’ Y A)i                           (105)
                                              B,||B||F â‰¤1
                                                               1
                                         =        sup    hB T , Î£âˆ’1 U T (U Î£V T âˆ’ XSY T )i        (106)
                                              B,||B||F â‰¤1      n
                                              1                              c)i
                                         =          sup hU Î£âˆ’1 B T , (M âˆ’ M                       (107)
                                              n B,||B||F â‰¤1
                                              1                                 c||F .
                                         â‰¤          sup ||U Î£âˆ’1 B T ||F ||M âˆ’ M                   (108)
                                              n B,||B||F â‰¤1
On the other hand
              ||U Î£âˆ’1 B T ||2F = Tr(BÎ£âˆ’1 U T U Î£âˆ’1 B T ) = nTr(B T BÎ£âˆ’2 ) â‰¤ nÎ£âˆ’2             2
                                                                                    min ||B||F ,
whereby the last inequality follows from the fact that Î£ is diagonal. Together (104) and (108), this
implies the thesis.
References
[AFK+ 01] Y. Azar, A. Fiat, A. Karlin, F. McSherry, and J. Saia, Spectral analysis of data, Pro-
             ceedings of the thirty-third annual ACM symposium on Theory of computing (New York,
             NY, USA), ACM, 2001, pp. 619â€“626.
[AM07]       D. Achlioptas and F. McSherry, Fast computation of low-rank matrix approximations, J.
             ACM 54 (2007), no. 2, 9.
[Arm66]      L. Armijo, Minimization of functions having lipschitz continuous first partial derivatives,
             Pacific J. Math. 16 (1966), no. 1, 1â€“3.
[BDJ99]      M. W. Berry, Z. DrmacÌ, and E. R. Jessup, Matrices, vector spaces, and information
             retrieval, SIAM Review 41 (1999), no. 2, 335â€“362.
                                                           26

[Ber92] M. W. Berry, Large scale sparse singular value computations, International Journal of
        Supercomputer Applications 6 (1992), 13â€“49.
[CCS08] J-F Cai, E. J. CandeÌ€s, and Z. Shen, A singular value thresholding algorithm for matrix
        completion, arXiv:0810.3286, 2008.
[CR08]  E. J. CandeÌ€s and B. Recht, Exact matrix completion via convex optimization,
        arxiv:0805.4471, 2008.
[CRT06] E. J. Candes, J. K. Romberg, and T. Tao, Robust uncertainty principles: exact signal
        reconstruction from highly incomplete frequency information, IEEE Trans. on Inform.
        Theory 52 (2006), 489â€“ 509.
[CT09]  E. J. CandeÌ€s and T. Tao, The power of convex relaxation: Near-optimal matrix comple-
        tion, arXiv:0903.1476, 2009.
[Don06] D. L. Donoho, Compressed Sensing, IEEE Trans. on Inform. Theory 52 (2006), 1289â€“
        1306.
[EAS99] A. Edelman, T. A. Arias, and S. T. Smith, The geometry of algorithms with orthogonality
        constraints, SIAM J. Matr. Anal. Appl. 20 (1999), 303â€“353.
[Faz02] M. Fazel, Matrix rank minimization with applications, Ph.D. thesis, Stanford University,
        2002.
[FKS89] J. Friedman, J. Kahn, and E. SzemereÌdi, On the second eigenvalue in random regular
        graphs, Proceedings of the Twenty-First Annual ACM Symposium on Theory of Com-
        puting (Seattle, Washington, USA), ACM, may 1989, pp. 587â€“598.
[FKV04] A. Frieze, R. Kannan, and S. Vempala, Fast monte-carlo algorithms for finding low-rank
        approximations, J. ACM 51 (2004), no. 6, 1025â€“1041.
[FO05]  U. Feige and E. Ofek, Spectral techniques applied to sparse random graphs, Random Struct.
        Algorithms 27 (2005), no. 2, 251â€“275.
[KMO08] R. H. Keshavan, A. Montanari, and S. Oh, Learning low rank matrices from O(n) entries,
        Proc. of the Allerton Conf. on Commun., Control and Computing, September 2008.
[KOM09] R. H. Keshavan, S. Oh, and A. Montanari, Matrix completion from a few entries,
        arXiv:0901.3150, January 2009.
[Net]   Netflix prize.
[RFP07] B. Recht, M. Fazel, and P. Parrilo, Guaranteed minimum rank solutions of matrix equa-
        tions via nuclear norm minimization, arxiv:0706.4138, 2007.
[SC09]  A. Singer and M. Cucuringu, Uniqueness of low-rank matrix completion by rigidity theory,
        arXiv:0902.3846, January 2009.
                                               27

