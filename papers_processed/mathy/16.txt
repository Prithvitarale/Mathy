JMLR: Workshop and Conference Proceedings vol 23 (2012) 33.1â€“33.34          25th Annual Conference on Learning Theory
                   A Method of Moments for Mixture Models and
                                       Hidden Markov Models
Animashree Anandkumar                                                               A . ANANDKUMAR @ UCI . EDU
University of California, Irvine
Daniel Hsu                                                                            DAHSU @ MICROSOFT. COM
Microsoft Research
Sham M. Kakade                                                                    SKAKADE @ MICROSOFT. COM
Microsoft Research
Editor: Shie Mannor, Nathan Srebro, Robert C. Williamson
                                                      Abstract
     Mixture models are a fundamental tool in applied statistics and machine learning for treating data
     taken from multiple subpopulations. The current practice for estimating the parameters of such
     models relies on local search heuristics (e.g., the EM algorithm) which are prone to failure, and ex-
     isting consistent methods are unfavorable due to their high computational and sample complexity
     which typically scale exponentially with the number of mixture components. This work develops an
     efficient method of moments approach to parameter estimation for a broad class of high-dimensional
     mixture models with many components, including multi-view mixtures of Gaussians (such as mix-
     tures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous
     unsupervised learning results for mixture models that were not achieved by previous works; and,
     because of its simplicity, it offers a viable alternative to EM for practical deployment.
1. Introduction
Mixture models are a fundamental tool in applied statistics and machine learning for treating data
taken from multiple subpopulations (Titterington et al., 1985). In a mixture model, the data are
generated from a number of possible sources, and it is of interest to identify the nature of the indi-
vidual sources. As such, estimating the unknown parameters of the mixture model from sampled
dataâ€”especially the parameters of the underlying constituent distributionsâ€”is an important statis-
tical task. For most mixture models, including the widely used mixtures of Gaussians and hidden
Markov models (HMMs), the current practice relies on the Expectation-Maximization (EM) algo-
rithm, a local search heuristic for maximum likelihood estimation. However, EM has a number
of well-documented drawbacks regularly faced by practitioners, including slow convergence and
suboptimal local optima (Redner and Walker, 1984).
     An alternative to maximum likelihood and EM, especially in the context of mixture models,
is the method of moments approach. The method of moments dates back to the origins of mix-
ture models with Pearsonâ€™s solution for identifying the parameters of a mixture of two univariate
Gaussians (Pearson, 1894). In this approach, model parameters are chosen to specify a distribu-
tion whose p-th order moments, for several values of p, are equal to the corresponding empirical
moments observed in the data. Since Pearsonâ€™s work, the method of moments has been studied
and adapted for a variety of problems; their intuitive appeal is also complemented with a guarantee
 c 2012 A. Anandkumar, D. Hsu & S.M. Kakade.

                                      A NANDKUMAR H SU K AKADE
of statistical consistency under mild conditions. Unfortunately, the method often runs into trouble
with large mixtures of high-dimensional distributions. This is because the equations determining
the parameters are typically based on moments of order equal to the number of model parameters,
and high-order moments are exceedingly difficult to estimate accurately due to their large variance.
    This work develops a computationally efficient method of moments based on only low-order
moments that can be used to estimate the parameters of a broad class of high-dimensional mixture
models with many components. The resulting estimators can be implemented with standard numer-
ical linear algebra routines (singular value and eigenvalue decompositions), and the estimates have
low variance because they only involve low-order moments. The class of models covered by the
method includes certain multivariate Gaussian mixture models and HMMs, as well as mixture mod-
els with no explicit likelihood equations. The method exploits the availability of multiple indirect
â€œviewsâ€ of a modelâ€™s underlying latent variable that determines the source distribution, although
the notion of a â€œviewâ€ is rather general. For instance, in an HMM, the past, present, and future
observations can be thought of as different noisy views of the present hidden state; in a mixture
of product distributions (such as axis-aligned Gaussians), the coordinates in the output space can
be partitioned (say, randomly) into multiple non-redundant â€œviewsâ€. The new method of moments
leads to unsupervised learning guarantees for mixture models under mild rank conditions that were
not achieved by previous works; in particular, the sample complexity of accurate parameter estima-
tion is shown to be polynomial in the number of mixture components and other relevant quantities.
Finally, due to its simplicity, the new method (or variants thereof) also offers a viable alternative to
EM and maximum likelihood for practical deployment.
1.1. Related work
Gaussian mixture models. The statistical literature on mixture models is vast (a more thorough
treatment can be found in the texts of Titterington et al. (1985) and Lindsay (1995)), and many
advances have been made in computer science and machine learning over the past decade or so, in
part due to their importance in modern applications. The use of mixture models for clustering data
comprises a large part of this work, beginning with the work of Dasgupta (1999) on learning mix-
tures of k well-separated d-dimensional Gaussians. This and subsequent work (Arora and Kannan,
2001; Dasgupta and Schulman, 2007; Vempala and Wang, 2002; Kannan et al., 2005; Achlioptas
and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al.,
2009) have focused on efficient algorithms that provably recover the parameters of the constituent
Gaussians from data generated by such a mixture distribution, provided that the distance between
each pair of means is sufficiently large (roughly either dc or k c times the standard deviation of the
Gaussians, for some c > 0). Such separation conditions are natural to expect in many clustering
applications, and a number of spectral projection techniques have been shown to enhance the sep-
aration (Vempala and Wang, 2002; Kannan et al., 2005; Brubaker and Vempala, 2008; Chaudhuri
et al., 2009). More recently, techniques have been developed for learning mixtures of Gaussians
without any separation condition (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant,
2010), although the computational and sample complexities of these methods grow exponentially
with the number of mixture components k. This dependence has also been shown to be inevitable
without further assumptions (Moitra and Valiant, 2010).
Method of moments. The latter works of Belkin and Sinha (2010), Kalai et al. (2010), and Moitra
and Valiant (2010) (as well as the algorithms of Feldman et al. (2005, 2006) for a related but differ-
                                                 33.2

                     A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
ent learning objective) can be thought of as modern implementations of the method of moments, and
their exponential dependence on k is not surprising given the literature on other moment methods
for mixture models. In particular, a number of moment methods for both discrete and continuous
mixture models have been developed using techniques such as the Vandermonde decompositions
of Hankel matrices (Lindsay, 1989; Lindsay and Basak, 1993; Boley et al., 1997; Gravin et al.,
2012). In these methods, following the spirit of Pearsonâ€™s original solution, the model parame-
ters are derived from the roots of polynomials whose coefficients are based on moments up to the
â„¦(k)-th order. The accurate estimation of such moments generally has computational and sample
complexity exponential in k.
Spectral approach to parameter estimation with low-order moments. The present work is based
on a notable exception to the above situation, namely Changâ€™s spectral decomposition technique for
discrete Markov models of evolution (Chang, 1996) (see also Mossel and Roch (2006) and Hsu
et al. (2009) for adaptations to other discrete mixture models such as discrete HMMs). This spectral
technique depends only on moments up to the third-order; consequently, the resulting algorithms
have computational and sample complexity that scales only polynomially in the number of mixture
components k. The success of the technique depends on a certain rank condition of the transition
matrices; but this condition is much milder than separation conditions of clustering works, and
it remains sufficient even when the dimension of the observation space is very large (Hsu et al.,
2009). In this work, we extend Changâ€™s spectral technique to develop a general method of moments
approach to parameter estimation, which is applicable to a large class of mixture models and HMMs
with both discrete and continuous component distributions in high-dimensional spaces. Like the
moment methods of Moitra and Valiant (2010) and Belkin and Sinha (2010), our algorithm does not
require a separation condition; but unlike those previous methods, the algorithm has computational
and sample complexity polynomial in k.
      Some previous spectral approaches for related learning problems only use second-order mo-
ments, but these approaches can only estimate a subspace containing the parameter vectors and
not the parameters themselves (McSherry, 2001). Indeed, it is known that the parameters of even
very simple discrete mixture models are not generally identifiable from only second-order mo-
ments (Chang, 1996)1 . We note that moments beyond the second-order (specifically, fourth-order
moments) have been exploited in the methods of Frieze et al. (1996) and Nguyen and Regev (2009)
for the problem of learning a parallelepiped from random samples, and that these methods are very
related to techniques used for independent component analysis (HyvaÌˆrinen and Oja, 2000). Adapt-
ing these techniques for other parameter estimation problems is an enticing possibility.
Multi-view learning. The spectral technique we employ depends on the availability of multiple
views, and such a multi-view assumption has been exploited in previous works on learning mixtures
of well-separated distributions (Chaudhuri and Rao, 2008; Chaudhuri et al., 2009). In these previous
works, a projection based on a canonical correlation analysis (Hotelling, 1935) between two views
is used to reinforce the separation between the mixture components, and to cancel out noise orthogo-
nal to the separation directions. The present work, which uses similar correlation-based projections,
shows that the availability of a third view of the data can remove the separation condition entirely.
The multi-view assumption substantially generalizes the case where the component distributions
are product distributions (such as axis-aligned Gaussians), which has been previously studied in the
  1. See Appendix G for an example of Chang (1996) demonstrating the non-identifiability of parameters from only
     second-order moments in a simple class of Markov models.
                                                       33.3

                                      A NANDKUMAR H SU K AKADE
literature (Dasgupta, 1999; Vempala and Wang, 2002; Chaudhuri and Rao, 2008; Feldman et al.,
2005, 2006); the combination of this and a non-degeneracy assumption is what allows us to avoid
the sample complexity lower bound of Moitra and Valiant (2010) for Gaussian mixture models. The
multi-view assumption also naturally arises in many applications, such as in multimedia data with
(say) text, audio, and video components (Blaschko and Lampert, 2008; Chaudhuri et al., 2009); as
well as in linguistic data, where the different words in a sentence or paragraph are considered noisy
predictors of the underlying semantics (Gale et al., 1992). In the vein of this latter example, we
consider estimation in a simple bag-of-words document topic model as a warm-up to our general
method; even this simpler model illustrates the power of pair-wise and triple-wise (i.e., bigram and
trigram) statistics that were not exploited by previous works on multi-view learning.
1.2. Outline
Section 2 first develops the method of moments in the context of a simple discrete mixture model
motivated by document topic modeling; an explicit algorithm and convergence analysis are also
provided. The general setting is considered in Section 3, where the main algorithm and its accom-
panying correctness and efficiency guarantee are presented. Applications to learning multi-view
mixtures of Gaussians and HMMs are discussed in Section 4. Proofs and additional discussion are
provided in the appendices.
1.3. Notations
The standard inner product between vectors ~u and ~v is denoted by h~u, ~v i = ~u>~v . We denote the
p-norm of a vector ~v by k~v kp . For a matrix A âˆˆ RmÃ—n , we let kAk2 denote its spectral norm
kAk2 := sup~v6=~0 kA~v k2 /k~v k2 , kAkF denote its Frobenius norm, Ïƒi (A) denote the i-th largest
singular value, and Îº(A) := Ïƒ1 (A)/ÏƒP                                                             nâˆ’1 :=
                                              min(m,n) (A) denote its condition number. Let âˆ†
                             n                    n                                                n
{(p1 , p2 , . . . , pn ) âˆˆ R : pi â‰¥ 0 âˆ€i,         i=1 pi = 1} denote the probability simplex in R , and
let S nâˆ’1                   n                                             n             d
             := {~u âˆˆ R : k~uk2 = 1} denote the unit sphere in R . Let ~ei âˆˆ R denote the i-th
coordinate vector whose i-th entry is 1 and the rest are zero. Finally, for a positive integer n, let
[n] := {1, 2, . . . , n}.
2. Warm-up: bag-of-words document topic modeling
We first describe our method of moments in the simpler context of bag-of-words models for docu-
ments. Proofs of lemmas and theorems in this section are given in Appendix A.
2.1. Setting
Suppose a document corpus can be partitioned by topic, with each document being assigned a single
topic. Further, suppose the words in a document are drawn independently from a multinomial
distribution corresponding to the documentâ€™s topic. Let k be the number of distinct topics in the
corpus, d be the number of distinct words in the vocabulary, and ` â‰¥ 3 be the number of words in
each document (so the documents may be quite short).
     The generative process for a document is given as follows:
    1. The documentâ€™s topic is drawn according to the multinomial distribution specified by the
       probability vector w    ~ = (w1 , w2 , . . . , wk ) âˆˆ âˆ†kâˆ’1 . This is modeled as a discrete random
                                                        33.4

                             A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
         variable h such that
                                                     Pr[h = j] = wj ,        j âˆˆ [k].
    2. Given the topic h, the documentâ€™s ` words are drawn independently according to the multi-
         nomial distribution specified by the probability vector Âµ                ~ h âˆˆ âˆ†dâˆ’1 . The random vectors
        ~x1 , ~x2 , . . . , ~x` âˆˆ Rd represent the ` words by setting
                                   ~xv = ~ei â‡” the v-th word in the document is i,               i âˆˆ [d]
         (the reason for this encoding of words will become clear in the next section). Therefore, for
         each word v âˆˆ [`] in the document,
                                                                  ~ j i = Mi,j ,
                                    Pr[~xv = ~ei |h = j] = h~ei , Âµ                   i âˆˆ [d], j âˆˆ [k],
         where M âˆˆ RdÃ—k is the matrix of conditional probabilities M := [~                          Âµ2 | Â· Â· Â· |~
                                                                                                Âµ1 |~           Âµk ].
This probabilistic model has the conditional independence structure depicted in Figure 3(a) as a
directed graphical model.
     We assume the following condition on w              ~ and M .
Condition 1 (Non-degeneracy: document topic model) wj >0 for all j âˆˆ[k], and M has rank k.
This condition requires that each topic has non-zero probability, and also prevents any topicâ€™s word
distribution from being a mixture of the other topicsâ€™ word distributions.
2.2. Pair-wise and triple-wise probabilities
Define Pairs âˆˆ RdÃ—d to be the matrix of pair-wise probabilities whose (i, j)-th entry is
                                      Pairsi,j := Pr[~x1 = ~ei , ~x2 = ~ej ],    i, j âˆˆ [d].
Also define Triples âˆˆ RdÃ—dÃ—d to be the third-order tensor of triple-wise probabilities whose
(i, j, Îº)-th entry is
                              Triplesi,j,Îº := Pr[~x1 = ~ei , ~x2 = ~ej , ~x3 = ~eÎº ],    i, j, Îº âˆˆ [d].
The identification of words with coordinate vectors allows Pairs and Triples to be viewed as ex-
pectations of tensor products of the random vectors ~x1 , ~x2 , and ~x3 :
                                Pairs = E[~x1 âŠ— ~x2 ]     and    Triples = E[~x1 âŠ— ~x2 âŠ— ~x3 ].                             (1)
We may also view Triples as a linear operator Triples : Rd â†’ RdÃ—d given by
                                            Triples(~Î· ) := E[(~x1 âŠ— ~x2 )h~Î· , ~x3 i].
In other words, the (i, j)-th entry of Triples(~Î· ) for ~Î· = (Î·1 , Î·2 , . . . , Î·d ) is
                                                  Xd                        Xd
                               Triples(~Î· )i,j =      Î·x Triplesi,j,x =         Î·x Triples(~ex )i,j .
                                                  x=1                       x=1
     The following lemma shows that Pairs and Triples(~Î· ) can be viewed as certain matrix products
involving the model parameters M and w.               ~
Lemma 1 Pairs=M diag(w)M                ~      >
                                                 and Triples(~Î· )=M diag(M > ~Î· ) diag(w)M       ~      >
                                                                                                            for all ~Î· âˆˆ Rd .
                                                               33.5

                                        A NANDKUMAR H SU K AKADE
2.3. Observable operators and their spectral properties
The pair-wise and triple-wise probabilities can be related in a way that essentially reveals the con-
ditional probability matrix M . This is achieved through a matrix called an â€œobservable operatorâ€.
Similar observable operators were previously used to characterize multiplicity automata (SchuÌˆtzenberger,
1961; Jaeger, 2000) and, more recently, for learning discrete HMMs (via an operator parameteriza-
tion) (Hsu et al., 2009).
Lemma 2 Assume Condition 1. Let U âˆˆ RdÃ—k and V âˆˆ RdÃ—k be matrices such that both U > M
and V > M are invertible. Then U > PairsV is invertible, and for all ~Î· âˆˆ Rd , the â€œobservable
operatorâ€ B(~Î· ) âˆˆ RkÃ—k , given by
                               B(~Î· ) := (U > Triples(~Î· )V )(U > PairsV )âˆ’1 ,
satisfies
                                  B(~Î· ) = (U > M ) diag(M > ~Î· )(U > M )âˆ’1 .
     The matrix B(~Î· ) is called â€œobservableâ€ because it is only a function of the observable variablesâ€™
joint probabilities (e.g., Pr[~x1 = ~ei , ~x2 = ~ej ]). In the case ~Î· = ~ex for some x âˆˆ [d], the matrix
B(~ex ) is similar (in the linear algebraic sense) to the diagonal matrix diag(M >~ex ); the collection
of matrices {diag(M >~ex ) : x âˆˆ [d]} (together with w)        ~ can be used to compute joint probabilities
under the model (see, e.g., Hsu et al. (2009)). Note that the columns of U > M are eigenvectors of
B(~ex ), with the j-th column having an associated eigenvalue equal to Pr[~xv = x|h = j]. If the
word x has distinct probabilities under every topic, then B(~ex ) has exactly k distinct eigenvalues,
each having geometric multiplicity one and corresponding to a column of U > M .
2.4. Topic-word distribution estimator and convergence guarantee
The spectral properties of the observable operators B(~Î· ) implied by Lemma 2 suggest the estimation
procedure (Algorithm A) in Figure 1. The procedure is essentially a plug-in approach based on the
equations relating the various moments in Lemma 2. We focus on estimating M ; the mixing weights
~ can be handled as a secondary step (see Appendix B.5).
w
On the choice of ~Î· . As discussed in the previous section, a suitable choice for ~Î· can be based
on prior knowledge about the topic-word distributions, such as ~Î· = ~ex for some x âˆˆ [d] that has
different conditional probabilities under each topic. In the absence of such information, one may
select ~Î· randomly from the subspace range(UÌ‚ ). Specifically, take ~Î· := UÌ‚ Î¸~ where Î¸~ âˆˆ Rk is a
random unit vector distributed uniformly over S kâˆ’1 .
     The following theorem establishes the convergence rate of Algorithm A.
Theorem 3 There exists a constant C > 0 such that the following holds. Pick any Î´ âˆˆ (0, 1).
Assume the document topic model from Section 2.1 satisfies Condition 1. Further, assume that in
Algorithm A, Pairs          \ are, respectively, the empirical averages of N independent copies
                [ and Triples
of ~x1 âŠ— ~x2 and ~x1 âŠ— ~x2 âŠ— ~x3 ; and that ~Î· = UÌ‚ Î¸~ where Î¸~ âˆˆ Rk is an independent random unit vector
distributed uniformly over S kâˆ’1 . If
                                                       k 7 Â· ln(1/Î´)
                                    N â‰¥CÂ·                                   ,
                                               Ïƒk (M )6 Â· Ïƒk (Pairs)4 Â· Î´ 2
                                                       33.6

                      A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
   Algorithm A
       1. Obtain empirical frequencies of word pairs and triples from a given sample of documents, and
                             [ âˆˆ RdÃ—d and Triples
          form the tables Pairs                        \ âˆˆ RdÃ—dÃ—d corresponding to the population quantities
          Pairs and Triples.
       2. Let UÌ‚ âˆˆ RdÃ—k and VÌ‚ âˆˆ RdÃ—k be, respectively, matrices of orthonormal left and right singular
                      [ corresponding to its top k singular values.
          vectors of Pairs
       3. Pick ~Î· âˆˆ Rd (see remark in the main text), and compute the right eigenvectors Î¾Ë†1 , Î¾Ë†2 , . . . , Î¾Ë†k (of
          unit Euclidean norm) of
                                            BÌ‚(~Î· ) := (UÌ‚ > Triples(~        [ VÌ‚ )âˆ’1 .
                                                              \ Î· )VÌ‚ )(UÌ‚ > Pairs
          (Fail if not possible.)
       4. Let ÂµÌ‚j := UÌ‚ Î¾Ë†j /h~1, UÌ‚ Î¾Ë†j i for all j âˆˆ [k].
       5. Return MÌ‚ := [ÂµÌ‚1 |ÂµÌ‚2 | Â· Â· Â· |ÂµÌ‚k ].
                        Figure 1: Topic-word distribution estimator (Algorithm A).
then with probability at least 1 âˆ’ Î´, the parameters returned by Algorithm A have the following
guarantee: there exists a permutation Ï„ on [k] and scalars c1 , c2 , . . . , ck âˆˆ R such that, for each
j âˆˆ [k],                                                                                   r
                                                                           k5                ln(1/Î´)
                kcj ÂµÌ‚j âˆ’ Âµ ~ Ï„ (j) k2 â‰¤ C Â· k~     ÂµÏ„ (j) k2 Â·        4            2
                                                                                         Â·           .
                                                                Ïƒk (M ) Â· Ïƒk (Pairs) Â· Î´        N
    Some illustrative empirical results using Algorithm A are presented in Appendix A.5. A few
remarks about the theorem are in order.
On boosting the confidence. Although the convergence depends polynomially on 1/Î´, where Î´ is
the failure probability, it is possible to boost the confidence by repeating Step 3 of Algorithm A with
different random ~Î· until the eigenvalues of BÌ‚(~Î· ) are sufficiently separated (as judged by confidence
intervals).
On the scaling factors cj . With a larger sample complexity that depends on d, an error bound can
be established for kÂµÌ‚j âˆ’ Âµ      ~ Ï„ (j) k1 directly (without the unknown scaling factors cj ), but we do not
pursue this as the cj are removed in Algorithm B anyway.
3. A method of moments for multi-view mixture models
We now consider a much broader class of mixture models and present a general method of moments
in this context. Proofs of lemmas and theorems in this section are given in Appendix B.
3.1. General setting
Consider the following multi-view mixture model; k denotes the number of mixture components,
and ` denotes the number of views. We assume ` â‰¥ 3 throughout. Let w                       ~ = (w1 , w2 , . . . , wk ) âˆˆ
âˆ†kâˆ’1 be a vector of mixing weights, and let h be a (hidden) discrete random variable with Pr[h =
j] = wj for all j âˆˆ [k]. Let ~x1 , ~x2 , . . . , ~x` âˆˆ Rd be ` random vectors that are conditionally
independent given h; the directed graphical model is depicted in Figure 3(a).
                                                                33.7

                                          A NANDKUMAR H SU K AKADE
      Define the conditional mean vectors as
                                   ~ v,j := E[~xv |h = j],
                                   Âµ                          v âˆˆ [`], j âˆˆ [k],
 and let Mv âˆˆ RdÃ—k be the matrix whose j-th column is Âµ          ~ v,j . Note that we do not specify anything
 else about the (conditional) distribution of ~xv â€”it may be continuous, discrete, or even a hybrid
 depending on h.
      We assume the following conditions on w       ~ and the Mv .
 Condition 2 (Non-degeneracy: general setting) wj > 0 for all j âˆˆ [k], and Mv has rank k for
 all v âˆˆ [`].
 We remark that it is easy to generalize to the case where views have different dimensionality (e.g.,
~xv âˆˆ Rdv for possibly different dimensions dv ). For notational simplicity, we stick to the same
 dimension for each view. Moreover, Condition 2 can be relaxed in some cases; we discuss one such
 case in Section 4.1 in the context of Gaussian mixture models.
      Because the conditional distribution of ~xv is not specified beyond its conditional means, it is
 not possible to develop a maximum likelihood approach to parameter estimation. Instead, as in the
 document topic model, we develop a method of moments based on solving polynomial equations
 arising from eigenvalue problems.
 3.2. Observable moments and operators
 We focus on the moments concerning {~x1 , ~x2 , ~x3 }, but the same properties hold for other triples of
 the random vectors {~xa , ~xb , ~xc } âŠ† {~xv : v âˆˆ [`]} as well.
      As in (1), we define the matrix P1,2 âˆˆ RdÃ—d of second-order moments, and the tensor P1,2,3 âˆˆ
 RdÃ—dÃ—d of third-order moments, by
                        P1,2 := E[~x1 âŠ— ~x2 ] and         P1,2,3 := E[~x1 âŠ— ~x2 âŠ— ~x3 ].
 Again, P1,2,3 is regarded as the linear operator P1,2,3 : ~Î· 7â†’ E[(~x1 âŠ— ~x2 )h~Î· , ~x3 i].
      Lemma 4 and Lemma 5 are straightforward generalizations of Lemma 1 and Lemma 2.
 Lemma 4 P1,2 =M1 diag(w)M      ~ 2> and P1,2,3 (~Î· )=M1 diag(M3> ~Î· ) diag(w)M     ~ 2> for all ~Î· âˆˆ Rd .
 Lemma 5 Assume Condition 2. For v âˆˆ {1, 2, 3}, let Uv âˆˆ RdÃ—k be a matrix such that Uv> Mv is
 invertible. Then U1> P1,2 U2 is invertible, and for all ~Î· âˆˆ Rd , the â€œobservable operatorâ€ B1,2,3 (~Î· ) âˆˆ
 RkÃ—k , given by B1,2,3 (~Î· ) := (U1> P1,2,3 (~Î· )U2 )(U1> P1,2 U2 )âˆ’1 , satisfies
                               B1,2,3 (~Î· ) = (U1> M1 ) diag(M3> ~Î· )(U1> M1 )âˆ’1 .
 In particular, the k roots of the polynomial Î» 7â†’ det(B1,2,3 (~Î· ) âˆ’ Î»I) are {h~Î· , Âµ   ~ 3,j i : j âˆˆ [k]}.
      Recall that Algorithm A relates the eigenvectors of B(~Î· ) to the matrix of conditional means M .
 The eigenvectors are only defined up to a scaling of each vector, so without prior knowledge of the
 correct scaling, they are not sufficient to recover the parameters M . Nevertheless, the eigenvalues
 also carry information about the parameters, as shown in Lemma 5, and it is possible to reconstruct
 the parameters from different the observation operators applied to different vectors ~Î· . This idea is
 captured in the following lemma.
                                                       33.8

                          A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
 Lemma 6 Consider the setting and definitions from Lemma 5. Let Î˜ âˆˆ RkÃ—k be an invertible
 matrix, and let Î¸~i> âˆˆ Rk be its i-th row. Moreover, for all i âˆˆ [k], let Î»i,1 , Î»i,2 , . . . , Î»i,k denote the
 k eigenvalues of B1,2,3 (U3 Î¸~i ) in the order specified by the matrix of right eigenvectors U1> M1 . Let
 L âˆˆ RkÃ—k be the matrix whose (i, j)-th entry is Î»i,j . Then
                                                          Î˜U3> M3 = L.
       Observe that the unknown parameters M3 are expressed as the solution to a linear system in
 the above equation, where the elements of the right-hand side L are the roots of k-th degree poly-
 nomials derived from the second- and third-order observable moments (namely, the characteristic
 polynomials of the B1,2,3 (U3 Î¸~i ), âˆ€i âˆˆ [k]). This template is also found in other moment methods
 based on decompositions of a Hankel matrix. A crucial distinction, however, is that the k-th degree
 polynomials in Lemma 6 only involve low-order moments, whereas standard methods may involve
 up to â„¦(k)-th order moments which are difficult to estimate (Lindsay, 1989; Lindsay and Basak,
 1993; Gravin et al., 2012).
 3.3. Main result: general estimation procedure and sample complexity bound
 The lemmas in the previous section suggest the estimation procedure (Algorithm B) presented in
 Figure 2.
    Algorithm B
         1. Compute empirical averages from N independent copies of ~x1 âŠ— ~x2 to form PÌ‚1,2 âˆˆ RdÃ—d .
              Similarly do the same for ~x1 âŠ— ~x3 to form PÌ‚1,3 âˆˆ RkÃ—k , and for ~x1 âŠ— ~x2 âŠ— ~x3 to form PÌ‚1,2,3 âˆˆ
              RdÃ—dÃ—d .
         2. Let UÌ‚1 âˆˆ RdÃ—k and UÌ‚2 âˆˆ RdÃ—k be, respectively, matrices of orthonormal left and right singular
              vectors of PÌ‚1,2 corresponding to its top k singular values. Let UÌ‚3 âˆˆ RdÃ—k be the matrix of
              orthonormal right singular vectors of PÌ‚1,3 corresponding to its top k singular values.
         3. Pick an invertible matrix Î˜ âˆˆ RkÃ—k , with its i-th row denoted as Î¸~i> âˆˆ Rk . In the absence of any
              prior information about M3 , a suitable choice for Î˜ is a random rotation matrix.
              Form the matrix BÌ‚1,2,3 (UÌ‚3 Î¸~1 ) := (UÌ‚1> PÌ‚1,2,3 (UÌ‚3 Î¸~1 )UÌ‚2 )(UÌ‚1> PÌ‚1,2 UÌ‚2 )âˆ’1 .
              Compute RÌ‚1 âˆˆ RkÃ—k (with unit Euclidean norm columns) that diagonalizes BÌ‚1,2,3 (UÌ‚3 Î¸~1 ), i.e.,
              RÌ‚âˆ’1 BÌ‚1,2,3 (UÌ‚3 Î¸~1 )RÌ‚1 = diag(Î»Ì‚1,1 , Î»Ì‚1,2 , . . . , Î»Ì‚1,k ). (Fail if not possible.)
                 1
         4. For each i âˆˆ {2, . . . , k}, obtain the diagonal entries Î»Ì‚i,1 , Î»Ì‚i,2 , . . . , Î»Ì‚i,k of RÌ‚1âˆ’1 BÌ‚1,2,3 (UÌ‚3 Î¸~i )RÌ‚1 ,
              and form the matrix LÌ‚ âˆˆ RkÃ—k whose (i, j)-th entry is Î»Ì‚i,j .
         5. Return MÌ‚3 := UÌ‚3 Î˜âˆ’1 LÌ‚.
                         Figure 2: General method of moments estimator (Algorithm B).
       As stated, the Algorithm B yields an estimator for M3 , but the method can easily be applied to
 estimate Mv for all other views v. One caveat is that the estimators may not yield the same ordering
 of the columns, due to the unspecified order of the eigenvectors obtained in the third step of the
 method, and therefore some care is needed to obtain a consistent ordering. We outline one solution
 in Appendix B.4.
       The sample complexity of Algorithm B depends on the specific concentration properties of
~x1 , ~x2 , ~x3 . We abstract away this dependence in the following condition.
                                                                    33.9

                                             A NANDKUMAR H SU K AKADE
Condition 3 There exist positive scalars N0 , C1,2 , C1,3 , C1,2,3 , and a function f (N, Î´) (decreasing
in N and Î´) such that for any N â‰¥ N0 and Î´ âˆˆ (0, 1),
           h                                         i
   1. Pr kPÌ‚a,b âˆ’ Pa,b k2 â‰¤ Ca,b Â· f (N, Î´) â‰¥ 1 âˆ’ Î´ for {a, b} âˆˆ {{1, 2}, {1, 3}},
                         h                                                                  i
   2. âˆ€~v âˆˆ Rd , Pr kPÌ‚1,2,3 (~v ) âˆ’ P1,2,3 (~v )k2 â‰¤ C1,2,3 Â· k~v k2 Â· f (N, Î´) â‰¥ 1 âˆ’ Î´.
Moreover (for technical convenience), PÌ‚1,3 is independent of PÌ‚1,2,3 (which may be achieved, say, by
splitting a sample of size 2N ).
For the discrete models such as the document topic model of Section 2.1 and discrete HMMs (Mos-
sel and Roch, 2006; Hsu                     âˆš Condition 3 holds with N0 = C1,2 = C1,3 = C1,2,3 = 1,
                             p et al., 2009),
and f (N, Î´) = (1 + ln(1/Î´))/ N . Using standard techniques (e.g., Chaudhuri et al. (2009);
Vershynin (2012)), the condition can also be shown to hold for mixtures of various continuous
distributions such as multivariate Gaussians.
    Now we are ready to present the main theorem of this section.
Theorem 7 There exists a constant C > 0 such that the following holds. Assume the three-view
mixture model satisfies Condition 2 and Condition 3. Pick any  âˆˆ (0, 1) and Î´ âˆˆ (0, Î´0 ). Further,
assume Î˜ âˆˆ RkÃ—k is an independent random rotation matrix distributed uniformly over the Stiefel
manifold {Q âˆˆ RkÃ—k : Q> Q = I}. If the number of samples N satisfies N â‰¥ N0 and
                                mini6=j kM3 (~ei âˆ’ ~ej )k2 Â· Ïƒk (P1,2 )            Î´
        f (N, Î´/k) â‰¤ C Â·                           5            4
                                                                           Â·               Â· ,
                                         C1,2,3 Â· k Â· Îº(M1 )                 ln(k/Î´)
                                     (                                                                        )
                                        mini6=j kM3 (~ei âˆ’ ~ej )k2 Â· Ïƒk (P1,2 )2              Î´    Ïƒk (P1,3 )
           f (N, Î´) â‰¤ C Â· min                                                         Â·          ,              Â·
                                           C1,2 Â· kP1,2,3 k2 Â· k 5 Â· Îº(M1 )4             ln(k/Î´)     C1,3
where kP1,2,3 k2 := max~v6=~0 kP1,2,3 (~v )k2 , then with probability at least 1 âˆ’ 5Î´, Algorithm B returns
MÌ‚3 = [ÂµÌ‚3,1 |ÂµÌ‚3,2 | Â· Â· Â· |ÂµÌ‚3,k ] with the following guarantee: there exists a permutation Ï„ on [k] such
that for each j âˆˆ [k],
                                                 ~ 3,Ï„ (j) k2 â‰¤ max k~
                                        kÂµÌ‚3,j âˆ’ Âµ                        Âµ3,j 0 k2 Â· .
                                                                 j 0 âˆˆ[k]
4. Applications
In addition to the document clustering model from Section 2, a number of natural latent variable
models fit into this multi-view framework. We describe two such cases in this section: Gaussian
mixture models and HMMs, both of which have been (at least partially) studied in the literature. In
both cases, the estimation technique of Algorithm B leads to new learnability results that were not
achieved by previous works.
4.1. Multi-view and axis-aligned Gaussian mixture models
The standard Gaussian mixture model is parameterized by a mixing weight wj , mean vector Âµ                         ~j âˆˆ
RD , and covariance matrix Î£j âˆˆ RDÃ—D for each mixture component j âˆˆ [k]. The hidden dis-
crete random variable h selects a component j with probability Pr[h = j] = wj ; the conditional
                                                            33.10

                           A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
 distribution of the observed random vector ~x given h is a multivariate Gaussian with mean Âµ                              ~ h and
 covariance Î£h .
       The multi-view assumption for Gaussian mixture models asserts that for each component j, the
 covariance Î£j has a block diagonal structure Î£j = blkdiag(Î£1,j , Î£2,j , . . . , Î£`,j ) (a special case
 is an axis-aligned Gaussian). The various blocks correspond to the ` different views of the data
~x1 , ~x2 , . . . , ~x` âˆˆ Rd (for d = D/`), which are conditionally independent given h. The mean vector
 for each component j is similarly partitioned into the views as Âµ                ~ j = (~ Âµ1,j , Âµ                ~ `,j ). In the
                                                                                                   ~ 2,j , . . . , Âµ
 case of an axis-aligned Gaussian, each covariance matrix Î£j is diagonal, and therefore the original
 coordinates [D] can be partitioned into ` = O(D/k) views (each of dimension d = â„¦(k)) in any
 way (say, randomly).2
       Condition 2 requires that the conditional mean matrix Mv = [~                      Âµv,2 | Â· Â· Â· |~
                                                                                    Âµv,1 |~             Âµv,k ] for each view
 v have full column rank (see Appendix D.2 for a possible relaxation). This is similar to the non-
 degeneracy and spreading conditions used in previous studies of multi-view clustering (Chaudhuri
 and Rao, 2008; Chaudhuri et al., 2009). In these previous works, the multi-view and non-degeneracy
 assumptions are shown to reduce the minimum separation required for various efficient algorithms
 to learn the model parameters. In comparison, Algorithm B does not require a minimum separation
 condition at all.
       Condition 3 can be established for this class of mixture models (in fact, even when the com-
 ponent distributions are simply subgaussian; see Appendix D.3 for details). Therefore, Algorithm
 B can be used to recover the means of each component distribution (and the covariances can be
 recovered as well; see Appendix D.4).
 4.2. Hidden Markov models
 A hidden Markov model is a latent variable model in which a hidden state sequence h1 , h2 , . . . , h`
 forms a Markov chain h1 â†’ h2 â†’ Â· Â· Â· â†’ h` over k possible states [k]; and given the state ht
 at time t âˆˆ [k], the observation ~xt at time t (a random vector taking values in Rd ) is condition-
 ally independent of all other observations and states. The directed graphical model is depicted in
 Figure 3(b).
       The vector ~Ï€ âˆˆ âˆ†kâˆ’1 is the initial state distribution: Pr[h1 = i] = Ï€i for all i âˆˆ [k]. For
 simplicity, we only consider time-homogeneous HMMs, although it is possible to generalize to
 the time-varying setting. The matrix T âˆˆ RkÃ—k is a stochastic matrix describing the hidden state
 Markov chain: Pr[ht+1 = i|ht = j] = Ti,j for all i, j âˆˆ [k], t âˆˆ [` âˆ’ 1]. Finally, the columns of
 the matrix O = [~o1 |~o2 | Â· Â· Â· |~ok ] âˆˆ RdÃ—k are the conditional means of the observation ~xt at time t
 given the corresponding hidden state ht : E[~xt |ht = i] = O~ei = ~oi for all i âˆˆ [k], t âˆˆ [`]. Note that
 both discrete and continuous observations are readily handled in this framework. For instance, the
 conditional distribution of ~xt given ht = i (for i âˆˆ [k]) could be a high-dimensional multivariate
 Gaussian with mean ~oi âˆˆ Rd . Such models were not handled by previous methods (Mossel and
 Roch, 2006; Hsu et al., 2009).
       The restriction of the HMM to three time steps, say t âˆˆ {1, 2, 3}, is an instance of the three-view
 mixture model.
 Proposition 8 If the hidden variable h (from the three-view mixture model of Section 3.1) is iden-
 tified with the second hidden state h2 , then {~x1 , ~x2 , ~x3 } are conditionally independent given h,
   2. For product distributions (e.g., axis-aligned Gaussians) satisfying a certain incoherence condition, Condition 2 can
      be established using a random partitioning of the coordinates; see Appendix D.1 for details.
                                                           33.11

                                    A NANDKUMAR H SU K AKADE
and the parameters of the resulting three-view mixture model on (h, ~x1 , ~x2 , ~x3 ) are w ~ := T ~Ï€ ,
M1 := O diag(~Ï€ )T > diag(T ~Ï€ )âˆ’1 , M2 := O, and M3 := OT .
From Proposition 8, it is easy to verify that B3,1,2 (~Î· ) = (U3> OT ) diag(O> ~Î· )(U3> OT )âˆ’1 . There-
fore, after recovering the observation conditional mean matrix O using Algorithm B, the Markov
chain transition matrix can be recovered using the matrix of right eigenvectors R of B3,1,2 (~Î· ) and
the equation (U3> O)âˆ’1 R = T (up to scaling of the columns).
Acknowledgments
We thank Kamalika Chaudhuri and Tong Zhang for many useful discussions, Karl Stratos for com-
ments on an early draft, and David Sontag and an anonymous reviewer for some pointers to related
work.
References
D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In COLT, 2005.
R. Ahlswede and A. Winter. Strong converse for identification via quantum channels. IEEE Trans-
   actions on Information Theory, 48(3):569â€“579, 2002.
S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In STOC, 2001.
M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, 2010.
M. B. Blaschko and C. H. Lampert. Correlational spectral clustering. In CVPR, 2008.
D. L. Boley, F. T. Luk, and D. Vandevoorde. Vandermonde factorization of a Hankel matrix. In
   Scientific Computing, 1997.
S. C. Brubaker and S. Vempala. Isotropic PCA and affine-invariant clustering. In FOCS, 2008.
J. T. Chang. Full reconstruction of Markov models on evolutionary trees: Identifiability and consis-
   tency. Mathematical Biosciences, 137:51â€“73, 1996.
K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and inde-
   pendence. In COLT, 2008.
K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical
   correlation analysis. In ICML, 2009.
S. Dasgupta. Learning mixutres of Gaussians. In FOCS, 1999.
S. Dasgupta and A. Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Ran-
   dom Structures and Algorithms, 22(1):60â€“65, 2003.
S. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated, spherical
   Gaussians. Journal of Machine Learning Research, 8(Feb):203â€“226, 2007.
J. Feldman, R. Oâ€™Donnell, and R. Servedio. Learning mixtures of product distributions over discrete
   domains. In FOCS, 2005.
                                               33.12

                   A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
J. Feldman, R. Oâ€™Donnell, and R. Servedio. PAC learning mixtures of axis-aligned Gaussians with
   no separation assumption. In COLT, 2006.
A. M. Frieze, M. Jerrum, and R. Kannan. Learning linear transformations. In FOCS, 1996.
W. A. Gale, K. W. Church, and D. Yarowsky. One sense per discourse. In 4th DARPA Speech and
   Natural Language Workshop, 1992.
N. Gravin, J. Lasserre, D. Pasechnik, and S. Robins. The inverse moment problem for convex
   polytopes. Discrete and Computational Geometry, 2012. To appear.
H. Hotelling. The most predictable criterion. Journal of Educational Psychology, 26(2):139â€“142,
   1935.
D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. In
   COLT, 2009.
D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models.
   Journal of Computer and System Sciences, 2012. To appear.
A. HyvaÌˆrinen and E. Oja. Independent component analysis: algorithms and applications. Neural
   Networks, 13(4â€“5):411â€“430, 2000.
H. Jaeger. Observable operator models for discrete stochastic time series. Neural Computation, 12
   (6), 2000.
A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In STOC,
   2010.
R. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models. In
   COLT, 2005.
B. G. Lindsay. Moment matrices: applications in mixtures. Annals of Statistics, 17(2):722â€“740,
   1989.
B. G. Lindsay. Mixture models: theory, geometry and applications. American Statistical Associa-
   tion, 1995.
B. G. Lindsay and P. Basak. Multivariate normal mixtures: a fast consistent method. Journal of the
   American Statistical Association, 88(422):468â€“476, 1993.
F. McSherry. Spectral partitioning of random graphs. In FOCS, 2001.
A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In FOCS,
   2010.
E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models. Annals of
   Applied Probability, 16(2):583â€“614, 2006.
P. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU signa-
   tures. Journal of Cryptology, 22(2):139â€“160, 2009.
                                               33.13

                                           A NANDKUMAR H SU K AKADE
                                     h                          h1         h2     Â·Â·Â·        h`
                         ~x1     ~x2     Â·Â·Â·    ~x`            ~x1        ~x2               ~x`
                                     (a)                                    (b)
                Figure 3: (a) The multi-view mixture model. (b) A hidden Markov model.
K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of
   the Royal Society, London, A., page 71, 1894.
B. Recht. A simpler approach to matrix completion, 2009. arXiv:0910.0651v2.
R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM algorithm.
   SIAM Review, 26(2):195â€“239, 1984.
M. P. SchuÌˆtzenberger. On the definition of a family of automata. Information and Control, 4:
   245â€“270, 1961.
G. W. Stewart and Ji-Guang Sun. Matrix Perturbation Theory. Academic Press, 1990.
D. M. Titterington, A. F. M. Smith, and U. E. Makov. Statistical analysis of finite mixture distribu-
   tions. Wiley, 1985.
S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In FOCS,
   2002.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and
   G. Kutyniok, editors, Compressed Sensing, Theory and Applications, chapter 5, pages 210â€“268.
   Cambridge University Press, 2012.
Appendix A. Proofs and details from Section 2
In this section, we provide omitted proofs and discussion from Section 2 (deferring most pertur-
bation arguments to Appendix C), and also present some illustrative empirical results on text data
using a modified version of Algorithm A.
A.1. Proof of Lemma 1
Since ~x1 , ~x2 , and ~x3 are conditionally independent given h,
                                                  X k
      Pairsi,j = Pr[~x1 = ~ei , ~x2 = ~ej ] =          Pr[~x1 = ~ei , ~x2 = ~ej |h = t] Â· Pr[h = t]
                                                  t=1
                     X k                                                                  Xk
                  =        Pr[~x1 = ~ei |h = t] Â· Pr[~x2 = ~ej |h = t] Â· Pr[h = t] =           Mi,t Â· Mj,t Â· wt
                     t=1                                                                  t=1
                                                        33.14

                     A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
                             >
so Pairs = M diag(w)M   ~      . Moreover, writing ~Î· = (Î·1 , Î·2 , . . . , Î·d ),
                            Xd
         Triples(~Î· )i,j =       Î·x Pr[~x1 = ~ei , ~x2 = ~ej , ~x3 = ~ex ]
                            x=1
                            Xd X  k                                        Xk
                         =           Î·x Â· Mi,t Â· Mj,t Â· Mx,t Â· wt =            Mi,t Â· Mj,t Â· wt Â· (M > ~Î· )t
                            x=1 t=1                                        t=1
so Triples(~Î· ) = M diag(M > ~Î· ) diag(w)M  ~       >
                                                      .
A.2. Proof of Lemma 2
Since diag(w)~  0 by Condition 1 and U > PairsV = (U > M ) diag(w)M                   ~     >
                                                                                               V by Lemma 1, it
                >
follows that U PairsV is invertible by the assumptions on U and V . Moreover, also by Lemma 1,
           B(~Î· ) = (U > Triples(~Î· )V ) (U > PairsV )âˆ’1
                  = (U > M diag(M > ~Î· ) diag(w)M    ~    >
                                                            V ) (U > PairsV )âˆ’1
                  = (U > M ) diag(M > ~Î· )(U > M )âˆ’1 (U > M diag(w)M         ~    >
                                                                                    V ) (U > PairsV )âˆ’1
                  = (U > M ) diag(M > ~Î· )(U > M )âˆ’1 .
A.3. Accuracy of moment estimates
Lemma 9 Fix Î´ âˆˆ (0, 1). Let Pairs   [ be the empirical average of N independent copies of ~x1 âŠ— ~x2 ,
          \ be the empirical average of N independent copies of (~x1 âŠ— ~x2 )h~Î· , ~x3 i. Then
and let Triples
          "                               p             #
                                     1 +    ln(1/Î´)
             [ âˆ’ PairskF â‰¤
    1. Pr kPairs                           âˆš              â‰¥ 1 âˆ’ Î´, and
                                             N
          "                                                                 p            #
                                                                k~
                                                                 Î· k 2 (1 +    ln(1/Î´))
    2. Pr âˆ€~Î· âˆˆ Rd , kTriples(~
                          \ Î· ) âˆ’ Triples(~Î· )kF â‰¤                        âˆš                â‰¥ 1 âˆ’ Î´.
                                                                            N
Proof The first claim follows from applying Lemma 24 to the vectorizations of Pairs               [ and Pairs
(whereupon the Frobenius norm is the Euclidean norm of the vectorized matrices). For the second
claim, we also apply Lemma 24 to Triples \ and Triples in the same way to obtain, with probability
at least 1 âˆ’ Î´,
                       d X d X  d                                                p
                     X
                                     \ i,j,x âˆ’ Triplesi,j,x ) â‰¤       2    (1 +    ln(1/Î´))2
                                   (Triples                                                    .
                                                                                   N
                     i=1 j=1 x=1
                                                        33.15

                                             A NANDKUMAR H SU K AKADE
Now condition on this event. For any ~Î· = (Î·1 , Î·2 , . . . , Î·d ) âˆˆ Rd ,
                                                          d X   d X   d                                      2
                                                       X
               \ Î· ) âˆ’ Triples(~Î· )k2F =
            kTriples(~                                                          \ i,j,x âˆ’ Triplesi,j,x )
                                                                           Î·x (Triples
                                                       i=1 j=1 x=1
                                                       X  d X   d            Xd
                                                    â‰¤               k~Î· k22        \ i,j,x âˆ’ Triplesi,j,x )2
                                                                                 (Triples
                                                       i=1 j=1               x=1
                                                                       p
                                                        k~Î· k22 (1  +       ln(1/Î´))2
                                                    â‰¤
                                                                       N
where the first inequality follows by Cauchy-Schwarz.
A.4. Proof of Theorem 3
Let E1 be the event in which
                                                                              p
                                                                       1 + ln(1/Î´)
                                         kPairs âˆ’ Pairsk2 â‰¤
                                          [                                    âˆš                                            (2)
                                                                                 N
and                                                                                   p
                                                                         kvk2 (1 + ln(1/Î´))
                             kTriples(~v ) âˆ’ Triples(~v )k2 â‰¤
                                \                                                   âˆš                                       (3)
                                                                                      N
for all ~v âˆˆ Rd . By Lemma 9, a union bound, and the fact that kAk2 â‰¤ kAkF , we have Pr[E1 ] â‰¥
1 âˆ’ 2Î´. Now condition on E1 , and let E2 be the event in which
                                                                                            âˆš
                              ~                                  ~     >                       2Ïƒk (UÌ‚ > M ) Â· Î´
           Î³ := min |hUÌ‚ Î¸, M (~ei âˆ’ ~ej )i| = min |hÎ¸, UÌ‚ M (~ei âˆ’ ~ej )i| >                     âˆš k           .          (4)
                   i6=j                                 i6=j                                        ek 2
                                                                   âˆš
By Lemma 15 and the fact kUÌ‚ > M (~ei âˆ’ ~ej )k2 â‰¥ 2Ïƒk (UÌ‚ > M ), we have Pr[E2 |E1 ] â‰¥ 1 âˆ’ Î´, and
thus Pr[E1 âˆ© E2 ] â‰¥ (1 âˆ’ 2Î´)(1 âˆ’ Î´) â‰¥ 1 âˆ’ 3Î´. So henceforth condition on this joint event E1 âˆ© E2 .
                   kPairsâˆ’Pairsk
                     \             2              Îµ0                               Îµ0
    Let Îµ0 :=          Ïƒk (Pairs)    , Îµ1 :=     1âˆ’Îµ0 , and Îµ2 := (1âˆ’Îµ21 )Â·(1âˆ’Îµ0 âˆ’Îµ21 ) . The conditions on N and
                                                 1âˆš
                                                                                                                 p
the bound in (2) implies that Îµ0 <             1+ 2
                                                     â‰¤ 12 , so Lemma 10 implies that Ïƒk (UÌ‚ > M ) â‰¥ 1 âˆ’ Îµ21 Â·
                                 kM k2
Ïƒk (M ), Îº(UÌ‚ > M ) â‰¤ âˆš                      , and that UÌ‚ > PairsVÌ‚ is invertible. By Lemma 2,
                              1âˆ’Îµ21 Â·Ïƒk (M )
            BÌƒ(~Î· ) := (UÌ‚ > Triples(~Î· )VÌ‚ )(UÌ‚ > PairsVÌ‚ )âˆ’1 = (UÌ‚ > M ) diag(M > ~Î· )(UÌ‚ > M )âˆ’1 .
Thus, Lemma 11 implies
                                                     \ Î· ) âˆ’ Triples(~Î· )k2
                                                  kTriples(~                                    Îµ2
                       kBÌ‚(~Î· ) âˆ’ BÌƒ(~Î· )k2 â‰¤                                           +              .                    (5)
                                                       (1 âˆ’ Îµ0 ) Â· Ïƒk (Pairs)              Ïƒk (Pairs)
Let R := UÌ‚ > M diag(kUÌ‚ > M~e1 k2 , kUÌ‚ > M~e2 k2 , . . . , kUÌ‚ > M~ek k2 )âˆ’1 and Îµ3 := kBÌ‚(~Î·)âˆ’BÌƒ(~            Î³
                                                                                                                   Î· )k2 Â·Îº(R)
                                                                                                                               .
Note that R has                                                  âˆ’1                        >
                                   columns, and that R BÌƒ(~Î· )R = diag(M ~Î· ). By Lemma 14 and the
                âˆš unit norm âˆš
fact kM k2 â‰¤ kkM k1 = k,
                                                                                           âˆš
                          âˆ’1              >                    kM k2                          k
                      kR k2 â‰¤ Îº(UÌ‚ M ) â‰¤ p                        2
                                                                                  â‰¤p        2
                                                                                                                            (6)
                                                          1 âˆ’ Îµ1 Â· Ïƒk (M )             1 âˆ’ Îµ1 Â· Ïƒk (M )
                                                              33.16

                        A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
and
                                                                                 k
                                     Îº(R) â‰¤ Îº(UÌ‚ > M )2 â‰¤                                     .                                 (7)
                                                                   (1 âˆ’    Îµ21 ) Â· Ïƒk (M )2
The conditions on N and the bounds in (2), (3), (4), (5), and (7) imply that Îµ3 < 12 . By Lemma 12,
there exists a permutation Ï„ on [k] such that, for all j âˆˆ [k],
                       ksj Î¾Ë†j âˆ’ UÌ‚ > Âµ~ Ï„ (j) /c0j k2 = ksj Î¾Ë†j âˆ’ R~eÏ„ (j) k2 â‰¤ 4k Â· kRâˆ’1 k2 Â· Îµ3                              (8)
where sj := sign(hÎ¾Ë†j , UÌ‚ > Âµ   ~ Ï„ (j) i) and c0j := kUÌ‚ > Âµ ~ Ï„ (j) k2 â‰¤ k~   ÂµÏ„ (j) k2 (the eigenvectors Î¾Ë†j are unique
up to sign sj because each eigenvalue has geometric multiplicity 1). Since Âµ                               ~ Ï„ (j) âˆˆ range(U ),
Lemma 10 and the bounds in (8) and (6) imply
                                                      q
                   ksj UÌ‚ Î¾Ë†j âˆ’ Âµ~ Ï„ (j) /c0j k2 â‰¤ ksj Î¾Ë†j âˆ’ UÌ‚ > Âµ     ~ Ï„ (j) /c0j k22 + k~ÂµÏ„ (j) /c0j k22 Â· Îµ21
                                                 â‰¤ ksj Î¾Ë†j âˆ’ UÌ‚ > Âµ ~ Ï„ (j) /c0j k2 + k~  ÂµÏ„ (j) /c0j k2 Â· Îµ1
                                                 â‰¤ 4k Â· kRâˆ’1 k2 Â· Îµ3 + Îµ1
                                                                    âˆš
                                                                       k
                                                 â‰¤ 4k Â· p                             Â· Îµ3 + Îµ1 .
                                                             1 âˆ’ Îµ21 Â· Ïƒk (M )
Therefore, for cj := sj c0j h~1, UÌ‚ Î¾Ë†j i, we have
                                                                                               âˆš                           
                                                                                                   k
     kcj ÂµÌ‚j âˆ’ Âµ~ Ï„ (j) k2 =   kc0j sj UÌ‚ Î¾Ë†j âˆ’Âµ               ÂµÏ„ (j) k2 Â· 4k Â· p
                                                 ~ Ï„ (j) k2 â‰¤ k~                                                 Â· Îµ3 + Îµ1 .
                                                                                          1 âˆ’ Îµ21 Â· Ïƒk (M )
Making all of the substitutions into the above bound gives
                                                                                                âˆš
                                                                                                   ek Â· k2
                                                                                                             
   kcj ÂµÌ‚j âˆ’ Âµ~ Ï„ (j) k2               4k 1.5                       k
                         â‰¤p                              Â·                           Â·p
        k~
         ÂµÏ„ (j) k2              1 âˆ’ Îµ21 Â· Ïƒk (M ) (1 âˆ’ Îµ21 ) Â· Ïƒk (M )2                   2(1 âˆ’ Îµ21 ) Â· Ïƒk (M ) Â· Î´
                                                                                                                              
                                    \ Î· ) âˆ’ Triples(~Î· )k2
                                 kTriples(~                                                 [ âˆ’ Pairsk2
                                                                                           kPairs
                             Â·                                           +
                                       (1 âˆ’ Îµ0 ) Â· Ïƒk (Pairs)                (1 âˆ’ Îµ21 ) Â· (1 âˆ’ Îµ0 âˆ’ Îµ21 ) Â· Ïƒk (Pairs)2
                                     [ âˆ’ Pairsk2
                                  kPairs
                             +
                                (1 âˆ’ Îµ0 ) Â· Ïƒk (Pairs)
                                                                      r
                                                   k5                     ln(1/Î´)
                         â‰¤CÂ·                  4              2
                                                                    Â·                  .
                                  Ïƒk (M ) Â· Ïƒk (Pairs) Â· Î´                    N
A.5. Some illustrative empirical results
As a demonstration of feasibility, we applied a modified version of Algorithm A to a subset of arti-
cles from the â€œ20 Newsgroupsâ€ dataset, specifically those in comp.graphics, rec.sport.baseball,
sci.crypt, and soc.religion.christian, where ~x1 , ~x2 , ~x3 represent three words from
the beginning (first third), middle (middle third), and end (last third) of an article. We used k = 25
(although results were similar for k âˆˆ {10, 15, 20, 25, 30}) and d = 5441 (after removing a standard
set of 524 stop-words and applying Porter stemming). Instead of using a single ~Î· and extracting all
eigenvectors of BÌ‚(~Î· ), we extracted a single eigenvector Î¾~x from BÌ‚(~ex ) for several words x âˆˆ [d]
(these xâ€™s were chosen using an automatic heuristic based on their statistical leverage scores in
 [ Below, for each such (BÌ‚(~ex ), Î¾~x ), we report the top 15 words y ordered by ~e>
Pairs).                                                                                                                 ~
                                                                                                                   y UÌ‚ Î¾x value.
                                                             33.17

                                          A NANDKUMAR H SU K AKADE
                BÌ‚(~eformat )  BÌ‚(~egod )       BÌ‚(~ekey )     BÌ‚(~epolygon )   BÌ‚(~eteam )   BÌ‚(~etoday )
                   source         god             key            polygon            win          game
                     find        write             bit             time           game            tiger
                     post        jesus            chip             save             run             bit
                    image      christian        system             refer           team            run
                      feal       christ         encrypt            book            year          pitch
                  intersect     people             car            source            don            day
                    email        time        repository            man            watch          team
                       rpi        apr              ve            routine           good           true
                     time         sin            public          netcom           score             lot
                  problem        bible          escrow              gif          yankees         book
                      file        day            secure           record           pitch           lost
                  program       church            make          subscribe          start      colorado
                       gif      person          clipper          change             bit            fan
                       bit       book             write         algorithm          time            apr
                     jpeg         life             nsa             scott         wonder          watch
The first and fourth topics appear to be about computer graphics (comp.graphics), the fifth and
sixth about baseball (rec.sports.baseball), the third about encryption (sci.crypt), and
the second about Christianity (soc.religion.christian).
    We also remark that Algorithm A can be implemented so that it makes just two passes over the
training data, and that simple hashing or random projection tricks can reduce the memory require-
ment to O(k 2 + kd) (i.e., Pairs
                               [ and Triples \ never need to be explicitly formed).
Appendix B. Proofs and details from Section 3
In this section, we provide omitted proofs and discussion from Section 3.
B.1. Proof of Lemma 4
By conditional independence,
 P1,2 = E[E[~x1 âŠ— ~x2 |h]] = E[E[~x1 |h] âŠ— E[~x2 |h]]
                                                                      X  k              
                              = E[(M1~eh ) âŠ— (M2~eh )] = M1                   wt~et âŠ— ~et M2> = M1 diag(w)M  ~ 2> .
                                                                         t=1
Similarly,
         P1,2,3 (~Î· ) = E[E[(~x1 âŠ— ~x2 )h~Î· , ~x3 i|h]] = E[E[~x1 |h] âŠ— E[~x2 |h]h~Î· , E[~x3 |h]i]
                                                                         X   k                            
                       = E[(M1~eh ) âŠ— (M2~eh )h~Î· , M3~eh i] = M1               wt~eh âŠ— ~eh h~Î· , M3~eh i M2>
                                                                            t=1
                                      >                    >
                       = M1 diag(M3 ~Î· ) diag(w)M   ~ 2.
                                                           33.18

                      A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
B.2. Proof of Lemma 5
We have U1> P1,2 U2 = (U1> M1 ) diag(w)(M     ~       >
                                                      2 U2 ) by Lemma 4, which is invertible by the assump-
tions on Uv and Condition 2. Moreover, also by Lemma 4,
        B1,2,3 (~Î· ) = (U1> P1,2,3 (~Î· )U2 ) (U1> P1,2 U2 )âˆ’1
                     = (U1> M1 diag(M3> ~Î· ) diag(w)M   ~ 2> U2 ) (U1> P1,2 U2 )âˆ’1
                     = (U1> M1 ) diag(M3> ~Î· )(U1> M1 )âˆ’1 (U1> M1 diag(w)M           ~ 2> U2 ) (U1> P1,2 U2 )âˆ’1
                     = (U1> M1 ) diag(M3> ~Î· )(U1> M1 )âˆ’1 (U1> P1,2 U2 ) (U1> P1,2 U2 )âˆ’1
                     = (U1> M1 ) diag(M3> ~Î· )(U1> M1 )âˆ’1 .
B.3. Proof of Lemma 6
By Lemma 5,
    (U1> M1 )âˆ’1 B1,2,3 (U3 Î¸~i )(U1> M1 ) = diag(M3> U3 Î¸~i )
                                              = diag(hÎ¸~i , U > M3~e1 i, hÎ¸~i , U > M3~e2 i, . . . hÎ¸~i , U > M3~ek i)
                                                              3                      3                     3
                                              = diag(Î»i,1 , Î»i,2 , . . . , Î»i,k )
for all i âˆˆ [k], and therefore
                     ï£® ~
                       hÎ¸1 , U3> M3~e1 i hÎ¸~1 , U3> M3~e2 i Â· Â· Â·      hÎ¸~1 , U3> M3~ek i
                                                                                          ï£¹
                     ï£¯hÎ¸~2 , U > M3~e1 i hÎ¸~2 , U > M3~e2 i Â· Â· Â·      hÎ¸~2 , U3> M3~e3 i ï£º
                              3                   3
               L=ï£¯                                                                        ï£º = Î˜U3> M3 .
                     ï£¯                                                                    ï£º
                               ..                  ..           ..                ..
                     ï£°          .                   .              .               .      ï£»
                      hÎ¸~k , U3 M3~e1 i hÎ¸~k , U3 M3~e2 i Â· Â· Â·
                               >                  >
                                                                       hÎ¸~k , U M3~ek i
                                                                                >
                                                                                3
B.4. Ordering issues
Although Algorithm B only explicitly yields estimates for M3 , it can easily be applied to estimate
Mv for all other views v. The main caveat is that the estimators may not yield the same ordering
of the columns, due to the unspecified order of the eigenvectors obtained in the third step of the
method, and therefore some care is needed to obtain a consistent ordering. However, this ordering
issue can be handled by exploiting consistency across the multiple views.
    The first step is to perform the estimation of M3 using Algorithm B as is. Then, to estimate M2 ,
one may re-use the eigenvectors in RÌ‚1 to diagonalize BÌ‚1,3,2 (~Î· ), as B1,2,3 (~Î· ) and B1,3,2 (~Î· ) share the
same eigenvectors. The same goes for estimating Mv for other all other views v except v = 1.
    It remains to provide a way to estimate M1 . Observe that M2 can be estimated in at least two
ways: via the operators BÌ‚1,3,2 (~Î· ), or via the operators BÌ‚3,1,2 (~Î· ). This is because the eigenvalues
of B3,1,2 (~Î· ) and B1,3,2 (~Î· ) are the identical. Because the eigenvalues are also sufficiently separated
from each other, the eigenvectors RÌ‚3 of BÌ‚3,1,2 (~Î· ) can be put in the same order as the eigenvectors
RÌ‚1 of BÌ‚1,3,2 by (approximately) matching up their respective corresponding eigenvalues. Finally,
the appropriately re-ordered eigenvectors RÌ‚3 can then be used to diagonalize BÌ‚3,2,1 (~Î· ) to estimate
M1 .
                                                         33.19

                                          A NANDKUMAR H SU K AKADE
B.5. Estimating the mixing weights
Given the estimate of MÌ‚3 , one can obtain an estimate of w           ~ using
                                                  wÌ‚ := MÌ‚3â€  EÌ‚[~x3 ]
where Aâ€  denotes the Moore-Penrose pseudoinverse of A (though other generalized inverses may
work as well), and EÌ‚[~x3 ] is the empirical average of ~x3 . This estimator is based on the following
observation:
                                 E[~x3 ] = E[E[~x3 |h]] = M3 E[~eh ] = M3 w          ~
and therefore
                                            M3â€  E[~x3 ] = M3â€  M3 w   ~ =w ~
since M3 has full column rank.
B.6. Proof of Theorem 7
The proof is similar to that of Theorem 3, so we just describe the essential differences. As before,
most perturbation arguments are deferred to Appendix C.
    First, let E1 be the event in which
                                       kPÌ‚1,2 âˆ’ P1,2 k2 â‰¤ C1,2 Â· f (N, Î´),
                                       kPÌ‚1,3 âˆ’ P1,3 k2 â‰¤ C1,3 Â· f (N, Î´)
and
                          kPÌ‚1,2,3 (UÌ‚3 Î¸~i ) âˆ’ P1,2,3 (UÌ‚3 Î¸~i )k2 â‰¤ C1,2,3 Â· f (N, Î´/k)
for all i âˆˆ [k]. Therefore by Condition 3 and a union bound, we have Pr[E1 ] â‰¥ 1 âˆ’ 3Î´. Second, let
E2 be the event in which
                                                                   minj6=j 0 kUÌ‚3> M3 (~ej âˆ’ ~ej 0 )k2 Â· Î´
               Î³ := min min0 |hÎ¸~i , UÌ‚3> M3 (~ej âˆ’ ~ej 0 )i| >                 âˆš k
                    iâˆˆ[k] j6=j                                                     ek 2 k
and
                                                        maxjâˆˆ[k] kM3~ej k2                             
              Î»max := max |hÎ¸~i , UÌ‚3> M3~ej i| â‰¤
                                                                                         p
                                                                   âˆš              1 + 2 ln(k 2 /Î´) .
                        i,jâˆˆ[k]                                      k
Since each Î¸~i is distributed uniformly over S kâˆ’1 , it follows from Lemma 15 and a union bound that
Pr[E2 |E1 ] â‰¥ 1 âˆ’ 2Î´. Therefore Pr[E1 âˆ© E2 ] â‰¥ (1 âˆ’ 3Î´)(1 âˆ’ 2Î´) â‰¥ 1 âˆ’ 5Î´.
    Let U3 âˆˆ RdÃ—k be the matrix of top k orthonormal left singular vectors of M3 . By Lemma 10
and the conditions on N , we have Ïƒk (UÌ‚3> U3 ) â‰¥ 1/2, and therefore
                                                                       âˆš 3           p
             mini6=i0 kM3 (~ei âˆ’ ~ei0 )k2 Â· Î´              Î»max          ek (1 + 2 ln(k 2 /Î´)) 0
        Î³â‰¥              âˆš                          and              â‰¤                                  Â· Îº (M3 )
                       2 ek k2 k
                                
                                                              Î³                        Î´
where
                                                       maxiâˆˆ[m] kM3~ei k2
                                    Îº0 (M3 ) :=                                    .
                                                   mini6=i0 kM3 (~ei âˆ’ ~ei0 )k2
                                                         33.20

                          A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
     Let ~Î·i := UÌ‚3 Î¸~i for i âˆˆ [k]. By Lemma 10, UÌ‚1> P1,2 UÌ‚2 is invertible, so we may define BÌƒ1,2,3 (~Î·i ) :=
(UÌ‚1 P1,2,3 (~Î·i )UÌ‚2 )(UÌ‚1> P1,2 UÌ‚2 )âˆ’1 . By Lemma 5,
    >
                                       BÌƒ1,2,3 (~Î·i ) = (UÌ‚1> M1 ) diag(M3> ~Î·i )(UÌ‚1> M1 )âˆ’1 .
Also define R := UÌ‚1> M1 diag(kUÌ‚1> M1~e1 k2 , kUÌ‚1> M1~e2 k2 , . . . , kUÌ‚1> M1~ek k2 )âˆ’1 . Using most of the
same arguments in the proof of Theorem 3, we have
                                 kRâˆ’1 k2 â‰¤ 2Îº(M1 ),                                                                              (9)
                                                             2
                                       Îº(R) â‰¤ 4Îº(M1 ) ,                                                                         (10)
                                                    2kPÌ‚1,2,3 (~Î·i ) âˆ’ P1,2,3 (~Î·i )k2 2kP1,2,3 k2 Â· kPÌ‚1,2 âˆ’ P1,2 k2
      kBÌ‚1,2,3 (~Î·i ) âˆ’ BÌƒ1,2,3 (~Î·i )k2 â‰¤                                              +                                     .
                                                                Ïƒk (P1,2 )                             Ïƒk (P1,2 )2
By Lemma 12, the operator BÌ‚1,2,3 (~Î·1 ) has k distinct eigenvalues, and hence its matrix of right
eigenvectors RÌ‚1 is unique up to column scaling and ordering. This in turn implies that RÌ‚1âˆ’1 is unique
                                                                                                           âˆ’1
up to row scaling and ordering. Therefore, for each i âˆˆ [k], the Î»Ì‚i,j = ~e>                          j RÌ‚1 BÌ‚1,2,3 (~Î·i )RÌ‚1~ej for
j âˆˆ [k] are uniquely defined up to ordering. Moreover, by Lemma 13 and the above bounds on
kBÌ‚1,2,3 (~Î·i ) âˆ’ BÌƒ1,2,3 (~Î·i )k2 and Î³, there exists a permutation Ï„ on [k] such that, for all i, j âˆˆ [k],
                                                                                        
   |Î»Ì‚i,j âˆ’ Î»i,Ï„ (j) | â‰¤ 3Îº(R) + 16k 1.5 Â· Îº(R) Â· kRâˆ’1 k22 Â· Î»max /Î³ Â· kBÌ‚1,2,3 (~Î·i ) âˆ’ BÌƒ1,2,3 (~Î·i )k2
                                                                                      
                         â‰¤ 12Îº(M1 )2 + 256k 1.5 Â· Îº(M1 )4 Â· Î»max /Î³ Â· kBÌ‚1,2,3 (~Î·i ) âˆ’ BÌƒ1,2,3 (~Î·i )k2 (11)
where the second inequality uses (9) and (10). Let Î½Ì‚j := (Î»Ì‚1,j , Î»Ì‚2,j , . . . , Î»Ì‚k,j ) âˆˆ Rk and ~Î½j :=
(Î»1,j , Î»2,j , . . . , Î»k,j ) âˆˆ Rk . Observe                                   >
                                                      âˆš that ~Î½j = Î˜UÌ‚3 kM3~ej = Î˜UÌ‚3 Âµ
                                                                                                 >
                                                                                                   ~ 3,j by Lemma 6. By the
orthogonality of Î˜, the fact k~v k2 â‰¤ kk~v kâˆ for ~v âˆˆ R , and (11)
kÎ˜âˆ’1 Î½Ì‚j âˆ’ UÌ‚3> Âµ    ~ 3,Ï„ (j) k2 = kÎ˜âˆ’1 (Î½Ì‚j âˆ’ ~Î½Ï„ (j) )k2
                                   = kÎ½Ì‚j âˆ’ ~Î½Ï„ (j) k2
                                        âˆš
                                   â‰¤ k Â· kÎ½Ì‚j âˆ’ ~Î½Ï„ (j) kâˆ
                                        âˆš
                                   = k Â· max |Î»Ì‚i,j âˆ’ Î»i,Ï„ (j) |
                                                  i
                                         âˆš                                                         
                                   â‰¤ 12 k Â· Îº(M1 )2 + 256k 2 Â· Îº(M1 )4 Â· Î»max /Î³ Â· kBÌ‚1,2,3 (~Î·i ) âˆ’ BÌƒ1,2,3 (~Î·i )k2 .
Finally, by Lemma 10 (as applied to PÌ‚1,3 and P1,3 ),
                                                                                                   kPÌ‚1,3 âˆ’ P1,3 k2
               kÂµÌ‚3,j âˆ’ Âµ   ~ 3,Ï„ (j) k2 â‰¤ kÎ˜âˆ’1 Î½Ì‚j âˆ’ UÌ‚3> Âµ       ~ 3,Ï„ (j) k2 + 2k~Âµ3,Ï„ (j) k2 Â·                    .
                                                                                                       Ïƒk (P1,3 )
Making all of the substitutions into the above bound gives
                                                                                       
                                   C 5                    4    0           ln(k/Î´)       C1,2,3 Â· f (N, Î´/k)
    kÂµÌ‚3,j  âˆ’Âµ  ~ 3,Ï„ (j) k2 â‰¤           Â· k Â· Îº(M1 ) Â· Îº (M3 ) Â·                    Â·
                                    6                                           Î´              Ïƒk (P1,2 )
                                                                                    
                                                  kP1,2,3 k2 Â· C1,2 Â· f (N/Î´)             C                    C1,3 Â· f (N, Î´)
                                              +                          2
                                                                                       + Â· k~    Âµ3,Ï„ (j) k2 Â·
                                                            Ïƒk (P1,2 )                    6                       Ïƒk (P1,3 )
                                   1                                         
                               â‰¤          max k~  Âµ3,j 0 k2 + k~ Âµ3,Ï„ (j) k2 Â· 
                                   2 j 0 âˆˆ[k]
                               â‰¤ max k~      Âµ3,j 0 k2 Â· .
                                   j 0 âˆˆ[k]
                                                                    33.21

                                          A NANDKUMAR H SU K AKADE
Appendix C. Perturbation analysis for observable operators
The following lemma establishes the accuracy of approximating the fundamental subspaces (i.e.,
the row and column spaces) of a matrix X by computing the singular value decomposition of a
perturbation XÌ‚ of X.
Lemma 10 Let X âˆˆ RmÃ—n be a matrix of rank k. Let U âˆˆ RmÃ—k and V âˆˆ RnÃ—k be matrices
with orthonormal columns such that range(U ) and range(V ) are spanned by, respectively, the left
and right singular vectors of X corresponding to its k largest singular values. Similarly define
UÌ‚ âˆˆ RmÃ—k and VÌ‚ âˆˆ RnÃ—k relative to a matrix XÌ‚ âˆˆ RmÃ—n . Define X := kXÌ‚ âˆ’ Xk2 , Îµ0 := Ïƒk(X)                      X
                                                                                                                       ,
               Îµ0                      1
and Îµ1 := 1âˆ’Îµ0 . Assume Îµ0 < 2 . Then
    1. Îµ1 < 1;
    2. Ïƒk (XÌ‚) = Ïƒk (UÌ‚ > XÌ‚ VÌ‚ ) â‰¥ (1 âˆ’ Îµ0 ) Â· Ïƒk (X) > 0;
                        p
    3. Ïƒk (UÌ‚ > U ) â‰¥ 1 âˆ’ Îµ21 ;
                        p
    4. Ïƒk (VÌ‚ > V ) â‰¥ 1 âˆ’ Îµ21 ;
    5. Ïƒk (UÌ‚ > X VÌ‚ ) â‰¥ (1 âˆ’ Îµ21 ) Â· Ïƒk (X);
    6. for any Î±Ì‚ âˆˆ Rk and ~v âˆˆ range(U ), kUÌ‚ Î±Ì‚ âˆ’ ~v k22 â‰¤ kÎ±Ì‚ âˆ’ UÌ‚ >~v k22 + k~v k22 Â· Îµ21 .
Proof The first claim follows from the assumption on Îµ0 . The second claim follows from the
assumptions and Weylâ€™s theorem (Lemma 20). Let the columns of UÌ‚âŠ¥ âˆˆ RmÃ—(mâˆ’k) be an or-
thonormal basis for the orthogonal complement of range(UÌ‚ ), so that kUÌ‚âŠ¥> U k2 â‰¤ X /Ïƒk (XÌ‚) â‰¤ Îµ1
by Wedinâ€™s theorem (Lemma 21). The third claim then follows because kUÌ‚ > U k22 = 1 âˆ’ kUÌ‚âŠ¥> U k22 â‰¥
1 âˆ’ Îµ21 . The fourth claim is analogous to the third claim, and the fifth claim follows from the
third and fourth. The sixth claim follows writing ~v = U Î±               ~ for some Î±   ~ âˆˆ Rk , and using the de-
composition kUÌ‚ Î±Ì‚ âˆ’ ~v k2 = kUÌ‚ Î±Ì‚ âˆ’ UÌ‚ UÌ‚ ~v k2 + kUÌ‚âŠ¥ UÌ‚âŠ¥ ~v k2 = kÎ±Ì‚ âˆ’ UÌ‚ >~v k22 + kUÌ‚âŠ¥> (U Î±
                               2                    >    2            >     2                                  ~ )k22 â‰¤
kÎ±Ì‚ âˆ’ UÌ‚ >~v k22 + kUÌ‚âŠ¥> U k22 k~ Î±k22 â‰¤ kÎ±Ì‚ âˆ’ UÌ‚ >~v k22 + k~   Î±k22 Â· Îµ21 = kÎ±Ì‚ âˆ’ UÌ‚ > U Î±~ k22 + k~v k22 Â· Îµ21 where
the last inequality follows from the argument for the third claim, and the last equality uses the or-
thonormality of the columns of U .
     The next lemma bounds the error of the observation operator in terms of the errors in estimating
the second-order and third-order moments.
Lemma 11 Consider the setting and definitions from Lemma 10, and let Y âˆˆ RmÃ—n and YÌ‚ âˆˆ
                                              Îµ0
RmÃ—n be given. Define Îµ2 := (1âˆ’Îµ2 )Â·(1âˆ’Îµ            âˆ’Îµ2 )
                                                           and Y := kYÌ‚ âˆ’ Y k2 . Assume Îµ0 < 1+1âˆš2 . Then
                                          1       0   1
                                                                                                           Îµ2
    1. UÌ‚ > XÌ‚ VÌ‚ and UÌ‚ > X VÌ‚ are both invertible, and k(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 âˆ’ (UÌ‚ > X VÌ‚ )âˆ’1 k2 â‰¤         Ïƒk (X) ;
                                                                               Y          kY k2 Â·Îµ2
    2. k(UÌ‚ > YÌ‚ VÌ‚ )(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 âˆ’ (UÌ‚ > Y VÌ‚ )(UÌ‚ > X VÌ‚ )âˆ’1 k2 â‰¤    (1âˆ’Îµ0 )Â·Ïƒk (X) +  Ïƒk (X) .
                                                           33.22

                        A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
Proof Let SÌ‚ := UÌ‚ > XÌ‚ VÌ‚ and SÌƒ := UÌ‚ > X VÌ‚ . By Lemma 10, UÌ‚ > XÌ‚ VÌ‚ is invertible, Ïƒk (SÌƒ) â‰¥
Ïƒk (UÌ‚ > U ) Â· Ïƒk (X) Â· Ïƒk (VÌ‚ > V ) â‰¥ (1 âˆ’ Îµ21 ) Â· Ïƒk (X) (so SÌƒ is also invertible), and kSÌ‚ âˆ’ SÌƒk2 â‰¤
                      Îµ0                                                            Îµ0
Îµ0 Â· Ïƒk (X) â‰¤ 1âˆ’Îµ        2 Â· Ïƒk (SÌƒ). The assumption on Îµ0 implies 1âˆ’Îµ2 < 1; therefore the Lemma 23
                         1                                                             1
                                    kSÌ‚âˆ’SÌƒk2 /Ïƒk (SÌƒ)                      Îµ2
implies   kSÌ‚ âˆ’1   âˆ’  SÌƒ âˆ’1 k 2 â‰¤                      Â·   1
                                                         Ïƒk (SÌƒ)
                                                                  â‰¤     Ïƒk (X) ,  which proves the first claim. For the
                                   1âˆ’kSÌ‚âˆ’SÌƒk2 /Ïƒk (SÌƒ)
second claim, observe that
      k(UÌ‚ > YÌ‚ VÌ‚ )(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 âˆ’ (UÌ‚ > Y VÌ‚ )(UÌ‚ > X VÌ‚ )âˆ’1 k2
       â‰¤ k(UÌ‚ > YÌ‚ VÌ‚ )(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 âˆ’ (UÌ‚ > Y VÌ‚ )(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 k2
         + k(UÌ‚ > Y VÌ‚ )(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 âˆ’ (UÌ‚ > Y VÌ‚ )(UÌ‚ > X VÌ‚ )âˆ’1 k2
       â‰¤ kUÌ‚ > YÌ‚ VÌ‚ âˆ’ UÌ‚ > Y VÌ‚ k2 Â· k(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 k2 + kUÌ‚ > Y VÌ‚ k2 Â· k(UÌ‚ > XÌ‚ VÌ‚ )âˆ’1 âˆ’ (UÌ‚ > X VÌ‚ )âˆ’1 k2
                    Y               kY k2 Â· Îµ2
       â‰¤                          +
          (1 âˆ’ Îµ0 ) Â· Ïƒk (X)          Ïƒk (X)
where the first inequality follows from the triangle inequality, the second follows from the sub-
multiplicative property of the spectral norm, and the last follows from Lemma 10 and the first
claim.
     The following lemma establishes standard eigenvalue and eigenvector perturbation bounds.
Lemma 12 Let A âˆˆ RkÃ—k be a diagonalizable matrix with k distinct real eigenvalues Î»1 , Î»2 , . . . , Î»k âˆˆ
R corresponding to the (right) eigenvectors Î¾~1 , Î¾~2 , . . . , Î¾~k âˆˆ Rk all normalized to have kÎ¾~i k2 = 1.
Let R âˆˆ RkÃ—k be the matrix whose i-th column is Î¾~i . Let AÌ‚ âˆˆ RkÃ—k be a matrix. Define
A := kAÌ‚ âˆ’ Ak2 , Î³A := mini6=j |Î»i âˆ’ Î»j |, and Îµ3 := Îº(R)Â·                         A                   1
                                                                                  Î³A . Assume Îµ3 < 2 . Then there
exists a permutation Ï„ on [k] such that the following holds:
    1. AÌ‚ has k distinct real eigenvalues Î»Ì‚1 , Î»Ì‚2 , . . . , Î»Ì‚k âˆˆ R, and |Î»Ì‚Ï„ (i) âˆ’Î»i | â‰¤ Îµ3 Â·Î³A for all i âˆˆ [k];
    2. AÌ‚ has corresponding (right) eigenvectors Î¾Ë†1 , Î¾Ë†2 , . . . , Î¾Ë†k âˆˆ Rk , normalized to have kÎ¾Ë†i k2 = 1,
        which satisfy kÎ¾Ë†Ï„ (i) âˆ’ Î¾~i k2 â‰¤ 4(k âˆ’ 1) Â· kRâˆ’1 k2 Â· Îµ3 for all i âˆˆ [k];
    3. the matrix RÌ‚ âˆˆ RkÃ—k whose i-th column is Î¾Ë†Ï„ (i) satisfies kRÌ‚âˆ’Rk2 â‰¤ kRÌ‚âˆ’RkF â‰¤ 4k 1/2 (k âˆ’
        1) Â· kRâˆ’1 k2 Â· Îµ3 .
Proof The Bauer-Fike theorem (Lemma 22) implies that for every eigenvalue Î»Ì‚i of AÌ‚, there exists
an eigenvalue Î»j of A such that |Î»Ì‚i âˆ’ Î»j | â‰¤ kRâˆ’1 (AÌ‚ âˆ’ A)Rk2 â‰¤ Îµ3 Â· Î³A . Therefore, the assumption
on Îµ3 implies that there exists a permutation Ï„ such that |Î»Ì‚Ï„ (i) âˆ’ Î»i | â‰¤ Îµ3 Â· Î³A < Î³2A . In particular,
                           h       Î³A          Î³A i
                             Î»i âˆ’     , Î»i +          âˆ© {Î»Ì‚1 , Î»Ì‚2 , . . . , Î»Ì‚k } = 1,  âˆ€i âˆˆ [k].                 (12)
                                    2           2
Since AÌ‚ is real, all non-real eigenvalues of AÌ‚ must come in conjugate pairs; so the existence of a
non-real eigenvalue of AÌ‚ would contradict (12). This proves the first claim.
     For the second claim, assume for notational simplicity that the permutation Ï„ is the identity
permutation. Let RÌ‚ âˆˆ RkÃ—k be the matrix whose i-th column is Î¾Ë†i . Define Î¶~i> âˆˆ Rk to be the
i-th row of Râˆ’1 (i.e., the i-th left eigenvector of A), and similarly define Î¶Ì‚i> âˆˆ Rk to be the i-th
                                                           33.23

                                                 A NANDKUMAR H SU K AKADE
row of     RÌ‚âˆ’1 . Fix a particular i âˆˆ [k]. Since {Î¾~1 , Î¾~2 , . . . , Î¾~k } forms a basis for Rk , we can write
Î¾Ë†i = kj=1 ci,j Î¾~j for some coefficients ci,1 , ci,2 , . . . , ci,k âˆˆ R. We may assume ci,i â‰¥ 0 (or else we
         P
replace Î¾Ë†i with âˆ’Î¾Ë†i ). The fact that kÎ¾Ë†i k2 = kÎ¾~j k2 = 1 for all j âˆˆ [k] and the triangle inequality
imply 1 = kÎ¾Ë†i k2 â‰¤ ci,i kÎ¾~i k2 +                   |ci,j |kÎ¾~j k2 = ci,i +
                                             P                                 P
                                                j6=i                               j6=i |ci,j |, and therefore
                                                                      X                           X
                             kÎ¾Ë†i âˆ’ Î¾~i k2 â‰¤ |1 âˆ’ ci,i |kÎ¾~i k2 +           |ci,j kÎ¾~j k2 â‰¤ 2          |ci,j |
                                                                       j6=i                       j6=i
again by the triangle inequality. Therefore, it suffices to show |ci,j | â‰¤ 2kRâˆ’1 k2 Â· Îµ3 for j 6= i to
prove the second claim. P
      Observe that AÎ¾Ë†i = A( ki0 =1 ci,i0 Î¾~i0 ) = ki0 =1 ci,i0 Î»i0 Î¾~i0 , and therefore
                                                               P
                   X  k                                                             X k
                         ci,i0 Î»i0 Î¾~i0 + (AÌ‚ âˆ’ A)Î¾Ë†i = AÌ‚Î¾Ë†i = Î»Ì‚i Î¾Ë†i = Î»i             ci,i0 Î¾~i0 + (Î»Ì‚i âˆ’ Î»i )Î¾Ë†i .
                   i0 =1                                                           i0 =1
Multiplying through the above equation by Î¶~j> , and using the fact that Î¶~j> Î¾~i0 = 1{j = i0 } gives
                                     ci,j Î»j + Î¶~i> (AÌ‚ âˆ’ A)Î¾Ë†i = Î»i ci,j + (Î»Ì‚i âˆ’ Î»i )Î¶~j> Î¾Ë†i .
The above equation rearranges to (Î»j âˆ’ Î»i )ci,j = (Î»Ì‚i âˆ’ Î»i )Î¶~j> Î¾Ë†i + Î¶~j> (A âˆ’ AÌ‚)Î¾Ë†i and therefore
                        kÎ¶~j k2 Â· (|Î»Ì‚i âˆ’ Î»i | + k(AÌ‚ âˆ’ A)Î¾Ë†i k2 )           kRâˆ’1 k2 Â· (|Î»Ì‚i âˆ’ Î»i | + kAÌ‚ âˆ’ Ak2 )
            |ci,j | â‰¤                                                     â‰¤
                                           |Î»j âˆ’ Î»i |                                           |Î»j âˆ’ Î»i |
by the Cauchy-Schwarz and triangle inequalities and the sub-multiplicative property of the spectral
norm. The bound |ci,j | â‰¤ 2kRâˆ’1 k2 Â· Îµ3 then follows from the first claim.
      The third claim follows from standard comparisons of matrix norms.
      The next lemma gives perturbation bounds for estimating the eigenvalues of simultaneously
diagonalizable matrices A1 , A2 , . . . , Ak . The eigenvectors RÌ‚ are taken from a perturbation of the
first matrix A1 , and are then subsequently used to approximately diagonalize the perturbations of the
remaining matrices A2 , . . . , Ak . In practice, one may use Jacobi-like procedures to approximately
solve the joint eigenvalue problem.
Lemma 13 Let A1 , A2 , . . . , Ak âˆˆ RkÃ—k be diagonalizable matrices that are diagonalized by the
same matrix invertible R âˆˆ RkÃ—k with unit length columns kR~ej k2 = 1, such that each Ai has k
distinct real eigenvalues:
                                            Râˆ’1 Ai R = diag(Î»i,1 , Î»i,2 , . . . , Î»i,k ).
Let AÌ‚1 , AÌ‚2 , . . . , AÌ‚k âˆˆ RkÃ—k be given. Define A := maxi kAÌ‚i âˆ’ Ai k2 , Î³A := mini minj6=j 0 |Î»i,j âˆ’
Î»i,j 0 |, Î»max := maxi,j |Î»i,j |, Îµ3 := Îº(R)Â·              A
                                                       Î³A , and Îµ4 := 4k
                                                                                  1.5 Â· kRâˆ’1 k2 Â· Îµ . Assume Îµ < 1 and
                                                                                                    2    3             3 2
Îµ4 < 1. Then there exists a permutation Ï„ on [k] such that the following holds.
    1. The matrix AÌ‚1 has k distinct real eigenvalues Î»Ì‚1,1 , Î»Ì‚1,2 , . . . , Î»Ì‚1,k âˆˆ R, and |Î»Ì‚1,j âˆ’ Î»1,Ï„ (j) | â‰¤
          Îµ3 Â· Î³A for all j âˆˆ [k].
                                                                  33.24

                          A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
     2. There exists a matrix RÌ‚ âˆˆ RkÃ—k whose j-th column is a right eigenvector corresponding to
         Î»Ì‚1,j , scaled so kRÌ‚~ej k2 = 1 for all j âˆˆ [k], such that kRÌ‚ âˆ’ RÏ„ k2 â‰¤ kRÎµâˆ’1           4
                                                                                                    k2
                                                                                                       , where RÏ„ is the
         matrix obtained by permuting the columns of R with Ï„ .
                                                                                                             Îµ4
     3. The matrix RÌ‚ is invertible and its inverse satisfies kRÌ‚âˆ’1 âˆ’ RÏ„âˆ’1 k2 â‰¤ kRâˆ’1 k2 Â·                  1âˆ’Îµ4 ;
     4. For all i âˆˆ {2, 3, . . . , k} and all j âˆˆ [k], the (j, j)-th element of RÌ‚âˆ’1 AÌ‚i RÌ‚, denoted by
         Î»Ì‚i,j := ~e>     âˆ’1
                     j RÌ‚ AÌ‚i RÌ‚~  ej , satisfies
                                                                              
                                                   Îµ4                    Îµ4
                  |Î»Ì‚i,j âˆ’ Î»i,Ï„ (j) | â‰¤ 1 +                Â· 1+ âˆš                   Â· Îµ3 Â· Î³ A
                                                 1 âˆ’ Îµ4                k Â· Îº(R)
                                                                                                  
                                                         1              1            1        Îµ4
                                         + Îº(R) Â·              +âˆš               +âˆš Â·                  Â· Îµ4 Â· Î»max .
                                                      1 âˆ’ Îµ4         k Â· Îº(R)         k 1 âˆ’ Îµ4
         If Îµ4 â‰¤ 12 , then |Î»Ì‚i,j âˆ’ Î»i,Ï„ (j) | â‰¤ 3Îµ3 Â· Î³A + 4Îº(R) Â· Îµ4 Â· Î»max .
Proof The first and second claims follow from applying Lemma 12 to A1 and AÌ‚1 . The third claim
follows from applying Lemma 23 to RÌ‚ and RÏ„ . To prove the last claim, first define Î¶~j> âˆˆ Rk (Î¶Ì‚j> ) to
be the j-th row of RÏ„âˆ’1 (RÌ‚âˆ’1 ), and Î¾~j âˆˆ Rk (Î¾Ë†j ) to be the j-th column of RÏ„ (RÌ‚), so Î¶~j> Ai Î¾~j = Î»i,Ï„ (j)
and Î¶Ì‚j> AÌ‚i Î¾Ë†j = ~e>     âˆ’1
                       j RÌ‚ AÌ‚i RÌ‚~ ej = Î»Ì‚i,j . By the triangle and Cauchy-Schwarz inequalities and the sub-
multiplicative property of the spectral norm,
|Î»Ì‚i,j âˆ’ Î»i,Ï„ (j) |
 = |Î¶Ì‚ > AÌ‚i Î¾Ë†j âˆ’ Î¶~> Ai Î¾~j |
       j              j
 = |Î¶~j> (AÌ‚i âˆ’ Ai )Î¾~j + Î¶~j> (AÌ‚i âˆ’ Ai )(Î¾Ë†j âˆ’ Î¾~j ) + (Î¶Ì‚j âˆ’ Î¶~j )> (AÌ‚i âˆ’ Ai )Î¾~j
    + (Î¶Ì‚j âˆ’ Î¶~j )> (AÌ‚i âˆ’ Ai )(Î¾Ë†j âˆ’ Î¾~j ) + (Î¶Ì‚j âˆ’ Î¶~j )> Ai Î¾~j + Î¶~j> Ai (Î¾Ë†j âˆ’ Î¾~j ) + (Î¶Ì‚j âˆ’ Î¶~j )> Ai (Î¾Ë†j âˆ’ Î¾~j )|
 â‰¤ |Î¶~> (AÌ‚i âˆ’ Ai )Î¾~j | + |Î¶~> (AÌ‚i âˆ’ Ai )(Î¾Ë†j âˆ’ Î¾~j )| + |(Î¶Ì‚j âˆ’ Î¶~j )> (AÌ‚i âˆ’ Ai )Î¾~j |
       j                         j
    + |(Î¶Ì‚j âˆ’ Î¶~j )> (AÌ‚i âˆ’ Ai )(Î¾Ë†j âˆ’ Î¾~j )| + |(Î¶Ì‚j âˆ’ Î¶~j )> Ai Î¾~j | + |Î¶~j> Ai (Î¾Ë†j âˆ’ Î¾~j )| + |(Î¶Ì‚j âˆ’ Î¶~j )> Ai (Î¾Ë†j âˆ’ Î¾~j )|
 â‰¤ kÎ¶~j k2 Â· kAÌ‚i âˆ’ Ai k2 Â· kÎ¾~j k2 + kÎ¶~j k2 Â· kAÌ‚i âˆ’ Ai k2 Â· kÎ¾Ë†j âˆ’ Î¾~j k2 + kÎ¶Ì‚j âˆ’ Î¶~j k2 Â· kAÌ‚i âˆ’ Ai k2 kÎ¾~j k2
    + kÎ¶Ì‚j âˆ’ Î¶~j k2 Â· kAÌ‚i âˆ’ Ai k2 Â· kÎ¾Ë†j âˆ’ Î¾~j k2
    + kÎ¶Ì‚j âˆ’ Î¶~j k2 Â· kÎ»i,Ï„ (j) Î¾~j k2 + kÎ»i,Ï„ (j) Î¶~j k2 Â· kÎ¾Ë†j âˆ’ Î¾~j k2 + kÎ¶Ì‚j âˆ’ Î¶~j k2 Â· kAi k2 Â· kÎ¾Ë†j âˆ’ Î¾~j k2 .
                                                                                                                        (13)
Observe that kÎ¶~j k2 â‰¤ kRâˆ’1 k2 , kÎ¾~j k2 â‰¤ kRk2 , kÎ¶Ì‚j âˆ’ Î¶~j k2 â‰¤ kRÌ‚âˆ’1 âˆ’ RÏ„âˆ’1 k2 â‰¤ kRâˆ’1 k2 Â· 1âˆ’Îµ                       Îµ4
                                                                                                                           4
                                                                                                                             ,
kÎ¾Ë†j âˆ’ Î¾~j k2 â‰¤ 4kÂ·kRâˆ’1 k2 Â·Îµ3 (by Lemma 12), and kAi k2 â‰¤ kRk2 Â·(maxj |Î»i,j |)Â·kRâˆ’1 k2 . Therefore,
                                                            33.25

                                          A NANDKUMAR H SU K AKADE
continuing from (13), |Î»Ì‚i,j âˆ’ Î»i,Ï„ (j) | is bounded as
                                                                                                         Îµ4
|Î»Ì‚i,j âˆ’ Î»i,Ï„ (j) | â‰¤ kRâˆ’1 k2 Â· kRk2 Â· A + kRâˆ’1 k2 Â· A Â· 4k Â· kRâˆ’1 k2 Â· Îµ3 + kRâˆ’1 k2 Â·                      Â· A Â· kRk2
                                                                                                      1 âˆ’ Îµ4
                                       Îµ4
                      + kRâˆ’1 k2 Â·             Â· A Â· 4k Â· kRâˆ’1 k2 Â· Îµ3
                                    1 âˆ’ Îµ4
                                                Îµ4
                      + Î»max Â· kRâˆ’1 k2 Â·                Â· kRk2 + Î»max Â· kRâˆ’1 k2 Â· 4k Â· kRâˆ’1 k2 Â· Îµ3
                                             1 âˆ’ Îµ4
                                       Îµ4
                      + kRâˆ’1 k2 Â·             Â· kRk2 Â· Î»max Â· kRâˆ’1 k2 Â· 4k Â· kRâˆ’1 k2 Â· Îµ3
                                    1 âˆ’ Îµ4
                                       Îµ4                          Îµ4
                    = Îµ3 Â· Î³ A + âˆš               Â· Îµ3 Â· Î³ A +           Â· Îµ3 Â· Î³ A
                                    k Â· Îº(R)                   1 âˆ’ Îµ4
                              Îµ4           Îµ4
                      +âˆš              Â·            Â· Îµ3 Â· Î³ A
                           k Â· Îº(R) 1 âˆ’ Îµ4
                                    1                         1                    Îº(R)      Îµ4
                      + Îº(R) Â·            Â· Îµ4 Â· Î»max + âˆš Â· Îµ4 Â· Î»max + âˆš Â·                       Â· Îµ4 Â· Î»max .
                                 1 âˆ’ Îµ4                        k                     k 1 âˆ’ Îµ4
Rearranging gives the claimed inequality.
Lemma 14 Let V âˆˆ RkÃ—k be an invertible matrix, and let R âˆˆ RkÃ—k be the matrix whose j-th
column is V ~ej /kV ~ej k2 . Then kRk2 â‰¤ Îº(V ), kRâˆ’1 k2 â‰¤ Îº(V ), and Îº(R) â‰¤ Îº(V )2 .
Proof We have R = V diag(kV ~e1 k2 , kV ~e2 k2 , . . . , kV ~ek k2 )âˆ’1 , so by the sub-multiplicative prop-
erty of the spectral norm, kRk2 â‰¤ kV k2 / minj kV ~ej k2 â‰¤ kV k2 /Ïƒk (V ) = Îº(V ). Similarly,
kRâˆ’1 k2 â‰¤ kV âˆ’1 k2 Â· maxj kV ~ej k2 â‰¤ kV âˆ’1 k2 Â· kV k2 = Îº(V ).
      The next lemma shows that randomly projecting a collection of vectors to R does not collapse
any two too close together, nor does it send any of them too far away from zero.
Lemma 15 Fix any Î´ âˆˆ (0, 1) and matrix A âˆˆ RmÃ—n (with m â‰¤ n). Let Î¸~ âˆˆ Rm be a random
vector distributed uniformly over S mâˆ’1 .
                                                                           
     1. Pr min |hÎ¸,  ~ A(~ei âˆ’ ~ej )i| > mini6=j kA(~ âˆš
                                                           ei âˆ’ ~ej )k2 Â· Î´
                                                                               â‰¥ 1 âˆ’ Î´.
                                                         em n2
                                                                
             i6=j
                                                                          
                            ~            kA~ei k2          p
     2. Pr âˆ€i âˆˆ [m], |hÎ¸, A~ei i| â‰¤ âˆš                 1 + 2 ln(m/Î´) â‰¥ 1 âˆ’ Î´.
                                              m
Proof For the first claim, let Î´0 := Î´/ n2 . By Lemma 25, for any fixed pair {i, j} âŠ† [n] and
                                                      
           âˆš
Î² := Î´0 / e,
                                                                                                         
         ~                                             1      Î´0               1        2            2
  Pr |hÎ¸, A(~ei âˆ’ ~ej )i| â‰¤ kA(~ei âˆ’ ~ej )k2 Â· âˆš Â· âˆš â‰¤ exp                       (1 âˆ’ (Î´0 /e) + ln(Î´0 /e)) â‰¤ Î´0 .
                                                       m       e               2
Therefore the first claim follows by a union bound over all n2 pairs {i, j}.
                                                                             
                                                           33.26

                    A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
                                                                              p
    For the second claim, apply Lemma 25 with Î² := 1 + t and t := 2 ln(m/Î´) to obtain
                                                                                             
                  ~           kA~ei k2                         1               2
            Pr |hÎ¸, A~ei i| â‰¥  âˆš       Â· (1 + t) â‰¤ exp             1 âˆ’ (1 + t) + 2 ln(1 + t)
                                  m                            2
                                                                                   
                                                               1               2
                                                  â‰¤ exp            1 âˆ’ (1 + t) + 2t
                                                               2
                                                          2 /2
                                                  = eâˆ’t         = Î´/m.
Therefore the second claim follows by taking a union bound over all i âˆˆ [m].
Appendix D. Proofs and details from Section 4
In this section, we provide omitted proofs and details from Section 4.
D.1. Learning mixtures of product distributions
In this section, we show how to use Algorithm B with mixtures of product distributions in Rn that
satisfy an incoherence condition on the means Âµ    ~ 1, Âµ            ~ k âˆˆ Rn of k component distributions.
                                                        ~ 2, . . . , Âµ
Note that product distributions are just a special case of the more general class of multi-view distri-
butions, which are directly handled by Algorithm B.
    The basic idea is to randomly partition the coordinates into ` â‰¥ 3 â€œviewsâ€, each of roughly the
same dimension. Under the assumption that the component distributions are product distributions,
the multi-view assumption is satisfied. What remains to be checked is that the non-degeneracy
condition (Condition 2) is satisfied. Theorem 16 (below) shows that it suffices that the original
matrix of component means have rank k and satisfy the following incoherence condition.
Condition 4 (Incoherence condition) Let Î´ âˆˆ (0, 1), ` âˆˆ [n], and M = [~             Âµ1 |~           Âµk ] âˆˆ RnÃ—k
                                                                                        Âµ2 | Â· Â· Â· |~
be given; let M = U SV be the thin singular value decomposition of M , where U âˆˆ RnÃ—k is a
                            >
matrix of orthonormal columns, S = diag(Ïƒ1 (M ), Ïƒ2 (M ), . . . , Ïƒk (M )) âˆˆ RkÃ—k , and V âˆˆ RkÃ—k
is orthogonal; and let                                                  
                                                     n         >       2
                                      cM := max         Â· kU ~ej k2 .
                                             jâˆˆ[n] k
The following inequality holds:
                                                  9      bn/`c
                                          cM â‰¤       Â·               .
                                                 32 k Â· ln `Â·k   Î´
Note that cM is always in the interval [1, n/k]; it is smallest when the left singular vectors in U have
     âˆš
Â±1/ n entries (as in a Hadamard basis), and largest when the singular vectors are the coordinate
axes. Roughly speaking, the incoherence condition requires that the non-degeneracy of a matrix
M be witnessed by many vertical blocks of M . When the condition is satisfied, then with high
probability, a random partitioning of the coordinates into ` groups induces a block partitioning of
M into ` matrices M1 , M2 , . . . , M` (with roughly equal number of rows) such that the k-th largest
singular value of Mv is not much smaller than that of M (for each v âˆˆ [`]).
    Chaudhuri and Rao (2008) show that under a similar condition (which they call a spreading con-
dition), a random partitioning of the coordinates into two â€œviewsâ€ preserves the separation between
                                                  33.27

                                        A NANDKUMAR H SU K AKADE
the means of k component distributions. They then follow this preprocessing with a projection
based on the correlations across the two views (similar to CCA). However, their overall algorithm
requires a minimum separation condition on the means of the component distributions. In contrast,
Algorithm B does not require a minimum separation condition at all in this setting.
Theorem 16 Assume Condition 4 holds. With probability at least 1âˆ’Î´, a uniformly chosen random
partitioning of [n] into ` disjoint sets [n] = I1 âˆª I2 âˆª Â· Â· Â· âˆª I` , each of size at least
                                                                        
                                                  32                 `Â·k
                                       |Iv | â‰¥       Â· cM Â· k Â· ln         ,
                                                  9                    Î´
has the following property: for eachp     v âˆˆ [`], the matrix Mv âˆˆ R|Iv |Ã—k formed by selecting the rows
of M indexed by Iv and scaling by n/|Iv |, satisfies
                                             Ïƒk (Mv ) â‰¥ Ïƒk (M )/2.
Proof Follows from Lemma 17 (below) together with a union bound.
Lemma 17 Assume Condition 4 holds. Consider a random subset {J1 , J2 , . . . , Jd } âŠ† [n] of size d
chosen uniformly at random without replacement, and let M            f be the random d Ã— k matrix given by
                                                           ï£® > ï£¹
                                                            ~eJ1 M
                                                            ~e>J2 M ï£º
                                                    r
                                            f := n Â· ï£¯
                                                           ï£¯         ï£º
                                            M              ï£¯ .. ï£º .
                                                       d ï£° . ï£»
                                                            ~e>Jd M
If
                                                  32                 k
                                            dâ‰¥       Â· cM Â· k Â· ln ,
                                                   9                 Î´
then                                    h                          i
                                     Pr Ïƒk (M f) â‰¥ Ïƒk (M )/2 â‰¥ 1 âˆ’ Î´.
Proof Let {I1 , I2 , . . . , Id } âŠ† [n] be a random subset of size d chosen uniformly at random with
replacement, and let M  c be the random d Ã— k matrix given by
                                                           ï£® > ï£¹
                                                             ~eI1 M
                                                               >
                                                       n ï£¯~eI2 M ï£º
                                                    r      ï£¯
                                            M :=         Â· ï£¯ . ï£º.
                                             c                       ï£º
                                                       d ï£° . ï£»   .
                                                             ~e>
                                                               Id M
By Proposition 18, for any Ï„ > 0,
                                    Pr[Ïƒk (M f) < Ï„ ] â‰¤ Pr[Ïƒk (M     c) < Ï„ ].
Therefore, henceforth, we just work with M       c (i.e., sampling with replacement).
                                                      33.28

                    A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
     Note that                                         q
                                           Ïƒk (Mc) =            c> M
                                                          Î»min (M     c).
For each j âˆˆ [d], let Xj := n Â· (U >~eIj ) âŠ— (U >~eIj ), so
                                      d                                    X    d    
                       >c         nX >                                      1
                   M M=
                    c                                       >
                                         (M ~eIj ) âŠ— (M ~eIj ) = V S               Xj SV >
                                  d                                         d
                                    j=1                                        j=1
and
                                                      X  d                            X   d     
                    >c                     2          1                         2        1
           Î»min (M M ) â‰¥ Î»min (S) Â· Î»min
                  c                                           Xj = Ïƒk (M ) Â· Î»min               Xj .
                                                      d                                  d
                                                         j=1                               j=1
Observe that
                                        Xn
                           E[Xj ] =          Pr[Ij = i] Â· n Â· (U >~ei ) âŠ— (U >~ei ) = I
                                        i=1
and that
                     Î»max (Xj ) â‰¤ n Â· max{kU >~ei k22 } = cM Â· k,             almost surely.
                                            iâˆˆ[n]
By Lemma 26 (a Chernoff bound on extremal eigenvalues of random symmetric matrices),
                                          d
                           "         X                  #
                                      1                 1                   2
                       Pr Î»min                Xj â‰¤          â‰¤ k Â· eâˆ’d(3/4) /(2cM k) â‰¤ Î´.
                                      d                 4
                                         j=1
The claim follows.
Proposition 18 (Reduction to sampling with replacement) Consider any m Ã— n matrix A. For
any t âˆˆ [m], let A et be a random t Ã— n submatrix of A formed by choosing a random subset of t
rows of A uniformly at random without replacement; and let A             bt be a random t Ã— n submatrix of A
formed by choosing a random subset of t rows of A uniformly at random with replacement. Fix any
t âˆˆ [m] and Ï„ > 0. Then
                                    Pr[Ïƒn (A  et ) < Ï„ ] â‰¤ Pr[Ïƒn (A  bt ) < Ï„ ].
Proof This argument is similar to one given by Recht (2009). We first prove that Pr[Ïƒn (A               et ) < Ï„ ]
                                     0
is non-increasing in t. For any t â‰¤ t, consider the following coupling between A             et and A
                                                                                                    et0 :
    1. First, sample t row indices in [m] uniformly at random without replacement, and select those
       rows in A to form A   et .
    2. Then, given these t row indices, choose t âˆ’ t0 of them uniformly at random without replace-
       ment, and remove them to form A         et0 .
Since Ïƒn (Aet0 ) â‰¤ Ïƒn (Aet ),
                                        Ïƒn (A et ) < Ï„ =â‡’ Ïƒn (A   et0 ) < Ï„,
and consequently
                                    Pr[Ïƒn (A  et ) < Ï„ ] â‰¤ Pr[Ïƒn (A  et0 ) < Ï„ ].                            (14)
                                                        33.29

                                               A NANDKUMAR H SU K AKADE
      Now we prove the proposition. Let Uniquet âˆˆ [t] be the number of distinct row indices selected
 to form A   bt . Then
                                           Xt
                           Pr[A bt < Ï„ ] =     Pr[A bt < Ï„ | Uniquet = i] Pr[Uniquet = i]
                                           i=1
                                           Xt
                                         =     Pr[A ei < Ï„ ] Pr[Uniquet = i]
                                           i=1
                                           Xt
                                         â‰¥     Pr[A et < Ï„ ] Pr[Uniquet = i]  (by (14), as i â‰¤ t)
                                           i=1
                                                        Xt
                                         = Pr[Aet < Ï„ ]      Pr[Uniquet = i]
                                                        i=1
                                         = Pr[Aet < Ï„ ].
 D.2. Relaxation of Condition 2 using higher-order moments
 Even if Condition 2 does not hold (e.g., if Âµ           ~ v,j â‰¡ m ~ âˆˆ Rd (say) for all v âˆˆ [`], j âˆˆ [k] so all of
 the component distributions have the same mean), one may still apply Algorithm B to the model
 (h, ~y1 , ~y2 , . . . , ~y` ) where ~yv âˆˆ Rd+d(d+1)/2 is the random vector that include both first- and second-
 order terms of ~xv , i.e., ~yv is the concatenation of xv and the upper triangular part of ~xv âŠ— ~xv . In this
 case, Condition 2 is replaced by a requirement that the matrices
                 Mv0 := E[~yv |h = 1] E[~yv |h = 2] Â· Â· Â· E[~yv |h = k] âˆˆ R(d+d(d+1)/2)Ã—k
                                                                                
 of conditional means and covariances have full rank. This requirement can be met even if the means
 ~ v,j of the mixture components are all the same. Extending this to higher-order terms is immediate.
 Âµ
 D.3. Empirical moments for multi-view mixtures of subgaussian distributions
 The required concentration behavior of the empirical moments used by Algorithm B can be easily
 established for multi-view Gaussian mixture models using known techniques (Chaudhuri et al.,
 2009). This is clear for the second-order statistics PÌ‚a,b for {a, b} âˆˆ {{1, 2}, {1, 3}}, and remains
 true for the third-order statistics PÌ‚1,2,3 because ~x3 is conditionally independent of ~x1 and ~x2 given
 h. The magnitude of hUÌ‚3 Î¸~i , ~x3 i can be bounded for all samples (with a union bound; recall that
 we make the simplifying assumption that PÌ‚1,3 is independent of PÌ‚1,2,3 , and therefore so are UÌ‚3 and
 PÌ‚1,2,3 ). Therefore, one effectively only needs spectral norm error bounds for second-order statistics,
 as provided by existing techniques.
      Indeed, it is possible to establish Condition 3 in the case where the conditional distribution of
~xv given h (for each view v) is subgaussian. Specifically, we assume that there exists some Î± > 0
 such that for each view v and each component j âˆˆ [k],
                                                                    
                                           âˆ’1/2
    E exp Î»h~u, cov(~xv |h = j)                 (~xv âˆ’ E[~xv |h = j])i â‰¤ exp(Î±Î»2 /2), âˆ€Î» âˆˆ R, ~u âˆˆ S dâˆ’1
                                                              33.30

                     A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
where cov(~x|h = j) := E[(~xv âˆ’ E[~xv |h = j]) âŠ— (~xv âˆ’ E[~xv |h = j])|h = j] is assumed to be
positive definite. Using standard techniques (e.g., Vershynin (2012)), Condition 3 can be shown to
hold under the above conditions with the following parameters (for some universal constant c > 0):
           wmin := min wj
                       jâˆˆ[k]
                          Î±3/2 (d + log(1/Î´))            Î±3/2 (d + log(1/Î´))
             N0 := c Â·                             log
                                   wmin                            wmin
                                n                                                                         o2
                                                          1/2
            Ca,b := c Â· max k cov(~xv |h = j)k2 , kE[~xv |h = j]k2 : v âˆˆ {a, b}, j âˆˆ [k]
                                                                                                      o3
                                                         1/2
           C1,2,3 := c Â· max k cov(~xv |h = j)k2 , kE[~xv |h = j]k2 : v âˆˆ [3], j âˆˆ [k]
                       r                   s         p
                          k 2 log(1/Î´)        Î±3/2 log(N/Î´)(d + log(1/Î´))
        f (N, Î´) :=                      +                                                 .
                                N                                wmin N
D.4. Recovering the component covariances
While Algorithm B recovers just the means of the mixture components, we remark that a slight
variation can be used to recover the covariances as well. Note that
                    E[~xv âŠ— ~xv |h] = (Mv ~eh ) âŠ— (Mv ~eh ) + Î£v,h = Âµ         ~ v,h âŠ— Âµ ~ v,h + Î£v,h
for all v âˆˆ [`]. For a pair of vectors Ï†   ~ âˆˆ Rd and Ïˆ     ~ âˆˆ Rd , define the matrix Q1,2,3 (Ï†,       ~ âˆˆ RdÃ—d of
                                                                                                      ~ Ïˆ)
fourth-order moments by Q1,2,3 (Ï†,        ~ := E[(~x1 âŠ— ~x2 )hÏ†,
                                        ~ Ïˆ)                                   ~ ~x3 i].
                                                                      ~ ~x3 ihÏˆ,
Proposition 19 Under the setting of Lemma 5, the matrix given by
                                     ~ Ïˆ)
                             F1,2,3 (Ï†, ~ := (U > Q1,2,3 (Ï†,        ~ 2 )(U > P1,2 U2 )âˆ’1
                                                                ~ Ïˆ)U
                                                   1                           1
                  ~ Ïˆ)
satisfies F1,2,3 (Ï†, ~ = (U > M1 ) diag(hÏ†,     ~ Âµ          ~ Âµ
                                                     ~ 3,t ihÏˆ,              ~ Î£3,t Ïˆi
                                                                 ~ 3,t i + hÏ†,        ~ : t âˆˆ [k])(U > M1 )âˆ’1 and
                                 1                                                                       1
hence is diagonalizable (in fact, by the same matrices as B1,2,3 (~Î· )).
Proof As in the proof of Lemma 4, it is easy to show that
                    ~ Ïˆ)
            Q1,2,3 (Ï†,  ~ = E[E[~x1 |h] âŠ— E[~x2 |h]hÏ†,     ~ E[~x3 âŠ— ~x3 |h]Ïˆi] ~
                                               ~ (~
                            = M1 E[~eh âŠ— ~eh hÏ†,     Âµ3,h âŠ— Âµ                    ~
                                                                ~ 3,h + Î£3,h )Ïˆi]M       >
                                                                                        2
                            = M1 diag(hÏ†,  ~ Âµ        ~ Âµ
                                             ~ 3,t ihÏˆ,   ~ 3,t i + hÏ†,~ Î£3,t Ïˆi~ : t âˆˆ [k]) diag(w)M~ 2> .
The claim then follows from the same arguments used in the proof of Lemma 5.
D.5. Proof of Proposition 8
The conditional independence properties follow from the HMM conditional independence assump-
tions. To check the parameters, observe first that
                             Pr[h2 = j|h1 = i] Â· Pr[h1 = i]               Tj,i Ï€i
  Pr[h1 = i|h2 = j] =                                                 =            = ~ei diag(~Ï€ )T > diag(T ~Ï€ )âˆ’1~ej
                                         Pr[h2 = j]                      (T ~Ï€ )j
                                                         33.31

                                    A NANDKUMAR H SU K AKADE
by Bayesâ€™ rule. Therefore
             M1~ej = E[~x1 |h2 = j] = OE[~eh1 |h2 = j] = O diag(~Ï€ )T > diag(T ~Ï€ )âˆ’1~ej .
The rest of the parameters are similar to verify.
Appendix E. General results from matrix perturbation theory
The lemmas in this section are standard results from matrix perturbation theory, taken from Stewart
and Sun (1990).
Lemma 20 (Weylâ€™s theorem) Let A, E âˆˆ RmÃ—n with m â‰¥ n be given. Then
                                max |Ïƒi (A + E) âˆ’ Ïƒi (A)| â‰¤ kEk2 .
                                iâˆˆ[n]
Proof See Theorem 4.11, p. 204 in Stewart and Sun (1990).
Lemma 21 (Wedinâ€™s theorem) Let A, E âˆˆ RmÃ—n with m â‰¥             n be given. Let A have the singular
value decomposition          ï£® > ï£¹                       ï£®            ï£¹
                                U1                          Î£1    0
                             ï£° U2> ï£» A V1 V2 = ï£° 0
                                                    
                                                                 Î£2 ï£» .
                                U3>                         0     0
Let AÌƒ := A+E, with analogous singular value decomposition (UÌƒ1 , UÌƒ2 , UÌƒ3 , Î£Ìƒ1 , Î£Ìƒ2 , VÌƒ1 VÌƒ2 ). Let Î¦ be
the matrix of canonical angles between range(U1 ) and range(UÌƒ1 ), and Î˜ be the matrix of canonical
angles between range(V1 ) and range(VÌƒ1 ). If there exists Î´, Î± > 0 such that mini Ïƒi (Î£Ìƒ1 ) â‰¥ Î± + Î´
and maxi Ïƒi (Î£2 ) â‰¤ Î±, then
                                                               kEk2
                                max{k sin Î¦k2 , k sin Î˜k2 } â‰¤         .
                                                                 Î´
Proof See Theorem 4.4, p. 262 in Stewart and Sun (1990).
Lemma 22 (Bauer-Fike theorem) Let A, E âˆˆ RkÃ—k be given. If A = V diag(Î»1 , Î»2 , . . . , Î»k )V âˆ’1
for some invertible V âˆˆ RkÃ—k , and AÌƒ := A + E has eigenvalues Î»Ìƒ1 , Î»Ìƒ2 , . . . , Î»Ìƒk , then
                                 max min |Î»Ìƒi âˆ’ Î»j | â‰¤ kV âˆ’1 EV k2 .
                                 iâˆˆ[k] jâˆˆ[k]
Proof See Theorem 3.3, p. 192 in Stewart and Sun (1990).
Lemma 23 (Perturbation of inverses) Let A, E âˆˆ RkÃ—k be given. If A is invertible, and kAâˆ’1 Ek2 <
1, then AÌƒ := A + E is invertible, and
                                                      kEk2 kAâˆ’1 k22
                                 kAÌƒâˆ’1 âˆ’ Aâˆ’1 k2 â‰¤                   .
                                                     1 âˆ’ kAâˆ’1 Ek2
Proof See Theorem 2.5, p. 118 in Stewart and Sun (1990).
                                                 33.32

                     A M ETHOD OF M OMENTS FOR M IXTURE M ODELS AND HMM S
Appendix F. Probability inequalities
Lemma 24 (Accuracy of empirical probabilities) Fix Âµ         ~ = (Âµ1 , Âµ2 , . . . , Âµn ) âˆˆ âˆ†mâˆ’1 . Let ~x be a
random vector for which Pr[~    x = ~ei ] = Âµi for all i âˆˆ [m], and let ~x1 , ~x2 , . . . , ~xn be n independent
copies of ~x. Set ÂµÌ‚ := (1/n) ni=1 ~xi . For all t > 0,
                              P
                                                          âˆš 
                                                        1+ t
                                     Pr kÂµÌ‚ âˆ’ Âµ ~ k2 > âˆš         â‰¤ eâˆ’t .
                                                           n
Proof
âˆš        This is a standard application of McDiarmidâ€™s inequality (using the fact that kÂµÌ‚ âˆ’ Âµ           ~ k2 has
  2/n bounded differences when a single ~xi is changed), together with the bound E[kÂµÌ‚ âˆ’ Âµ               ~ k2 ] â‰¤
   âˆš
1/ n. See Proposition 19 in Hsu et al. (2012).
Lemma 25 (Random projection) Let Î¸~ âˆˆ Rn be a random vector distributed uniformly over
S nâˆ’1 , and fix a vector ~v âˆˆ Rn .
   1. If Î² âˆˆ (0, 1), then
                                                                                            
                                                   1              1
                               ~ ~v i| â‰¤ k~v k2 Â· âˆš Â· Î² â‰¤ exp (1 âˆ’ Î² + ln Î² ) .
                                                                              2             2
                         Pr |hÎ¸,
                                                    n             2
   2. If Î² > 1, then
                                                                                            
                               ~                   1              1           2             2
                         Pr |hÎ¸, ~v i| â‰¥ k~v k2 Â· âˆš Â· Î² â‰¤ exp (1 âˆ’ Î² + ln Î² ) .
                                                    n             2
Proof This is a special case of Lemma 2.2 from Dasgupta and Gupta (2003).
Lemma 26 (Matrix Chernoff bound) Let X be a symmetric random m Ã— m matrix such that
0  X  rI almost surely, and set l := Î»min (E[X]). Let X1 , X2 , . . . , Xn be i.i.d. copies of X. For
any  âˆˆ [0, 1],
                                         n
                            "      X                       #
                                     1                                         2
                        Pr Î»min             Xi â‰¤ (1 âˆ’ ) Â· l â‰¤ m Â· eâˆ’n l/(2r) .
                                     n
                                        i=1
Proof This is a direct corollary of Theorem 19 from Ahlswede and Winter (2002).
Appendix G. Insufficiency of second-order moments
Chang (1996) shows that a simple class of Markov models used in mathematical phylogenetics
cannot be identified from pair-wise probabilities alone. Below, we restate (a specialization of) this
result in terms of the document topic model from Section 2.1.
Proposition 27 (Chang, 1996) Consider the model from Section 2.1 on (h, x1 , x2 , . . . , x` ) with
parameters M and w.    ~ Let Q âˆˆ RkÃ—k be an invertible matrix such that the following hold:
                                                     33.33

                                     A NANDKUMAR H SU K AKADE
    1. ~1> Q = ~1> ;
    2. M Qâˆ’1 , Q diag(w)M~    >
                                diag(M w)~ âˆ’1 , and Qw    ~ have non-negative entries;
                ~ > is a diagonal matrix.
    3. Q diag(w)Q
Then the marginal distribution over (x1 , x2 ) is identical to that in the case where the model has
parameters MÌƒ := M Qâˆ’1 and wÌƒ := Qw.     ~
A simple example for d = k = 2 can be obtained from
                                                                          ï£®          âˆš            ï£¹
                                                                               1+   1+4p(1âˆ’p)
                    p    1âˆ’p                       1/2                       p       âˆš 2
         M :=                    ,       w~ :=            ,        Q := ï£°                         ï£»
                  1âˆ’p      p                       1/2                     1âˆ’p
                                                                                   1âˆ’ 1+4p(1âˆ’p)
                                                                                          2
for some p âˆˆ (0, 1). We take p = 0.25, in which case Q satisfies the conditions of Proposition 27,
and
                                                                                 
                                  0.25 0.75                                         0.5
                          M=                   ,                             w~=         ,
                                  0.75 0.25                                         0.5
                                                                                         
                          âˆ’1      0.6614 0.1129                                     0.7057
               MÌƒ = M Q â‰ˆ                             ,                wÌƒ = Qw~â‰ˆ              .
                                  0.3386 0.8871                                     0.2943
                        ~ and (MÌƒ , wÌƒ) give rise to the same pair-wise probabilities
In this case, both (M, w)
                                                                                  
                                     >                              0.3125 0.1875
                               ~
                      M diag(w)M        = MÌƒ diag(wÌƒ)MÌƒ > â‰ˆ                          .
                                                                    0.1875 0.3125
However, the triple-wise probabilities, for Î· = (1, 0), differ: for (M, w),  ~ we have
                                                                              
                                      >                 >       0.2188 0.0938
                          M diag(M Î·) diag(w)M   ~          â‰ˆ                    ;
                                                                0.0938 0.0938
while for (MÌƒ , wÌƒ), we have
                                                                              
                                      >                 >       0.2046 0.1079
                          MÌƒ diag(MÌƒ Î·) diag(wÌƒ)MÌƒ â‰ˆ                             .
                                                                0.1079 0.0796
                                                    33.34

