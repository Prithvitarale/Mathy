{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a15df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import argparse\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel #pip install transformers\n",
    "from sklearn.linear_model import LogisticRegression #pip install sklearn\n",
    "from sklearn import svm\n",
    "import random\n",
    "import numpy as np\n",
    "data_dir = \".\\papers_processed\\*\"\n",
    "chunk_size = 750\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "mathy_embeddings = [[], []]\n",
    "non_mathy_embeddings = [[], []]\n",
    "data = [mathy_embeddings, non_mathy_embeddings]\n",
    "mathy_test = []\n",
    "mathy_train = []\n",
    "non_mathy_test = []\n",
    "non_mathy_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f3e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa415998",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BertTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b7ff266fc752>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#txt-content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'BertTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "for j, dir_path in enumerate(glob.glob(data_dir)): #mathy, non_mathy\n",
    "    dir_path = os.path.join(dir_path, \"*\")\n",
    "    for k, dirr in enumerate(glob.glob(dir_path)): #test, train\n",
    "        dirr = os.path.join(dirr, \"*\")\n",
    "        for l, pdf in enumerate(glob.glob(dirr)): #txts\n",
    "            pdf = open(pdf, \"r\", encoding=\"utf8\")\n",
    "            pdf_text = pdf.read()\n",
    "            pdf.close()\n",
    "            embedding_vector = None\n",
    "            r = range(0, len(pdf_text), chunk_size)\n",
    "            n = len(r)\n",
    "            for i in r:#txt-content\n",
    "                inputs = tokenizer(pdf_text[i:i+chunk_size], return_tensors=\"pt\")\n",
    "                outputs = model(**inputs)\n",
    "                if embedding_vector is None:\n",
    "                    embedding_vector = outputs.last_hidden_state[0][0]\n",
    "                else:\n",
    "                    embedding_vector += outputs.last_hidden_state[0][0]\n",
    "            embedding_vector /= n\n",
    "            # embedding_vector.requires_grad=False\n",
    "            data[j][k].append([embedding_vector.detach().numpy(), j])\n",
    "mathy_test = data[0][0]\n",
    "mathy_train = data[0][1]\n",
    "non_mathy_test = data[1][0]\n",
    "non_mathy_train = data[1][1]\n",
    "training_data = mathy_train + non_mathy_train\n",
    "random.shuffle(training_data)\n",
    "train_x, train_y = zip(*training_data)\n",
    "test_data = mathy_test + non_mathy_test\n",
    "random.shuffle(test_data)\n",
    "test_x, test_y = zip(*test_data)\n",
    "print(\"data processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LR:\\n\")\n",
    "# x, y = self.get_training_data()\n",
    "x, y = train_x, train_y\n",
    "model = LogisticRegression(max_iter=300).fit(x, y)\n",
    "score = model.score(x, y)\n",
    "predictions = model.predict(x)\n",
    "print(predictions)\n",
    "print(y)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10573031",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_x, train_y\n",
    "predictions = self.lr.predict(x)\n",
    "mathy_predictions = predictions==0\n",
    "non_mathy_predictions = predictions\n",
    "total_mathy_predictions = np.sum(mathy_predictions)\n",
    "total_non_mathy_predictions = np.sum(non_mathy_predictions)\n",
    "mathy_template = y==0\n",
    "correct_mathy = mathy_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
